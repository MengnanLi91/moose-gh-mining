{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wMy0yNFQyMjoyNToxOS0wNTowMM4AMgdF"
    },
    "edges": [
      {
        "node": {
          "title": "Time dependent ConvectiveHeatFluxBC",
          "author": {
            "login": "makeclean"
          },
          "bodyText": "This may be a totally obvious question, but I would like to make the T_infinity value of an ADConvectiveHeatFluxBC or ConvectiveHeatFluxBC depend upon time, or at different timesteps different values of the T_infinity would be changeable. I've been able to have a functional description of the HTC, but not the T_infinity. Any suggestions?",
          "url": "https://github.com/idaholab/moose/discussions/17443",
          "updatedAt": "2022-06-21T23:35:06Z",
          "publishedAt": "2021-03-29T09:47:07Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "T_infinity is a material property ADConvectiveHeatFluxBC, same as the heat transfer coefficient, so it can be set using a GenericFunctionMaterial, which will use a function with the desired time dependence of the Tinf\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/17443#discussioncomment-544461",
                  "updatedAt": "2022-06-21T23:35:07Z",
                  "publishedAt": "2021-03-29T15:42:44Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Convergence issues due to huge differences in stiffness tensor",
          "author": {
            "login": "Bala-1005"
          },
          "bodyText": "Hello everyone,\nI am working on kks model for solidification with the two phases being solid and liquid. I have included the tensor mechanics module too similar to the one shown in the example \"/moose/modules/combined/examples/phase_field-mechanics/kks_mechanics_VTS.i\".\nThe stiffness of the liquid phase is several orders lower than the solid(~10^9). This has caused some convergence issues close to the boundary and when two solid parts of the domain interact. I am not sure how to alleviate this problem as this seems to be the only issue with the code. I have fiddled around with different boundary conditions but that has not helped at all.\nHow do I solve this issue?\nThanks,\nBala",
          "url": "https://github.com/idaholab/moose/discussions/16922",
          "updatedAt": "2022-06-11T08:58:44Z",
          "publishedAt": "2021-02-08T17:33:41Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "Have you tried using automatic_scaling? This can be enabled in the Executioner block with \"automatic_scaling = true\"",
                  "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-355553",
                  "updatedAt": "2022-06-11T08:58:45Z",
                  "publishedAt": "2021-02-10T06:48:51Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "I did try that but it doesn't seem to help. I also tried to use ActivateElementsCoupled as instructed in #16715(This is a related issue where I wanted to activate tensor mechanics only for the solid and interface part but not the liquid phase). But it gives me the following error.\n\"A 'ActivateElementsCoupled' is not a registered object.\nIf you are trying to find this object in a dynamically linked library, make sure that\nthe library can be found either in your \"Problem/library_path\" parameter or in the\nMOOSE_LIBRARY_PATH environment variable.\"\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-356506",
                          "updatedAt": "2022-06-11T08:58:52Z",
                          "publishedAt": "2021-02-10T13:23:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Did you build your application with tensor_mechanics? It says what module is included in the Makefile",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-491470",
                          "updatedAt": "2022-06-11T08:59:00Z",
                          "publishedAt": "2021-03-17T00:34:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Yes. I did build my application with tensor_mechanics. Besides, I had the physics wrong when I wanted to activate only the solid part of the domain for tensor mechanics. Right now I have applied tensor mechanics to the entire domain. Convergence is still an issue at the boundary.",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-491479",
                          "updatedAt": "2022-06-11T08:59:00Z",
                          "publishedAt": "2021-03-17T00:37:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "If you are using phase-field variable to indicate the phase domain, you should not use ActivateElementsCoupled. That will assign phase domain element-wise.\nIs your simulation running fine with two comparable stiffness tensors? Are you using a direct solver LU in the executioner block?",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-494244",
                          "updatedAt": "2022-06-11T09:00:12Z",
                          "publishedAt": "2021-03-17T15:21:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Yes. The simulation runs fine with comparable stiffness tensors. I was able to solve the issue at the boundary by deactivating some of the boundary conditions. But when two solids meet, the simulation ceases to converge. I have attached an image\nat the point where the simulation stops.\n\nAlso, here is the executioner block.\n[Executioner]\ntype = Transient\nsolve_type = 'NEWTON'\npetsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type'\npetsc_options_value = 'asm      ilu          nonzero'\nl_max_its = 30\nnl_max_its = 50\nnl_rel_tol = 1e-08\nautomatic_scaling = true\nend_time = 25\ndt = 0.008\n[./Adaptivity]\ninitial_adaptivity = 3 # Number of times mesh is adapted to initial condition\nrefine_fraction = 0.6 # Fraction of high error that will be refined\ncoarsen_fraction = 0.1 # Fraction of low error that will coarsened\nmax_h_level = 3 # Max number of refinements used, starting from initial mesh (before uniform refinement)\n[../]\n[]\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-495597",
                          "updatedAt": "2022-06-11T09:00:12Z",
                          "publishedAt": "2021-03-17T20:30:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Hello @jiangwen84. I did try to run the simulation with LU solver. The issue of non convergence when two solid parts of the domain meet still persists. There is no issue when the stiffness matrices are comparable.\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-498837",
                          "updatedAt": "2022-06-11T09:00:46Z",
                          "publishedAt": "2021-03-18T14:56:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "It is a bit difficult to tell what will be issue. Looks like the convergence issue might due to your physics, i.e. some quantities become unphysical, and the large material contrast trigger this issue?\nAnyway, could you attach your output? It might be useful to take a look at the linear and nonlinear iterations behavior.",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-501697",
                          "updatedAt": "2022-06-11T09:00:46Z",
                          "publishedAt": "2021-03-19T05:32:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "I was able to slightly solve the issue by increasing the number of nonlinear iterations to 100. But only for several timesteps. The issue returned with convergence being the issue. The output file is very large and therefore I am attaching the part where it starts to cut timesteps and stop converging.\noutput.txt\nerror.txt\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-518258",
                          "updatedAt": "2022-06-11T09:00:46Z",
                          "publishedAt": "2021-03-23T15:10:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "Try to set solve_type = 'PJFNK'\nMaybe you want to loose your convergence tolerance a bit, i.e., nl_rel_tol = 1e-7 and nl_abs_tol=1e-7",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-518346",
                          "updatedAt": "2022-06-11T09:00:46Z",
                          "publishedAt": "2021-03-23T15:22:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Thank you @jiangwen84 and everyone. I was able to solve the issue by increasing the nonlinear iterations number, loosening the tolerance a bit, and changing the boundary conditions from a displacement boundary condition to pressure boundary conditions. The simulation runs quite well now.\nOne final problem, I hope :),  is that there are a few memory issues. Any suggestions on how to solve these issues? Here is the error file. I used --malloc-dump while running the file.\nel2b_KHS.txt\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-535529",
                          "updatedAt": "2022-06-11T09:00:50Z",
                          "publishedAt": "2021-03-26T18:22:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MOOSE sub app freezing with no error message",
          "author": {
            "login": "s-dunnim"
          },
          "bodyText": "Hi, I'm using the Level Set module with Navier Stokes, and running a model involving a reinitialisation sub app. I'm getting a problem where at some point during the reinitialisation MOOSE will just stop, with no error message. If I run top in a separate terminal, I can see the processes are still running, but residuals are no longer being printed, and there are no more timesteps being added to the output file. This is happening at apparently random times; even with the same input file, it can happen a few timesteps in, or right towards the end.\nI can only recreate the problem in parallel, so used the lldb debugger in parallel to try and find the cause - it seems there is a problem in INSADTauMaterial which is being used by INSADStabilized3Eqn, that ends up in a divide by zero error. I've attached the backtrace information from the debugger here (interestingly the problem happened in the main app, not the sub app, when I was running the debugger... this may point to a completely separate issue, especially as the sub app itself doesn't use the Navier Stokes materials. However the freezing behaviour is the same - and if they aren't linked, how does one problem arise in the opt app and the other one in debug mode?). I've also attached the input files.\nDoes anyone know what could be causing this, and how I can prevent it? Thanks in advance.\nls_b.i.txt\nreinit_sub.i.txt",
          "url": "https://github.com/idaholab/moose/discussions/17438",
          "updatedAt": "2022-06-27T08:31:32Z",
          "publishedAt": "2021-03-26T19:46:35Z",
          "category": {
            "name": "Q&A Modules: Navier-Stokes"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "s-dunnim"
                  },
                  "bodyText": "I have found the solution (if not the cause) to the freezing problem in the sub app - something was going wrong with the SolutionTimeAdaptiveDT timestepper I was using, it works fine if I simply use a constant dt. This doesn't explain the debugger behaviour as far as I can tell, but I'm going to let that slide as it isn't affecting my model.",
                  "url": "https://github.com/idaholab/moose/discussions/17438#discussioncomment-543262",
                  "updatedAt": "2022-06-27T08:32:58Z",
                  "publishedAt": "2021-03-29T10:41:54Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Uniaxial tensile simulation of polycrystalline model based on crystal plasticity",
          "author": {
            "login": "PengWei97"
          },
          "bodyText": "Dear MOOSE experts,\nI recently wrote an input file based on the files prop_grain_read.i and crysp_user_object.i to simulate plastic uniaxial stretching of polycrystalline crystals, and established a two-dimensional model of 0.21*0.21mm. There are 200 grains in it. Stretching in the y-axis direction.\n284 time steps, time = 0.327438, stretching by 0.03%.\nBut the calculation is very slow. Is there any suggestion to speed up the calculation?\nThe content of the written input file and output terminal is as follows,\nprop_grain_read_cp.i\nprop_grain_read_cp.txt\nAny suggestions or recommendations to fix these problems would be greatly appreciated.\nThank you\nWei Peng",
          "url": "https://github.com/idaholab/moose/discussions/17423",
          "updatedAt": "2022-06-24T09:23:28Z",
          "publishedAt": "2021-03-24T15:22:42Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "@PengWei97 I need to let you know that we are currently switching the UO-based CP (the one you used) to stress-update based CP. One on-going PR is #17405. The new version might give you a bit speedup.\nFor your current input file, I suggest you add maximum_substep_iteration = 4 in your FiniteStrainUObasedCP block. It will allow you to take larger dt. With sub stepping, each time step will take longer time, but it can speed up your simulation by taking less number of time steps.\n@sapitts @dewenyushu  any other thoughts?",
                  "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-527866",
                  "updatedAt": "2022-06-24T09:23:32Z",
                  "publishedAt": "2021-03-25T03:53:45Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dewenyushu"
                          },
                          "bodyText": "Hi @PengWei97 I agree with @jiangwen84 that using sub-stepping should allow larger dt thus speed up computation in general.\nFor the new stress-update based CP base class  (#17405). As Wen mentioned, this is a new implementation which will be more actively developed and maintained than the UO-based version in the future. With this new class, we do see some improvements in terms of the total linear/nonlinear iterations for several existing models (and some of them are improved a lot!). However, we are yet to quantify the improvement or examine the consistency of this improvement among all existing models.",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-530003",
                          "updatedAt": "2022-06-24T09:23:34Z",
                          "publishedAt": "2021-03-25T14:41:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Hi @jiangwen84 ,Thank you for your guidance.\nI added maximum_substep_iteration = 5 to theFiniteStrainUObasedCPblock according to your suggestion. But it is still calculated at the minimum incremental step set in the seventh step, and it took five and a half hours to calculate to \"Time Step 805, time = 0.897438, dt = 0.001\". Similarly, I used umat calculations on abaqus, and it took less than half an hour to complete the uniaxial stretching simulation. I think it must be an unreasonable setting in that place. The following is the input file. Do you have any suggestions for improvement?\n`[Mesh]\ntype = GeneratedMesh\ndim = 2\nelem_type = QUAD4\ndisplacements = 'disp_x disp_y'\nnx = 100\nny = 100\nxmax = 0.21\nymax = 0.21\n[]\n[Variables]\n[./disp_x]\nblock = 0\n[../]\n[./disp_y]\nblock = 0\n[../]\n[]\n[GlobalParams]\nvolumetric_locking_correction=true\n[]\n[AuxVariables]\n[./stress_yy]\norder = CONSTANT\nfamily = MONOMIAL\nblock = 0\n[../]\n[./e_yy]\norder = CONSTANT\nfamily = MONOMIAL\nblock = 0\n[../]\n[]\n[Functions]\n[./tdisp]\ntype = ParsedFunction\nvalue = 2e-4*t # Approximately 0.1% loading per second\n[../]\n[]\n[UserObjects]\n[./prop_read]\ntype = ElementPropertyReadFile\n# need read\nprop_file_name = 'input_file.txt' # euler angle\n# Enter file data as prop#1, prop#2, .., prop#nprop\nnprop = 3\nread_type = grain\nngrain = 200\n# rand_seed = 25346\nrve_type = periodic\n# Periodic or non-periodic grain distribution: Default is non-periodic\n[../]\n[]\n[AuxKernels]\n[./stress_yy]\ntype = RankTwoAux\nvariable = stress_yy\nrank_two_tensor = stress\nindex_j = 1\nindex_i = 1\nexecute_on = 'initial timestep_end'\nblock = 0\n[../]\n[./e_yy]\ntype = RankTwoAux\nvariable = e_yy\nrank_two_tensor = lage\nindex_j = 1\nindex_i = 1\nexecute_on = timestep_end\nblock = 0\n[../]\n[]\n[BCs]\n[./fix_x]\ntype = DirichletBC\nvariable = disp_x\nboundary = 'left'\nvalue = 0\n[../]\n[./fix_y]\ntype = DirichletBC\nvariable = disp_y\nboundary = 'bottom'\nvalue = 0\n[../]\n[./tdisp]\ntype = FunctionDirichletBC\nvariable = disp_y\nboundary = top\nfunction = tdisp\n[../]\n[]\n[UserObjects]\n[./slip_rate_gss]\ntype = CrystalPlasticitySlipRateGSS\nvariable_size = 12\nslip_sys_file_name = input_slip_sys.txt # slip system\nnum_slip_sys_flowrate_props = 2\nflowprops = '1 4 0.001 0.1 5 8 0.001 0.1 9 12 0.001 0.1'\nuo_state_var_name = state_var_gss\n[../]\n[./slip_resistance_gss]\ntype = CrystalPlasticitySlipResistanceGSS\nvariable_size = 12\nuo_state_var_name = state_var_gss\n[../]\n[./state_var_gss]\ntype = CrystalPlasticityStateVariable\nvariable_size = 12\ngroups = '0 4 8 12'\ngroup_values = '60.8 60.8 60.8'\nuo_state_var_evol_rate_comp_name = state_var_evol_rate_comp_gss\nscale_factor = 1.0\n[../]\n[./state_var_evol_rate_comp_gss]\ntype = CrystalPlasticityStateVarRateComponentGSS\nvariable_size = 12\nhprops = '1.0 541.5 109.8 2.5'\nuo_slip_rate_name = slip_rate_gss\nuo_state_var_name = state_var_gss\n[../]\n[]\n[Materials]\n[./crysp]\ntype = FiniteStrainUObasedCP\nstol = 1e-2\ntan_mod_type = exact\nuo_slip_rates = 'slip_rate_gss'\nuo_slip_resistances = 'slip_resistance_gss'\nuo_state_vars = 'state_var_gss'\nuo_state_var_evol_rate_comps = 'state_var_evol_rate_comp_gss'\nmaximum_substep_iteration = 5 # add for an advice of jiangwen84\n[../]\n[./strain]\ntype = ComputeFiniteStrain\ndisplacements = 'disp_x disp_y'\n[../]\n[./elasticity_tensor]\ntype = ComputeElasticityTensorCP\nC_ijkl = '1.684e5 1.214e5 1.214e5 1.684e5 1.214e5 1.684e5 0.754e5 0.754e5 0.754e5'\nfill_method = symmetric9\nread_prop_user_object = prop_read\n[../]\n[]\n[Postprocessors]\n[./stress_yy]\ntype = ElementAverageValue\nvariable = stress_yy\nblock = 'ANY_BLOCK_ID 0'\n[../]\n[./e_yy]\ntype = ElementAverageValue\nvariable = e_yy\nblock = 'ANY_BLOCK_ID 0'\n[../]\n[]\n[Preconditioning]\n[./smp]\ntype = SMP\nfull = true\n[../]\n[]\n[Executioner]\ntype = Transient\n#Preconditioned JFNK (default)\nsolve_type = 'PJFNK'\nl_max_its = 30\nl_tol = 1.0e-4\nnl_max_its = 20\nnl_rel_tol = 1.0e-9\nend_time = 1\ndtmin = 0.001\n[./TimeStepper]\ntype = IterationAdaptiveDT\ndt = 0.05\noptimal_iterations = 6\n[../]\n[]\n[Outputs]\nfile_base = prop_grain_read_out_xiu02\nexodus = true\ncsv = true\n[]\n[Kernels]\n[./TensorMechanics]\ndisplacements = 'disp_x disp_y'\nuse_displaced_mesh = true\n[../]\n[]\n`",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-532930",
                          "updatedAt": "2022-06-24T09:23:35Z",
                          "publishedAt": "2021-03-26T07:07:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Hi @PengWei97 I agree with @jiangwen84 that using sub-stepping should allow larger dt thus speed up computation in general.\nFor the new stress-update based CP base class (#17405). As Wen mentioned, this is a new implementation which will be more actively developed and maintained than the UO-based version in the future. With this new class, we do see some improvements in terms of the total linear/nonlinear iterations for several existing models (and some of them are improved a lot!). However, we are yet to quantify the improvement or examine the consistency of this improvement among all existing models.\n\nHi @dewenyushu,Thank you for your advice.\nI tried to clone the relevant *.C and *.h files in my development branch into the application I created, but an error was displayed when compiling, the error is as follows,\nerror_make.txt\nIs it because this class is still under development, so we can't use it now, or is it for other reasons?",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-532965",
                          "updatedAt": "2022-06-24T09:23:36Z",
                          "publishedAt": "2021-03-26T07:21:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "Try one more thing:\nadd\npetsc_options_iname = '-pc_type'\npetsc_options_value = 'lu'\nto [Executioner] block.",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-535234",
                          "updatedAt": "2022-09-02T07:34:12Z",
                          "publishedAt": "2021-03-26T17:12:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Try one more thing:\nadd\npetsc_options_iname = '-pc_type'\npetsc_options_value = 'lu'\nto [Executioner] block.\n\nhi @jiangwen84 \uff0cfollow your suggestion, this problem has been solved.\nThank you very much for your guidance.\nwei",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-537754",
                          "updatedAt": "2022-09-02T07:34:12Z",
                          "publishedAt": "2021-03-27T14:29:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewenyushu"
                          },
                          "bodyText": "Hi @PengWei97 I agree with @jiangwen84 that using sub-stepping should allow larger dt thus speed up computation in general.\nFor the new stress-update based CP base class (#17405). As Wen mentioned, this is a new implementation which will be more actively developed and maintained than the UO-based version in the future. With this new class, we do see some improvements in terms of the total linear/nonlinear iterations for several existing models (and some of them are improved a lot!). However, we are yet to quantify the improvement or examine the consistency of this improvement among all existing models.\n\nHi @dewenyushu,Thank you for your advice.\nI tried to clone the relevant *.C and *.h files in my development branch into the application I created, but an error was displayed when compiling, the error is as follows,\nerror_make.txt\nIs it because this class is still under development, so we can't use it now, or is it for other reasons?\n\nThanks for trying this out! By looking at your log file, it seems like you will need to checkout the Kalidindi class (CrystalPlasticityKalidindiUpdate.C and .h) from my PR as well since I have made some changes for it being able to run with the new base class.\nThe new base class is something that is still in development right now and transforming all existing CP models to the new base class will require some more work. Currently, only the Kalidindi CP model works with this new base class. So I will not be surprised if there being some glitches when transforming your model without some changes. If you come across some new issues, please keep us updated.",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-538290",
                          "updatedAt": "2022-09-02T07:34:11Z",
                          "publishedAt": "2021-03-27T16:58:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Hi @PengWei97 I agree with @jiangwen84 that using sub-stepping should allow larger dt thus speed up computation in general.\nFor the new stress-update based CP base class (#17405). As Wen mentioned, this is a new implementation which will be more actively developed and maintained than the UO-based version in the future. With this new class, we do see some improvements in terms of the total linear/nonlinear iterations for several existing models (and some of them are improved a lot!). However, we are yet to quantify the improvement or examine the consistency of this improvement among all existing models.\n\nHi @dewenyushu,Thank you for your advice.\nI tried to clone the relevant *.C and *.h files in my development branch into the application I created, but an error was displayed when compiling, the error is as follows,\nerror_make.txt\nIs it because this class is still under development, so we can't use it now, or is it for other reasons?\n\nThanks for trying this out! By looking at your log file, it seems like you will need to checkout the Kalidindi class (CrystalPlasticityKalidindiUpdate.C and .h) from my PR as well since I have made some changes for it being able to run with the new base class.\nThe new base class is something that is still in development right now and transforming all existing CP models to the new base class will require some more work. Currently, only the Kalidindi CP model works with this new base class. So I will not be surprised if there being some glitches when transforming your model without some changes. If you come across some new issues, please keep us updated.\n\nThank you very much. I will try to modify it.",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-539696",
                          "updatedAt": "2022-09-02T07:34:12Z",
                          "publishedAt": "2021-03-28T02:15:51Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MooseDocs w/ Animal Codes",
          "author": {
            "login": "roskofnj"
          },
          "bodyText": "Each Animal code has it's own moosedoc.py which is used to start a document server on the localhost (127.0.0.1:8000). I would like to have all the Animal code document servers running at the same time. However I cannot launch multiple document servers on different ports - when the second doc server is launched it simply overwrites the first.\nIs there a way to start a document server that contains all documentation for all Animal codes, as well as RELAP7? It seems strange that I cannot start different servers on different ports.\nThanks very much for any help you're able to offer.",
          "url": "https://github.com/idaholab/moose/discussions/17436",
          "updatedAt": "2022-09-01T18:37:57Z",
          "publishedAt": "2021-03-26T15:08:47Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "cticenhour"
                  },
                  "bodyText": "You can change the port for the MooseDocs build command using the --port option. So, if I wanted to change my documentation website port to 9000, I would just run the following in my app's doc directory:\n./moosedocs.py build --port 9000 --serve\n\nand the built pages would be available on http://127.0.0.1:9000. To see all of the options for the build command, type ./moosedocs.py build -h in your doc directory.",
                  "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535177",
                  "updatedAt": "2022-09-01T18:37:57Z",
                  "publishedAt": "2021-03-26T16:55:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "roskofnj"
                          },
                          "bodyText": "This is exactly what I've trying to do. But any time I try to launch a separate instance it overwrites the first. For example:\n\nin the Griffin directory, I'll do ./moosedocs.py build --serve --port 8001 and then Griffin document server launches correctly at 127.0.0.1:8001\nin the Sockeye directory I'll do ./moosedocs.py build --serve --port 8002 and the Sockeye document server correctly launches at 127.0.0.1:8002. But then if I visit 127.0.0.1:8001 (which used to be a Griffin doc server) it now shows Sockeye.\n\nSo it seems like when I launch a second server it is always overwriting the first.",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535594",
                          "updatedAt": "2022-09-01T18:37:57Z",
                          "publishedAt": "2021-03-26T18:37:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "I deleted my previous comment, as I just got to test the --host alternative option (setting each documentation instance as a separate local IP) on a MacOS machine. Because both 127.x.x.x URLs are loopback interfaces (referring to the same machine), it will be overwritten regardless of the IP difference. @loganharbour's option is the proper answer here.",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535710",
                          "updatedAt": "2022-09-01T18:38:06Z",
                          "publishedAt": "2021-03-26T19:09:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roskofnj"
                          },
                          "bodyText": "Thanks very much. I actually tried just what you were originally thinking.\nI way out of my league on this server stuff, so I really appreciate you both (@loganharbour ) for bearing with me on this.",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535779",
                          "updatedAt": "2022-09-01T18:38:06Z",
                          "publishedAt": "2021-03-26T19:25:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "loganharbour"
                          },
                          "bodyText": "Thanks for coming to us with questions! This information may be able to help someone in the future.\nI'd also like to add that in terms of documentation, if you have any suggestions to make the process any clearer to the user, we welcome issues on GitHub!",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535791",
                          "updatedAt": "2022-09-01T18:38:06Z",
                          "publishedAt": "2021-03-26T19:29:43Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Note that while the serve option is the easiest way to get the documentation going, it isn't required to view the documentation. We suggest using --serve because (1) it is convenient, and (2) it auto updates from source so that you can make changes and see the changes almost real time.\nIf you would like to just generate the raw static site in one go (which you can also open with your browser by opening the html), you can generate with\n./moosedocs.py build --destination <directory>\n\nin which you fill <directory> with a directory where you want the site to be built into. Once built, you can traverse into said directory, start with the index.html page and browse as needed without having to start the local service to host the site. You could do this for all of the applications that you need documentation for, and put them in a convenient location for easy access.",
                  "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535196",
                  "updatedAt": "2022-09-01T18:38:08Z",
                  "publishedAt": "2021-03-26T17:01:12Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "roskofnj"
                          },
                          "bodyText": "Great, I appreciate this alternative approach. I'll give it a try. might be easier and less prone to issues than my original attempt. At this point I don't think I'll be doing too many changes to the documentation, so it should be sufficient.",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535601",
                          "updatedAt": "2022-09-01T18:38:08Z",
                          "publishedAt": "2021-03-26T18:39:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Question about explicit solve (TimeIntegrators\\ActuallyExplicitEuler)",
          "author": {
            "login": "jlinBE"
          },
          "bodyText": "Hi, I am testing explicit solves using moose\\modules\\tensor_mechanics\\examples\\coal_mining\\coarse.i\nAttached is an input file where I've turned off everything except the elastic behavior.\ncoarse_explicit.zip\nIf I use the ExplicitEuler solver, the sim runs, but convergence is much worse; if I use ActuallyExplicitEuler, I get an error message:\n[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Object is in wrong state\n[0]PETSC ERROR: Matrix is missing diagonal entry 0\n[0]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[0]PETSC ERROR: Petsc Release Version 3.13.3, unknown\n[0]PETSC ERROR: ../../tensor_mechanics-opt on a  named DELL-DBECK-2019 by moose Tue Nov  3 11:40:06 2020\n[0]PETSC ERROR: Configure options --COPTFLAGS=-O3 --CXXOPTFLAGS=-O3 --FOPTFLAGS=-O3 --with-x=0 --with-mpi=1 --with-ssl=0 --with-openmp=1 --with-debugging=0 --with-cxx-dialect=C++11 --with-shared-libraries=1 --download-mumps=1 --download-hypre=1 --download-metis=1 --download-slepc=1 --download-ptscotch=1 --download-parmetis=1 --download-scalapack=1 --download-superlu_dist=1 --with-fortran-bindings=0 --with-sowing=0 EOF --download-fblaslapack=1 AR=${PREFIX}/bin/x86_64-conda_cos6-linux-gnu-ar CC=mpicc CXX=mpicxx FC=mpifort FC=mpifort FC=mpifort CFLAGS=\"-march=nocona -mtune=haswell\" CXXFLAGS=\"-march=nocona -mtune=haswell\" LDFLAGS=\"-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/moose/miniconda3/envs/moose/lib -Wl,-rpath-link,/home/moose/miniconda3/envs/moose/lib -L/home/moose/miniconda3/envs/moose/lib\" --prefix=/home/moose/miniconda3/envs/moose\n[0]PETSC ERROR: #1 MatILUFactorSymbolic_SeqAIJ() line 1685 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/mat/impls/aij/seq/aijfact.c\n[0]PETSC ERROR: #2 MatILUFactorSymbolic() line 6620 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/mat/interface/matrix.c\n[0]PETSC ERROR: #3 PCSetUp_ILU() line 135 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/impls/factor/ilu/ilu.c\n[0]PETSC ERROR: #4 PCSetUp() line 898 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #5 KSPSetUp() line 376 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #6 PCSetUpOnBlocks_BJacobi_Singleblock() line 613 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/impls/bjacobi/bjacobi.c\n[0]PETSC ERROR: #7 PCSetUpOnBlocks() line 927 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #8 KSPSetUpOnBlocks() line 214 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #9 KSPSolve_Private() line 634 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #10 KSPSolve() line 853 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nAny idea what the above means?  Also, a question about switching to explicit solve, in general: is it a drag-and-drop process where I simply add it into the input file?  Or am I supposed to change things elsewhere?",
          "url": "https://github.com/idaholab/moose/discussions/16052",
          "updatedAt": "2022-08-26T20:59:25Z",
          "publishedAt": "2020-11-03T00:53:38Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@fdkong Could you take a look at this when you have some time.",
                  "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-121486",
                  "updatedAt": "2022-09-13T07:58:00Z",
                  "publishedAt": "2020-11-04T19:50:25Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Hi, I am testing explicit solves using moose\\modules\\tensor_mechanics\\examples\\coal_mining\\coarse.i\nAttached is an input file where I've turned off everything except the elastic behavior.\ncoarse_explicit.zip\nIf I use the ExplicitEuler solver, the sim runs, but convergence is much worse; if I use ActuallyExplicitEuler, I get an error message:\n[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Object is in wrong state\n[0]PETSC ERROR: Matrix is missing diagonal entry 0\n\nThe error means you might miss kernels for certain variables. Or you have a saddle point system. If you have a saddle point system, you need to explicitly add zeros for all variables to the diagonal of matrix.\nIf you could find a lot of examples about how to use ActuallyExplicitEuler in moose repo. If you are interested in ActuallyExplicitEuler, I would suggest you start from a simple example such as: ee-2d-linear.i .\n\n[0]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[0]PETSC ERROR: Petsc Release Version 3.13.3, unknown\n[0]PETSC ERROR: ../../tensor_mechanics-opt on a named DELL-DBECK-2019 by moose Tue Nov 3 11:40:06 2020\n[0]PETSC ERROR: Configure options --COPTFLAGS=-O3 --CXXOPTFLAGS=-O3 --FOPTFLAGS=-O3 --with-x=0 --with-mpi=1 --with-ssl=0 --with-openmp=1 --with-debugging=0 --with-cxx-dialect=C++11 --with-shared-libraries=1 --download-mumps=1 --download-hypre=1 --download-metis=1 --download-slepc=1 --download-ptscotch=1 --download-parmetis=1 --download-scalapack=1 --download-superlu_dist=1 --with-fortran-bindings=0 --with-sowing=0 EOF --download-fblaslapack=1 AR=${PREFIX}/bin/x86_64-conda_cos6-linux-gnu-ar CC=mpicc CXX=mpicxx FC=mpifort FC=mpifort FC=mpifort CFLAGS=\"-march=nocona -mtune=haswell\" CXXFLAGS=\"-march=nocona -mtune=haswell\" LDFLAGS=\"-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/moose/miniconda3/envs/moose/lib -Wl,-rpath-link,/home/moose/miniconda3/envs/moose/lib -L/home/moose/miniconda3/envs/moose/lib\" --prefix=/home/moose/miniconda3/envs/moose\n[0]PETSC ERROR: #1 MatILUFactorSymbolic_SeqAIJ() line 1685 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/mat/impls/aij/seq/aijfact.c\n[0]PETSC ERROR: #2 MatILUFactorSymbolic() line 6620 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/mat/interface/matrix.c\n[0]PETSC ERROR: #3 PCSetUp_ILU() line 135 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/impls/factor/ilu/ilu.c\n[0]PETSC ERROR: #4 PCSetUp() line 898 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #5 KSPSetUp() line 376 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #6 PCSetUpOnBlocks_BJacobi_Singleblock() line 613 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/impls/bjacobi/bjacobi.c\n[0]PETSC ERROR: #7 PCSetUpOnBlocks() line 927 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #8 KSPSetUpOnBlocks() line 214 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #9 KSPSolve_Private() line 634 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #10 KSPSolve() line 853 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nAny idea what the above means? Also, a question about switching to explicit solve, in general: is it a drag-and-drop process where I simply add it into the input file? Or am I supposed to change things elsewhere?\n\nThis is a simple block you need for triggering  explicit solve. The solver is still pretty new, and it is not popular as the implicit method in moose.  I will not be surprised  if there are some uncovered corner cases.\n  [./TimeIntegrator]\n    type = ActuallyExplicitEuler\n  [../]",
                          "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-121521",
                          "updatedAt": "2022-09-13T07:58:01Z",
                          "publishedAt": "2020-11-04T20:58:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "@fdkong\nI am having the same error, I see you mentioned\n\nThe error means you might miss kernels for certain variables. Or you have a saddle point system. If you have a saddle point system, you need to explicitly add zeros for all variables to the diagonal of matrix.\n\nHow do we actually add zeros to the matrix via an input file?\nI see the only difference I have between the example and my input file is the 'TimeDerivative' kernel, which I don't have, I instead update the material property (tensor mechanics) with respect to time.\nIt works fine if I run it using an implicit method.\nCould you please help with this? Thank you!\nKind regards,\nTraiwit\ntest_transient1.zip",
                          "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-501473",
                          "updatedAt": "2022-09-13T07:58:03Z",
                          "publishedAt": "2021-03-19T03:40:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "@Traiwit Explicit time integration won't work without a mass matrix, i.e. how do you invert a zero matrix? Currently the Actually explicit euler integrator only works with InertialForce.",
                          "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-534103",
                          "updatedAt": "2022-09-13T07:58:05Z",
                          "publishedAt": "2021-03-26T12:42:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jlinBE"
                  },
                  "bodyText": "Thanks, I'll try it with simpler 2d cases and get back to you.",
                  "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-124940",
                  "updatedAt": "2022-09-13T07:58:06Z",
                  "publishedAt": "2020-11-09T11:51:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Installing MOOSE on ARM64 architecture",
          "author": {
            "login": "shikhar413"
          },
          "bodyText": "Hello MOOSE team,\nI am a new MOOSE user and I am trying to build MOOSE on an ARM64 machine. I've had to rely on building most dependencies from source since many conda packages are built only on x86_64 machines. When compiling PETSC and libMesh by running ./scripts/update_and_rebuild_petsc.sh and ./scripts/update_and_rebuild_libmesh.sh, however, I run into the following messages:\nPETSC Compilation Error:\n\nlibMesh Compilation Error:\n\nThese errors occur while running make after configure has finished being called. I am running on ARM64 Ubuntu Server 20.04 and have installed MOOSE dependencies according to the instructions on https://mooseframework.inl.gov/getting_started/installation/manual_installation_gcc.html. I don't know how I can modify the PETSC and LibMesh install scripts to get around these issues, any guidance would be very much appreciated.",
          "url": "https://github.com/idaholab/moose/discussions/17416",
          "updatedAt": "2022-10-13T19:54:09Z",
          "publishedAt": "2021-03-23T15:56:47Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@dschwen You got MOOSE running on a Raspberry PI, correct? Do you have any insights?",
                  "url": "https://github.com/idaholab/moose/discussions/17416#discussioncomment-527792",
                  "updatedAt": "2023-07-31T17:51:54Z",
                  "publishedAt": "2021-03-25T03:15:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "shikhar413"
                  },
                  "bodyText": "The folks at Argonne MCS were able to lead me in the right direction wrt the PETSc installation. For some reason the Fortran compiler was unable to locate the shared libraries for mumps and manually setting the PETSc flag in scripts/update_and_rebuild_petsc.sh as FFLAGS=\"-fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument\" fixed the PETSc compilation issue.\nAdding the full commands to build MOOSE on ARM Ubuntu 20.04 for anyone else who might run into similar issues, happy to submit a PR to add these instructions to some documentation in the repo if there is interest as well. Might be good to also start testing builds on ARM architectures, especially to support installation on the new M1 Macs.\n#!/bin/sh\n\n# Install gcc, python3\nsudo apt install build-essential libssl-dev libffi-dev python3-dev manpages-dev\n\n# Install MPICH\nsudo apt install mpich\n\n# Install pip\nsudo apt install python3-pip\n\n# Install MOOSE dependencies\npip3 install coverage\npip3 install reportlab\npip3 install mako\npip3 install numpy\npip3 install scipy\npip3 install scikit-learn\nsudo apt install hdf5-tools\nsudo apt install hdf5-helpers\nsudo apt install python3-h5py\npip3 install scikit-image\npip3 install requests\nsudo apt install python3-vtk7\nsudo apt install python3-vtkplotter\npip3 install pyyaml\npip3 install matplotlib\npip3 install lxml\npip3 install pyflakes\npip3 install pandas\npip3 install mock\nsudo apt install python3-yaml\nsudo apt install python3-pyqt5\nsudo apt install swig\npip3 install --no-cache-dir pybtex livereload daemonlite pylint lxml pylatexenc anytree\n\n# Add locally installed binaries to path\necho \"export PATH=$HOME/.local/bin:$PATH\" >> ~/.bashrc\n\n# Set environment variables for MOOSE installation\necho \"export CC=mpicc\" >> ~/.bashrc\necho \"export CXX=mpicxx\" >> ~/.bashrc\necho \"export FC=mpif90\" >> ~/.bashrc\necho \"export F90=mpif90\" >> ~/.bashrc\n\n# Point default python environment to python3 (necessary for running MOOSE tests)\nsudo apt-get install python-is-python3\n\n# Install PETSC dependencies\nsudo apt install bison valgrind flex\n\n# Clone MOOSE repo\nmoose_dir=$HOME/projects\nmkdir -p $moose_dir\ncd $moose_dir\ngit clone https://github.com/idaholab/moose.git\ncd moose\ngit checkout master\n\n# Manually add flag for Fortran flags to PETSc installation script\nsed -i 's/hypre=1/hypre=1 --FFLAGS=\"-fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument\"/g' scripts/update_and_rebuild_petsc.sh\n\n# Compile PETSc\nunset PETSC_DIR PETSC_ARCH\n./scripts/update_and_rebuild_petsc.sh\ncd petsc\nmake PETSC_DIR=$moose_dir/moose/scripts/../petsc PETSC_ARCH=arch-moose check\n\n# Compile libMesh\ncd $moose_dir/moose\n./scripts/update_and_rebuild_libmesh.sh\n\n# Compile and test MOOSE\ncd $moose_dir/moose/test\nmake -j 4\n./run_tests -j 4",
                  "url": "https://github.com/idaholab/moose/discussions/17416#discussioncomment-532299",
                  "updatedAt": "2023-07-31T17:51:54Z",
                  "publishedAt": "2021-03-26T02:30:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Does MOOSE support electrical simulation?",
          "author": {
            "login": "tongfen"
          },
          "bodyText": "Hi, I am new to MOOSE. I am looking to simulate electrical-mechanical coupling. Does MOOSE support this? Thank you very much.",
          "url": "https://github.com/idaholab/moose/discussions/17389",
          "updatedAt": "2022-06-24T03:46:46Z",
          "publishedAt": "2021-03-19T15:29:54Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "cticenhour"
                  },
                  "bodyText": "If all you need is an electrostatic field simulation, then the Diffusion kernel (or one of the custom versions: FuncDiffusion or MatDiffusion if you want to couple in a material property for the permittivity) could help you set up Poisson's Equation to solve for potential. Then you can set up an AuxKernel to get electric field as an AuxVariable from the electrostatic potential. If you need full Maxwell's, an electromagnetic module for MOOSE has been built but is not cleared for public release yet. It should be coming very soon though.",
                  "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-503809",
                  "updatedAt": "2022-06-24T03:46:46Z",
                  "publishedAt": "2021-03-19T15:49:39Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "tongfen"
                          },
                          "bodyText": "Hi Cticenhour,\nThank you so much for your information. It is still beyond my understanding. I will spend some time to figure it out. Thanks again!",
                          "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-504941",
                          "updatedAt": "2022-06-24T03:46:56Z",
                          "publishedAt": "2021-03-19T20:24:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "No problem! Feel free to post more questions you begin learning MOOSE.",
                          "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-505022",
                          "updatedAt": "2022-06-24T03:46:56Z",
                          "publishedAt": "2021-03-19T20:54:24Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mangerij"
                  },
                  "bodyText": "Are you asking about piezoelectric materials?",
                  "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-510795",
                  "updatedAt": "2022-06-24T03:46:56Z",
                  "publishedAt": "2021-03-21T21:05:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "tongfen"
                          },
                          "bodyText": "Not exactly, do you have any idea about using MOOSE to simulate piezoresistive materials? Thanks.",
                          "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-511095",
                          "updatedAt": "2022-06-24T03:46:56Z",
                          "publishedAt": "2021-03-21T23:53:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mangerij"
                          },
                          "bodyText": "In my limited knowledge of piezoresistivity, wouldn't you just need to couple elastic deformations to a change in conductivity? This would just be a postprocessor added to what is already in Moose: tensor mechanics\nOr is there coupling between the current and the strain?",
                          "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-514191",
                          "updatedAt": "2022-06-24T03:47:24Z",
                          "publishedAt": "2021-03-22T16:50:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "This would just be a postprocessor\n\nMore likely a Material that couples in the stress or strain and computes a Diffusivity (or electrical conductivity) at each quadrature point.",
                          "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-514937",
                          "updatedAt": "2022-06-24T03:47:26Z",
                          "publishedAt": "2021-03-22T19:53:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "tongfen"
                          },
                          "bodyText": "Mangerij and Dschwen, I see. Thank you for your reply. I got some useful information from you both.",
                          "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-530051",
                          "updatedAt": "2022-12-20T07:33:14Z",
                          "publishedAt": "2021-03-25T14:50:08Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "See also #17362",
                  "url": "https://github.com/idaholab/moose/discussions/17389#discussioncomment-514948",
                  "updatedAt": "2022-07-14T09:37:27Z",
                  "publishedAt": "2021-03-22T19:56:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "using Auxvaribles from previous step for extra-stress calculation",
          "author": {
            "login": "Traiwit"
          },
          "bodyText": "Hi guys,\nI defined Auxvaribale as below, I just wonder how can I use it for extra-stress calculation? seems like I cannot just call the variable name.\n[AuxVariables]\n  [./stress_xx]\n    order = SECOND\n    family = MONOMIAL\n  [../]\n  [./stress_yy]\n    order = SECOND\n    family = MONOMIAL\n  [../]\n  [./stress_zz]\n    order = SECOND\n    family = MONOMIAL\n  [../]\n[]\n[AuxKernels]\n  [./matl_s11]\n    type = RankTwoAux\n    rank_two_tensor = stress\n    index_i = 0\n    index_j = 0\n    variable = stress_xx\n  [../]\n  [./matl_s22]\n    type = RankTwoAux\n    rank_two_tensor = stress\n    index_i = 1\n    index_j = 1\n    variable = stress_yy\n  [../]\n  [./matl_s33]\n    type = RankTwoAux\n    rank_two_tensor = stress\n    index_i = 2\n    index_j = 2\n    variable = stress_zz\n  [../]\n[]\n\nbelow is for the extra-stress\n[./void_stress]\n  type = ComputeExtraStressConstant\n  block = 1\n  extra_stress_tensor = '-stress_xx -stress_yy -stress_zz 0 0 0'\n  prefactor = Ex_stress1\n[../]\n\n[./Ex_stress1]\n  type = ParsedFunction\n  value = 'if(t<2,1,1e-4)'\n[../]\n\nhere is the error message: cannot convert '-stress_zz' to float\nIt should be quite straightforward, but I couldn't find the way around it.\nThank you!\nTraiwit",
          "url": "https://github.com/idaholab/moose/discussions/17408",
          "updatedAt": "2022-10-20T13:29:33Z",
          "publishedAt": "2021-03-22T23:45:35Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "Traiwit"
                  },
                  "bodyText": "note that: the objective here is to remove the stress for a particular block, hence, extra_stress_tensor equal to previous step stress but in opposite direction",
                  "url": "https://github.com/idaholab/moose/discussions/17408#discussioncomment-515782",
                  "updatedAt": "2022-10-20T13:29:35Z",
                  "publishedAt": "2021-03-23T00:59:06Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "You should be able to setup the stress to only be calculated on the desired block and setup another on the other block that is zero, since the stress calculations are Material objects.\nBut, I am not familiar with mechanics module; perhaps some mechanics folks could jump in @jiangwen84 or @sapitts with the correct approach.",
                  "url": "https://github.com/idaholab/moose/discussions/17408#discussioncomment-527803",
                  "updatedAt": "2022-10-20T13:29:35Z",
                  "publishedAt": "2021-03-25T03:21:45Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "The name of ComputeExtraStressConstant indicates that it only works for a constant stress tensor. If the stress in your simulation is not a constant, i.e., with spatial variation, you cannot use this object.\nCould you explain a bit more on what you are trying to do? Do you want zero stress in the void?",
                          "url": "https://github.com/idaholab/moose/discussions/17408#discussioncomment-527843",
                          "updatedAt": "2022-10-20T13:29:39Z",
                          "publishedAt": "2021-03-25T03:43:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi, yes, I instead modified the computestress .C file by adding the prefactor term, so now I can multiply the old stress by some prefactor which can reduce the stress in the particular block (creating the void).\nAll good now, thanks guys!\n{\n\n  _stress[_qp] = _stress_old[_qp]* _prefactor[_qp]+ _elasticity_tensor[_qp] * elastic_strain_increment;\n\n  computeQpJacobian();\n}",
                          "url": "https://github.com/idaholab/moose/discussions/17408#discussioncomment-527872",
                          "updatedAt": "2022-10-20T13:29:40Z",
                          "publishedAt": "2021-03-25T03:59:21Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Failing test (CRASH)",
          "author": {
            "login": "pastoreg"
          },
          "bodyText": "Hi all,\nI have an MR that is failing because of a test failure (CRASH).  It seems the test runs fine in serial or with mpiexec, however, ./run_tests --n-threads=2 reproduces the error, as below.  I am not sure what may be causing this and would appreciate any help.  Thanks!\n(moose) gpastore@x86_64-apple-darwin13 bison % ./run_tests --n-threads=2 --re=test:tensor_mechanics/fecral_plasticity/power-law_strain_hardening.test_power-law_plasticity_fecral_tm\ntest:tensor_mechanics/fecral_plasticity/power-law_strain_hardening.test_power-law_plasticity_fecral_tm: Exit Code: -11\ntest:tensor_mechanics/fecral_plasticity/power-law_strain_hardening.test_power-law_plasticity_fecral_tm: ################################################################################\ntest:tensor_mechanics/fecral_plasticity/power-law_strain_hardening.test_power-law_plasticity_fecral_tm: Tester failed, reason: CRASH\ntest:tensor_mechanics/fecral_plasticity/power-law_strain_hardening.test_power-law_plasticity_fecral_tm:\ntest:tensor_mechanics/fecral_plasticity/power-law_strain_hardening.test_power-law_plasticity_fecral_tm [...] FAILED (CRASH)\nFinal Test Results:\n\ntest:tensor_mechanics/fecral_plasticity/power-law_strain_hardening.test_power-law_plasticity_fecral_tm [...] FAILED (CRASH)\n\nRan 1 tests in 1.5 seconds. Average test time 0.2 seconds, maximum test time 0.2 seconds.\nGiovanni\n--\nGiovanni Pastore, PhD\nResearch Associate Professor\nDepartment of Nuclear Engineering\nUniversity of Tennessee\nKnoxville, TN 37916\ngpastore@utk.edu",
          "url": "https://github.com/idaholab/moose/discussions/17361",
          "updatedAt": "2022-07-08T00:05:32Z",
          "publishedAt": "2021-03-17T21:24:26Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "pastoreg"
                  },
                  "bodyText": "Never mind, I figured this out.\nThe crash was due to a segfault caused by a pretty simple error.  Below is a code snippet from a function called by my constructor:\nif (!_fe_problem.hasFunction(\"FeCrAl_hardening_function_296K\"))\n{\nconst std::vector hf0_plastic_strains = {0.0e+00,\n1.0e-03,\n2.0e-03,\n3.0e-03,\n4.0e-03,\n5.0e-03,\n1.0e-02,\n2.0e-02,\n3.0e-02,\n4.0e-02,\n5.0e-02,\n6.0e-02,\n7.0e-02,\n8.0e-02,\n9.0e-02,\n1.0e-01,\n1.1e-01,\n1.6e-01};\nconst std::vector hf0_stresses = {4.55e+08,\n4.65e+08,\n4.71e+08,\n4.75e+08,\n4.79e+08,\n4.82e+08,\n4.94e+08,\n5.12e+08,\n5.26e+08,\n5.39e+08,\n5.51e+08,\n5.63e+08,\n5.74e+08,\n5.85e+08,\n5.96e+08,\n6.07e+08,\n6.17e+08,\n6.72e+08};\nInputParameters hf0_params = _app.getFactory().getValidParams(\"PiecewiseLinear\");\nhf0_params.set<std::vector<Real>>(\"x\") = hf0_plastic_strains;\nhf0_params.set<std::vector<Real>>(\"y\") = hf0_stresses;\n_fe_problem.addFunction(\"PiecewiseLinear\", \"FeCrAl_hardening_function_296K\", hf0_params);\n\n}\n_hf_temperatures.push_back(296.);\n_hardening_functions.push_back(dynamic_cast<const PiecewiseLinear *>(\n&_fe_problem.getFunction(\"FeCrAl_hardening_function_296K\")));\nIn the version that was giving the test crash (segfaulting), the last two lines were inside the if statement, hence, the _hf_temperatures and _hardening_functions ended up having size zero in multi-thread.\nThanks,\nGiovanni",
                  "url": "https://github.com/idaholab/moose/discussions/17361#discussioncomment-514149",
                  "updatedAt": "2022-07-08T00:05:39Z",
                  "publishedAt": "2021-03-22T16:38:44Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Thanks for posting the solution.",
                          "url": "https://github.com/idaholab/moose/discussions/17361#discussioncomment-527817",
                          "updatedAt": "2022-07-08T00:06:06Z",
                          "publishedAt": "2021-03-25T03:25:19Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}