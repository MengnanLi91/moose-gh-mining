{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0xMi0wNlQwODoyMzoyOC0wNjowMM4ARnkr"
    },
    "edges": [
      {
        "node": {
          "title": "errors when update and rebuild libmesh",
          "author": {
            "login": "Joseph-0123"
          },
          "bodyText": "Dear all,\nOur uni cluster finished maintenance last week. But I find I cannot use MOOSE. So I update and rebuild MOOSE following the HPC Cluster.\nBut when I run ./scripts/update_and_rebuild_libmesh.sh, I have the following errors. Could you please help me fix it? Thanks a lot.\nJoseph\n(base) cd ~/projects/moose\n(base) ./scripts/update_and_rebuild_libmesh.sh\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n/home/projects/moose/scripts/diagnostics.sh: line 40: --version: command not found\nPETSc submodule will be used. PETSc submodule is our default solver.\nIMPORTANT: If you did not run the update_and_rebuild_petsc.sh script yet, please run it before building libMesh\n---------------------------------------------\n----------- Configuring libMesh -------------\n---------------------------------------------\nchecking build system type... x86_64-pc-linux-gnu\nchecking host system type... x86_64-pc-linux-gnu\nchecking target system type... x86_64-pc-linux-gnu\nchecking for a BSD-compatible install... /homeprojects/moose/scripts/../libmesh/build-aux/install-sh -C\nchecking whether build environment is sane... yes\nchecking for a thread-safe mkdir -p... /usr/bin/mkdir -p\nchecking for gawk... gawk\nchecking whether make sets $(MAKE)... yes\nchecking whether make supports nested variables... yes\nchecking whether UID '231419' is supported by ustar format... yes\nchecking whether GID '44347' is supported by ustar format... yes\nchecking how to create a ustar tar archive... gnutar\nchecking whether make supports nested variables... (cached) yes\nchecking whether to enable maintainer-specific portions of Makefiles... no\nchecking for src/base/libmesh.C... no\n<<< Configuring build directory for VPATH build >>>\nchecking for perl... /usr/bin/perl\nnote: MPI library path not given...\ntrying prefix=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\nnote: MPI library path not given...\ntrying prefix=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\nchecking whether make supports the include directive... yes (GNU style)\nchecking whether the C compiler works... no\nconfigure: error: in `/home/projects/moose/libmesh/build':\nconfigure: error: C compiler cannot create executables\nSee `config.log' for more details\nRunning make -j 1...\nmake: *** No targets specified and no makefile found.  Stop.",
          "url": "https://github.com/idaholab/moose/discussions/22720",
          "updatedAt": "2022-12-08T21:10:33Z",
          "publishedAt": "2022-11-16T10:20:49Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "The update_and_rebuild_petsc.sh has no problem. But the update_and_rebuild_libmesh.sh has errors.",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4156724",
                  "updatedAt": "2022-11-16T12:55:43Z",
                  "publishedAt": "2022-11-16T12:55:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "hello\nIt seems there are issues with the compiler.\nCould you please report the output of the diagnostics script in moose/scripts",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4158901",
                          "updatedAt": "2022-11-16T16:27:23Z",
                          "publishedAt": "2022-11-16T16:27:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "hello\nIt seems there are issues with the compiler.\nCould you please report the output of the diagnostics script in moose/scripts\n\nThanks for your help. I get the following information.\n(base) ./scripts/diagnostics.sh\nWed Nov 16 17:37:48 CET 2022\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.4 (Ootpa) Release: 8.4 Codename: Ootpa\nMOOSE Package Version: Custom Build\nCPU Count: 80\nMemory Free: 320052.148 MB\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nVariable which $CC check:\n/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin/mpicc\n$CC --version:\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n./scripts/diagnostics.sh: line 40: --version: command not found\nMPICC:\nwhich mpicc:\n/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin/mpicc\nmpicc -show:\nCOMPILER :\nPython:\n/home/miniconda3/bin/python\nPython 3.7.13\nMODULES:\nCurrently Loaded Modules:\n\ncompiler/gnu/10.2   2) mpi/openmpi/4.0\n\nPETSC_DIR not set\nENVIRONMENT:\nCONDA_SHLVL=1\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arc=01;31:.arj=01;31:.taz=01;31:.lha=01;31:.lz4=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.tzo=01;31:.t7z=01;31:.zip=01;31:.z=01;31:.dz=01;31:.gz=01;31:.lrz=01;31:.lz=01;31:.lzo=01;31:.xz=01;31:.zst=01;31:.tzst=01;31:.bz2=01;31:.bz=01;31:.tbz=01;31:.tbz2=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.war=01;31:.ear=01;31:.sar=01;31:.rar=01;31:.alz=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.cab=01;31:.wim=01;31:.swm=01;31:.dwm=01;31:.esd=01;31:.jpg=01;35:.jpeg=01;35:.mjpg=01;35:.mjpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.webm=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.ogv=01;35:.ogx=01;35:.aac=01;36:.au=01;36:.flac=01;36:.m4a=01;36:.mid=01;36:.midi=01;36:.mka=01;36:.mp3=01;36:.mpc=01;36:.ogg=01;36:.ra=01;36:.wav=01;36:.oga=01;36:.opus=01;36:.spx=01;36:.xspf=01;36:\nLD_LIBRARY_PATH=/home/moose-compilers/gcc-10.2.0/lib64:/home/moose-compilers/gcc-10.2.0/lib:/home/moose-compilers/gcc-10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0:/home/moose-compilers/gcc-10.2.0/libexec/gcc/x86_64-pc-linux-gnu/10.2.0:/home/moose-compilers/mpich-3.3/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64\nCONDA_EXE=/home/miniconda3/bin/conda\n__LMOD_REF_COUNT_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/bin:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin:1;/home/.local/bin:1;/home/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\nModuleTable002=LAp9LApbIm1waS9vcGVubXBpIl0gPSB7CmZuID0gIi9zb2Z0d2FyZS9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29tcGlsZXIvZ251LzEwLjIvbXBpL29wZW5tcGkvNC4wLmx1YSIsCmZ1bGxOYW1lID0gIm1waS9vcGVubXBpLzQuMCIsCmxvYWRPcmRlciA9IDIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAibXBpL29wZW5tcGkvNC4wIiwKd1YgPSAiMDAwMDAwMDA0Lip6ZmluYWwiLAp9LAp9LAptcGF0aEEgPSB7CiIvc29mdHdhcmUvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvbXBpbGVyL2dudS8xMC4yIiwgIi9vcHQvYndocGMva2l0L21vZHVsZWZpbGVzIiwgIi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZp\nSSH_CONNECTION=2a00:1398:4:7e02:e5a8:991c:1523:d3e 49843 2a00:1398:4:1805::810d:3814 22\nMODULES_RUN_QUARANTINE=LD_LIBRARY_PATH LD_PRELOAD\nLANG=en_US.UTF-8\nLMOD_SYSTEM_NAME=uc2\nHISTTIMEFORMAT=%F %T\nGNU_HOME=/opt/bwhpc/common/compiler/gnu/10.2.0\nLMOD_FAMILY_COMPILER_VERSION=10.2\nHOSTNAME=uc2n995.localdomain\nOLDPWD=/home/projects/moose/scripts\nGNU_MAN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man\nC_INCLUDE_PATH=/home//moose-compilers/mpich-3.3/include:\nMPI_INC_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/include\nMPI_MAN_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man\nFPATH=/home//moose-compilers/mpich-3.3/include:\nGNU_BIN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/bin\nCONDA_PREFIX=/home/miniconda3\n__LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/lib:1;/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64:1\nMPIDIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\nGNU_VERSION=10.2.0\nMPI_LIB_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib\n_CE_M=\nCC=mpicc\nPROJ_LIB=/home/miniconda3/share/proj\nXDG_SESSION_ID=11076\nMODULES_CMD=/usr/share/Modules/libexec/modulecmd.tcl\nUSER=qz9211\nGNU_LIB_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib\n__LMOD_REF_COUNT_MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:1;/opt/bwhpc/modulefiles:1;/opt/bwhpc/common/modulefiles/Core:1\nPWD=/home/projects/moose\nHOME=/home/\nTMP=/scratch\nCONDA_PYTHON_EXE=/home/miniconda3/bin/python\nSSH_CLIENT=2a00:1398:4:7e02:e5a8:991c:1523:d3e 49843 22\nLMOD_VERSION=8.6.16\nLMOD_PAGER=less\nFAMILY_COMPILER_VERSION=10.2\nF77=mpif77\nXDG_DATA_DIRS=/home/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\nFAMILY_COMPILER=compiler/gnu\n_CE_CONDA=\nMKL_NUM_THREADS=1\nTMPDIR=/scratch\nLMOD_sys=Linux\nModuleTable001=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJjb21waWxlci9nbnUiLAp9LAptVCA9IHsKWyJjb21waWxlci9nbnUiXSA9IHsKZm4gPSAiL29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZS9jb21waWxlci9nbnUvMTAuMi5sdWEiLApmdWxsTmFtZSA9ICJjb21waWxlci9nbnUvMTAuMiIsCmxvYWRPcmRlciA9IDEsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAxLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29tcGlsZXIvZ251LzEwLjIiLAp3ViA9ICJeMDAwMDAwMTAuMDAwMDAwMDAyLip6ZmluYWwi\nLOADEDMODULES=compiler/gnu/10.2:mpi/openmpi/4.0\nFC=mpif90\n__LMOD_REF_COUNT_MANPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man:1;/opt/lmod/lmod/share/man:1\nSCRATCH=/scratch\nModuleTable003=bGVzL0NvcmUiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL29wdC9id2hwYy9raXQvbW9kdWxlZmlsZXM6L29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZSIsCn0K\nLMOD_ROOT=/opt/lmod\nCONDA_PROMPT_MODIFIER=(base)\nSSH_TTY=/dev/pts/33\nMAIL=/var/spool/mail/qz9211\nMPI_VERSION=4.0.5-gnu-10.2\nCXX=mpicxx\nMPI_HOME=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\nMPI_LIB64_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64\nSHELL=/bin/bash\nTERM=xterm\nLMOD_SITE_NAME=TIK\nModuleTable_Sz=3\nLMOD_FAMILY_COMPILER=compiler/gnu\nLSDF=/lsdf\nTMOUT=36000\nGNU_LIB64_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib64\nMPI_BIN_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin\nSHLVL=2\nMANPATH=/home/moose-compilers/mpich-3.3/share/man:/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man:/opt/lmod/lmod/share/man::\nTEMP=/scratch\nF90=mpif90\nMODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:/opt/modulefiles:/opt/bwhpc/common/modulefiles/Core\nLOGNAME=qz9211\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/231419/bus\nCLUSTER=uc2\nXDG_RUNTIME_DIR=/run/user/231419\nCPLUS_INCLUDE_PATH=/home/moose-compilers/mpich-3.3/include:\nMODULEPATH_modshare=/usr/share/modulefiles:1:/usr/share/Modules/modulefiles:1:/etc/modulefiles:1\nMODULEPATH_ROOT=/opt/modulefiles\nLMOD_PACKAGE_PATH=/etc/lmod/?.lua;;\nPATH=/home/miniconda3/bin:/home/miniconda3/condabin:/home/moose-compilers/gcc-10.2.0/bin:/home/moose-compilers/mpich-3.3/bin:/home/moose-compilers/miniconda/bin:/opt/bwhpc/common/compiler/gnu/10.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin:/home/.local/bin:/home/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\nGNU_INC_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/include\nLMFILES=/opt/bwhpc/common/modulefiles/Core/compiler/gnu/10.2.lua:/software/bwhpc/common/modulefiles/Compiler/gnu/10.2/mpi/openmpi/4.0.lua\nMODULESHOME=/opt/lmod/lmod\nLMOD_SETTARG_FULL_SUPPORT=no\nCONDA_DEFAULT_ENV=base\nMPI_EXA_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/examples\nHISTSIZE=1000\nLMOD_PKG=/opt/lmod/lmod\nLMOD_CMD=/opt/lmod/lmod/libexec/lmod\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nOMP_NUM_THREADS=1\nLMOD_DIR=/opt/lmod/lmod/libexec\nBASH_FUNC_module%%=() {  eval $(/opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash $)\n}\nBASH_FUNC__module_raw%%=() {  unset _mlshdbg;\nif [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\ncase \"$-\" in\nvx*)\nset +vx;\n_mlshdbg='vx'\n;;\nv)\nset +v;\n_mlshdbg='v'\n;;\nx)\nset +x;\nmlshdbg='x'\n;;\n)\n_mlshdbg=''\n;;\nesac;\nfi;\nunset _mlre _mlIFS;\nif [ -n \"${IFS+x}\" ]; then\n_mlIFS=$IFS;\nfi;\nIFS=' ';\nfor _mlv in ${MODULES_RUN_QUARANTINE:-};\ndo\nif [ \"${_mlv}\" = \"${_mlv##[!A-Za-z0-9]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\nif [ -n \"eval 'echo ${'$_mlv'+x}'\" ]; then\n_mlre=\"${_mlre:-}${_mlv}_modquar='eval 'echo ${'$_mlv'}'' \";\nfi;\nmlrv=\"MODULES_RUNENV${_mlv}\";\n_mlre=\"${_mlre:-}${_mlv}='eval 'echo ${'$_mlrv':-}'' \";\nfi;\ndone;\nif [ -n \"${_mlre:-}\" ]; then\neval eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '\"$@\"';\nelse\neval /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\";\nfi;\n_mlstatus=$?;\nif [ -n \"${_mlIFS+x}\" ]; then\nIFS=$_mlIFS;\nelse\nunset IFS;\nfi;\nunset _mlre _mlv _mlrv _mlIFS;\nif [ -n \"${_mlshdbg:-}\" ]; then\nset -$_mlshdbg;\nfi;\nunset _mlshdbg;\nreturn $_mlstatus\n}\nBASH_FUNC_switchml%%=() {  typeset swfound=1;\nif [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\ntypeset swname='main';\nif [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\ntypeset swfound=0;\nunset MODULES_USE_COMPAT_VERSION;\nfi;\nelse\ntypeset swname='compatibility';\nif [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\ntypeset swfound=0;\nMODULES_USE_COMPAT_VERSION=1;\nexport MODULES_USE_COMPAT_VERSION;\nfi;\nfi;\nif [ $swfound -eq 0 ]; then\necho \"Switching to Modules $swname version\";\nsource /usr/share/Modules/init/bash;\nelse\necho \"Cannot switch to Modules $swname version, command not found\";\nreturn 1;\nfi\n}\nBASH_FUNC_ml%%=() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\n}\n_=/usr/bin/env\n(base)",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4159082",
                          "updatedAt": "2022-11-16T16:42:37Z",
                          "publishedAt": "2022-11-16T16:42:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so your python is still the miniconda python\nyou still have a problem with compilers, your path contains an mpich you probably installed yourself in home/moose-compilers,\nand openmpi from the module\nwhat s worse is that that openmpi from the module is BROKEN. It gives you errors on every invocation.\nI would recommend:\n\nclean the environment further from both the openmpi and mpich in there\nfind a non-broken mpi module to use. It should not give these \"fabrics\" and \"symbol lookup\" errors",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4292601",
                          "updatedAt": "2022-12-02T13:46:31Z",
                          "publishedAt": "2022-12-02T13:46:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "The MPI compiler is broken it sends an error message.\n\nYou need to address this before you can build libmesh.\n\nIf this is something your cluster administrator installed for you then you ll need to contact them\n\u2026\n Le 16 nov. 2022 \u00e0 09:42, Joseph-0123 ***@***.***> a \u00e9crit :\n\n \ufeff\n hello\n\n It seems there are issues with the compiler.\n\n Could you please report the output of the diagnostics script in moose/scripts\n\n Thanks for your help. I get the following information.\n\n (base) ./scripts/diagnostics.sh\n Wed Nov 16 17:37:48 CET 2022\n\n System Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.4 (Ootpa) Release: 8.4 Codename: Ootpa\n\n MOOSE Package Version: Custom Build\n\n CPU Count: 80\n\n Memory Free: 320052.148 MB\n mpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n\n Variable which $CC check:\n /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin/mpicc\n\n $CC --version:\n\n mpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n mpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n mpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n ./scripts/diagnostics.sh: line 40: --version: command not found\n\n MPICC:\n which mpicc:\n /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin/mpicc\n mpicc -show:\n\n COMPILER :\n\n Python:\n /home/miniconda3/bin/python\n Python 3.7.13\n\n MODULES:\n\n Currently Loaded Modules:\n\n compiler/gnu/10.2 2) mpi/openmpi/4.0\n PETSC_DIR not set\n\n ENVIRONMENT:\n CONDA_SHLVL=1\n LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arc=01;31:.arj=01;31:.taz=01;31:.lha=01;31:.lz4=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.tzo=01;31:.t7z=01;31:.zip=01;31:.z=01;31:.dz=01;31:.gz=01;31:.lrz=01;31:.lz=01;31:.lzo=01;31:.xz=01;31:.zst=01;31:.tzst=01;31:.bz2=01;31:.bz=01;31:.tbz=01;31:.tbz2=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.war=01;31:.ear=01;31:.sar=01;31:.rar=01;31:.alz=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.cab=01;31:.wim=01;31:.swm=01;31:.dwm=01;31:.esd=01;31:.jpg=01;35:.jpeg=01;35:.mjpg=01;35:.mjpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.webm=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.ogv=01;35:.ogx=01;35:.aac=01;36:.au=01;36:.flac=01;36:.m4a=01;36:.mid=01;36:.midi=01;36:.mka=01;36:.mp3=01;36:.mpc=01;36:.ogg=01;36:.ra=01;36:.wav=01;36:.oga=01;36:.opus=01;36:.spx=01;36:.xspf=01;36:\n LD_LIBRARY_PATH=/home/moose-compilers/gcc-10.2.0/lib64:/home/moose-compilers/gcc-10.2.0/lib:/home/moose-compilers/gcc-10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0:/home/moose-compilers/gcc-10.2.0/libexec/gcc/x86_64-pc-linux-gnu/10.2.0:/home/moose-compilers/mpich-3.3/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64\n CONDA_EXE=/home/miniconda3/bin/conda\n __LMOD_REF_COUNT_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/bin:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin:1;/home/.local/bin:1;/home/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\n ModuleTable002=LAp9LApbIm1waS9vcGVubXBpIl0gPSB7CmZuID0gIi9zb2Z0d2FyZS9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29tcGlsZXIvZ251LzEwLjIvbXBpL29wZW5tcGkvNC4wLmx1YSIsCmZ1bGxOYW1lID0gIm1waS9vcGVubXBpLzQuMCIsCmxvYWRPcmRlciA9IDIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAibXBpL29wZW5tcGkvNC4wIiwKd1YgPSAiMDAwMDAwMDA0Lip6ZmluYWwiLAp9LAp9LAptcGF0aEEgPSB7CiIvc29mdHdhcmUvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvbXBpbGVyL2dudS8xMC4yIiwgIi9vcHQvYndocGMva2l0L21vZHVsZWZpbGVzIiwgIi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZp\n SSH_CONNECTION=2a00:1398:4:7e02:e5a8:991c:1523:d3e 49843 2a00:1398:4:1805::810d:3814 22\n MODULES_RUN_QUARANTINE=LD_LIBRARY_PATH LD_PRELOAD\n LANG=en_US.UTF-8\n LMOD_SYSTEM_NAME=uc2\n HISTTIMEFORMAT=%F %T\n GNU_HOME=/opt/bwhpc/common/compiler/gnu/10.2.0\n LMOD_FAMILY_COMPILER_VERSION=10.2\n HOSTNAME=uc2n995.localdomain\n OLDPWD=/home/projects/moose/scripts\n GNU_MAN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man\n C_INCLUDE_PATH=/home//moose-compilers/mpich-3.3/include:\n MPI_INC_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/include\n MPI_MAN_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man\n FPATH=/home//moose-compilers/mpich-3.3/include:\n GNU_BIN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/bin\n CONDA_PREFIX=/home/miniconda3\n __LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/lib:1;/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64:1\n MPIDIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\n GNU_VERSION=10.2.0\n MPI_LIB_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib\n _CE_M=\n CC=mpicc\n PROJ_LIB=/home/miniconda3/share/proj\n XDG_SESSION_ID=11076\n MODULES_CMD=/usr/share/Modules/libexec/modulecmd.tcl\n USER=qz9211\n GNU_LIB_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib\n __LMOD_REF_COUNT_MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:1;/opt/bwhpc/modulefiles:1;/opt/bwhpc/common/modulefiles/Core:1\n PWD=/home/projects/moose\n HOME=/home/\n TMP=/scratch\n CONDA_PYTHON_EXE=/home/miniconda3/bin/python\n SSH_CLIENT=2a00:1398:4:7e02:e5a8:991c:1523:d3e 49843 22\n LMOD_VERSION=8.6.16\n LMOD_PAGER=less\n FAMILY_COMPILER_VERSION=10.2\n F77=mpif77\n XDG_DATA_DIRS=/home/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\n FAMILY_COMPILER=compiler/gnu\n _CE_CONDA=\n MKL_NUM_THREADS=1\n TMPDIR=/scratch\n LMOD_sys=Linux\n ModuleTable001=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJjb21waWxlci9nbnUiLAp9LAptVCA9IHsKWyJjb21waWxlci9nbnUiXSA9IHsKZm4gPSAiL29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZS9jb21waWxlci9nbnUvMTAuMi5sdWEiLApmdWxsTmFtZSA9ICJjb21waWxlci9nbnUvMTAuMiIsCmxvYWRPcmRlciA9IDEsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAxLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29tcGlsZXIvZ251LzEwLjIiLAp3ViA9ICJeMDAwMDAwMTAuMDAwMDAwMDAyLip6ZmluYWwi\n LOADEDMODULES=compiler/gnu/10.2:mpi/openmpi/4.0\n FC=mpif90\n __LMOD_REF_COUNT_MANPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man:1;/opt/lmod/lmod/share/man:1\n SCRATCH=/scratch\n ModuleTable003=bGVzL0NvcmUiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL29wdC9id2hwYy9raXQvbW9kdWxlZmlsZXM6L29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZSIsCn0K\n LMOD_ROOT=/opt/lmod\n CONDA_PROMPT_MODIFIER=(base)\n SSH_TTY=/dev/pts/33\n MAIL=/var/spool/mail/qz9211\n MPI_VERSION=4.0.5-gnu-10.2\n CXX=mpicxx\n MPI_HOME=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\n MPI_LIB64_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64\n SHELL=/bin/bash\n TERM=xterm\n LMOD_SITE_NAME=TIK\n ModuleTable_Sz=3\n LMOD_FAMILY_COMPILER=compiler/gnu\n LSDF=/lsdf\n TMOUT=36000\n GNU_LIB64_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib64\n MPI_BIN_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin\n SHLVL=2\n MANPATH=/home/moose-compilers/mpich-3.3/share/man:/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man:/opt/lmod/lmod/share/man::\n TEMP=/scratch\n F90=mpif90\n MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:/opt/modulefiles:/opt/bwhpc/common/modulefiles/Core\n LOGNAME=qz9211\n DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/231419/bus\n CLUSTER=uc2\n XDG_RUNTIME_DIR=/run/user/231419\n CPLUS_INCLUDE_PATH=/home/moose-compilers/mpich-3.3/include:\n MODULEPATH_modshare=/usr/share/modulefiles:1:/usr/share/Modules/modulefiles:1:/etc/modulefiles:1\n MODULEPATH_ROOT=/opt/modulefiles\n LMOD_PACKAGE_PATH=/etc/lmod/?.lua;;\n PATH=/home/miniconda3/bin:/home/miniconda3/condabin:/home/moose-compilers/gcc-10.2.0/bin:/home/moose-compilers/mpich-3.3/bin:/home/moose-compilers/miniconda/bin:/opt/bwhpc/common/compiler/gnu/10.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin:/home/.local/bin:/home/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\n GNU_INC_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/include\n LMFILES=/opt/bwhpc/common/modulefiles/Core/compiler/gnu/10.2.lua:/software/bwhpc/common/modulefiles/Compiler/gnu/10.2/mpi/openmpi/4.0.lua\n MODULESHOME=/opt/lmod/lmod\n LMOD_SETTARG_FULL_SUPPORT=no\n CONDA_DEFAULT_ENV=base\n MPI_EXA_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/examples\n HISTSIZE=1000\n LMOD_PKG=/opt/lmod/lmod\n LMOD_CMD=/opt/lmod/lmod/libexec/lmod\n LESSOPEN=||/usr/bin/lesspipe.sh %s\n OMP_NUM_THREADS=1\n LMOD_DIR=/opt/lmod/lmod/libexec\n BASH_FUNC_module%%=() { eval $(/opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash $)\n }\n BASH_FUNC__module_raw%%=() { unset _mlshdbg;\n if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\n case \"$-\" in\n vx*)\n set +vx;\n _mlshdbg='vx'\n ;;\n v)\n set +v;\n _mlshdbg='v'\n ;;\n x)\n set +x;\n mlshdbg='x'\n ;;\n )\n _mlshdbg=''\n ;;\n esac;\n fi;\n unset _mlre _mlIFS;\n if [ -n \"${IFS+x}\" ]; then\n _mlIFS=$IFS;\n fi;\n IFS=' ';\n for _mlv in ${MODULES_RUN_QUARANTINE:-};\n do\n if [ \"${_mlv}\" = \"${_mlv##[!A-Za-z0-9]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\n if [ -n \"eval 'echo ${'$_mlv'+x}'\" ]; then\n _mlre=\"${_mlre:-}${_mlv}_modquar='eval 'echo ${'$_mlv'}'' \";\n fi;\n mlrv=\"MODULES_RUNENV${_mlv}\";\n _mlre=\"${_mlre:-}${_mlv}='eval 'echo ${'$_mlrv':-}'' \";\n fi;\n done;\n if [ -n \"${_mlre:-}\" ]; then\n eval eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '\"$@\"';\n else\n eval /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\";\n fi;\n _mlstatus=$?;\n if [ -n \"${_mlIFS+x}\" ]; then\n IFS=$_mlIFS;\n else\n unset IFS;\n fi;\n unset _mlre _mlv _mlrv _mlIFS;\n if [ -n \"${_mlshdbg:-}\" ]; then\n set -$_mlshdbg;\n fi;\n unset _mlshdbg;\n return $_mlstatus\n }\n BASH_FUNC_switchml%%=() { typeset swfound=1;\n if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\n typeset swname='main';\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\n typeset swfound=0;\n unset MODULES_USE_COMPAT_VERSION;\n fi;\n else\n typeset swname='compatibility';\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\n typeset swfound=0;\n MODULES_USE_COMPAT_VERSION=1;\n export MODULES_USE_COMPAT_VERSION;\n fi;\n fi;\n if [ $swfound -eq 0 ]; then\n echo \"Switching to Modules $swname version\";\n source /usr/share/Modules/init/bash;\n else\n echo \"Cannot switch to Modules $swname version, command not found\";\n return 1;\n fi\n }\n BASH_FUNC_ml%%=() { eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\n }\n _=/usr/bin/env\n (base)\n\n \u2014\n Reply to this email directly, view it on GitHub, or unsubscribe.\n You are receiving this because you commented.",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4159518",
                  "updatedAt": "2022-11-16T17:32:08Z",
                  "publishedAt": "2022-11-16T17:32:08Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "Thanks for your help, I have contacted the cluster administrator.",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4165736",
                  "updatedAt": "2022-11-17T09:56:35Z",
                  "publishedAt": "2022-11-17T09:56:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "Sorry for my question again. I have the following errors (ending part of the contents) again when I run ./scripts/update_and_rebuild_libmesh.sh,\nizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _Size = long int; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<FPoptimizer_CodeTree::ParamComparer<complex<long double> > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algo.h:1921:38:\nlib/autoptr.hh:51:19: warning: pointer used after \u2018void operator delete(void*, std::size_t)\u2019 [-Wuse-after-free]\nIn member function \u2018void FPOPT_autoptr<Ref>::Forget() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019,\n    inlined from \u2018void FPOPT_autoptr<Ref>::Set(Ref*) [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019 at lib/autoptr.hh:62:11,\n    inlined from \u2018FPOPT_autoptr<Ref>& FPOPT_autoptr<Ref>::operator=(const FPOPT_autoptr<Ref>&) [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019 at lib/autoptr.hh:16:60,\n    inlined from \u2018FPoptimizer_CodeTree::CodeTree<std::complex<long double> >& FPoptimizer_CodeTree::CodeTree<std::complex<long double> >::operator=(const FPoptimizer_CodeTree::CodeTree<std::complex<long double> >&)\u2019 at fpoptimizer/codetree.hh:37:11,\n    inlined from \u2018std::_Require<std::__not_<std::__is_tuple_like<_Tp> >, std::is_move_constructible<_Tp>, std::is_move_assignable<_Tp> > std::swap(_Tp&, _Tp&) [with _Tp = FPoptimizer_CodeTree::CodeTree<complex<long double> >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/move.h:205:11,\n    inlined from \u2018void std::iter_swap(_ForwardIterator1, _ForwardIterator2) [with _ForwardIterator1 = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _ForwardIterator2 = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algobase.h:182:11,\n    inlined from \u2018_RandomAccessIterator std::__unguarded_partition(_RandomAccessIterator, _RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<FPoptimizer_CodeTree::ParamComparer<complex<long double> > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algo.h:1874:18,\n    inlined from \u2018_RandomAccessIterator std::__unguarded_partition_pivot(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<FPoptimizer_CodeTree::ParamComparer<complex<long double> > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algo.h:1889:40,\n    inlined from \u2018void std::__introsort_loop(_RandomAccessIterator, _RandomAccessIterator, _Size, _Compare) [with _RandomAccessIterator = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _Size = long int; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<FPoptimizer_CodeTree::ParamComparer<complex<long double> > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algo.h:1921:38:\nlib/autoptr.hh:45:22: note: call to \u2018void operator delete(void*, std::size_t)\u2019 here\nIn member function \u2018void FPOPT_autoptr<Ref>::Forget() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019,\n    inlined from \u2018FPOPT_autoptr<Ref>::~FPOPT_autoptr() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019 at lib/autoptr.hh:23:30,\n    inlined from \u2018FPoptimizer_CodeTree::CodeTree<Value_t>::~CodeTree() [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/codetree.cc:104:5,\n    inlined from \u2018void {anonymous}::CodeTreeParserData<Value_t>::AddConst(const Value_t&) [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/readbytecode.cc:305:9,\n    inlined from \u2018void FPoptimizer_CodeTree::CodeTree<Value_t>::GenerateFrom(const typename FunctionParserBase<Value_t>::Data&, const std::vector<FPoptimizer_CodeTree::CodeTree<Value_t> >&, bool) [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/readbytecode.cc:487:33:\nlib/autoptr.hh:44:8: warning: pointer used after \u2018void operator delete(void*, std::size_t)\u2019 [-Wuse-after-free]\nIn member function \u2018void FPOPT_autoptr<Ref>::Forget() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019,\n    inlined from \u2018FPOPT_autoptr<Ref>::~FPOPT_autoptr() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019 at lib/autoptr.hh:23:30,\n    inlined from \u2018FPoptimizer_CodeTree::CodeTree<Value_t>::~CodeTree() [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/codetree.cc:104:5,\n    inlined from \u2018void {anonymous}::CodeTreeParserData<Value_t>::AddConst(const Value_t&) [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/readbytecode.cc:304:17,\n    inlined from \u2018void FPoptimizer_CodeTree::CodeTree<Value_t>::GenerateFrom(const typename FunctionParserBase<Value_t>::Data&, const std::vector<FPoptimizer_CodeTree::CodeTree<Value_t> >&, bool) [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/readbytecode.cc:487:33:\nlib/autoptr.hh:45:22: note: call to \u2018void operator delete(void*, std::size_t)\u2019 here\n  CXXLD    libdevel.la\n  CXX      libopt_la-fparser.lo\n  CXX      libopt_la-fparser_ad.lo\n  CXX      libopt_la-Faddeeva.lo\n  CXX      libopt_la-fpoptimizer.lo\n  CXXLD    libopt.la\n  CXX      liboprof_la-fparser.lo\n  CXX      liboprof_la-fparser_ad.lo\n  CXX      liboprof_la-Faddeeva.lo\n  CXX      liboprof_la-fpoptimizer.lo\n  CXXLD    liboprof.la\nmake[4]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/fparser'\nmake[3]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/fparser'\nmake[2]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/fparser'\nMaking all in nanoflann\nmake[2]: Entering directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/nanoflann'\nmake[2]: Nothing to be done for 'all'.\nmake[2]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/nanoflann'\nMaking all in timpi\nmake[2]: Entering directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/timpi'\nMaking all in src\nmake[3]: Entering directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/timpi/src'\n  CXX      apps/version.o\n  CXX      parallel/src/libtimpi_dbg_la-communicator.lo\n  CXX      parallel/src/libtimpi_dbg_la-message_tag.lo\n  CXX      parallel/src/libtimpi_dbg_la-request.lo\n  CXX      utilities/src/libtimpi_dbg_la-semipermanent.lo\n  CXX      utilities/src/libtimpi_dbg_la-timpi_assert.lo\n  CXX      utilities/src/libtimpi_dbg_la-timpi_init.lo\n  CXX      utilities/src/libtimpi_dbg_la-timpi_version.lo\n  CXXLD    libtimpi_dbg.la\n  CXXLD    timpi_version-dbg\nlibtool: warning: '-version-info' is ignored for programs\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'\ncollect2: error: ld returned 1 exit status\nmake[3]: *** [Makefile:950: timpi_version-dbg] Error 1\nmake[3]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/timpi/src'\nmake[2]: *** [Makefile:614: all-recursive] Error 1\nmake[2]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/timpi'\nmake[1]: *** [Makefile:1033: all-recursive] Error 1\nmake[1]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib'\nmake: *** [Makefile:33070: all-recursive] Error 1",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283042",
                  "updatedAt": "2022-12-05T14:30:51Z",
                  "publishedAt": "2022-12-01T12:46:22Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This is still this fabric library\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283572",
                          "updatedAt": "2022-12-01T13:55:08Z",
                          "publishedAt": "2022-12-01T13:55:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "This is still this fabric library\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'\n\n\nThanks for your reminder. Could you please tell me what are the detailed solutions for this problem? I do not find any similar answers about this problem in our Discussion community.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283736",
                          "updatedAt": "2022-12-01T14:13:44Z",
                          "publishedAt": "2022-12-01T14:13:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "@roystgnr I am not sure what to make of this issue... PETSc seems to have built fine (you can see the link lines in the last post of this thread).\nDo you have any idea as to what may be going on? I think Joseph's environment is sound... Edit: I am responding here because the latest error results from building libMesh can be seen above.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4313787",
                          "updatedAt": "2022-12-05T14:34:23Z",
                          "publishedAt": "2022-12-05T14:32:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "Do you have any idea as to what may be going on?\n\nDifferent MPI versions getting mixed?  If that happens, anywhere, the eventual disaster looks something like this.  There can only be one MPI that can get invoked by libMesh, and it has to be the exact same binary that PETSc used, and if you try to build with one but don't clean out every library file before \"starting over\" with another, that still counts as different MPI versions getting mixed.\nmake V=1 might be useful to see what the linker thinks it's trying to do here.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4317685",
                          "updatedAt": "2022-12-05T22:22:00Z",
                          "publishedAt": "2022-12-05T22:21:59Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "The output of the diagnostics script in moose/scripts is shown as follows. It looks like there are no errors.\nThu Dec  1 13:41:04 CET 2022\n\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.4 (Ootpa) Release: 8.4 Codename: Ootpa\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 80\n\nMemory Free: 290202.082 MB\n\n$CC not set\n\nMPICC:\nwhich mpicc:\n\t/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin/mpicc\nmpicc -show:\n\tgcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\n\nCOMPILER gcc:\ngcc (GCC) 12.1.0\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n\t/home/miniconda3/bin/python\n\tPython 3.7.13\n\nMODULES:\n\nCurrently Loaded Modules:\n  1) compiler/gnu/12.1   2) mpi/openmpi/4.1\n\n \n\nPETSC_DIR not set\n\nENVIRONMENT:\nCONDA_SHLVL=1\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36:\nLD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/12.1.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64:/home/moose-compilers/gcc-10.2.0/lib64:/home/moose-compilers/gcc-10.2.0/lib:/home/moose-compilers/gcc-10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0:/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/libexec/gcc/x86_64-pc-linux-gnu/10.2.0:/home/kit/agw/qz9211/moose-compilers/mpich-3.3/lib\nCONDA_EXE=/home/kit/agw/qz9211/miniconda3/bin/conda\n__LMOD_REF_COUNT_PATH=/opt/bwhpc/common/compiler/gnu/12.1.0/bin:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:1;/home/kit/agw/qz9211/miniconda3/bin:1;/home/kit/agw/qz9211/miniconda3/condabin:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/bin:1;/home/kit/agw/qz9211/moose-compilers/mpich-3.3/bin:1;/home/kit/agw/qz9211/moose-compilers/miniconda/bin:1;/home/kit/agw/qz9211/.local/bin:1;/home/kit/agw/qz9211/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\n_ModuleTable002_=MS4qemZpbmFsIiwKfSwKWyJtcGkvb3Blbm1waSJdID0gewpmbiA9ICIvc29mdHdhcmUvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvbXBpbGVyL2dudS8xMi4xL21waS9vcGVubXBpLzQuMS5sdWEiLApmdWxsTmFtZSA9ICJtcGkvb3Blbm1waS80LjEiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIm1waS9vcGVubXBpLzQuMSIsCndWID0gIjAwMDAwMDAwNC4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKIi9zb2Z0d2FyZS9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29tcGlsZXIvZ251LzEyLjEiLCAiL29wdC9id2hwYy9raXQvbW9kdWxlZmlsZXMiLCAiL29wdC9i\nSSH_CONNECTION=2a00:1398:4:7e02:3dc8:2104:ec1c:ecb5 54776 2a00:1398:4:1805::810d:3813 22\nMODULES_RUN_QUARANTINE=LD_LIBRARY_PATH LD_PRELOAD\nLANG=en_US.UTF-8\nLMOD_SYSTEM_NAME=uc2\nHISTTIMEFORMAT=%F %T \nGNU_HOME=/opt/bwhpc/common/compiler/gnu/12.1.0\nLMOD_FAMILY_COMPILER_VERSION=12.1\nHOSTNAME=uc2n996.localdomain\nOLDPWD=/home/kit/agw/qz9211/projects/moose/petsc\nGNU_MAN_DIR=/opt/bwhpc/common/compiler/gnu/12.1.0/share/man\nC_INCLUDE_PATH=/home/kit/agw/qz9211/moose-compilers/mpich-3.3/include:\nMPI_INC_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include\nMPI_MAN_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man\nFPATH=/home/kit/agw/qz9211/moose-compilers/mpich-3.3/include:\nGNU_BIN_DIR=/opt/bwhpc/common/compiler/gnu/12.1.0/bin\nCONDA_PREFIX=/home/kit/agw/qz9211/miniconda3\n__LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/12.1.0/lib64:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/lib64:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/lib:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/libexec/gcc/x86_64-pc-linux-gnu/10.2.0:1;/home/kit/agw/qz9211/moose-compilers/mpich-3.3/lib:1\nMPIDIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1\nGNU_VERSION=12.1.0\nMPI_LIB_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64\n_CE_M=\nPROJ_LIB=/home/kit/agw/qz9211/miniconda3/share/proj\nXDG_SESSION_ID=12297\nMODULES_CMD=/usr/share/Modules/libexec/modulecmd.tcl\nUSER=qz9211\nGNU_LIB_DIR=/opt/bwhpc/common/compiler/gnu/12.1.0/lib64\n__LMOD_REF_COUNT_MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/12.1:1;/opt/bwhpc/kit/modulefiles:1;/opt/bwhpc/common/modulefiles/Core:1\nPWD=/home/kit/agw/qz9211/projects/moose\nHOME=/home/kit/agw/qz9211\nTMP=/scratch\nCONDA_PYTHON_EXE=/home/kit/agw/qz9211/miniconda3/bin/python\nSSH_CLIENT=2a00:1398:4:7e02:3dc8:2104:ec1c:ecb5 54776 22\nLMOD_VERSION=8.6.16\nLMOD_PAGER=less\nKIT_FAMILY_COMPILER_VERSION=12.1\nF77=mpif77\nXDG_DATA_DIRS=/home/kit/agw/qz9211/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\nKIT_FAMILY_COMPILER=compiler/gnu\n_CE_CONDA=\nMKL_NUM_THREADS=1\nTMPDIR=/scratch\nLMOD_sys=Linux\n_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IDg2NDAwLApjX3Nob3J0VGltZSA9IDcuNjk0MTI4OTkwMTczMywKZGVwdGhUID0ge30sCmZhbWlseSA9IHsKY29tcGlsZXIgPSAiY29tcGlsZXIvZ251IiwKfSwKbVQgPSB7ClsiY29tcGlsZXIvZ251Il0gPSB7CmZuID0gIi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvcmUvY29tcGlsZXIvZ251LzEyLjEubHVhIiwKZnVsbE5hbWUgPSAiY29tcGlsZXIvZ251LzEyLjEiLApsb2FkT3JkZXIgPSAxLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMSwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNvbXBpbGVyL2dudS8xMi4xIiwKd1YgPSAiMDAwMDAwMDEyLjAwMDAwMDAw\nLOADEDMODULES=compiler/gnu/12.1:mpi/openmpi/4.1\nFC=mpif90\n__LMOD_REF_COUNT_MANPATH=/opt/bwhpc/common/compiler/gnu/12.1.0/share/man:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man:1;/home/kit/agw/qz9211/moose-compilers/mpich-3.3/share/man:1;/opt/lmod/lmod/share/man:1\nSCRATCH=/scratch\n_ModuleTable003_=d2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZSIsCn0sCnN5c3RlbUJhc2VNUEFUSCA9ICIvb3B0L2J3aHBjL2tpdC9tb2R1bGVmaWxlczovb3B0L2J3aHBjL2NvbW1vbi9tb2R1bGVmaWxlcy9Db3JlIiwKfQo=\nLMOD_ROOT=/opt/lmod\nCONDA_PROMPT_MODIFIER=(base) \nSSH_TTY=/dev/pts/5\nMAIL=/var/spool/mail/qz9211\nMPI_VERSION=4.1.4-gnu-12.1\nMPI_HOME=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1\nSHELL=/bin/bash\nTERM=xterm\nLMOD_SITE_NAME=KIT\n_ModuleTable_Sz_=3\nLMOD_FAMILY_COMPILER=compiler/gnu\nLSDF=/lsdf\nTMOUT=36000\nMPI_BIN_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin\nSHLVL=2\nMANPATH=/opt/bwhpc/common/compiler/gnu/12.1.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man:/home/kit/agw/qz9211/moose-compilers/mpich-3.3/share/man:/opt/lmod/lmod/share/man::\nTEMP=/scratch\nF90=mpif90\nMODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/12.1:/opt/bwhpc/kit/modulefiles:/opt/bwhpc/common/modulefiles/Core\nLOGNAME=qz9211\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/231419/bus\nCLUSTER=uc2\nXDG_RUNTIME_DIR=/run/user/231419\nCPLUS_INCLUDE_PATH=/home/kit/agw/qz9211/moose-compilers/mpich-3.3/include:\nMODULEPATH_modshare=/usr/share/modulefiles:1:/usr/share/Modules/modulefiles:1:/etc/modulefiles:1\nMODULEPATH_ROOT=/opt/modulefiles\nLMOD_PACKAGE_PATH=/etc/lmod/?.lua;;\nPATH=/opt/bwhpc/common/compiler/gnu/12.1.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:/home/kit/agw/qz9211/miniconda3/bin:/home/kit/agw/qz9211/miniconda3/condabin:/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/bin:/home/kit/agw/qz9211/moose-compilers/mpich-3.3/bin:/home/kit/agw/qz9211/moose-compilers/miniconda/bin:/home/kit/agw/qz9211/.local/bin:/home/kit/agw/qz9211/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\nGNU_INC_DIR=/opt/bwhpc/common/compiler/gnu/12.1.0/include\n_LMFILES_=/opt/bwhpc/common/modulefiles/Core/compiler/gnu/12.1.lua:/software/bwhpc/common/modulefiles/Compiler/gnu/12.1/mpi/openmpi/4.1.lua\nMODULESHOME=/opt/lmod/lmod\nLMOD_SETTARG_FULL_SUPPORT=no\nCONDA_DEFAULT_ENV=base\nMPI_EXA_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/examples\nHISTSIZE=1000\nLMOD_PKG=/opt/lmod/lmod\nLMOD_CMD=/opt/lmod/lmod/libexec/lmod\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nOMP_NUM_THREADS=1\nLMOD_DIR=/opt/lmod/lmod/libexec\nBASH_FUNC_module%%=() {  eval $(/opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash $*)\n}\nBASH_FUNC__module_raw%%=() {  unset _mlshdbg;\n if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\n case \"$-\" in \n *v*x*)\n set +vx;\n _mlshdbg='vx'\n ;;\n *v*)\n set +v;\n _mlshdbg='v'\n ;;\n *x*)\n set +x;\n _mlshdbg='x'\n ;;\n *)\n _mlshdbg=''\n ;;\n esac;\n fi;\n unset _mlre _mlIFS;\n if [ -n \"${IFS+x}\" ]; then\n _mlIFS=$IFS;\n fi;\n IFS=' ';\n for _mlv in ${MODULES_RUN_QUARANTINE:-};\n do\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\n if [ -n \"`eval 'echo ${'$_mlv'+x}'`\" ]; then\n _mlre=\"${_mlre:-}${_mlv}_modquar='`eval 'echo ${'$_mlv'}'`' \";\n fi;\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\n _mlre=\"${_mlre:-}${_mlv}='`eval 'echo ${'$_mlrv':-}'`' \";\n fi;\n done;\n if [ -n \"${_mlre:-}\" ]; then\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '\"$@\"'`;\n else\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\n fi;\n _mlstatus=$?;\n if [ -n \"${_mlIFS+x}\" ]; then\n IFS=$_mlIFS;\n else\n unset IFS;\n fi;\n unset _mlre _mlv _mlrv _mlIFS;\n if [ -n \"${_mlshdbg:-}\" ]; then\n set -$_mlshdbg;\n fi;\n unset _mlshdbg;\n return $_mlstatus\n}\nBASH_FUNC_switchml%%=() {  typeset swfound=1;\n if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\n typeset swname='main';\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\n typeset swfound=0;\n unset MODULES_USE_COMPAT_VERSION;\n fi;\n else\n typeset swname='compatibility';\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\n typeset swfound=0;\n MODULES_USE_COMPAT_VERSION=1;\n export MODULES_USE_COMPAT_VERSION;\n fi;\n fi;\n if [ $swfound -eq 0 ]; then\n echo \"Switching to Modules $swname version\";\n source /usr/share/Modules/init/bash;\n else\n echo \"Cannot switch to Modules $swname version, command not found\";\n return 1;\n fi\n}\nBASH_FUNC_ml%%=() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\n}\n_=/usr/bin/env",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283062",
                  "updatedAt": "2022-12-01T14:03:16Z",
                  "publishedAt": "2022-12-01T12:48:47Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Your environment is not sane. I see multiple compilers and MPI wrappers being made available among your PATH:\nPATH=/opt/bwhpc/common/compiler/gnu/12.1.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin/home/kit/agw/qz9211/miniconda3/bin:/home/kit/agw/qz9211/miniconda3/condabin:/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/bin:/home/kit/agw/qz9211/moose-compilers/mpich-3.3/bin:/home/kit/agw/qz9211/moose-compilers/miniconda/bin:/home/kit/agw/qz9211/.local/bin:/home/kit/agw/qz9211/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\n\n(GCC 12 and 10 at: /opt/bwhpc/common/compiler/gnu/12.1.0/bin ,home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/bin\n(OpenMPI and MPICH at /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin, /home/kit/agw/qz9211/moose-compilers/mpich-3.3/bin)\nIt looks like these two compilers are coming from the System and your implementation of Conda. My advice would be to remove the Conda portion and attempt to use your clusters primary OpenMPI compiler.\nAlso, you need to set CC, CXX, FC, F77, F90, otherwise libMesh will build using 'gcc' and not the necessary MPI wrapper 'mpicc':\nexport CC=mpicc CXX=mpicxx FC=mpif90 F77=mpif77 F90=mpif90\nI would expect when loading the following modules:\nCurrently Loaded Modules:\n  1) compiler/gnu/12.1   2) mpi/openmpi/4.1\n\n...that the above CC, CXX exports would be done for you, however they are not. So you'll need to remember to export these variables each time you wish to use this system's MPI wrapper.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283742",
                          "updatedAt": "2022-12-01T14:16:18Z",
                          "publishedAt": "2022-12-01T14:14:08Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "**Hello Milljm,\nThanks for your suggestion.\nAfter I received your email, I started to use the cluster's primary OpenMPI compiler and delete my downloaded compiler.**\nI delete my downloaded folders of miniconda3, moose-compilers, and mpich-3.3. And I change the .bashrc contents.\n**However, I get the same errors after I run  ./scripts/update_and_rebuild_libmesh.sh\nAnd I got the following infromation after I run the diagnostics script in moose/scripts. The deleted olders of miniconda3, moose-compilers, and mpich-3.3 shown again.**\n _**.bashrc contents**_\nif [ -f /etc/bashrc ]; then\n\t. /etc/bashrc\nfi\n\nPATH=\"$HOME/.local/bin:$HOME/bin:$PATH\"\nexport PATH\n\nPACKAGES_DIR=$/opt/bwhpc/common/\n\nexport CC=mpicc\nexport CXX=mpicxx\nexport F90=mpif90\nexport F77=mpif77\nexport FC=mpif90\n\nmodule load compiler/gnu/10.2\nmodule load mpi/openmpi/4.1\n\nexport PATH=$PACKAGES_DIR/compiler/gnu/10.2.0/bin:$PACKAGES_DIR/mpi/openmpi/4.1.4-gnu-10.2/bin:$PATH\nexport LD_LIBRARY_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin:$LD_LIBRARY_PATH\nexport C_INCLUDE_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include:$CPLUS_INCLUDE_PATH\nexport FPATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include:$FPATH\nexport MANPATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/share/man:$MANPATH\n\nMETHOD=dbg make -j 8\n\n\n\n\n_**diagnostics log**_\n\n\n\n\n\n\n\nFri Dec  2 13:25:12 CET 2022\n\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.4 (Ootpa) Release: 8.4 Codename: Ootpa\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 80\n\nMemory Free: 286905.648 MB\n\nVariable `which $CC` check:\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin/mpicc\n\n$CC --version:\ngcc (GCC) 10.2.0\nCopyright (C) 2020 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nMPICC:\nwhich mpicc:\n\t/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin/mpicc\nmpicc -show:\n\tgcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\n\nCOMPILER gcc:\ngcc (GCC) 10.2.0\nCopyright (C) 2020 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n\t/usr/bin/python\n\tPython 3.6.8\n\nMODULES:\n\nCurrently Loaded Modules:\n  1) compiler/gnu/10.2   2) mpi/openmpi/4.1\n\n \n\nPETSC_DIR not set\n\nENVIRONMENT:\nCONDA_SHLVL=1\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36:\nLD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64:$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin\nCONDA_EXE=/home/kit/agw/qz9211/miniconda3/bin/conda\n__LMOD_REF_COUNT_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/bin:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin:1;/home/kit/agw/qz9211/miniconda3/bin:1;/home/kit/agw/qz9211/miniconda3/condabin:1;$/opt/bwhpc/common/compiler/gnu/12.1.0/bin:1;$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:1;/home/kit/agw/qz9211/.local/bin:1;/home/kit/agw/qz9211/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\n_ModuleTable002_=LAp9LApbIm1waS9vcGVubXBpIl0gPSB7CmZuID0gIi9zb2Z0d2FyZS9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29tcGlsZXIvZ251LzEwLjIvbXBpL29wZW5tcGkvNC4xLmx1YSIsCmZ1bGxOYW1lID0gIm1waS9vcGVubXBpLzQuMSIsCmxvYWRPcmRlciA9IDIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAibXBpL29wZW5tcGkvNC4xIiwKd1YgPSAiMDAwMDAwMDA0LjAwMDAwMDAwMS4qemZpbmFsIiwKfSwKfSwKbXBhdGhBID0gewoiL3NvZnR3YXJlL2J3aHBjL2NvbW1vbi9tb2R1bGVmaWxlcy9Db21waWxlci9nbnUvMTAuMiIsICIvb3B0L2J3aHBjL2tpdC9tb2R1bGVmaWxlcyIsICIvb3B0L2J3aHBjL2NvbW1v\nSSH_CONNECTION=2a00:1398:4:7e02:8d30:91d9:4b2a:4edb 53157 2a00:1398:4:1805::810d:3813 22\nMODULES_RUN_QUARANTINE=LD_LIBRARY_PATH LD_PRELOAD\nLANG=en_US.UTF-8\nLMOD_SYSTEM_NAME=uc2\nHISTTIMEFORMAT=%F %T \nGNU_HOME=/opt/bwhpc/common/compiler/gnu/10.2.0\nLMOD_FAMILY_COMPILER_VERSION=10.2\nHOSTNAME=uc2n996.localdomain\nOLDPWD=/home/kit/agw/qz9211\nGNU_MAN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man\nC_INCLUDE_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include:\nMPI_INC_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include\nMPI_MAN_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/share/man\nFPATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include:\nGNU_BIN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/bin\nCONDA_PREFIX=/home/kit/agw/qz9211/miniconda3\n__LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/lib:1;/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64:1;$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:1\nMPIDIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2\nGNU_VERSION=10.2.0\nMPI_LIB_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64\n_CE_M=\nCC=mpicc\nPROJ_LIB=/home/kit/agw/qz9211/miniconda3/share/proj\nXDG_SESSION_ID=13033\nMODULES_CMD=/usr/share/Modules/libexec/modulecmd.tcl\nUSER=qz9211\nGNU_LIB_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib\n__LMOD_REF_COUNT_MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:1;/opt/bwhpc/kit/modulefiles:1;/opt/bwhpc/common/modulefiles/Core:1\nPWD=/home/kit/agw/qz9211/projects/moose\nHOME=/home/kit/agw/qz9211\nTMP=/scratch\nCONDA_PYTHON_EXE=/home/kit/agw/qz9211/miniconda3/bin/python\nSSH_CLIENT=2a00:1398:4:7e02:8d30:91d9:4b2a:4edb 53157 22\nLMOD_VERSION=8.6.16\nLMOD_PAGER=less\nKIT_FAMILY_COMPILER_VERSION=10.2\nF77=mpif77\nXDG_DATA_DIRS=/home/kit/agw/qz9211/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\nKIT_FAMILY_COMPILER=compiler/gnu\n_CE_CONDA=\nMKL_NUM_THREADS=1\nTMPDIR=/scratch\nLMOD_sys=Linux\n_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IDg2NDAwLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJjb21waWxlci9nbnUiLAp9LAptVCA9IHsKWyJjb21waWxlci9nbnUiXSA9IHsKZm4gPSAiL29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZS9jb21waWxlci9nbnUvMTAuMi5sdWEiLApmdWxsTmFtZSA9ICJjb21waWxlci9nbnUvMTAuMiIsCmxvYWRPcmRlciA9IDEsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAxLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29tcGlsZXIvZ251LzEwLjIiLAp3ViA9ICJeMDAwMDAwMTAuMDAwMDAwMDAyLip6ZmluYWwi\nLOADEDMODULES=compiler/gnu/10.2:mpi/openmpi/4.1\nFC=mpif90\n__LMOD_REF_COUNT_MANPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/share/man:1;$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man:1;/opt/lmod/lmod/share/man:1\nSCRATCH=/scratch\n_ModuleTable003_=bi9tb2R1bGVmaWxlcy9Db3JlIiwKfSwKc3lzdGVtQmFzZU1QQVRIID0gIi9vcHQvYndocGMva2l0L21vZHVsZWZpbGVzOi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvcmUiLAp9Cg==\nLMOD_ROOT=/opt/lmod\nCONDA_PROMPT_MODIFIER=(base) \nSSH_TTY=/dev/pts/10\nMAIL=/var/spool/mail/qz9211\nMPI_VERSION=4.1.4-gnu-10.2\nCXX=mpicxx\nMPI_HOME=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2\nSHELL=/bin/bash\nTERM=xterm\nLMOD_SITE_NAME=KIT\n_ModuleTable_Sz_=3\nLMOD_FAMILY_COMPILER=compiler/gnu\nLSDF=/lsdf\nTMOUT=36000\nGNU_LIB64_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib64\nMPI_BIN_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin\nSHLVL=2\nMANPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/share/man:$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man:/opt/lmod/lmod/share/man::\nTEMP=/scratch\nF90=mpif90\nMODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:/opt/bwhpc/kit/modulefiles:/opt/bwhpc/common/modulefiles/Core\nLOGNAME=qz9211\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/231419/bus\nCLUSTER=uc2\nXDG_RUNTIME_DIR=/run/user/231419\nCPLUS_INCLUDE_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include:\nMODULEPATH_modshare=/usr/share/modulefiles:1:/usr/share/Modules/modulefiles:1:/etc/modulefiles:1\nMODULEPATH_ROOT=/opt/modulefiles\nLMOD_PACKAGE_PATH=/etc/lmod/?.lua;;\nPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin:/home/kit/agw/qz9211/miniconda3/bin:/home/kit/agw/qz9211/miniconda3/condabin:$/opt/bwhpc/common/compiler/gnu/12.1.0/bin:$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:/home/kit/agw/qz9211/.local/bin:/home/kit/agw/qz9211/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\nGNU_INC_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/include\n_LMFILES_=/opt/bwhpc/common/modulefiles/Core/compiler/gnu/10.2.lua:/software/bwhpc/common/modulefiles/Compiler/gnu/10.2/mpi/openmpi/4.1.lua\nMODULESHOME=/opt/lmod/lmod\nLMOD_SETTARG_FULL_SUPPORT=no\nCONDA_DEFAULT_ENV=base\nMPI_EXA_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/examples\nHISTSIZE=1000\nLMOD_PKG=/opt/lmod/lmod\nLMOD_CMD=/opt/lmod/lmod/libexec/lmod\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nOMP_NUM_THREADS=1\nLMOD_DIR=/opt/lmod/lmod/libexec\nBASH_FUNC_module%%=() {  eval $(/opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash $*)\n}\nBASH_FUNC__module_raw%%=() {  unset _mlshdbg;\n if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\n case \"$-\" in \n *v*x*)\n set +vx;\n _mlshdbg='vx'\n ;;\n *v*)\n set +v;\n _mlshdbg='v'\n ;;\n *x*)\n set +x;\n _mlshdbg='x'\n ;;\n *)\n _mlshdbg=''\n ;;\n esac;\n fi;\n unset _mlre _mlIFS;\n if [ -n \"${IFS+x}\" ]; then\n _mlIFS=$IFS;\n fi;\n IFS=' ';\n for _mlv in ${MODULES_RUN_QUARANTINE:-};\n do\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\n if [ -n \"`eval 'echo ${'$_mlv'+x}'`\" ]; then\n _mlre=\"${_mlre:-}${_mlv}_modquar='`eval 'echo ${'$_mlv'}'`' \";\n fi;\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\n _mlre=\"${_mlre:-}${_mlv}='`eval 'echo ${'$_mlrv':-}'`' \";\n fi;\n done;\n if [ -n \"${_mlre:-}\" ]; then\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '\"$@\"'`;\n else\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\n fi;\n _mlstatus=$?;\n if [ -n \"${_mlIFS+x}\" ]; then\n IFS=$_mlIFS;\n else\n unset IFS;\n fi;\n unset _mlre _mlv _mlrv _mlIFS;\n if [ -n \"${_mlshdbg:-}\" ]; then\n set -$_mlshdbg;\n fi;\n unset _mlshdbg;\n return $_mlstatus\n}\nBASH_FUNC_switchml%%=() {  typeset swfound=1;\n if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\n typeset swname='main';\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\n typeset swfound=0;\n unset MODULES_USE_COMPAT_VERSION;\n fi;\n else\n typeset swname='compatibility';\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\n typeset swfound=0;\n MODULES_USE_COMPAT_VERSION=1;\n export MODULES_USE_COMPAT_VERSION;\n fi;\n fi;\n if [ $swfound -eq 0 ]; then\n echo \"Switching to Modules $swname version\";\n source /usr/share/Modules/init/bash;\n else\n echo \"Cannot switch to Modules $swname version, command not found\";\n return 1;\n fi\n}\nBASH_FUNC_ml%%=() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\n}\n_=/usr/bin/env",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4292468",
                  "updatedAt": "2022-12-02T13:43:49Z",
                  "publishedAt": "2022-12-02T13:26:10Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "When you made these changes, did you go back and rebuild PETSc? If not, can you perform the following? I want to see how PETSc is being linked, since it too uses MPI in order to build (yet libMesh configure dies early on with MPI issues):\ncd home/projects/moose/petsc/arch-moose/lib\nldd libpetsc.3.16.so\n(I believe libpetsc.3.16.so is the correct filename). If the file does not exist, can you provide what is listed in this location?",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4313580",
                          "updatedAt": "2022-12-05T14:09:45Z",
                          "publishedAt": "2022-12-05T14:09:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "When you made these changes, did you go back and rebuild PETSc? If not, can you perform the following? I want to see how PETSc is being linked, since it too uses MPI in order to build (yet libMesh configure dies early on with MPI issues):\ncd home/projects/moose/petsc/arch-moose/lib\nldd libpetsc.3.16.so\n(I believe libpetsc.3.16.so is the correct filename). If the file does not exist, can you provide what is listed in this location?\n\nI have the libpetsc.so.3.16 in the folder of /projects/moose/petsc/arch-moose/lib.\nAnd I can get the following contents when I run ldd libpetsc.so.3.16.\n    linux-vdso.so.1 (0x00007fff9f9ef000)\n    libHYPRE-2.23.0.so => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libHYPRE-2.23.0.so (0x000015023c2ac000)\n    libstrumpack.so => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libstrumpack.so (0x000015023b7cf000)\n    libsuperlu_dist.so.7 => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libsuperlu_dist.so.7 (0x000015023b4ce000)\n    libhdf5_hl.so.200 => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libhdf5_hl.so.200 (0x000015023b2aa000)\n    libhdf5.so.200 => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libhdf5.so.200 (0x000015023ac2d000)\n    libparmetis.so => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libparmetis.so (0x000015023a9ed000)\n    libmetis.so => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libmetis.so (0x000015023a78a000)\n    libm.so.6 => /usr/lib64/libm.so.6 (0x000015023a408000)\n    libX11.so.6 => /usr/lib64/libX11.so.6 (0x000015023a0c5000)\n    libstdc++.so.6 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libstdc++.so.6 (0x0000150239ccf000)\n    libdl.so.2 => /usr/lib64/libdl.so.2 (0x0000150239acb000)\n    libmpi_usempif08.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libmpi_usempif08.so.40 (0x0000150239893000)\n    libmpi_usempi_ignore_tkr.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libmpi_usempi_ignore_tkr.so.40 (0x0000150239688000)\n    libmpi_mpifh.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libmpi_mpifh.so.40 (0x000015023941e000)\n    libmpi.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libmpi.so.40 (0x0000150238f4d000)\n    libgfortran.so.5 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libgfortran.so.5 (0x0000150238a82000)\n    libgcc_s.so.1 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libgcc_s.so.1 (0x0000150238869000)\n    libquadmath.so.0 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libquadmath.so.0 (0x0000150238621000)\n    libpthread.so.0 => /usr/lib64/libpthread.so.0 (0x0000150238401000)\n    librt.so.1 => /usr/lib64/librt.so.1 (0x00001502381f9000)\n    libgomp.so.1 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libgomp.so.1 (0x0000150237fb8000)\n    libc.so.6 => /usr/lib64/libc.so.6 (0x0000150237bf3000)\n    /lib64/ld-linux-x86-64.so.2 (0x000015023e25c000)\n    libxcb.so.1 => /lib64/libxcb.so.1 (0x00001502379ca000)\n    liblustreapi.so.1 => /lib64/liblustreapi.so.1 (0x0000150237798000)\n    libgpfs.so => /lib64/libgpfs.so (0x0000150237582000)\n    libopen-rte.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-rte.so.40 (0x0000150237261000)\n    libopen-pal.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-pal.so.40 (0x0000150236dae000)\n    libucp.so.0 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libucp.so.0 (0x0000150236a82000)\n    libuct.so.0 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libuct.so.0 (0x000015023683d000)\n    libucm.so.0 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libucm.so.0 (0x0000150236622000)\n    libucs.so.0 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libucs.so.0 (0x000015023627e000)\n    libfabric.so.1 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libfabric.so.1 (0x00001502357d3000)\n    librdmacm.so.1 => /lib64/librdmacm.so.1 (0x00001502355b8000)\n    libibverbs.so.1 => /lib64/libibverbs.so.1 (0x0000150235398000)\n    libpmi2.so.0 => /lib64/libpmi2.so.0 (0x0000150235180000)\n    libpmi.so.0 => /lib64/libpmi.so.0 (0x0000150234f7a000)\n    libutil.so.1 => /lib64/libutil.so.1 (0x0000150234d76000)\n    libz.so.1 => /lib64/libz.so.1 (0x0000150234b5e000)\n    libhwloc.so.15 => /lib64/libhwloc.so.15 (0x000015023490e000)\n    libevent_core-2.1.so.6 => /lib64/libevent_core-2.1.so.6 (0x00001502346d5000)\n    libevent_pthreads-2.1.so.6 => /lib64/libevent_pthreads-2.1.so.6 (0x00001502344d2000)\n    libXau.so.6 => /lib64/libXau.so.6 (0x00001502342ce000)\n    libreadline.so.7 => /lib64/libreadline.so.7 (0x000015023407f000)\n    libnuma.so.1 => /lib64/libnuma.so.1 (0x0000150233e73000)\n    libuuid.so.1 => /lib64/libuuid.so.1 (0x0000150233c6b000)\n    libatomic.so.1 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libatomic.so.1 (0x0000150233a63000)\n    libnl-3.so.200 => /lib64/libnl-3.so.200 (0x0000150233840000)\n    libnl-route-3.so.200 => /lib64/libnl-route-3.so.200 (0x00001502335ba000)\n    libresolv.so.2 => /lib64/libresolv.so.2 (0x00001502333a3000)\n    libslurm_pmi.so => /usr/lib64/slurm/libslurm_pmi.so (0x0000150232fc8000)\n    libcrypto.so.1.1 => /lib64/libcrypto.so.1.1 (0x0000150232ae2000)\n    libtinfo.so.6 => /lib64/libtinfo.so.6 (0x00001502328b5000)",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4313683",
                          "updatedAt": "2022-12-05T14:22:34Z",
                          "publishedAt": "2022-12-05T14:22:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "Hello, @roystgnr , I get the following information after I run  make V=1 in the  projects/moose/petsc/ folder.\n==========================================\n \nSee documentation/faq.html and documentation/bugreporting.html\nfor help with installation problems.  Please send EVERYTHING\nprinted out below when reporting problems.  Please check the\nmailing list archives and consider subscribing.\n \n  https://petsc.org/release/community/mailing/\n \n==========================================\nStarting make run on uc2n994.localdomain at Tue, 06 Dec 2022 09:50:34 +0100\nMachine characteristics: Linux uc2n994.localdomain 4.18.0-305.65.1.el8_4.x86_64 #1 SMP Thu Sep 22 08:28:21 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------\nUsing PETSc directory: /home/kit/agw/qz9211/projects/moose/petsc\nUsing PETSc arch: arch-moose\n-----------------------------------------\nPETSC_VERSION_RELEASE    1\nPETSC_VERSION_MAJOR      3\nPETSC_VERSION_MINOR      16\nPETSC_VERSION_SUBMINOR   6\nPETSC_VERSION_PATCH      0\nPETSC_VERSION_DATE       \"unknown\"\nPETSC_VERSION_GIT        \"unknown\"\nPETSC_VERSION_DATE_GIT   \"unknown\"\nPETSC_VERSION_EQ(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_ PETSC_VERSION_EQ\nPETSC_VERSION_LT(MAJOR,MINOR,SUBMINOR)          \\\nPETSC_VERSION_LE(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_GT(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_GE(MAJOR,MINOR,SUBMINOR) \\\n-----------------------------------------\nUsing configure Options: --download-hypre=1 --with-shared-libraries=1 --download-hdf5=1 --download-hdf5-fortran-bindings=0   --with-debugging=no --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-strumpack=1 --download-scalapack=1 --download-slepc=1 --with-mpi=1 --with-openmp=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0 --with-64-bit-indices \nUsing configuration flags:\n#define INCLUDED_PETSCCONF_H\n#define PETSC_ARCH \"arch-moose\"\n#define PETSC_ATTRIBUTEALIGNED(size) __attribute((aligned(size)))\n#define PETSC_Alignx(a,b)   \n#define PETSC_BLASLAPACK_UNDERSCORE 1\n#define PETSC_CLANGUAGE_C 1\n#define PETSC_CXX_INLINE inline\n#define PETSC_CXX_RESTRICT __restrict\n#define PETSC_C_INLINE inline\n#define PETSC_C_RESTRICT __restrict\n#define PETSC_DEPRECATED_ENUM(why) __attribute((deprecated))\n#define PETSC_DEPRECATED_FUNCTION(why) __attribute((deprecated))\n#define PETSC_DEPRECATED_MACRO(why) _Pragma(why)\n#define PETSC_DEPRECATED_TYPEDEF(why) __attribute((deprecated))\n#define PETSC_DIR \"/home/kit/agw/qz9211/projects/moose/petsc\"\n#define PETSC_DIR_SEPARATOR '/'\n#define PETSC_FORTRAN_CHARLEN_T size_t\n#define PETSC_FORTRAN_TYPE_INITIALIZE  = -2\n#define PETSC_FUNCTION_NAME_C __func__\n#define PETSC_FUNCTION_NAME_CXX __func__\n#define PETSC_HAVE_ACCESS 1\n#define PETSC_HAVE_ATOLL 1\n#define PETSC_HAVE_ATTRIBUTEALIGNED 1\n#define PETSC_HAVE_BUILTIN_EXPECT 1\n#define PETSC_HAVE_BZERO 1\n#define PETSC_HAVE_C99_COMPLEX 1\n#define PETSC_HAVE_CLOCK 1\n#define PETSC_HAVE_CXX 1\n#define PETSC_HAVE_CXX_COMPLEX 1\n#define PETSC_HAVE_CXX_COMPLEX_FIX 1\n#define PETSC_HAVE_CXX_DIALECT_CXX03 1\n#define PETSC_HAVE_CXX_DIALECT_CXX11 1\n#define PETSC_HAVE_DLADDR 1\n#define PETSC_HAVE_DLCLOSE 1\n#define PETSC_HAVE_DLERROR 1\n#define PETSC_HAVE_DLFCN_H 1\n#define PETSC_HAVE_DLOPEN 1\n#define PETSC_HAVE_DLSYM 1\n#define PETSC_HAVE_DOUBLE_ALIGN_MALLOC 1\n#define PETSC_HAVE_DRAND48 1\n#define PETSC_HAVE_DYNAMIC_LIBRARIES 1\n#define PETSC_HAVE_ERF 1\n#define PETSC_HAVE_FBLASLAPACK 1\n#define PETSC_HAVE_FCNTL_H 1\n#define PETSC_HAVE_FENV_H 1\n#define PETSC_HAVE_FLOAT_H 1\n#define PETSC_HAVE_FORK 1\n#define PETSC_HAVE_FORTRAN_FLUSH 1\n#define PETSC_HAVE_FORTRAN_GET_COMMAND_ARGUMENT 1\n#define PETSC_HAVE_FORTRAN_TYPE_STAR 1\n#define PETSC_HAVE_FORTRAN_UNDERSCORE 1\n#define PETSC_HAVE_GETCWD 1\n#define PETSC_HAVE_GETDOMAINNAME 1\n#define PETSC_HAVE_GETHOSTBYNAME 1\n#define PETSC_HAVE_GETHOSTNAME 1\n#define PETSC_HAVE_GETPAGESIZE 1\n#define PETSC_HAVE_GETRUSAGE 1\n#define PETSC_HAVE_HDF5 1\n#define PETSC_HAVE_HYPRE 1\n#define PETSC_HAVE_IMMINTRIN_H 1\n#define PETSC_HAVE_INTTYPES_H 1\n#define PETSC_HAVE_ISINF 1\n#define PETSC_HAVE_ISNAN 1\n#define PETSC_HAVE_ISNORMAL 1\n#define PETSC_HAVE_LGAMMA 1\n#define PETSC_HAVE_LOG2 1\n#define PETSC_HAVE_LSEEK 1\n#define PETSC_HAVE_MALLOC_H 1\n#define PETSC_HAVE_MEMALIGN 1\n#define PETSC_HAVE_MEMMOVE 1\n#define PETSC_HAVE_METIS 1\n#define PETSC_HAVE_MMAP 1\n#define PETSC_HAVE_MPIEXEC_ENVIRONMENTAL_VARIABLE OMP\n#define PETSC_HAVE_MPIIO 1\n#define PETSC_HAVE_MPI_COMBINER_CONTIGUOUS 1\n#define PETSC_HAVE_MPI_COMBINER_DUP 1\n#define PETSC_HAVE_MPI_COMBINER_NAMED 1\n#define PETSC_HAVE_MPI_EXSCAN 1\n#define PETSC_HAVE_MPI_F90MODULE 1\n#define PETSC_HAVE_MPI_F90MODULE_VISIBILITY 1\n#define PETSC_HAVE_MPI_FEATURE_DYNAMIC_WINDOW 1\n#define PETSC_HAVE_MPI_FINALIZED 1\n#define PETSC_HAVE_MPI_GET_ACCUMULATE 1\n#define PETSC_HAVE_MPI_GET_LIBRARY_VERSION 1\n#define PETSC_HAVE_MPI_GPU_AWARE 1\n#define PETSC_HAVE_MPI_IALLREDUCE 1\n#define PETSC_HAVE_MPI_IBARRIER 1\n#define PETSC_HAVE_MPI_INIT_THREAD 1\n#define PETSC_HAVE_MPI_INT64_T 1\n#define PETSC_HAVE_MPI_IN_PLACE 1\n#define PETSC_HAVE_MPI_LONG_DOUBLE 1\n#define PETSC_HAVE_MPI_NEIGHBORHOOD_COLLECTIVES 1\n#define PETSC_HAVE_MPI_NONBLOCKING_COLLECTIVES 1\n#define PETSC_HAVE_MPI_ONE_SIDED 1\n#define PETSC_HAVE_MPI_PROCESS_SHARED_MEMORY 1\n#define PETSC_HAVE_MPI_REDUCE_LOCAL 1\n#define PETSC_HAVE_MPI_REDUCE_SCATTER 1\n#define PETSC_HAVE_MPI_REDUCE_SCATTER_BLOCK 1\n#define PETSC_HAVE_MPI_RGET 1\n#define PETSC_HAVE_MPI_TYPE_DUP 1\n#define PETSC_HAVE_MPI_TYPE_GET_ENVELOPE 1\n#define PETSC_HAVE_MPI_WIN_CREATE 1\n#define PETSC_HAVE_MUMPS 1\n#define PETSC_HAVE_NANOSLEEP 1\n#define PETSC_HAVE_NETDB_H 1\n#define PETSC_HAVE_NETINET_IN_H 1\n#define PETSC_HAVE_OMPI_MAJOR_VERSION 4\n#define PETSC_HAVE_OMPI_MINOR_VERSION 1\n#define PETSC_HAVE_OMPI_RELEASE_VERSION 4\n#define PETSC_HAVE_OPENMP 1\n#define PETSC_HAVE_PACKAGES \":blaslapack:fblaslapack:hdf5:hypre:mathlib:metis:mpi:mumps:openmp:parmetis:pthread:ptscotch:regex:scalapack:strumpack:superlu_dist:x11:\"\n#define PETSC_HAVE_PARMETIS 1\n#define PETSC_HAVE_POPEN 1\n#define PETSC_HAVE_PTHREAD 1\n#define PETSC_HAVE_PTHREAD_BARRIER_T 1\n#define PETSC_HAVE_PTHREAD_H 1\n#define PETSC_HAVE_PTSCOTCH 1\n#define PETSC_HAVE_PWD_H 1\n#define PETSC_HAVE_RAND 1\n#define PETSC_HAVE_READLINK 1\n#define PETSC_HAVE_REALPATH 1\n#define PETSC_HAVE_REAL___FLOAT128 1\n#define PETSC_HAVE_REGEX 1\n#define PETSC_HAVE_RTLD_GLOBAL 1\n#define PETSC_HAVE_RTLD_LAZY 1\n#define PETSC_HAVE_RTLD_LOCAL 1\n#define PETSC_HAVE_RTLD_NOW 1\n#define PETSC_HAVE_SCALAPACK 1\n#define PETSC_HAVE_SCHED_CPU_SET_T 1\n#define PETSC_HAVE_SCOTCH_PARMETIS_V3_NODEND 1\n#define PETSC_HAVE_SETJMP_H 1\n#define PETSC_HAVE_SLEEP 1\n#define PETSC_HAVE_SLEPC 1\n#define PETSC_HAVE_SNPRINTF 1\n#define PETSC_HAVE_SOCKET 1\n#define PETSC_HAVE_SO_REUSEADDR 1\n#define PETSC_HAVE_STDINT_H 1\n#define PETSC_HAVE_STRCASECMP 1\n#define PETSC_HAVE_STRINGS_H 1\n#define PETSC_HAVE_STRUCT_SIGACTION 1\n#define PETSC_HAVE_STRUMPACK 1\n#define PETSC_HAVE_SUPERLU_DIST 1\n#define PETSC_HAVE_SYSINFO 1\n#define PETSC_HAVE_SYS_PARAM_H 1\n#define PETSC_HAVE_SYS_PROCFS_H 1\n#define PETSC_HAVE_SYS_RESOURCE_H 1\n#define PETSC_HAVE_SYS_SOCKET_H 1\n#define PETSC_HAVE_SYS_SYSCTL_H 1\n#define PETSC_HAVE_SYS_SYSINFO_H 1\n#define PETSC_HAVE_SYS_TIMES_H 1\n#define PETSC_HAVE_SYS_TIME_H 1\n#define PETSC_HAVE_SYS_TYPES_H 1\n#define PETSC_HAVE_SYS_UTSNAME_H 1\n#define PETSC_HAVE_SYS_WAIT_H 1\n#define PETSC_HAVE_TGAMMA 1\n#define PETSC_HAVE_TIME 1\n#define PETSC_HAVE_TIME_H 1\n#define PETSC_HAVE_UNAME 1\n#define PETSC_HAVE_UNISTD_H 1\n#define PETSC_HAVE_USLEEP 1\n#define PETSC_HAVE_VA_COPY 1\n#define PETSC_HAVE_VSNPRINTF 1\n#define PETSC_HAVE_X 1\n#define PETSC_HAVE_XMMINTRIN_H 1\n#define PETSC_HDF5_HAVE_PARALLEL 1\n#define PETSC_IS_COLORING_MAX USHRT_MAX\n#define PETSC_IS_COLORING_VALUE_TYPE short\n#define PETSC_IS_COLORING_VALUE_TYPE_F integer2\n#define PETSC_LEVEL1_DCACHE_LINESIZE 64\n#define PETSC_LIB_DIR \"/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib\"\n#define PETSC_MAX_PATH_LEN 4096\n#define PETSC_MEMALIGN 16\n#define PETSC_MPICC_SHOW \"gcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\"\n#define PETSC_MPIU_IS_COLORING_VALUE_TYPE MPI_UNSIGNED_SHORT\n#define PETSC_PREFETCH_HINT_NTA _MM_HINT_NTA\n#define PETSC_PREFETCH_HINT_T0 _MM_HINT_T0\n#define PETSC_PREFETCH_HINT_T1 _MM_HINT_T1\n#define PETSC_PREFETCH_HINT_T2 _MM_HINT_T2\n#define PETSC_PYTHON_EXE \"/usr/bin/python\"\n#define PETSC_Prefetch(a,b,c) _mm_prefetch((const char*)(a),(c))\n#define PETSC_REPLACE_DIR_SEPARATOR '\\\\'\n#define PETSC_SIGNAL_CAST  \n#define PETSC_SIZEOF_ENUM 4\n#define PETSC_SIZEOF_INT 4\n#define PETSC_SIZEOF_LONG 8\n#define PETSC_SIZEOF_LONG_LONG 8\n#define PETSC_SIZEOF_SHORT 2\n#define PETSC_SIZEOF_SIZE_T 8\n#define PETSC_SIZEOF_VOID_P 8\n#define PETSC_SLSUFFIX \"so\"\n#define PETSC_UINTPTR_T uintptr_t\n#define PETSC_UNUSED __attribute((unused))\n#define PETSC_USE_64BIT_INDICES 1\n#define PETSC_USE_AVX512_KERNELS 1\n#define PETSC_USE_BACKWARD_LOOP 1\n#define PETSC_USE_CTABLE 1\n#define PETSC_USE_DEBUGGER \"gdb\"\n#define PETSC_USE_INFO 1\n#define PETSC_USE_ISATTY 1\n#define PETSC_USE_LOG 1\n#define PETSC_USE_MALLOC_COALESCED 1\n#define PETSC_USE_PROC_FOR_SIZE 1\n#define PETSC_USE_REAL_DOUBLE 1\n#define PETSC_USE_SHARED_LIBRARIES 1\n#define PETSC_USE_SINGLE_LIBRARY 1\n#define PETSC_USE_SOCKET_VIEWER 1\n#define PETSC_USE_VISIBILITY_C 1\n#define PETSC_USE_VISIBILITY_CXX 1\n#define PETSC_USING_64BIT_PTR 1\n#define PETSC_USING_F2003 1\n#define PETSC_USING_F90FREEFORM 1\n#define PETSC_VERSION_BRANCH_GIT \"HEAD\"\n#define PETSC_VERSION_DATE_GIT \"2022-07-19 17:19:37 -0500\"\n#define PETSC_VERSION_GIT \"v3.16.6-1-g477e44bbb55\"\n#define PETSC__BSD_SOURCE 1\n#define PETSC__DEFAULT_SOURCE 1\n#define PETSC__GNU_SOURCE 1\n-----------------------------------------\nUsing C compile: mpicc -o .o -c -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp    \nmpicc -show: gcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\nC compiler version: gcc (GCC) 10.2.0\nUsing C++ compile: mpicxx -o .o -c -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp  -fPIC -std=gnu++11  -fopenmp  -I/home/kit/agw/qz9211/projects/moose/petsc/include -I/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/include  -fopenmp \nmpicxx -show: g++ -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\nC++ compiler version: g++ (GCC) 10.2.0\nUsing Fortran compile: mpif90 -o .o -c -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O  -fopenmp   -I/home/kit/agw/qz9211/projects/moose/petsc/include -I/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/include   \nmpif90 -show: gfortran -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi\nFortran compiler version: GNU Fortran (GCC) 10.2.0\n-----------------------------------------\nUsing C/C++ linker: mpicc\nUsing C/C++ flags: -fopenmp -fopenmp   -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp\nUsing Fortran linker: mpif90\nUsing Fortran flags: -fopenmp -fopenmp   -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O  -fopenmp\n-----------------------------------------\nUsing system modules: compiler/gnu/10.2:mpi/openmpi/4.1\nUsing mpi.h: # 1 \"/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include/mpi.h\" 1\n-----------------------------------------\nUsing libraries: -Wl,-rpath,/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -L/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -Wl,-rpath,/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -L/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -Wl,-rpath,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0 -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0 -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib64 -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib64 -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib -lpetsc -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lstrumpack -lscalapack -lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lhdf5_hl -lhdf5 -lparmetis -lmetis -lm -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lrt -lquadmath -lstdc++ -ldl\n------------------------------------------\nUsing mpiexec: mpiexec --oversubscribe\n------------------------------------------\nUsing MAKE: /usr/bin/gmake\nUsing MAKEFLAGS: -j44 -l113.6  --no-print-directory -- PETSC_DIR=/home/kit/agw/qz9211/projects/moose/petsc PETSC_ARCH=arch-moose V=1\n==========================================\ngmake[3]: Nothing to be done for 'libs'.\n*** Building SLEPc ***\nChecking environment... \nCleaning arch dir /pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose...\ndone\nChecking PETSc installation... \nxxx==========================================================================xxx\nWARNING: Your PETSc and SLEPc repos may not be in sync (more than 30 days apart)\nxxx==========================================================================xxx\ndone\nChecking LAPACK library... done\nChecking SCALAPACK... done\nWriting various configuration files... done\n \n================================================================================\nSLEPc Configuration\n================================================================================\n\nSLEPc directory:\n  /pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc\n  It is a git repository on branch: tags/v3.16.2^0\nSLEPc prefix directory:\n  /pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose\nPETSc directory:\n  /home/kit/agw/qz9211/projects/moose/petsc\n  It is a git repository on branch: tags/v3.17.4~4^2\nArchitecture \"arch-moose\" with double precision real numbers\nSCALAPACK from SCALAPACK linked by PETSc\n\nxxx==========================================================================xxx\n Configure stage complete. Now build the SLEPc library with:\n   make SLEPC_DIR=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc PETSC_DIR=/home/kit/agw/qz9211/projects/moose/petsc PETSC_ARCH=arch-moose\nxxx==========================================================================xxx\n\n==========================================\nStarting make run on uc2n994.localdomain at Tue, 06 Dec 2022 09:51:10 +0100\nMachine characteristics: Linux uc2n994.localdomain 4.18.0-305.65.1.el8_4.x86_64 #1 SMP Thu Sep 22 08:28:21 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------\nUsing SLEPc directory: /pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc\nUsing PETSc directory: /home/kit/agw/qz9211/projects/moose/petsc\nUsing PETSc arch: arch-moose\n-----------------------------------------\nSLEPC_VERSION_RELEASE    1\nSLEPC_VERSION_MAJOR      3\nSLEPC_VERSION_MINOR      16\nSLEPC_VERSION_SUBMINOR   2\nSLEPC_VERSION_PATCH      0\nSLEPC_VERSION_DATE       \"unknown\"\nSLEPC_VERSION_GIT        \"unknown\"\nSLEPC_VERSION_DATE_GIT   \"unknown\"\nSLEPC_VERSION_EQ(MAJOR,MINOR,SUBMINOR) \\\nSLEPC_VERSION_ SLEPC_VERSION_EQ\nSLEPC_VERSION_LT(MAJOR,MINOR,SUBMINOR)          \\\nSLEPC_VERSION_LE(MAJOR,MINOR,SUBMINOR) \\\nSLEPC_VERSION_GT(MAJOR,MINOR,SUBMINOR) \\\nSLEPC_VERSION_GE(MAJOR,MINOR,SUBMINOR) \\\n-----------------------------------------\nUsing SLEPc configure options: --with-clean=1 --prefix=/home/kit/agw/qz9211/projects/moose/petsc/arch-moose\nUsing SLEPc configuration flags:\n#define INCLUDED_SLEPCCONF_H\n#define SLEPC_PETSC_DIR \"/home/kit/agw/qz9211/projects/moose/petsc\"\n#define SLEPC_PETSC_ARCH \"arch-moose\"\n#define SLEPC_DIR \"/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc\"\n#define SLEPC_LIB_DIR \"/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib\"\n#define SLEPC_VERSION_GIT \"v3.16.2\"\n#define SLEPC_VERSION_DATE_GIT \"2022-02-01 11:25:32 +0100\"\n#define SLEPC_VERSION_BRANCH_GIT \"tags/v3.16.2^0\"\n#define SLEPC_MISSING_LAPACK_GGSVD3 1\n#define SLEPC_HAVE_SCALAPACK 1\n#define SLEPC_SCALAPACK_HAVE_UNDERSCORE 1\n#define SLEPC_HAVE_PACKAGES \":scalapack:\"\n-----------------------------------------\nPETSC_VERSION_RELEASE    1\nPETSC_VERSION_MAJOR      3\nPETSC_VERSION_MINOR      16\nPETSC_VERSION_SUBMINOR   6\nPETSC_VERSION_PATCH      0\nPETSC_VERSION_DATE       \"unknown\"\nPETSC_VERSION_GIT        \"unknown\"\nPETSC_VERSION_DATE_GIT   \"unknown\"\nPETSC_VERSION_EQ(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_ PETSC_VERSION_EQ\nPETSC_VERSION_LT(MAJOR,MINOR,SUBMINOR)          \\\nPETSC_VERSION_LE(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_GT(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_GE(MAJOR,MINOR,SUBMINOR) \\\n-----------------------------------------\nUsing PETSc configure options: --download-hypre=1 --with-shared-libraries=1 --download-hdf5=1 --download-hdf5-fortran-bindings=0   --with-debugging=no --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-strumpack=1 --download-scalapack=1 --download-slepc=1 --with-mpi=1 --with-openmp=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0 --with-64-bit-indices \nUsing PETSc configuration flags:\n#define INCLUDED_PETSCCONF_H\n#define PETSC_ARCH \"arch-moose\"\n#define PETSC_ATTRIBUTEALIGNED(size) __attribute((aligned(size)))\n#define PETSC_Alignx(a,b)   \n#define PETSC_BLASLAPACK_UNDERSCORE 1\n#define PETSC_CLANGUAGE_C 1\n#define PETSC_CXX_INLINE inline\n#define PETSC_CXX_RESTRICT __restrict\n#define PETSC_C_INLINE inline\n#define PETSC_C_RESTRICT __restrict\n#define PETSC_DEPRECATED_ENUM(why) __attribute((deprecated))\n#define PETSC_DEPRECATED_FUNCTION(why) __attribute((deprecated))\n#define PETSC_DEPRECATED_MACRO(why) _Pragma(why)\n#define PETSC_DEPRECATED_TYPEDEF(why) __attribute((deprecated))\n#define PETSC_DIR \"/home/kit/agw/qz9211/projects/moose/petsc\"\n#define PETSC_DIR_SEPARATOR '/'\n#define PETSC_FORTRAN_CHARLEN_T size_t\n#define PETSC_FORTRAN_TYPE_INITIALIZE  = -2\n#define PETSC_FUNCTION_NAME_C __func__\n#define PETSC_FUNCTION_NAME_CXX __func__\n#define PETSC_HAVE_ACCESS 1\n#define PETSC_HAVE_ATOLL 1\n#define PETSC_HAVE_ATTRIBUTEALIGNED 1\n#define PETSC_HAVE_BUILTIN_EXPECT 1\n#define PETSC_HAVE_BZERO 1\n#define PETSC_HAVE_C99_COMPLEX 1\n#define PETSC_HAVE_CLOCK 1\n#define PETSC_HAVE_CXX 1\n#define PETSC_HAVE_CXX_COMPLEX 1\n#define PETSC_HAVE_CXX_COMPLEX_FIX 1\n#define PETSC_HAVE_CXX_DIALECT_CXX03 1\n#define PETSC_HAVE_CXX_DIALECT_CXX11 1\n#define PETSC_HAVE_DLADDR 1\n#define PETSC_HAVE_DLCLOSE 1\n#define PETSC_HAVE_DLERROR 1\n#define PETSC_HAVE_DLFCN_H 1\n#define PETSC_HAVE_DLOPEN 1\n#define PETSC_HAVE_DLSYM 1\n#define PETSC_HAVE_DOUBLE_ALIGN_MALLOC 1\n#define PETSC_HAVE_DRAND48 1\n#define PETSC_HAVE_DYNAMIC_LIBRARIES 1\n#define PETSC_HAVE_ERF 1\n#define PETSC_HAVE_FBLASLAPACK 1\n#define PETSC_HAVE_FCNTL_H 1\n#define PETSC_HAVE_FENV_H 1\n#define PETSC_HAVE_FLOAT_H 1\n#define PETSC_HAVE_FORK 1\n#define PETSC_HAVE_FORTRAN_FLUSH 1\n#define PETSC_HAVE_FORTRAN_GET_COMMAND_ARGUMENT 1\n#define PETSC_HAVE_FORTRAN_TYPE_STAR 1\n#define PETSC_HAVE_FORTRAN_UNDERSCORE 1\n#define PETSC_HAVE_GETCWD 1\n#define PETSC_HAVE_GETDOMAINNAME 1\n#define PETSC_HAVE_GETHOSTBYNAME 1\n#define PETSC_HAVE_GETHOSTNAME 1\n#define PETSC_HAVE_GETPAGESIZE 1\n#define PETSC_HAVE_GETRUSAGE 1\n#define PETSC_HAVE_HDF5 1\n#define PETSC_HAVE_HYPRE 1\n#define PETSC_HAVE_IMMINTRIN_H 1\n#define PETSC_HAVE_INTTYPES_H 1\n#define PETSC_HAVE_ISINF 1\n#define PETSC_HAVE_ISNAN 1\n#define PETSC_HAVE_ISNORMAL 1\n#define PETSC_HAVE_LGAMMA 1\n#define PETSC_HAVE_LOG2 1\n#define PETSC_HAVE_LSEEK 1\n#define PETSC_HAVE_MALLOC_H 1\n#define PETSC_HAVE_MEMALIGN 1\n#define PETSC_HAVE_MEMMOVE 1\n#define PETSC_HAVE_METIS 1\n#define PETSC_HAVE_MMAP 1\n#define PETSC_HAVE_MPIEXEC_ENVIRONMENTAL_VARIABLE OMP\n#define PETSC_HAVE_MPIIO 1\n#define PETSC_HAVE_MPI_COMBINER_CONTIGUOUS 1\n#define PETSC_HAVE_MPI_COMBINER_DUP 1\n#define PETSC_HAVE_MPI_COMBINER_NAMED 1\n#define PETSC_HAVE_MPI_EXSCAN 1\n#define PETSC_HAVE_MPI_F90MODULE 1\n#define PETSC_HAVE_MPI_F90MODULE_VISIBILITY 1\n#define PETSC_HAVE_MPI_FEATURE_DYNAMIC_WINDOW 1\n#define PETSC_HAVE_MPI_FINALIZED 1\n#define PETSC_HAVE_MPI_GET_ACCUMULATE 1\n#define PETSC_HAVE_MPI_GET_LIBRARY_VERSION 1\n#define PETSC_HAVE_MPI_GPU_AWARE 1\n#define PETSC_HAVE_MPI_IALLREDUCE 1\n#define PETSC_HAVE_MPI_IBARRIER 1\n#define PETSC_HAVE_MPI_INIT_THREAD 1\n#define PETSC_HAVE_MPI_INT64_T 1\n#define PETSC_HAVE_MPI_IN_PLACE 1\n#define PETSC_HAVE_MPI_LONG_DOUBLE 1\n#define PETSC_HAVE_MPI_NEIGHBORHOOD_COLLECTIVES 1\n#define PETSC_HAVE_MPI_NONBLOCKING_COLLECTIVES 1\n#define PETSC_HAVE_MPI_ONE_SIDED 1\n#define PETSC_HAVE_MPI_PROCESS_SHARED_MEMORY 1\n#define PETSC_HAVE_MPI_REDUCE_LOCAL 1\n#define PETSC_HAVE_MPI_REDUCE_SCATTER 1\n#define PETSC_HAVE_MPI_REDUCE_SCATTER_BLOCK 1\n#define PETSC_HAVE_MPI_RGET 1\n#define PETSC_HAVE_MPI_TYPE_DUP 1\n#define PETSC_HAVE_MPI_TYPE_GET_ENVELOPE 1\n#define PETSC_HAVE_MPI_WIN_CREATE 1\n#define PETSC_HAVE_MUMPS 1\n#define PETSC_HAVE_NANOSLEEP 1\n#define PETSC_HAVE_NETDB_H 1\n#define PETSC_HAVE_NETINET_IN_H 1\n#define PETSC_HAVE_OMPI_MAJOR_VERSION 4\n#define PETSC_HAVE_OMPI_MINOR_VERSION 1\n#define PETSC_HAVE_OMPI_RELEASE_VERSION 4\n#define PETSC_HAVE_OPENMP 1\n#define PETSC_HAVE_PACKAGES \":blaslapack:fblaslapack:hdf5:hypre:mathlib:metis:mpi:mumps:openmp:parmetis:pthread:ptscotch:regex:scalapack:strumpack:superlu_dist:x11:\"\n#define PETSC_HAVE_PARMETIS 1\n#define PETSC_HAVE_POPEN 1\n#define PETSC_HAVE_PTHREAD 1\n#define PETSC_HAVE_PTHREAD_BARRIER_T 1\n#define PETSC_HAVE_PTHREAD_H 1\n#define PETSC_HAVE_PTSCOTCH 1\n#define PETSC_HAVE_PWD_H 1\n#define PETSC_HAVE_RAND 1\n#define PETSC_HAVE_READLINK 1\n#define PETSC_HAVE_REALPATH 1\n#define PETSC_HAVE_REAL___FLOAT128 1\n#define PETSC_HAVE_REGEX 1\n#define PETSC_HAVE_RTLD_GLOBAL 1\n#define PETSC_HAVE_RTLD_LAZY 1\n#define PETSC_HAVE_RTLD_LOCAL 1\n#define PETSC_HAVE_RTLD_NOW 1\n#define PETSC_HAVE_SCALAPACK 1\n#define PETSC_HAVE_SCHED_CPU_SET_T 1\n#define PETSC_HAVE_SCOTCH_PARMETIS_V3_NODEND 1\n#define PETSC_HAVE_SETJMP_H 1\n#define PETSC_HAVE_SLEEP 1\n#define PETSC_HAVE_SLEPC 1\n#define PETSC_HAVE_SNPRINTF 1\n#define PETSC_HAVE_SOCKET 1\n#define PETSC_HAVE_SO_REUSEADDR 1\n#define PETSC_HAVE_STDINT_H 1\n#define PETSC_HAVE_STRCASECMP 1\n#define PETSC_HAVE_STRINGS_H 1\n#define PETSC_HAVE_STRUCT_SIGACTION 1\n#define PETSC_HAVE_STRUMPACK 1\n#define PETSC_HAVE_SUPERLU_DIST 1\n#define PETSC_HAVE_SYSINFO 1\n#define PETSC_HAVE_SYS_PARAM_H 1\n#define PETSC_HAVE_SYS_PROCFS_H 1\n#define PETSC_HAVE_SYS_RESOURCE_H 1\n#define PETSC_HAVE_SYS_SOCKET_H 1\n#define PETSC_HAVE_SYS_SYSCTL_H 1\n#define PETSC_HAVE_SYS_SYSINFO_H 1\n#define PETSC_HAVE_SYS_TIMES_H 1\n#define PETSC_HAVE_SYS_TIME_H 1\n#define PETSC_HAVE_SYS_TYPES_H 1\n#define PETSC_HAVE_SYS_UTSNAME_H 1\n#define PETSC_HAVE_SYS_WAIT_H 1\n#define PETSC_HAVE_TGAMMA 1\n#define PETSC_HAVE_TIME 1\n#define PETSC_HAVE_TIME_H 1\n#define PETSC_HAVE_UNAME 1\n#define PETSC_HAVE_UNISTD_H 1\n#define PETSC_HAVE_USLEEP 1\n#define PETSC_HAVE_VA_COPY 1\n#define PETSC_HAVE_VSNPRINTF 1\n#define PETSC_HAVE_X 1\n#define PETSC_HAVE_XMMINTRIN_H 1\n#define PETSC_HDF5_HAVE_PARALLEL 1\n#define PETSC_IS_COLORING_MAX USHRT_MAX\n#define PETSC_IS_COLORING_VALUE_TYPE short\n#define PETSC_IS_COLORING_VALUE_TYPE_F integer2\n#define PETSC_LEVEL1_DCACHE_LINESIZE 64\n#define PETSC_LIB_DIR \"/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib\"\n#define PETSC_MAX_PATH_LEN 4096\n#define PETSC_MEMALIGN 16\n#define PETSC_MPICC_SHOW \"gcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\"\n#define PETSC_MPIU_IS_COLORING_VALUE_TYPE MPI_UNSIGNED_SHORT\n#define PETSC_PREFETCH_HINT_NTA _MM_HINT_NTA\n#define PETSC_PREFETCH_HINT_T0 _MM_HINT_T0\n#define PETSC_PREFETCH_HINT_T1 _MM_HINT_T1\n#define PETSC_PREFETCH_HINT_T2 _MM_HINT_T2\n#define PETSC_PYTHON_EXE \"/usr/bin/python\"\n#define PETSC_Prefetch(a,b,c) _mm_prefetch((const char*)(a),(c))\n#define PETSC_REPLACE_DIR_SEPARATOR '\\\\'\n#define PETSC_SIGNAL_CAST  \n#define PETSC_SIZEOF_ENUM 4\n#define PETSC_SIZEOF_INT 4\n#define PETSC_SIZEOF_LONG 8\n#define PETSC_SIZEOF_LONG_LONG 8\n#define PETSC_SIZEOF_SHORT 2\n#define PETSC_SIZEOF_SIZE_T 8\n#define PETSC_SIZEOF_VOID_P 8\n#define PETSC_SLSUFFIX \"so\"\n#define PETSC_UINTPTR_T uintptr_t\n#define PETSC_UNUSED __attribute((unused))\n#define PETSC_USE_64BIT_INDICES 1\n#define PETSC_USE_AVX512_KERNELS 1\n#define PETSC_USE_BACKWARD_LOOP 1\n#define PETSC_USE_CTABLE 1\n#define PETSC_USE_DEBUGGER \"gdb\"\n#define PETSC_USE_INFO 1\n#define PETSC_USE_ISATTY 1\n#define PETSC_USE_LOG 1\n#define PETSC_USE_MALLOC_COALESCED 1\n#define PETSC_USE_PROC_FOR_SIZE 1\n#define PETSC_USE_REAL_DOUBLE 1\n#define PETSC_USE_SHARED_LIBRARIES 1\n#define PETSC_USE_SINGLE_LIBRARY 1\n#define PETSC_USE_SOCKET_VIEWER 1\n#define PETSC_USE_VISIBILITY_C 1\n#define PETSC_USE_VISIBILITY_CXX 1\n#define PETSC_USING_64BIT_PTR 1\n#define PETSC_USING_F2003 1\n#define PETSC_USING_F90FREEFORM 1\n#define PETSC_VERSION_BRANCH_GIT \"HEAD\"\n#define PETSC_VERSION_DATE_GIT \"2022-07-19 17:19:37 -0500\"\n#define PETSC_VERSION_GIT \"v3.16.6-1-g477e44bbb55\"\n#define PETSC__BSD_SOURCE 1\n#define PETSC__DEFAULT_SOURCE 1\n#define PETSC__GNU_SOURCE 1\n-----------------------------------------\nUsing C/C++ include paths: -I/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/include -I/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose/include      -I/home/kit/agw/qz9211/projects/moose/petsc/include -I/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/include\nUsing C/C++ compiler: mpicc -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp    \nUsing Fortran include/module paths: -I/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/include -I/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose/include -I/home/kit/agw/qz9211/projects/moose/petsc/include -I/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/include\nUsing Fortran compiler: mpif90 -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O  -fopenmp   \n-----------------------------------------\nUsing C/C++ linker: mpicc\nUsing C/C++ flags: -fopenmp -fopenmp   -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp\nUsing Fortran linker: mpif90\nUsing Fortran flags: -fopenmp -fopenmp   -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O  -fopenmp\n-----------------------------------------\nUsing libraries: -Wl,-rpath,/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose/lib -L/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose/lib -lslepc         -Wl,-rpath,/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -L/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -Wl,-rpath,/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -L/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -Wl,-rpath,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0 -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0 -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib64 -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib64 -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib -lpetsc -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lstrumpack -lscalapack -lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lhdf5_hl -lhdf5 -lparmetis -lmetis -lm -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lrt -lquadmath -lstdc++ -ldl\n------------------------------------------\nUsing mpiexec: mpiexec --oversubscribe\n------------------------------------------\nUsing MAKEFLAGS: -j44 -l113.6  --no-print-directory -- SLEPC_DIR=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc V=1 PETSC_ARCH=arch-moose PETSC_DIR=/home/kit/agw/qz9211/projects/moose/petsc\n==========================================\n/usr/bin/python /home/kit/agw/qz9211/projects/moose/petsc/config/gmakegen.py --petsc-arch=arch-moose --pkg-dir=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc --pkg-name=slepc --pkg-pkgs=sys,eps,svd,pep,nep,mfn,lme --pkg-arch=arch-moose\n/usr/bin/python /home/kit/agw/qz9211/projects/moose/petsc/config/gmakegentest.py --petsc-dir=/home/kit/agw/qz9211/projects/moose/petsc --petsc-arch=arch-moose --testdir=./arch-moose/tests --srcdir=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/src --pkg-name=slepc --pkg-pkgs=sys,eps,svd,pep,nep,mfn,lme --pkg-arch=arch-moose --pkg-dir=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4320936",
                  "updatedAt": "2022-12-06T13:34:04Z",
                  "publishedAt": "2022-12-06T08:55:33Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I believe what Roy is trying to tell us, is that by not having:\nexport CC=mpicc CXX=mpicxx F77=mpif77 F90=mpif90\n...set before running ./update_and_rebuild_petsc.sh and only during our last attempt at building libMesh, the two libraries have been built differently. It looks fine to me by way of what PETSc is being linked to. But it is probably a good idea to start over by building PETSc. Making sure the above exports are being done. And also now with Conda's MPI library truly out of the way.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4323216",
                          "updatedAt": "2022-12-06T13:44:00Z",
                          "publishedAt": "2022-12-06T13:43:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "I'm wondering about make V=1 in the libMesh folder, actually.  The linker there is what's complaining, and I'm curious about what it's really trying to do under the make hood.\nIt's interesting to see not a mention of fabric in those PETSc results, though.  That at least seems to clarify that it's something being brought in by MPI and not at a higher level.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4327838",
                          "updatedAt": "2022-12-06T23:44:24Z",
                          "publishedAt": "2022-12-06T23:44:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "I'm wondering about make V=1 in the libMesh folder, actually. The linker there is what's complaining, and I'm curious about what it's really trying to do under the make hood.\nIt's interesting to see not a mention of fabric in those PETSc results, though. That at least seems to clarify that it's something being brought in by MPI and not at a higher level.\n\nThanks for your tip. When I make V=1 in the libMesh folder, I get\nmake: *** No targets specified and no makefile found.  Stop.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4327874",
                          "updatedAt": "2022-12-06T23:53:58Z",
                          "publishedAt": "2022-12-06T23:53:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "when using the ./update_and_rebuild_libmesh.sh script, when it dies, you'll need to enter the following directory before running make V=1:\n/home/kit/agw/qz9211/projects/moose/libmesh/build\nI can install OpenMPI 4.1.4 on one of our test machines, see if we can replicate. I'll report back with my results...",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4333015",
                          "updatedAt": "2022-12-07T13:28:36Z",
                          "publishedAt": "2022-12-07T13:28:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Well, OpenMPI 4.1.4 with GCC 10.2.0, on Rocky-8 (as close as I can get to RHEL 8.x) worked for me, unfortunately.\nMy PETSc links:\n[rocky-8][~/projects/2nd_moose/petsc/arch-moose/lib]> ldd libpetsc.so\n\tlinux-vdso.so.1 (0x00007ffd15f7e000)\n\tlibHYPRE-2.23.0.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libHYPRE-2.23.0.so (0x00007f8553ad6000)\n\tlibstrumpack.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libstrumpack.so (0x00007f8552ff9000)\n\tlibsuperlu_dist.so.7 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libsuperlu_dist.so.7 (0x00007f8552cf8000)\n\tlibhdf5_hl.so.200 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libhdf5_hl.so.200 (0x00007f8552ad4000)\n\tlibhdf5.so.200 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libhdf5.so.200 (0x00007f8552456000)\n\tlibparmetis.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libparmetis.so (0x00007f8552216000)\n\tlibmetis.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libmetis.so (0x00007f8551fb3000)\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007f8551c31000)\n\tlibX11.so.6 => /lib64/libX11.so.6 (0x00007f85518ee000)\n\tlibstdc++.so.6 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libstdc++.so.6 (0x00007f8551513000)\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007f855130f000)\n\tlibmpi_usempif08.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_usempif08.so.40 (0x00007f85510ce000)\n\tlibmpi_usempi_ignore_tkr.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_usempi_ignore_tkr.so.40 (0x00007f8550ebf000)\n\tlibmpi_mpifh.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_mpifh.so.40 (0x00007f8550c51000)\n\tlibmpi.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi.so.40 (0x00007f855073d000)\n\tlibgfortran.so.5 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgfortran.so.5 (0x00007f85502a0000)\n\tlibgcc_s.so.1 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgcc_s.so.1 (0x00007f8550087000)\n\tlibquadmath.so.0 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libquadmath.so.0 (0x00007f854fe3f000)\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f854fc1f000)\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00007f854fa17000)\n\tlibgomp.so.1 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgomp.so.1 (0x00007f854f7d7000)\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007f854f412000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f8555a83000)\n\tlibxcb.so.1 => /lib64/libxcb.so.1 (0x00007f854f1e9000)\n\tlibopen-rte.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libopen-rte.so.40 (0x00007f854eec6000)\n\tlibopen-pal.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libopen-pal.so.40 (0x00007f854ebc2000)\n\tlibpmix.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/pmix-4.1.2-4pykhszatp6kjdo6wbskirkpu7ey5x6s/lib/libpmix.so.2 (0x00007f854e7d3000)\n\tlibutil.so.1 => /lib64/libutil.so.1 (0x00007f854e5cf000)\n\tlibz.so.1 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/zlib-1.2.13-p7bwlngjhz5hp73lkes7efviqft5tkdi/lib/libz.so.1 (0x00007f854e3b7000)\n\tlibhwloc.so.15 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/hwloc-2.8.0-cvypsspkurpffm54wjuadcasfsyvco47/lib/libhwloc.so.15 (0x00007f854e159000)\n\tlibevent_core-2.1.so.7 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libevent-2.1.12-xgyfn7vgkuendutqklq2u34f2ksqjmuj/lib/libevent_core-2.1.so.7 (0x00007f854df23000)\n\tlibevent_pthreads-2.1.so.7 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libevent-2.1.12-xgyfn7vgkuendutqklq2u34f2ksqjmuj/lib/libevent_pthreads-2.1.so.7 (0x00007f854dd20000)\n\tlibXau.so.6 => /lib64/libXau.so.6 (0x00007f854db1c000)\n\tlibpciaccess.so.0 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libpciaccess-0.16-idrbcbu2dgirjpz5pw5uhofkpvljr6vi/lib/libpciaccess.so.0 (0x00007f854d913000)\n\tlibxml2.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libxml2-2.10.1-e65t5nvaqvefmhcckjw6cnhczewjyyja/lib/libxml2.so.2 (0x00007f854d5a8000)\n\tliblzma.so.5 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/xz-5.2.7-wcfmgp6kbiza7qsued5ylgqctrn6jkwt/lib/liblzma.so.5 (0x00007f854d381000)\n\tlibiconv.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libiconv-1.16-vxufhfcwlegaeg64whhmea76sldq2ejv/lib/libiconv.so.2 (0x00007f854d083000)\n\nMy libmesh links:\n[rocky-8][~/projects/2nd_moose/libmesh/build/.libs]> ldd libmesh_opt.so\n\tlinux-vdso.so.1 (0x00007fff5e1f4000)\n\tlibnetcdf.so.13 => /home/milljm/projects/2nd_moose/libmesh/build/contrib/netcdf/v4/liblib/.libs/libnetcdf.so.13 (0x00007f2164d6b000)\n\tlibtimpi_opt.so.11 => /home/milljm/projects/2nd_moose/libmesh/build/contrib/timpi/src/.libs/libtimpi_opt.so.11 (0x00007f2164b5d000)\n\tlibglpk.so.40 => /lib64/libglpk.so.40 (0x00007f216487c000)\n\tlibz.so.1 => /lib64/libz.so.1 (0x00007f2164664000)\n\tlibslepc.so.3.16 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libslepc.so.3.16 (0x00007f21640ae000)\n\tlibpetsc.so.3.16 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libpetsc.so.3.16 (0x00007f2162716000)\n\tlibHYPRE-2.23.0.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libHYPRE-2.23.0.so (0x00007f2162101000)\n\tlibstrumpack.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libstrumpack.so (0x00007f2161624000)\n\tlibsuperlu_dist.so.7 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libsuperlu_dist.so.7 (0x00007f2161323000)\n\tlibhdf5_hl.so.200 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libhdf5_hl.so.200 (0x00007f21610ff000)\n\tlibhdf5.so.200 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libhdf5.so.200 (0x00007f2160a81000)\n\tlibparmetis.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libparmetis.so (0x00007f2160841000)\n\tlibmetis.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libmetis.so (0x00007f21605de000)\n\tlibX11.so.6 => /lib64/libX11.so.6 (0x00007f216029b000)\n\tlibmpi_usempif08.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_usempif08.so.40 (0x00007f216005a000)\n\tlibmpi_usempi_ignore_tkr.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_usempi_ignore_tkr.so.40 (0x00007f215fe4b000)\n\tlibmpi_mpifh.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_mpifh.so.40 (0x00007f215fbdd000)\n\tlibgfortran.so.5 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgfortran.so.5 (0x00007f215f740000)\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00007f215f538000)\n\tlibquadmath.so.0 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libquadmath.so.0 (0x00007f215f2f0000)\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007f215f0ec000)\n\tlibtirpc.so.3 => /lib64/libtirpc.so.3 (0x00007f215eeb9000)\n\tlibmpi.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi.so.40 (0x00007f215e9a5000)\n\tlibstdc++.so.6 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libstdc++.so.6 (0x00007f215e5ca000)\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007f215e248000)\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f215e028000)\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007f215dc63000)\n\tlibgcc_s.so.1 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgcc_s.so.1 (0x00007f215da4a000)\n\tlibgomp.so.1 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgomp.so.1 (0x00007f215d80a000)\n\tlibamd.so.2 => /lib64/libamd.so.2 (0x00007f215d5ff000)\n\tlibcolamd.so.2 => /lib64/libcolamd.so.2 (0x00007f215d3f7000)\n\tlibgmp.so.10 => /lib64/libgmp.so.10 (0x00007f215d15f000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f21667c7000)\n\tlibxcb.so.1 => /lib64/libxcb.so.1 (0x00007f215cf36000)\n\tlibopen-rte.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libopen-rte.so.40 (0x00007f215cc13000)\n\tlibopen-pal.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libopen-pal.so.40 (0x00007f215c90f000)\n\tlibpmix.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/pmix-4.1.2-4pykhszatp6kjdo6wbskirkpu7ey5x6s/lib/libpmix.so.2 (0x00007f215c520000)\n\tlibutil.so.1 => /lib64/libutil.so.1 (0x00007f215c31c000)\n\tlibhwloc.so.15 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/hwloc-2.8.0-cvypsspkurpffm54wjuadcasfsyvco47/lib/libhwloc.so.15 (0x00007f215c0be000)\n\tlibevent_core-2.1.so.7 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libevent-2.1.12-xgyfn7vgkuendutqklq2u34f2ksqjmuj/lib/libevent_core-2.1.so.7 (0x00007f215be88000)\n\tlibevent_pthreads-2.1.so.7 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libevent-2.1.12-xgyfn7vgkuendutqklq2u34f2ksqjmuj/lib/libevent_pthreads-2.1.so.7 (0x00007f215bc85000)\n\tlibgssapi_krb5.so.2 => /lib64/libgssapi_krb5.so.2 (0x00007f215ba30000)\n\tlibkrb5.so.3 => /lib64/libkrb5.so.3 (0x00007f215b746000)\n\tlibk5crypto.so.3 => /lib64/libk5crypto.so.3 (0x00007f215b52f000)\n\tlibcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007f215b32b000)\n\tlibsuitesparseconfig.so.4 => /lib64/libsuitesparseconfig.so.4 (0x00007f215b128000)\n\tlibXau.so.6 => /lib64/libXau.so.6 (0x00007f215af24000)\n\tlibpciaccess.so.0 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libpciaccess-0.16-idrbcbu2dgirjpz5pw5uhofkpvljr6vi/lib/libpciaccess.so.0 (0x00007f215ad1b000)\n\tlibxml2.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libxml2-2.10.1-e65t5nvaqvefmhcckjw6cnhczewjyyja/lib/libxml2.so.2 (0x00007f215a9b0000)\n\tlibkrb5support.so.0 => /lib64/libkrb5support.so.0 (0x00007f215a79f000)\n\tlibkeyutils.so.1 => /lib64/libkeyutils.so.1 (0x00007f215a59b000)\n\tlibcrypto.so.1.1 => /lib64/libcrypto.so.1.1 (0x00007f215a0b2000)\n\tlibresolv.so.2 => /lib64/libresolv.so.2 (0x00007f2159e9b000)\n\tliblzma.so.5 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/xz-5.2.7-wcfmgp6kbiza7qsued5ylgqctrn6jkwt/lib/liblzma.so.5 (0x00007f2159c74000)\n\tlibiconv.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libiconv-1.16-vxufhfcwlegaeg64whhmea76sldq2ejv/lib/libiconv.so.2 (0x00007f2159976000)\n\tlibselinux.so.1 => /lib64/libselinux.so.1 (0x00007f215974c000)\n\tlibpcre2-8.so.0 => /lib64/libpcre2-8.so.0 (0x00007f21594c8000)\n\nAttaching libmesh_diagnostics.log\nlibmesh_diagnostic.log",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334470",
                          "updatedAt": "2022-12-07T15:57:03Z",
                          "publishedAt": "2022-12-07T15:56:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "In all of my links, I do not see me linking to libfabric.\nI do not even see this library in my lib directory for OpenMPI. Hmm, I'll do a bit more research on just what this library is, that seems to come with OpenMPI. I suppose its an argument option I am not supplying while building OpenMPI.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334573",
                          "updatedAt": "2022-12-07T16:07:24Z",
                          "publishedAt": "2022-12-07T16:07:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "It would seem libFabric is separate from OpenMPI. My guess is when your cluster updated to OpenMPI 4.1.4, they failed to also build libFabric using GCC 10.2.0. This is my laymen explanation... Perhaps there exists an OpenMPI module you can load, which is using an older GCC?\nNormally when a cluster updates their MPI wrappers, what they are actually doing is adding to an existing bunch. Is it possible to load an early version of OpenMPI?\nCurrently Loaded Modules:\n\n    1) compiler/gnu/10.2 2) mpi/openmpi/4.0\n\nIs there perhaps mpi/openmpi/3? (strange that 4.0 means 4.1.4).",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334678",
                          "updatedAt": "2022-12-07T16:17:57Z",
                          "publishedAt": "2022-12-07T16:17:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I am rebuilding openmpi with fabrics=ofi, that should get me a libfabrics library. However, this library is seriously tailored for the backpane interconnect of the nodes, specialized hardware, for your cluster. It will be different for mine... So I am not sure how close I can replicate your environment for testing.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334797",
                          "updatedAt": "2022-12-07T16:29:35Z",
                          "publishedAt": "2022-12-07T16:29:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "It was OpenMPI 4.0 for quite some time, but after the last larger system update (we got noticed by the administrators) 4.0 isn't compatible anymore and they decided to recompile petsc\nand openmpi using Gnu10.2 and openmpi 4.1.\nAt least for my attempt to compile libmesh, I'm sure that I used proper modules loaded and linked:\nlibmesh_build_2022-12-06.16%3A44%3A43.log",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334836",
                          "updatedAt": "2022-12-07T16:34:00Z",
                          "publishedAt": "2022-12-07T16:34:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I rebuilt openmpi to make use of libfabric. However, while I was expecting libopen-pal.so to link to a version of libfabric I installed using spack, it instead linked to my system's versions:\n{ [milljm@sawtooth2] [3b55c2e] } [/apps/local/spack/software/gcc-8.4.0/openmpi-4.0.2-xkgotunrhknpvlly4cuq66owczlchudz/lib]\n$ ldd libopen-pal.so \n\tlinux-vdso.so.1 =>  (0x00007fffedb02000)\n\tlibfabric.so.1 => /usr/lib64/libfabric.so.1 (0x00007fffed05c000)      <------------- RIGHT HERE\n\tlibrdmacm.so.1 => /usr/lib64/librdmacm.so.1 (0x00007fffece40000)\n\tlibibverbs.so.1 => /usr/lib64/libibverbs.so.1 (0x00007fffecc27000)\n\tlibpsm2.so.2 => /usr/lib64/libpsm2.so.2 (0x00007fffec9c1000)\n\tlibrt.so.1 => /usr/lib64/librt.so.1 (0x00007fffec7b9000)\n\tlibutil.so.1 => /usr/lib64/libutil.so.1 (0x00007fffec5b6000)\n\tlibhwloc.so.15 => /apps/local/spack/software/gcc-8.4.0/hwloc-2.1.0-c37ukynvqpcswa3y6dppqyjc5qea2zon/lib/libhwloc.so.15 (0x00007fffec365000)\n\tlibudev.so.1 => /usr/lib64/libudev.so.1 (0x00007fffec14f000)\n\tlibpciaccess.so.0 => /apps/local/spack/software/gcc-8.4.0/libpciaccess-0.13.5-fjixzxnwbotbtdrb4dvldiyc3kul37ic/lib/libpciaccess.so.0 (0x00007fffebf46000)\n\tlibxml2.so.2 => /apps/local/spack/software/gcc-8.4.0/libxml2-2.9.10-odes7obyblstttzxon4o3foz5czn5und/lib/libxml2.so.2 (0x00007fffebbe5000)\n\tlibdl.so.2 => /usr/lib64/libdl.so.2 (0x00007fffeb9e1000)\n\tlibz.so.1 => /apps/local/spack/software/gcc-8.4.0/zlib-1.2.11-iode5iweyrwrwqi7vc4wbtvtszsqba5g/lib/libz.so.1 (0x00007fffeb7ca000)\n\tliblzma.so.5 => /apps/local/spack/software/gcc-8.4.0/xz-5.2.4-egk7d7shjuuzm4jl4nyoen64hgu6ft4f/lib/liblzma.so.5 (0x00007fffeb5a4000)\n\tlibiconv.so.2 => /apps/local/spack/software/gcc-8.4.0/libiconv-1.16-gxhwn4i3pmaieairp27f7dphdysssabc/lib/libiconv.so.2 (0x00007fffeb2a8000)\n\tlibm.so.6 => /usr/lib64/libm.so.6 (0x00007fffeafa6000)\n\tlibpthread.so.0 => /usr/lib64/libpthread.so.0 (0x00007fffead8a000)\n\tlibc.so.6 => /usr/lib64/libc.so.6 (0x00007fffea9bc000)\n\tlibnl-3.so.200 => /usr/lib64/libnl-3.so.200 (0x00007fffea79b000)\n\tlibnl-route-3.so.200 => /usr/lib64/libnl-route-3.so.200 (0x00007fffea52e000)\n\tlibpsm_infinipath.so.1 => /usr/lib64/libpsm_infinipath.so.1 (0x00007fffea2d8000)\n\tlibgcc_s.so.1 => /apps/local/spack/software/gcc-4.8.5/gcc-8.4.0-jacdabugmghmzya2rpfqpym2bpzjomy2/lib64/libgcc_s.so.1 (0x00007fffea0c0000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fffed8e3000)\n\tlibnuma.so.1 => /apps/local/spack/software/gcc-8.4.0/numactl-2.0.13-ewpw3l3emcqvu55ii2tylucw54lltg35/lib/libnuma.so.1 (0x00007fffe9eb5000)\n\tlibcap.so.2 => /usr/lib64/libcap.so.2 (0x00007fffe9cb0000)\n\tlibdw.so.1 => /usr/lib64/libdw.so.1 (0x00007fffe9a5f000)\n\tlibinfinipath.so.4 => /usr/lib64/libinfinipath.so.4 (0x00007fffe9850000)\n\tlibuuid.so.1 => /usr/lib64/libuuid.so.1 (0x00007fffe964b000)\n\tlibattr.so.1 => /usr/lib64/libattr.so.1 (0x00007fffe9446000)\n\tlibelf.so.1 => /usr/lib64/libelf.so.1 (0x00007fffe922e000)\n\tlibbz2.so.1 => /usr/lib64/libbz2.so.1 (0x00007fffe901e000)\n\nCan you perhaps see what libfabric your libopen-pal.so is linked to (please do this):\nldd /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-pal.so",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4335481",
                          "updatedAt": "2022-12-07T17:25:13Z",
                          "publishedAt": "2022-12-07T17:25:13Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "1runer"
                  },
                  "bodyText": "Dear all,\nI also wanna step in this discussion, because I have more or less the same issues with the cluster.\nInstead of Joseph-0123, I used the build-in versions of Petsc and OpenMPI.\nSome weeks ago, the administrators updated OpenMPI from 4.0 to 4.1 which makes it necessary to recompile moose/libmesh. WIth the newer version of OpenMPI I couldn't recompile libmesh failing at one of the very last steps.\nThe errors tells something  like @roystgnr mentioned regarding fabric.\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-pal.so: undefined reference to fi_open@FABRIC_1.5'`\nI attached the log file to this message.\nRecompilation_MOOSE.txt\nBest regards",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4330167",
                  "updatedAt": "2022-12-07T07:28:06Z",
                  "publishedAt": "2022-12-07T07:26:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "I am not sue what is going on on this machine. I am reading past threads in this post, and I am realizing we have used three different compiler combinations:\n/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'\n                              ^^^^^^^^^^^^^^ 3 different combos in play\n\nHow are you guys selecting these different compilers?  Modules? By hand somehow? Are your admins making changes to what is being sourced on-the-fly over the course of these past few days?",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4344154",
                  "updatedAt": "2022-12-08T15:00:05Z",
                  "publishedAt": "2022-12-08T14:59:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "That happens when you try to solve an issue by recompiling Moose with different combinations of openmpi/gnu/petsc.\nMaybe some history about our HPC:\nWe used MOOSE for quite some time successfully until November using OpenMPI 4.0 and Gnu 10.2.\nThen they decided to go for hardware and software upgrade resulting in a non-working (but still existing) old openmpi 4.0, which explains the very first message (undefined symbol: rdma_establish).\nSo they recommended to recompile MOOSE using their pre-built openmpi 4.1 which actually fails due to \"undefined reference to `fi_open@FABRIC_1.5'\".\nI hope I was able to clarify some things even if I didn't solve it.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4344250",
                          "updatedAt": "2022-12-08T15:13:13Z",
                          "publishedAt": "2022-12-08T15:13:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I spoke to our HPC admins. They looked over this issue and spoke to me at length that they too run (or have ran) into this issue in the past. That it's highly dependent on the cluster hardware and fabric and libraries.\nOne thing we can try to do, is have you build your own MPI wrapper and use that, instead of the wrapper being supplied by your cluster. Doing so would produce a wrapper not using fabrics at all.\nIf you have not done this before, building/installing/using your own MPI wrapper follows the same ./configure, make, make install paradigm -its pretty straight forward. I can try and help with this if you wish.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4346232",
                          "updatedAt": "2022-12-08T19:16:17Z",
                          "publishedAt": "2022-12-08T19:16:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "I spoke to our HPC admins. They looked over this issue and spoke to me at length that they too run (or have ran) into this issue in the past. That it's highly dependent on the cluster hardware and fabric and libraries.\nOne thing we can try to do, is have you build your own MPI wrapper and use that, instead of the wrapper being supplied by your cluster. Doing so would produce a wrapper not using fabrics at all.\nIf you have not done this before, building/installing/using your own MPI wrapper follows the same ./configure, make, make install paradigm -its pretty straight forward. I can try and help with this if you wish.\n\nHello, Milljm. I do appreciate your generous help in solving this problem. I will try to build my own MPI wrapper.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4346940",
                          "updatedAt": "2022-12-08T21:10:34Z",
                          "publishedAt": "2022-12-08T21:10:33Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Help understanding how vector variables are handled in auxkernels",
          "author": {
            "login": "Eilloo"
          },
          "bodyText": "Hi all,\nI've been trying to learn about auxkernels, with a view to extract velocity gradient components at an arbitrary node ID (not necessarily the node currently under consideration).\nFrom the documentation on the 'couplable' interface, I was hoping that I'd be able to do something along the lines of:\n_vel_grad_x(coupledGradient(\"velocity\", index))\nwhere \"velocity\" is an input vector variable, and 'index' is 0, 1, or 2 for x, y, and z.\nSuffice to say this didn't work, and so I began using some cout << statements in my auxkernel to try and work out how to index my velocity vector properly.\nUsing something along the lines of:\nstd::cout << _velocity[currentNodeID]\nprinted values looking like this:\n(x,y,z)=(2.18065e-314, 2.47033e-323, 2.18065e-314)\nor for some lines:\n(x,y,z)=(0, 4.82411e+228, 1.90859e+185)\nThe e-300 values I thought could be rounding errors and the velocity is zero. However, I can't explain the very large values, and none of the output velocities look close to those in the output file, whose velocity magnitudes, vary between 0 and 8e-2.\nMy question is, what have I been printing? My hope is that this will clarify how I should set about extracting the velocity gradient components properly.\nThanks!\nBelow are some snippets from the .h and .C files to clarify how I've defined things in more detail:\nIn the header file, under 'protected':\nconst VectorVariableValue & _velocity;\nconst VariableGradient & _vel_grad_x;\nIn the initialiser list in the .C file:\n_velocity(coupledVectorValue(\"velocity\")),\n_vel_grad_x(coupledGradient(\"velocity\", 0)),\nThe cout statement:\nstd::cout << _velocity[_current_node->id()] << \"\\n\";\nVelocity is defined with family=\"LAGRANGE_VEC\" in the input file.\nI'll also add that my simulation is 2D, so seeing any value in the 'z' component is surprising!",
          "url": "https://github.com/idaholab/moose/discussions/22895",
          "updatedAt": "2023-01-03T23:22:37Z",
          "publishedAt": "2022-12-06T16:37:56Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou are reading out of bounds, so there could be any value there.\nIf velocity is a vector variable, then to retrieve the gradient of it (the whole vector) you will need to save it in a\nconst VectorVariableGradient & _vel_grad\nand use\nconst ADVectorVariableGradient &\nto fill it.\nThen you will want to access each component with 0, 1, 2 for X Y Z\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4325086",
                  "updatedAt": "2022-12-06T16:58:30Z",
                  "publishedAt": "2022-12-06T16:58:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Eilloo"
                  },
                  "bodyText": "Ah, I hadn't seen that VectorVariableGradient is a type!\nTo make sure I understand, I shouldn't use the 'coupleable' interface to populate it, but instead look to access the ADVectorVariableGradient values directly?\nI see now that the nodeID's aren't what I thought and hence are out of bounds garbage when I try to index into _velocity using them... is there a way to convert from a node ID to an index so I can pull out the corresponding variable values?\nThanks for your help!",
                  "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4325240",
                  "updatedAt": "2022-12-06T17:18:08Z",
                  "publishedAt": "2022-12-06T17:18:07Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You should use the coupleable interface, but you need to use the right routine to get the vector variable gradient\nSo that part is tough. The coupleable interface only gets you the local variable value and for some items the neighbor values. It wont get you the general values of the variable everywhere based on the node ID.\nYou can use the variable pointer directly (use getVariable to retrieve it) and if you know where to index you can directly access the solution vector (wont work for higher orders very easily)\nhttps://mooseframework.inl.gov/docs/doxygen/moose/classMooseVariableFE.html#acbbf516d918a5de55a3b59832908cf98\nor you can use the functor base class of variables. Functors are a base class of variable and help with geometry-based evaluations (like element face, or particular point in space). That hasnt been explored much so far for node IDs, you ll need to do some development\nhttps://mooseframework.inl.gov/docs/doxygen/moose/classMoose_1_1FunctorBase.html",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4325716",
                          "updatedAt": "2022-12-06T18:04:15Z",
                          "publishedAt": "2022-12-06T18:04:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "with a view to extract velocity gradient components at an arbitrary node ID (not necessarily the node currently under consideration).\n\nbasically this is difficult to do in MOOSE right now.\nSome solutions will either be hacky or involve developments\nIf you can formulate your problem to always consider the local node, or nodes around the local element, then it will be easier",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4325722",
                          "updatedAt": "2022-12-06T18:05:43Z",
                          "publishedAt": "2022-12-06T18:05:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Thanks for the info @GiudGiud - I thought I'd say what I've ended up doing, for the sake of completeness:\nThere's a handy 'find_nodal_neighbors' method in MeshTools, so once you know which node you're interested in, it's easy to get a list of its neighbour nodes using this.\nYou can then get the dof associated with each velocity component at each of these neighbours using 'dof_number', and changing the 'component' argument. Getting the actual values is achieved in a similar way to the existing 'NearestNodeValue' Auxkernel.\nSubtracting these components from those associated with the node of interest essentially gives 'delta_velocity_x' (for, say, the x component), and you can divide this by the difference in x/y/z co-ordinates to get all 4 (2D) or 9 (3D) velocity gradient components.\nAveraging each component by summing and dividing by the number of neighbour nodes used to calculate them gives one set of velocity gradient components, associated with the original node of interest.\nNow, this isn't very elegant and involves a lot of nested loops, so I suspect things will get very slow very soon when I start looking at more complex meshes... but initial impressions are that it seems to work, in case anyone is interested!",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4342538",
                          "updatedAt": "2022-12-08T11:53:37Z",
                          "publishedAt": "2022-12-08T11:53:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yes that seems workable. Thank you for sharing",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4343669",
                          "updatedAt": "2022-12-08T14:07:10Z",
                          "publishedAt": "2022-12-08T14:07:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This won\u2019t work for all variable types obviously, only a subset of nodal variables",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4343678",
                          "updatedAt": "2022-12-08T14:07:52Z",
                          "publishedAt": "2022-12-08T14:07:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to add TensorMechanics into eigenvalue calculation",
          "author": {
            "login": "js-jixu"
          },
          "bodyText": "Hi, experts.\nI'm doing an eigenvalue calculation. My input file clearly contains TensorMechanics, but the calculated result does not contain TensorMechanics. When I look at the non-linear iteration residuals, other variables have values, only disp_x/y/z are 0:\n 0 Nonlinear |R| = 4.649184e+03\n    |residual|_2 of individual variables:\n                   velocity: 4647.58\n                   p:        4.07466e-05\n                   Tf:       8.7437e-13\n                   Ts:       122.101\n                   disp_x:   0\n                   disp_y:   0\n                   disp_z:   0\n\nHere is my blocks about TensorMechanics(Variables and BCs are omitted):\n[Kernels]\n  [disp_x_diff]\n    type = Diffusion\n    variable = disp_x\n    block = fluid\n    use_displaced_mesh = true\n  []\n  [disp_y_diff]\n    type = Diffusion\n    variable = disp_y\n    block = fluid\n    use_displaced_mesh = true\n  []\n  [disp_z_diff]\n    type = Diffusion\n    variable = disp_z\n    block = fluid\n    use_displaced_mesh = true\n  []\n[]\n\n[Modules/TensorMechanics/Master]\n  displacements = 'disp_x disp_y disp_z'\n  strain = FINITE\n  material_output_order = FIRST\n  generate_output = 'vonmises_stress stress_xx stress_yy stress_zz strain_xx strain_yy strain_zz'\n  [mechanics]\n    block = 'solid'\n    temperature = Ts\n    displacements = 'disp_x disp_y disp_z'\n    automatic_eigenstrain_names = true\n  []\n[]\n\n[Materials]\n  [elasticity_solid]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 2e7\n    poissons_ratio = 0.32\n    block = 'solid'\n    use_displaced_mesh = true\n  []\n  [thermal_expansion_solid]\n    type = ComputeThermalExpansionEigenstrain\n    temperature = Ts\n    thermal_expansion_coeff = 1e-5\n    stress_free_temperature = 273\n    eigenstrain_name = thermal_expansion\n    block = 'solid'\n    use_displaced_mesh = true\n  []\n  [stress_solid]\n    type = ComputeFiniteStrainElasticStress\n    block = 'solid'\n  []\n[]\n\nAnd I use InversePowerMethod in Executioner block:\n[Executioner]\n  type = InversePowerMethod\n  max_power_iterations = 50\n\n  normalization = 'powernorm'\n  normal_factor = 21230\n\n  bx_norm = 'bnorm'\n  k0 = 1\n  l_max_its = 200\n  eig_check_tol = 1e-3\n\n  solve_type = 'PJFNK'\n  petsc_options = '-snes_converged_reason -ksp_converged_reason -snes_linesearch_monitor'\n  petsc_options_iname = '-pc_type -pc_factor_shift_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu       NONZERO               superlu_dist'\n\n  line_search = none\n  automatic_scaling = true\n[]",
          "url": "https://github.com/idaholab/moose/discussions/22721",
          "updatedAt": "2023-01-03T03:12:42Z",
          "publishedAt": "2022-11-16T13:04:50Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "js-jixu"
                  },
                  "bodyText": "Is there a kind guy to take a look at my question?\ud83d\udc40",
                  "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4162945",
                  "updatedAt": "2022-11-17T01:58:33Z",
                  "publishedAt": "2022-11-17T01:58:32Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYou should not need to have [Modules/ ] before TensorMechanics.\nThis syntax of adding Modules has been deprecated\nUse the [Debug] show_action parameter to show if the action is running\nJudging by the 0 residual there's nothing acting on the displacement.\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4179274",
                          "updatedAt": "2022-11-18T17:38:16Z",
                          "publishedAt": "2022-11-18T17:38:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "Hi, Guillaume, sorry for the late reply.\n\nYou should not need to have [Modules/ ] before TensorMechanics.\n\nI removed [Modules/ ], but the terminal reported that section 'TensorMechanics' does not have an associated \"Action\". It may be that my moose version is too low. I haven't updated since September. So to get it working I added [Modules/ ] again.\n\nUse the [Debug] show_action parameter to show if the action is running.\n\nI added show_actions = true in [Debug]. It seems that TensorMechanics is taken into calculation:\n[DBG][ACT] TASK (             meta_action) TYPE (     CommonTensorMechanicsAction) NAME (          Master) Memory usage 68MB\n[DBG][ACT] TASK (             meta_action) TYPE (           TensorMechanicsAction) NAME (       mechanics) Memory usage 68MB\n[DBG][ACT] TASK (   check_copy_nodal_vars) TYPE (             CopyNodalVarsAction) NAME (          disp_x) Memory usage 68MB\n[DBG][ACT] TASK (   check_copy_nodal_vars) TYPE (             CopyNodalVarsAction) NAME (          disp_y) Memory usage 68MB\n[DBG][ACT] TASK (   check_copy_nodal_vars) TYPE (             CopyNodalVarsAction) NAME (          disp_z) Memory usage 68MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin1_y) Memory usage 105MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin1_x) Memory usage 105MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin2_y) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin2_x) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin3_y) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (         axial_z) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (        radial_x) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (        radial_y) Memory usage 106MB\n\nBut as I said before, disp_x/y/z are all 0 in the calculation results. This is not appropriate, because the temperature in different parts of the system changes. Then disp_x/y/z should also change.\nThere are figures for displacement and temperature distribution of results:",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4200707",
                          "updatedAt": "2022-11-22T02:17:15Z",
                          "publishedAt": "2022-11-22T02:16:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "I wonder if the problem will appear here. Since I have used InversePowerMethod to solve an eigenvalue problem. The ComputeThermalExpansionEigenstrain in the TensorMechanics module is also an eigenvalue solving problem. So maybe an eigenvalue problem cannot be nested within an eigenvalue problem?\nWhen I use executioner Transient or Steady for ComputeThermalExpansionEigenstrain, everything works fine. This problem occurs only when using InversePowerMethod.\nHowever, when I use NonlinearEigen, disp_x/y/z are not involved in the calculation in the power iteration. But at the end of the power iteration, when the nonlinear iteration is performed, disp_x/y/z participates in the calculation again. Unfortunately, the results of the calculations are still problematic. So I want TensorMechanics to be able to participate in power iteration.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4205850",
                          "updatedAt": "2022-11-22T13:34:41Z",
                          "publishedAt": "2022-11-22T13:34:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lynnmunday"
                          },
                          "bodyText": "ComputeThermalExpansionEigenstrain is not solving an eigenvalue problem.  Eigenstrain is any stress-free strain, I don't know why someone named it eigen-strain because I don't think it has anything to do with eigenvectors.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4256415",
                          "updatedAt": "2022-11-28T19:02:10Z",
                          "publishedAt": "2022-11-28T19:02:09Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lynnmunday"
                  },
                  "bodyText": "I don't understand your problem, you have diffusion and mechanics acting on the same variables.  What kind of physics are you modeling?  I haven't seen any examples for solving natural frequencies of a mechanical system in MOOSE.  You should look at the electromagnetics module in moose for tests that use the eigensolver.  Let me know of you get this to work, I want to solve mechanics eigenvalue problems too.",
                  "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4198808",
                  "updatedAt": "2022-11-21T20:11:20Z",
                  "publishedAt": "2022-11-21T20:09:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "Hi, Lynn.\n\nyou have diffusion and mechanics acting on the same variables. What kind of physics are you modeling?\n\nI want to use disp_x/y/z for simulating thermal expansion due to temperature rise. My model has fluid regions and solid regions. The solid region will expand due to the increase in temperature, which will also change the size of the fluid region. So I set Diffusion for dispxyz in the fluid area. Finally get such a result:\n\nI'm solving an eigenvalue problem which determines power distributions. I want to know the effect of temperature and displacement on power distribution, and power distribution in turn affects temperature and displacement. So I put the Kernels and BCs of temperature and displacement into the eigenvalue problem. But from the results, the temperature has changed, but disp_x/y/z has not changed.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4200859",
                          "updatedAt": "2022-11-22T02:44:07Z",
                          "publishedAt": "2022-11-22T02:44:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so the eigenvalue problem is not the tensor mechanics one, it's the one that determines the power distribution, which is coupled to fluid flow + mechanics?\nI would build a working input with a multiapp for either the eigenvalue problem or the tensor mechanics + fluid problem.\nThen when this works, you may consider a fully coupled solve",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4207912",
                          "updatedAt": "2022-11-22T16:04:16Z",
                          "publishedAt": "2022-11-22T16:04:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "so the eigenvalue problem is not the tensor mechanics one, it's the one that determines the power distribution, which is coupled to fluid flow + mechanics?\n\nYeah! I want to solve an eigenvalue problem which determines the distribution of power, and this problem has to be coupled with tensormechanics and fluid flow..\n\nI would build a working input with a multiapp for either the eigenvalue problem or the tensor mechanics + fluid problem. Then when this works, you may consider a fully coupled solve.\n\nI have a input file which couple the eigenvalue problem and fluid. It works well.\nAnd I have a steady input file which couples tensormechanics and fluid. Its power isn't solved by an eigenvalue problem but given as a function of z coordinate. This input file also works fine.\nSo I want to couple tensormechanics into the original input file which couples eigenvalue problem and fluid.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4208044",
                          "updatedAt": "2022-11-23T01:13:15Z",
                          "publishedAt": "2022-11-22T16:16:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "Is there any method for TensorMechanics to run in InversePowerMethod\uff1f",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4211662",
                          "updatedAt": "2022-11-23T01:30:17Z",
                          "publishedAt": "2022-11-23T01:30:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Tensor mechanics eigen problem  works for me\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Mesh]\n  [gmg]\n    type = GeneratedMeshGenerator\n    dim = 2\n    xmax = 10\n    nx = 20\n    ymax = 1\n    ny = 2\n  []\n[]\n\n[Modules]\n  [TensorMechanics]\n    [Master]\n      [all]\n        add_variables = true\n        new_system = true\n        formulation = TOTAL\n        volumetric_locking_correction = true\n      []\n    []\n  []\n[]\n\n[Kernels]\n  [x]\n    type = MassEigenKernel\n    variable = disp_x\n  []\n  [y]\n    type = MassEigenKernel\n    variable = disp_y\n  []\n[]\n\n[BCs]\n  [left_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = left\n    value = 0\n  []\n  [left_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = left\n    value = 0\n  []\n[]\n\n[Materials]\n  [C]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 1\n    poissons_ratio = 0.25\n  []\n  [stress]\n    type = ComputeLagrangianLinearElasticStress\n  []\n[]\n\n[AuxVariables]\n  [unorm_old]\n    [AuxKernel]\n      type = ParsedAux\n      function = 'sqrt(disp_x^2+disp_y^2)'\n      args = 'disp_x disp_y'\n      execute_on = 'INITIAL TIMESTEP_END'\n    []\n  []\n  [unorm]\n    [AuxKernel]\n      type = ParsedAux\n      function = 'sqrt(disp_x^2+disp_y^2)'\n      args = 'disp_x disp_y'\n      execute_on = 'LINEAR TIMESTEP_END'\n    []\n  []\n[]\n\n[Postprocessors]\n  [unorm]\n    type = ElementIntegralVariablePostprocessor\n    variable = unorm\n    execute_on = 'LINEAR'\n  []\n  [udiff]\n    type = ElementL2Difference\n    variable = unorm\n    other_variable = unorm_old\n    execute_on = 'LINEAR'\n  []\n[]\n\n[Preconditioning]\n  [smp]\n    type = SMP\n    full = true\n  []\n[]\n\n[Executioner]\n  type = InversePowerMethod\n\n  solve_type = PJFNK\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n\n  bx_norm = unorm\n  normalization = unorm\n  xdiff = udiff\n\n  min_power_iterations = 11\n  max_power_iterations = 400\n  Chebyshev_acceleration_on = true\n  eig_check_tol = 1e-12\n  k0 = 1000\n[]\n\n[Outputs]\n  exodus = true\n  print_linear_residuals = false\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4212000",
                          "updatedAt": "2022-11-23T02:46:13Z",
                          "publishedAt": "2022-11-23T02:46:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "Thank you for your input file @hugary1995 . I think maybe TensorMechanics can't be compatible with my eigenvalue problem. Gary's input file with TensorMechanics can run with InversePowerMethod, but when I substitute the TensorMechanics part of gary's input file with mine, it doesn't work. I mean, when I use TensorMechanics part of gary's input file + eigenvalue problem of my input file, the  disp_x/y/z will not take part in the calculation.\nI don't know why, but I have a guess that I mentioned before. From ComputeThermalExpansionEigenstrain, I think the calculation of displacement field is also an EigenValue calculation. And the eigenvalue calculation cannot be nested in another eigenvalue calculation. So when I do an eigenvalue calculation, I can't nest TensorMechanics into it.\ud83e\udd14",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4241396",
                          "updatedAt": "2022-11-26T06:46:33Z",
                          "publishedAt": "2022-11-26T06:46:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@YaqiWang knows a lot about our eigen systems",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4243574",
                          "updatedAt": "2022-11-26T17:47:00Z",
                          "publishedAt": "2022-11-26T17:47:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "I changed the initial condition of dispxyz, and now the residual of dispxyz in eigenvalue calculation will not be 0.\n 0 Nonlinear |R| = 4.649051e+03\n    |residual|_2 of individual variables:\n                   velocity: 4647.58\n                   p:        0.000258578\n                   Tf:       8.43338e-13\n                   Ts:       116.934\n                   disp_x:   0.0106197\n                   disp_y:   0.0106197\n                   disp_z:   0.000100545\n\nBut that doesn't solve my problem. First, the calculation results are actually related to the initial value of dispxyz. That is, I set different initial values of dispxyz then get different calculation results. What I do is an eigenvalue calculation. The final result should be independent of the initial value. Second, from the output results, the distribution of dispxyz is very abnormal. It doesn't seem like a reasonable result.\nThe first figure is the output of disp_x solved by the eigenvalue problem and the second figure is the reasonable results.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4245500",
                          "updatedAt": "2022-11-27T07:37:32Z",
                          "publishedAt": "2022-11-27T07:05:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "if the residuals are not 0, the problem is not converged and there is no reason for the results to be good.\nI d reduce the case down to a small mesh and try to get convergence with LU, then work on a field split",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4246987",
                          "updatedAt": "2022-11-27T15:00:42Z",
                          "publishedAt": "2022-11-27T15:00:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "The final residual will reach 0. I just intercepted the residual at the beginning of the first step. The problem I encountered at the beginning was that the residual of dispxyz in eigenvalue calculation was 0 from beginning to end, which was strange. Later, I changed the initial value of dispxyz. At the beginning of eigenvalue calculation, the residual error of dispxyz is not 0 (I think this indicates that dispxyz has participated in the calculation). But when the calculation is completed, the result is similar to the first picture. dispx and dispy is asymmetrical and very strange. I think the ideal result should be similar to the second picture",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4247180",
                          "updatedAt": "2022-11-27T15:41:34Z",
                          "publishedAt": "2022-11-27T15:41:33Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "libmesh-config missing after the installation via conda",
          "author": {
            "login": "max-hassani"
          },
          "bodyText": "Bug Description\nI followed the instruction to install moose via conda. Afterwards, I cloned the moose repository, and inside the test folder, make -j 4 and received the following error:\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/libtool: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/libtool: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\nChecking if header needs updating: /home/muhammad/gitRepos/moose/framework/include/base/MooseRevision.h...\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\nCompiling C++ (in opt mode) /home/muhammad/gitRepos/moose/framework/build/unity_src/src_Unity.C...\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/libtool: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\nmake: *** [/home/muhammad/gitRepos/moose/framework/build.mk:145: /home/muhammad/gitRepos/moose/framework/build/unity_src/src_Unity..opt.lo] Error 127\n\nIs there any other step that I missed?\nI am running on WSL2 (ubuntu).\nThe conda environment includes the following packages:\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\n_sysroot_linux-64_curr_repodata_hack 3                   h5bd9786_13    conda-forge\natk-1.0                   2.36.0               ha1a6a79_0\nautoconf                  2.69            pl5321hd708f79_11    conda-forge\nautomake                  1.16.1          pl5320ha770c72_1004    conda-forge\nbeautifulsoup4            4.11.1          py310h06a4308_0\nbinutils_impl_linux-64    2.36.1               h193b22a_2    conda-forge\nbinutils_linux-64         2.36                hf3e587d_10    conda-forge\nblas                      1.0                    openblas\nblosc                     1.21.0               h4ff587b_1\nbottleneck                1.3.5           py310ha9d4c09_0\nbrotli                    1.0.9                h5eee18b_7\nbrotli-bin                1.0.9                h5eee18b_7\nbrunsli                   0.1                  h2531618_0\nbzip2                     1.0.8                h7f98852_4    conda-forge\nc-ares                    1.18.1               h7f98852_0    conda-forge\nca-certificates           2022.6.15            ha878542_0    conda-forge\ncairo                     1.16.0               h19f5f5c_2\ncfitsio                   3.470                h5893167_7\ncharls                    2.2.0                h2531618_0\nclang-format-12           12.0.1          default_ha53f305_4    conda-forge\ncloudpickle               2.0.0              pyhd3eb1b0_0\ncmake                     3.24.1               h5432695_0    conda-forge\ncycler                    0.11.0             pyhd3eb1b0_0\ncytoolz                   0.11.0          py310h7f8727e_0\ndask-core                 2022.7.0        py310h06a4308_0\ndbus                      1.13.18              hb2f20db_0\ndeepdiff                  5.8.1              pyhd8ed1ab_0    conda-forge\nexpat                     2.4.8                h27087fc_0    conda-forge\nfont-ttf-dejavu-sans-mono 2.37                 hd3eb1b0_0\nfont-ttf-inconsolata      2.001                hcb22688_0\nfont-ttf-source-code-pro  2.030                hd3eb1b0_0\nfont-ttf-ubuntu           0.83                 h8b1ccd4_0\nfontconfig                2.13.1               h6c09931_0\nfonts-anaconda            1                    h8fa9717_0\nfonts-conda-ecosystem     1                    hd3eb1b0_0\nfonttools                 4.25.0             pyhd3eb1b0_0\nfreetype                  2.11.0               h70c0345_0\nfribidi                   1.0.10               h7b6447c_0\nfsspec                    2022.7.1        py310h06a4308_0\ngcc_impl_linux-64         10.4.0              h7ee1905_16    conda-forge\ngcc_linux-64              10.4.0              h9215b83_10    conda-forge\ngdk-pixbuf                2.42.8               h433bba3_0\ngfortran_impl_linux-64    10.4.0              h44b2e72_16    conda-forge\ngfortran_linux-64         10.4.0              h69d5af5_10    conda-forge\ngiflib                    5.2.1                h7b6447c_0\nglib                      2.69.1               h4ff587b_1\ngmp                       6.2.1                h295c915_3\ngmpy2                     2.1.2           py310heeb90bb_0\ngobject-introspection     1.72.0          py310hbb6d50b_0\ngraphite2                 1.3.14               h295c915_1\ngraphviz                  2.50.0               h3cd0ef9_0\ngst-plugins-base          1.14.0               h8213a91_2\ngstreamer                 1.14.0               h28cd5cc_2\ngtk2                      2.24.33              h73c1081_2\ngts                       0.7.6                hb67d8dd_3\ngxx_impl_linux-64         10.4.0              h7ee1905_16    conda-forge\ngxx_linux-64              10.4.0              h6e491c6_10    conda-forge\nharfbuzz                  4.3.0                hd55b92a_0\nhdf5                      1.12.1          mpi_mpich_h08b82f9_4    conda-forge\nicu                       58.2                 he6710b0_3\nimagecodecs               2021.8.26       py310hecf7e94_1\nimageio                   2.19.3          py310h06a4308_0\njinja2                    3.0.3              pyhd3eb1b0_0\njpeg                      9e                   h7f8727e_0\njxrlib                    1.1                  h7b6447c_2\nkernel-headers_linux-64   3.10.0              h4a8ded7_13    conda-forge\nkeyutils                  1.6.1                h166bdaf_0    conda-forge\nkiwisolver                1.4.2           py310h295c915_0\nkrb5                      1.19.3               h3790be6_0    conda-forge\nlatexcodec                2.0.1              pyh9f0ad1d_0    conda-forge\nlcms2                     2.12                 h3be6417_0\nld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge\nlerc                      3.0                  h295c915_0\nlibaec                    1.0.4                he6710b0_1\nlibblas                   3.9.0           15_linux64_openblas    conda-forge\nlibbrotlicommon           1.0.9                h5eee18b_7\nlibbrotlidec              1.0.9                h5eee18b_7\nlibbrotlienc              1.0.9                h5eee18b_7\nlibcblas                  3.9.0           15_linux64_openblas    conda-forge\nlibclang                  10.0.1          default_hb85057a_2\nlibclang-cpp12            12.0.1          default_ha53f305_4    conda-forge\nlibcurl                   7.83.1               h7bff187_0    conda-forge\nlibdeflate                1.8                  h7f8727e_5\nlibdrm-cos7-x86_64        2.4.97            h9b0a68f_1105    conda-forge\nlibedit                   3.1.20191231         he28a2e2_2    conda-forge\nlibev                     4.33                 h516909a_1    conda-forge\nlibevent                  2.1.12               h8f2d780_0\nlibffi                    3.3                  he6710b0_2\nlibgcc-devel_linux-64     10.4.0              h74af60c_16    conda-forge\nlibgcc-ng                 12.1.0              h8d9b700_16    conda-forge\nlibgd                     2.3.3                h695aa2c_1\nlibgfortran-ng            12.1.0              h69a702a_16    conda-forge\nlibgfortran5              12.1.0              hdcd56e2_16    conda-forge\nlibglu                    9.0.0             he1b5a44_1001    conda-forge\nlibglvnd-cos7-x86_64      1.0.1             h9b0a68f_1105    conda-forge\nlibglvnd-glx-cos7-x86_64  1.0.1             h9b0a68f_1105    conda-forge\nlibgomp                   12.1.0              h8d9b700_16    conda-forge\nlibice-cos7-x86_64        1.0.9             h9b0a68f_1105    conda-forge\nlibice-devel-cos7-x86_64  1.0.9             h9b0a68f_1105    conda-forge\nliblapack                 3.9.0           15_linux64_openblas    conda-forge\nlibllvm10                 10.0.1               hbcb73fb_5\nlibllvm12                 12.0.1               hf817b99_2    conda-forge\nlibnghttp2                1.47.0               hdcd2b5c_1    conda-forge\nlibnsl                    2.0.0                h7f98852_0    conda-forge\nlibopenblas               0.3.20               h043d6bf_1\nlibpng                    1.6.37               hbc83047_0\nlibpq                     12.9                 h16c4e8d_3\nlibrsvg                   2.54.4               h19fe530_0\nlibsanitizer              10.4.0              hde28e3b_16    conda-forge\nlibsm-cos7-x86_64         1.2.2             h9b0a68f_1105    conda-forge\nlibsm-devel-cos7-x86_64   1.2.2             h9b0a68f_1105    conda-forge\nlibssh2                   1.10.0               haa6b8db_3    conda-forge\nlibstdcxx-devel_linux-64  10.4.0              h74af60c_16    conda-forge\nlibstdcxx-ng              12.1.0              ha89aaad_16    conda-forge\nlibtiff                   4.4.0                hecacb30_0\nlibtool                   2.4.6             h9c3ff4c_1008    conda-forge\nlibuuid                   1.0.3                h7f8727e_2\nlibuv                     1.44.2               h166bdaf_0    conda-forge\nlibwebp                   1.2.2                h55f646e_0\nlibwebp-base              1.2.2                h7f8727e_0\nlibx11-common-cos7-x86_64 1.6.7             h9b0a68f_1105    conda-forge\nlibx11-cos7-x86_64        1.6.7             h9b0a68f_1105    conda-forge\nlibx11-devel-cos7-x86_64  1.6.7             h9b0a68f_1105    conda-forge\nlibxcb                    1.15                 h7f8727e_0\nlibxext-cos7-x86_64       1.3.3             h9b0a68f_1105    conda-forge\nlibxext-devel-cos7-x86_64 1.3.3             h9b0a68f_1105    conda-forge\nlibxkbcommon              1.0.1                hfa300c1_0\nlibxml2                   2.9.14               h74e7548_0\nlibxslt                   1.1.35               h4e12654_0\nlibxt-cos7-x86_64         1.1.5             h9b0a68f_1105    conda-forge\nlibxt-devel-cos7-x86_64   1.1.5             h9b0a68f_1105    conda-forge\nlibzlib                   1.2.12               h166bdaf_2    conda-forge\nlibzopfli                 1.0.3                he6710b0_0\nlivereload                2.6.3              pyh9f0ad1d_0    conda-forge\nlocket                    1.0.0           py310h06a4308_0\nlxml                      4.9.1           py310h1edc446_0\nlz4-c                     1.9.3                h295c915_1\nm4                        1.4.18            h516909a_1001    conda-forge\nmake                      4.3                  hd18ef5c_1    conda-forge\nmako                      1.1.4              pyhd3eb1b0_0\nmarkupsafe                2.1.1           py310h7f8727e_0\nmatplotlib                3.5.2           py310h06a4308_0\nmatplotlib-base           3.5.2           py310hf590b9c_0\nmesa-khr-devel-cos7-x86_64 18.3.4            h9b0a68f_1105    conda-forge\nmesa-libgl-cos7-x86_64    18.3.4            h9b0a68f_1105    conda-forge\nmesa-libgl-devel-cos7-x86_64 18.3.4            h9b0a68f_1105    conda-forge\nmesa-libglapi-cos7-x86_64 18.3.4            h9b0a68f_1105    conda-forge\nmesalib                   18.3.1               h590aaf7_0    conda-forge\nmock                      4.0.3              pyhd3eb1b0_0\nmoose-libmesh             2022.09.09              build_0    https://conda.software.inl.gov/public\nmoose-libmesh-vtk         9.1.0               h94e2b84_12    https://conda.software.inl.gov/public\nmoose-mpich               4.0.2                   build_2    https://conda.software.inl.gov/public\nmoose-petsc               3.16.6                  build_1    https://conda.software.inl.gov/public\nmoose-tools               2022.07.18      py310haa4b876_0    https://conda.software.inl.gov/public\nmpc                       1.1.0                h10f8cd9_1\nmpfr                      4.0.2                hb69a4c5_1\nmpi                       1.0                       mpich\nmpich                     4.0.2              h846660c_100    conda-forge\nmpich-mpicc               4.0.2              hb600da9_100    conda-forge\nmpich-mpicxx              4.0.2              h166bdaf_100    conda-forge\nmpich-mpifort             4.0.2              h924138e_100    conda-forge\nmpmath                    1.2.1                    pypi_0    pypi\nmunkres                   1.1.4                      py_0\nncurses                   6.3                  h27087fc_1    conda-forge\nnetworkx                  2.8.4           py310h06a4308_0\nninja                     1.10.2               h06a4308_5\nninja-base                1.10.2               hd09550d_5\nnspr                      4.33                 h295c915_0\nnss                       3.74                 h0370c37_0\nnumexpr                   2.8.3           py310h757a811_0\nnumpy                     1.23.1          py310hac523dd_0\nnumpy-base                1.23.1          py310h375b286_0\nopenjpeg                  2.4.0                h3ad879b_0\nopenssl                   1.1.1q               h166bdaf_0    conda-forge\nordered-set               4.1.0              pyhd8ed1ab_0    conda-forge\npackaging                 21.3               pyhd3eb1b0_0\npandas                    1.4.4           py310h6a678d5_0\npango                     1.50.7               h05da053_0\npartd                     1.2.0              pyhd3eb1b0_1\npcre                      8.45                 h295c915_0\nperl                      5.32.1          2_h7f98852_perl5    conda-forge\npillow                    9.2.0           py310hace64e9_1\npip                       22.2.2          py310h06a4308_0\npixman                    0.40.0               h7f8727e_1\npkg-config                0.29.2               h1bed415_8\nply                       3.11            py310h06a4308_0\npybtex                    0.24.0             pyhd8ed1ab_2    conda-forge\npylatexenc                2.10               pyhd8ed1ab_0    conda-forge\npyparsing                 3.0.9           py310h06a4308_0\npyqt                      5.15.7          py310h6a678d5_1\npyqt5-sip                 12.11.0                  pypi_0    pypi\npython                    3.10.4               h12debd9_0\npython-dateutil           2.8.2              pyhd3eb1b0_0\npython_abi                3.10                    2_cp310    conda-forge\npytz                      2022.1          py310h06a4308_0\npywavelets                1.3.0           py310h7f8727e_0\npyyaml                    6.0                      pypi_0    pypi\nqt-main                   5.15.2               h327a75a_7\nqt-webengine              5.15.9               hd2b0992_4\nqtwebkit                  5.212                h4eab89a_4\nreadline                  8.1.2                h7f8727e_1\nrhash                     1.4.3                h166bdaf_0    conda-forge\nscikit-image              0.19.2          py310h00e6091_0\nscipy                     1.9.1           py310hdfbd76f_0    conda-forge\nsetuptools                59.8.0          py310hff52083_1    conda-forge\nsip                       6.6.2           py310h6a678d5_0\nsix                       1.16.0             pyhd3eb1b0_1\nsnappy                    1.1.9                h295c915_0\nsoupsieve                 2.3.1              pyhd3eb1b0_0\nsqlite                    3.39.3               h5082296_0\nsympy                     1.10.1          py310h06a4308_0\nsysroot_linux-64          2.17                h4a8ded7_13    conda-forge\ntifffile                  2021.7.2           pyhd3eb1b0_2\ntk                        8.6.12               h1ccaba5_0\ntoml                      0.10.2             pyhd3eb1b0_0\ntoolz                     0.11.2             pyhd3eb1b0_0\ntornado                   6.2             py310h5eee18b_0\ntzdata                    2022c                h04d1e81_0\nwheel                     0.37.1             pyhd3eb1b0_0\nxorg-x11-proto-devel-cos7-x86_64 2018.4            h9b0a68f_1105    conda-forge\nxz                        5.2.6                h166bdaf_0    conda-forge\nyaml                      0.2.5                h7b6447c_0\nzfp                       0.5.5                h295c915_6\nzlib                      1.2.12               h166bdaf_2    conda-forge\nzstd                      1.5.2                h6239696_4    conda-forge",
          "url": "https://github.com/idaholab/moose/discussions/22911",
          "updatedAt": "2022-12-10T06:48:09Z",
          "publishedAt": "2022-10-02T14:07:29Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@muh-hassani sorry we missed this. We monitor the Discussions for build issues, not the issues.\nYou likely missed activating the moose environment before compiling",
                  "url": "https://github.com/idaholab/moose/discussions/22911#discussioncomment-4337343",
                  "updatedAt": "2022-12-07T21:17:49Z",
                  "publishedAt": "2022-12-07T21:17:48Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Nodal field question",
          "author": {
            "login": "jessecarterMOOSE"
          },
          "bodyText": "Just a general finite element question, but maybe specifically for the MOOSE implementation.\nLet's say I have an auxiliary field variable, like temperature, defined at nodes. In a finite deformation mechanics model, do the values of my field variables interpolated to the quadrature points change/update on nonlinear iterations? The nodal values are not changing but the displacements, and therefore the shape of the element, can change as the step converges.",
          "url": "https://github.com/idaholab/moose/discussions/22885",
          "updatedAt": "2022-12-07T14:51:51Z",
          "publishedAt": "2022-12-06T00:16:34Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "In MOOSE/LibMesh, Nodal variables and their interpolants do not change with mesh deformation (on the same tessellation). Their gradients do.\nIn a more general setting of FEM, interpolants could change if you use some less common parametric mapping for element coordinates. But I've never seen that in MOOSE.",
                  "url": "https://github.com/idaholab/moose/discussions/22885#discussioncomment-4322898",
                  "updatedAt": "2022-12-06T12:56:34Z",
                  "publishedAt": "2022-12-06T12:56:33Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "ok, so shouldn't matter if I'm using displaced or undisplaed mesh to get the interpolated values at the qp's?",
                          "url": "https://github.com/idaholab/moose/discussions/22885#discussioncomment-4325835",
                          "updatedAt": "2022-12-06T18:17:36Z",
                          "publishedAt": "2022-12-06T18:17:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Yep. It doesn't matter.",
                          "url": "https://github.com/idaholab/moose/discussions/22885#discussioncomment-4330040",
                          "updatedAt": "2022-12-07T07:08:04Z",
                          "publishedAt": "2022-12-07T07:07:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "\ud83d\udc4d",
                          "url": "https://github.com/idaholab/moose/discussions/22885#discussioncomment-4333833",
                          "updatedAt": "2022-12-07T14:51:38Z",
                          "publishedAt": "2022-12-07T14:51:38Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Why are the inelastic models inactivated after cracking in smeared cracking model?",
          "author": {
            "login": "xuxiaobei1995"
          },
          "bodyText": "Hi all,\nRecently I'm doing some work concerning the cracking. I read the codes in ComputeSmearedCrackingStress and find that the inelastic models are inactivated after the cracking happens. Does any one know the reason of this operation? How can I do to keep the inelastic models activated after cracking?\nBest Regards,\nXiaobei",
          "url": "https://github.com/idaholab/moose/discussions/22894",
          "updatedAt": "2022-12-07T07:48:26Z",
          "publishedAt": "2022-12-06T13:54:45Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@bwspenc I think you wrote this object",
                  "url": "https://github.com/idaholab/moose/discussions/22894#discussioncomment-4324077",
                  "updatedAt": "2022-12-06T15:11:04Z",
                  "publishedAt": "2022-12-06T15:11:03Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Recently I'm doing some work concerning the cracking. I read the codes in ComputeSmearedCrackingStress and find that the inelastic models are inactivated after the cracking happens. Does any one know the reason of this operation?\n\nThere are many types of inelastic(plastic)-damage models:\nE-D\nE-D-P\nE-D-PD\nE-P-D\nE-P-PD\nwhere E, P, D, PD divide the state of the material into 4 categories\nE: purely elastic phase, little damage growth, no plastic flow\nP: purely plastic phase, mostly plastic deformation, small elastic deformation, little damage growth\nD: purely softening phase, a lot of damage growth, little plastic flow\nPD: a mix of plastic flow and damage softening\nWe happen to have implemented the E-P-D type, that's it. I believe there were good reasons for that choice.\n\nHow can I do to keep the inelastic models activated after cracking?\n\nNot that I know from an input file, no. But that ComputeMultipleInelasticStress or its alike is a complicated beast, there might be some magic parameter that allows you to do that.\nIf you are willing to modify the source, yes.",
                  "url": "https://github.com/idaholab/moose/discussions/22894#discussioncomment-4330124",
                  "updatedAt": "2022-12-07T07:20:55Z",
                  "publishedAt": "2022-12-07T07:20:54Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "xuxiaobei1995"
                          },
                          "bodyText": "Hi hugary, In fact I found that the smeared cracking may not be suitable for my work. Anyway, thanks for your explanation!",
                          "url": "https://github.com/idaholab/moose/discussions/22894#discussioncomment-4330340",
                          "updatedAt": "2022-12-07T07:48:20Z",
                          "publishedAt": "2022-12-07T07:48:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "[PorousFlow] why a small amount of rainfall flooded the model (PorousFlowSink)",
          "author": {
            "login": "Traiwit"
          },
          "bodyText": "Hi guys, I'm working on the underground water modelling and want to add the rainfall into the model\nso here is a simple model (1km x 1km x 0.5 km 20^3 mesh) with 15% porosity and perm of 1e-15 m2\nhere are the BCs: basically (rho g h) at the 4 sides + rain at the top\nand the amount of rain basically is from a quick google search\n\"Australia has a very low annual average rainfall of 419 mm\"\nso 419 mm year-1 = 419* 1e-3 m3 m-2 year-1 = 419 kg m-2 year -1\nnote that my time unit is in YEARS\n  [./ini_pp]\n    type = ParsedFunction\n    value  = 9.81*1000*(500-z)\n  [../]\n\n     [./water_grad_left]\n       type = FunctionDirichletBC\n       variable = porepressure\n       boundary = 'left'\n       function = ini_pp\n       # save_in = fluxes_out\n     [../]\n\n     [./water_grad_right]\n       type = FunctionDirichletBC\n       variable = porepressure\n       boundary = 'right'\n       function = ini_pp\n        # save_in = fluxes_out\n     [../]\n\n     [./water_grad_top]\n       type = FunctionDirichletBC\n       variable = porepressure\n       boundary = 'top'\n       function = ini_pp\n        # save_in = fluxes_out\n     [../]\n\n     [./water_grad_bottom]\n       type = FunctionDirichletBC\n       variable = porepressure\n       boundary = 'bottom'\n       function = ini_pp\n        # save_in = fluxes_out\n     [../]\n\n          [rainfall_recharge]\n             type = PorousFlowSink\n             fluid_phase = 0\n             boundary = 'front'\n             variable = porepressure\n             flux_function = - 419\n             save_in = fluxes_out\n           []\n\nsimulation is run in steady-state, and here is the result:\n\nbut then when I change the perm to unrealistically high (1e-12 m2), this is what I got + the streamline move in a totally different way\n\nfeel like with an extremely high perm case more water is able to move out from the system (from top to bottom of the sides), but in a normal groundwater perm case, not all the water can move out from the system (streamline at the sides is minimal).\nnot sure if my PorousFlowSink flux_function is correct\n@WilkAndy I'm tagging you here since you are the porousflow expert\nKind regards,\nTraiwit",
          "url": "https://github.com/idaholab/moose/discussions/22867",
          "updatedAt": "2022-12-10T07:04:56Z",
          "publishedAt": "2022-12-05T05:07:07Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "Hi @Traiwit - there is nothing wrong with your BCs if you believe that 419mm/year is entering the groundwater system.  However, that is likely unphysical.  Recently, we quantified recharge to the Australia's groundwater system - have a look at https://doi.org/10.25919/waky-s739 .   There you will see values much less than 419mm/year.  I don't know what area you're looking at, but a value of 10mm/year is more likely.\nThe reason this is a lot smaller than the rainfall is that most water ends up as runoff, or is evapotranspirated.  If you are doing any reasonably realistic groundwater modelling, your model should likely include regional groundwater flow (can be set by using appropriate Dirichlet BCs, for instance) as well as evapotranspiration (i would use https://mooseframework.inl.gov/source/bcs/PorousFlowHalfCubicSink.html , and you can find estimates of the \"pan evaporation\" online - eg at the BOM http://www.bom.gov.au/climate/change/about/evap_averagemaps.shtml ).",
                  "url": "https://github.com/idaholab/moose/discussions/22867#discussioncomment-4311828",
                  "updatedAt": "2022-12-05T10:26:45Z",
                  "publishedAt": "2022-12-05T10:26:44Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Thanks for the answer @WilkAndy, if I understand correctly, IF i want to use only PorousFlowSink I need to consider: rainfall + evaporation and flow at the surface altogether right? and that would be 'surface recharge' data instead of just 'rainfall'",
                          "url": "https://github.com/idaholab/moose/discussions/22867#discussioncomment-4319239",
                          "updatedAt": "2022-12-06T03:35:46Z",
                          "publishedAt": "2022-12-06T03:35:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "Yes, that is true, @Traiwit , but your model will likely quite rapidly fill up, if only your \"side\" boundary conditions are letting water out.  Also, it will likely not be possible if your topography is non-horizontal, as you will have natural seep points at the bottom of valleys, where you should let water out by using a PorousFlowPiecewiseLinearSink.",
                          "url": "https://github.com/idaholab/moose/discussions/22867#discussioncomment-4320404",
                          "updatedAt": "2022-12-06T07:33:29Z",
                          "publishedAt": "2022-12-06T07:33:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi @WilkAndy\nNot sure how do I set PorousFlowPiecewiseLinearSink to the side to let water out WHILE maintaining the rho x g x h\nfor our real models, we define pwp at all the nodes on 4 sides including the mountain range from CSV > PiecewiseConstantFromCSV > BC.\n\n\nAlso nice that you brought up the non-horizontal topol, lets me explain the actual real model we are working on, it might not be related to what you mentioned, but you may find it interesting:\nYes, most of our topols are not completely flat, for example:\n\nwhere is orange surface is the pit with drainage BC\nand the topol BC just set so the water table is 10m under the top of the system\n[./drain]\ntype = PorousFlowPiecewiseLinearSinkBEH #exactly the same as PorousFlowPiecewiseLinearSink just add 1/element length to the equation\nvariable = porepressure\nboundary = 'DRAINSRFPIT'\npt_vals = '0 1e9'\nmultipliers = '0 1e9'\nflux_function = 1\nv = min_dist\nPT_shift = 0\nfluid_phase = 0\nuse_mobility = true\n[../]\n\n[./topol_linear]\ntype = PorousFlowPiecewiseLinearSink\nvariable = porepressure\nboundary = 'TOPOLSRF'\npt_vals = '-1e9 1e9'\nmultipliers = '-1e9 1e9'\nflux_function = 1\nv = min_dist\nPT_shift = -98100 #10m below surface\nfluid_phase = 0\n[../]\n\nIt has been working fine just like that, BUT for this particular problem/mine it has a cave area where the perm is high compared to the rest of the model\n\nhere is the result if I use the PorousFlowPiecewiseLinearSink for topol\n\nHence we are trying to use PorousFlowSink with real data to deal with this problem instead. I tried flux_function = -10 just to test it out, here is the result: (the system is drained too much but at the edge it still flooded with water)\n\n\nAt this current stage, I still don't know what to do, might try mixing PorousFlowPiecewiseLinearSink and PorousFlowSink to balance them out.\nPlease let me know if you have any idea on how to deal with this problem, otherwise, it's okay I find it quite overwhelming too.\nThank you!\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/22867#discussioncomment-4320774",
                          "updatedAt": "2022-12-06T08:30:52Z",
                          "publishedAt": "2022-12-06T08:30:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "I only meant applying the PorousFlowPiecewiseLinearSink on the top of the model, not on its sides.  So, just like you've done with your drain (but over the entire topography, not just the pit).\nLet's discuss your topol_linear.... If the water table ever drops below 10m below the topography, you are adding water to the system, and if it is ever above this point, you're substracting water.   Is this really what you want?\nUsually, you'd have:\n\na constant recharge acting over the entire topography (PorousFlowSink with negative flux_function - of course this could vary spatially and temporally, but constant is clearly easier to parameterise!)\nseepage - this is just your drain but acting over the entire topography, which will remove water from any places that it accumulates\nevapotranspiration everywhere on the topography (PorousFlowHalfCubicSink)\n\nThese will control the porepressure so that is almost nowhere positive, and sits close to the root depth of the evapotranspiration in many places, and can be quite negative near the tops of hills.\nBy the way, one reason the half-cubic or half-gaussian are advantageous numerically, is because they are smooth and differentiable, unlike the straightforward PieceiwseLinearSink.  Also, the use_mobility might give you better understanding of what to use for flux_function, and the use_relperm might provide quicker numerical convergence.",
                          "url": "https://github.com/idaholab/moose/discussions/22867#discussioncomment-4321057",
                          "updatedAt": "2022-12-06T09:09:07Z",
                          "publishedAt": "2022-12-06T09:09:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "By the way, feel free to ring/email, @Traiwit .   I'd prefer to sort it out on this github discussion, because then future people can learn from it, but sometimes phonecalls are much more efficient!",
                          "url": "https://github.com/idaholab/moose/discussions/22867#discussioncomment-4321089",
                          "updatedAt": "2022-12-06T09:11:37Z",
                          "publishedAt": "2022-12-06T09:11:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Thanks @WilkAndy, I will try to run more scenarios to gain more understanding of these BCs (mixing 2 or all of them), and then I will probs contact you tomorrow (Note that I'm in Sydney/AU time-zone)\nNote that we are running the problem in FullySat flow model for simplicity, which might give me less realistic results.\nThank you\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/22867#discussioncomment-4321173",
                          "updatedAt": "2022-12-06T09:23:02Z",
                          "publishedAt": "2022-12-06T09:23:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi @WilkAndy here is some interesting finding:\nNot sure if you see above, drain and topol BCs have different pt_vals and multipliers\ndrain\npt_vals = '0 1e9'\nmultipliers = '0 1e9'\n\ntopol\npt_vals = '-1e9 1e9'\nmultipliers = '-1e9 1e9'\n\nSo rain4 and rain6 are pretty much the results we want, it seems like PorousFlowSink with flux_function = -10 doesn't do much\nNow I think I should narrow down to the controlling of the flux_function of topol, I don't have a specific value for it yet, I just know for drain I want the surface to be dry (flux_function as high as possible while the simulation still converges, right now 1 is good for all cases so far), However, for topol I've been using flux_function = 1 for the none-cave models (working fine so far), but yeh still need to figure out for this case.\n\nRegards,\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/22867#discussioncomment-4329190",
                          "updatedAt": "2022-12-07T04:02:33Z",
                          "publishedAt": "2022-12-07T04:02:32Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to output the assigned area to each node for integrated BCs?",
          "author": {
            "login": "garciapintado"
          },
          "bodyText": "As I understand, when saved (save_in=True), the values from integrated boundary condition objects (e.g. NeumannBC) give the flux assigned to each boundary node in the considered sideset, where each node is assigned an area.\nPlease, is there any direct way (which I am then missing) to output the nodal area assigned to each  of these BC nodes so one could get the flux per unit area?\nThanks!",
          "url": "https://github.com/idaholab/moose/discussions/22878",
          "updatedAt": "2022-12-06T21:07:40Z",
          "publishedAt": "2022-12-05T18:00:23Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "I think the area around the node is baked into _JxW[0] for a kernel acting on a nodal variable.\nIt should be similar for a flux BC\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4318908",
                  "updatedAt": "2022-12-06T02:23:27Z",
                  "publishedAt": "2022-12-06T02:23:26Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "garciapintado"
                  },
                  "bodyText": "Thanks for answer, @GiudGiud . Sorry that I am not getting this.  As I've understood, I should create a Boundary restricted Nodal AuxKernel object (e.g., let\u2019s call it NodalAreaBC), inheriting from Auxkernel, and with computeValue as:\nReal\nNodalAreaBC::computeValue()\n{\n  if (isNodal())\n      return _JxW[0];\n  else\n      return _JxW[_qp];\n}\n\nWhich obviously is wrong as it does not work. That is, for an elemental auxiliary variable variable it outputs something proportional to the element size, but not quite the element area, and for a nodal auxiliary variable, as follows, it directly gives a segmentation error.\n[AuxVariables]\n  [node_area]\n    family = LAGRANGE\n    order = FIRST\n    #family = MONOMIAL\n    #order = CONSTANT\n  []\n[]\n\n[AuxKernels]\n  [node_area]\n    type = NodalAreaBC\n    variable = node_area\n    boundary = top\n  []\n[]\n\nCould you please advise further?",
                  "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4325232",
                  "updatedAt": "2022-12-06T18:09:09Z",
                  "publishedAt": "2022-12-06T17:17:19Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The behavior with the elemental variable is expected.\nDo you need this output in an auxiliary variable? Or can you use it on the fly in a calculation?\nWithout digging into it, I dont think the auxkernel will have the local nodal area computed at any point. I would expect boundary conditions (which will not allow you to output the quantity) to have it",
                          "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4325501",
                          "updatedAt": "2022-12-06T17:41:03Z",
                          "publishedAt": "2022-12-06T17:41:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "garciapintado"
                          },
                          "bodyText": "The point is that if I have an integratedBC for energy flux, whose output is, integrated for each node [J.s-1] through a boundary. This is nice as I can directly use also a postprocessor to get the nodal sum, and so the integrated energy flux [J.s-1] across all the boundary as a global value.\nSo far so good, but on the other hand, the element size along this boundary [with a rather high spatial variability] implies that plotting of this flux, as it is a product of the flux per unit area [J.m-2.s-1] ---what I would like to see in plots--- times the assigned nodal size [m2], is not so nice.\nThe only way I can think of doing this is by adding an Auxkernel, which would be calculated just once within MOOSE, indicating the corresponding nodal areas for the element boundaries. I could then later [out of MOOSE] divide the integratedBC saved_in value by the nodal areas.\nMaybe I'm missing an alternative?",
                          "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4325968",
                          "updatedAt": "2022-12-06T18:33:57Z",
                          "publishedAt": "2022-12-06T18:33:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Can you divide the integrated flux by the total surface area instead of the local nodal area?\nThe total area of a sideset is easy to get with a surface postprocessor",
                          "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4325995",
                          "updatedAt": "2022-12-06T18:36:45Z",
                          "publishedAt": "2022-12-06T18:36:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "garciapintado"
                          },
                          "bodyText": "Well, no. That would give the average flux intensity [J.m-2.s-1], but not the spatial distribution of the intensity (of course, the same units [J.m-2.s-1]).",
                          "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4326031",
                          "updatedAt": "2022-12-06T18:42:16Z",
                          "publishedAt": "2022-12-06T18:42:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "garciapintado"
                          },
                          "bodyText": "I've been for a few days trying to see if I was missing some MOOSE option for this. A \"promising\" object name was the auxkernel NodalVolumePD, from the peridynamics module , but it seems to be only for PD mesh objects.",
                          "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4326082",
                          "updatedAt": "2022-12-06T18:48:40Z",
                          "publishedAt": "2022-12-06T18:48:39Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "maxnezdyur"
                  },
                  "bodyText": "There is an user object called NodalArea. It may be what you are looking for.",
                  "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4326270",
                  "updatedAt": "2022-12-06T19:10:27Z",
                  "publishedAt": "2022-12-06T19:10:09Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "garciapintado"
                          },
                          "bodyText": "Right! In the contact module. It does not appear  listed in the Moose documentation Syntax Index, but it does within the documentation source list. I had completely missed it. I've just tried and works as expected! Many thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/22878#discussioncomment-4327016",
                          "updatedAt": "2022-12-06T21:07:36Z",
                          "publishedAt": "2022-12-06T21:07:35Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PorousFlow: constant field of energy source",
          "author": {
            "login": "garciapintado"
          },
          "bodyText": "Hello,\nI am wondering if in PorousFlow there is someway to use the available kernels to add a constant field [spatially distributed] of heat as source term for the energy continuity equation. I can\u2019t see how\u2026",
          "url": "https://github.com/idaholab/moose/discussions/22896",
          "updatedAt": "2022-12-06T19:33:29Z",
          "publishedAt": "2022-12-06T17:36:25Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "garciapintado"
                  },
                  "bodyText": "Please forget this discussion, I see that HeatSource kernel from the Heat conduction module can be used for this purpose",
                  "url": "https://github.com/idaholab/moose/discussions/22896#discussioncomment-4326433",
                  "updatedAt": "2022-12-06T19:33:25Z",
                  "publishedAt": "2022-12-06T19:33:25Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "A conjugate heat transfer problem about TH module.",
          "author": {
            "login": "Raven-pro"
          },
          "bodyText": "Hello developers!\nI am using the TH module to establish a conjugate heat transfer system where there is a 1D flow channel at the center, and there are two heat structures on each side.\nThe structure of the heat structure is the fuel as a heat source inside with cladding outside.\nSo the process is, the heat source generates heat, and the heat is conducted through the cladding, the flow takes away heat as its temperature increases.\nWhen the simulation reaches a steady state, the temperature distribution should be, the heat source having the highest temperature, and near the wall, the temperature is lower.\nHowever, the result is different from what it is supposed to be. I visualized it in the ParaView as below.\n\nSo, for each heat structure, the heat distribution is identical: the highest temperature on the left and the lowest on the right\nI have also attached my input file below. Can anybody help me with dealing with such a problem?\nT_in = 533.15            # K\n# m_dot_in = 1.3565      # kg/s\nm_dot_in = 0.96805       # kg/s\npress = 15.5e6           # Pa\n\n# plate channels parameters (x,y,z)\nplate_height = 0.75      # m\nplate_width = 0.0762     # m\nplate_thick = 0.00038    # m\n\n# core parameters\ncore_n_elems = 100\ncore_gap = 0.00228       # m\n\ntot_power = 2e9          # W/m3\n\n[GlobalParams]\n    initial_p = ${press}\n    initial_vel = 7\n    initial_T = ${T_in}\n    rdg_slope_reconstruction = full\n    closures = simple_closures\n    fp = water\n[]\n\n# [Functions]\n#     [power_func]\n#         type = ParsedFunction\n#         value = '1.570796326794897 * sin(z / 3.6576 * pi)'\n#     []\n# []\n\n[Functions]\n    [HeatFunction]\n      type = ParsedFunction\n      value = ${tot_power}\n    []\n[]\n\n# [AuxVariables]\n#     [power_density]\n#       family = MONOMIAL\n#       order = CONSTANT\n#       block = 'core_hs1:FUEL1 core_hs2:FUEL2'\n#     []\n# []\n\n# [AuxKernels]\n#     [mock_power_aux]\n#       type = FunctionAux\n#       variable = power_density\n#       function = HeatFunction\n#       block = 'core_hs1:FUEL1 core_hs2:FUEL2'\n#     []\n# []\n\n[Modules]\n    [FluidProperties]\n        [water]\n            type = StiffenedGasFluidProperties\n            gamma = 2.35\n            cv = 1816.0\n            q = -1.167e6\n            p_inf = 1.0e9\n            q_prime = 0\n        []\n    []\n[]\n\n[SolidProperties]\n    [cladding]\n        type = ThermalFunctionSolidProperties\n        rho = 2700.0\n        cp = 897\n        k = '131.25+t/8.33e2'\n    []\n\n    [fuel]\n        type = ThermalFunctionSolidProperties\n        rho = 6030.0\n        cp = '(0.892+0.00046*t-0.71*(0.749+0.00038*t))*1e3'\n        k = 50.7\n    []\n[]\n\n\n[Closures]\n    [simple_closures]\n        type = Closures1PhaseSimple\n    []\n[]\n########################Structure Material Prop#################\n[Materials]\n    [fuel-mat]\n      type = ADConstantDensityThermalSolidPropertiesMaterial\n      block = 'core_hs1:FUEL1 core_hs2:FUEL2'\n      sp = fuel\n      temperature = T_solid\n      T_ref = 300\n    []\n  \n    # [gap-mat]\n    #   type = ADGenericConstantMaterial\n    #   block = 'core_hs:GAP1 core_hs:GAP2'\n    #   prop_names = 'thermal_conductivity specific_heat density'\n    #   prop_values = '1.084498 1.0 1.0'\n    # []\n  \n    [clad-mat]\n      type = ADConstantDensityThermalSolidPropertiesMaterial\n      block = 'core_hs1:CLAD1 core_hs2:CLAD2'\n      sp = cladding\n      temperature = T_solid\n      T_ref = 300\n    []\n[]\n\n[Components]\n########################Single Phase Flow#######################\n\n############################channel 1###########################\n    [inlet1]\n        type = InletMassFlowRateTemperature1Phase\n        input = 'core_chan1:in'\n        m_dot = ${m_dot_in}\n        T = ${T_in}\n    []\n\n    [core_chan1]\n        type = FlowChannel1Phase\n        position = '0 0 0'\n        orientation = '0 0 1'\n        length = ${plate_height}\n        n_elems = ${core_n_elems}\n        A = ${fparse plate_width * core_gap}\n        D_h = ${fparse 2 * plate_width * core_gap/(plate_width + core_gap)}\n        f = 0.16\n    []\n\n    [outlet1]\n        type = Outlet1Phase\n        input = 'core_chan1:out'\n        p = ${press}\n    []\n########################Heat Structure##########################\n\n############################Core HS1############################\n    [total_power1]\n        type = TotalPower\n        power = ${fparse tot_power * plate_height * plate_thick * plate_width}\n    []\n    [core_hs1]\n        type = HeatStructureCylindrical\n        position = '0 0 0'           \n        orientation = '0 0 1'\n        length = ${plate_height}\n        n_elems = ${core_n_elems}\n\n        names = 'CLAD1 FUEL1'       # pay attention to block\n        widths = '0.00038 0.00038'\n        # materials = 'steel'\n        n_part_elems = '10 10'      # radial elements\n        initial_T = 564.15\n    []\n\n    [core_heating1]\n        type = HeatSourceFromTotalPower\n        hs = core_hs1\n        regions = 'FUEL1'\n        power = total_power1\n        # power_shape_function = power_func\n    []\n\n    # [core_heating1]\n    #     type = HeatSourceFromPowerDensity\n    #     hs = core_hs1\n    #     regions = 'FUEL1'\n    #     # power_density = power_density\n    #     power = total_power\n    # []\n\n    [core_ht1]\n        type = HeatTransferFromHeatStructure1Phase\n        flow_channel = core_chan1\n        hs = core_hs1\n        hs_side = outer\n        P_hf = ${plate_width}\n        Hw = 50000\n    []\n\n############################Core HS2############################\n    [total_power2]\n        type = TotalPower\n        power = ${fparse tot_power * plate_height * plate_thick * plate_width}\n    []\n    [core_hs2]\n        type = HeatStructureCylindrical\n        position = '0 -0.00076 0'           \n        orientation = '0 0 1'\n        length = ${plate_height}\n        n_elems = ${core_n_elems}\n\n        names = 'CLAD2 FUEL2'       # pay attention to block\n        widths = '0.00038 0.00038'\n        # materials = 'steel'\n        n_part_elems = '10 10'      # radial elements\n        initial_T = 564.15\n    []\n\n    [core_heating2]\n        type = HeatSourceFromTotalPower\n        hs = core_hs2\n        regions = 'FUEL2'\n        power = total_power2\n        # power_shape_function = power_func\n    []\n\n    # [core_heating2]\n    #     type = HeatSourceFromPowerDensity\n    #     hs = core_hs2\n    #     regions = 'FUEL2'\n    #     # power_density = power_density\n    #     power = total_power\n    # []\n\n    [core_ht2]\n        type = HeatTransferFromHeatStructure1Phase\n        flow_channel = core_chan1\n        hs = core_hs2\n        hs_side = outer\n        P_hf = ${plate_width}\n        Hw = 50000\n    []\n########################End#####################################\n[]\n\n[Executioner]\n    type = Transient\n    start_time = 0\n    end_time = 100000\n    dt = 10000\n  \n    line_search = basic\n    solve_type = PJFNK\n  \n    nl_rel_tol = 1e-5\n    nl_abs_tol = 1e-5\n    nl_max_its = 5\n[]\n  \n[Outputs]\n    exodus = true\n  \n    [console]\n      type = Console\n      max_rows = 1\n      outlier_variable_norms = false\n    []\n    print_linear_residuals = false\n[]",
          "url": "https://github.com/idaholab/moose/discussions/22843",
          "updatedAt": "2022-12-06T14:24:06Z",
          "publishedAt": "2022-12-01T12:53:15Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou need to pass different sides in hs_side for the two heat transfer components I think.\nFrom the docs https://mooseframework.inl.gov/modules/thermal_hydraulics/source/components/HeatTransferFromHeatStructure1Phase.html\nYou have these options:\nEND, INNER, OUTER, START\nI think you want one to be 'inner' and the other to be outer.\nAm I understanding this right that one cylinder is the inner cylinder and the other the outer cylinder?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4287897",
                  "updatedAt": "2022-12-01T23:49:39Z",
                  "publishedAt": "2022-12-01T23:49:24Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Raven-pro"
                          },
                          "bodyText": "Thanks, I tried to switch hs_side to inner, but the following warning came out.\n\nI don't know what inner and outer mean here. I want the heat to transfer from left to right in one cylinder and from right to left in the other cylinder, as the flow channel is at the center.\nBut the result seems that the heat can only transfer from left to right.",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4288739",
                          "updatedAt": "2022-12-02T02:42:41Z",
                          "publishedAt": "2022-12-02T02:42:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Raven-pro"
                          },
                          "bodyText": "Am I sitting something wrong in my input file?",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4288746",
                          "updatedAt": "2022-12-02T02:44:13Z",
                          "publishedAt": "2022-12-02T02:44:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so the flow is between two cylinders that are NOT concentric? do you have a sketch of what the channel looks like?",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4289397",
                          "updatedAt": "2022-12-02T04:34:25Z",
                          "publishedAt": "2022-12-02T04:34:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "as to what the meaning of inner and outer are, these structures are cylinders, so it's the inner/outer surfaces of a cylinder that has a hole inside if inner_radius > 0",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4289449",
                          "updatedAt": "2022-12-02T04:47:15Z",
                          "publishedAt": "2022-12-02T04:47:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Raven-pro"
                          },
                          "bodyText": "Yes, the cylinders are not concentric, they are parallel with the flow in the middle. The sketch is as below.\n\nCan TH module achieve such conjugate heat transfer?",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4289502",
                          "updatedAt": "2022-12-02T05:00:24Z",
                          "publishedAt": "2022-12-02T05:00:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "it can\nI think something is wrong with your positioning of each structure. They overlap and touch and it seems wrong\n\none heat structure is at y = -0.0076 and is 0.0076 * 2 wide so it touches y=0\nthe channel is at y=0\nthe other heat structure is at y=0 so it overlaps with the channel and it s also 0.0076*2 wide so it overlaps with the other heat structure.\n\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4293535",
                          "updatedAt": "2022-12-02T15:31:29Z",
                          "publishedAt": "2022-12-02T15:31:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Raven-pro"
                          },
                          "bodyText": "I don't get it, I thought the first heat structure is 0.0076 wide(2 * 0.0038).\nBecause the channel is at y = 0, and the left edge of the second structure just touches the channel.\nThe left edge of the first structure is -0.0076, so the right edge of it touches the channel as well.\nI don't think there is overlapping. Did I miss something?",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4297415",
                          "updatedAt": "2022-12-03T04:36:59Z",
                          "publishedAt": "2022-12-03T04:36:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The plot you showed seems to show the two heat structures are touching, when you want the space for the channel in between?",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4297434",
                          "updatedAt": "2022-12-03T04:53:54Z",
                          "publishedAt": "2022-12-03T04:53:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Raven-pro"
                          },
                          "bodyText": "The channel(the blue part) is 1D, and is in the middle of two heat structures(the red parts are fuels, and the orange parts are claddings).\nAs the channel is 1D, and I thought the heat structures should touch the channel(I don't know if that's true), so the right side of the left hs and the left side of the right hs touch the same flow channel, and it seems they touch each other.",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4297524",
                          "updatedAt": "2022-12-03T05:52:07Z",
                          "publishedAt": "2022-12-03T05:52:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I can see that in the sketch. In the exodus output you pasted first, is every component where it should be?\nThis just looks like an output issue at this point. We only output the RZ part of the cylinder, and the one you want to see in the figure is the symmetric with regards to the Z axis of this RZ figure.",
                          "url": "https://github.com/idaholab/moose/discussions/22843#discussioncomment-4299418",
                          "updatedAt": "2022-12-03T15:50:04Z",
                          "publishedAt": "2022-12-03T15:50:04Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}