{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wMS0zMVQxMTo0MjoyMi0wNjowMM4AXfaa"
    },
    "edges": [
      {
        "node": {
          "title": "How to specify serial computation",
          "author": {
            "login": "Ating24"
          },
          "bodyText": "I am using MOOSE's polynomial chaos for training on a cluster, calculating 256 deterministic samples. Using more kernels will keep initializing the equation, while using fewer kernels will exceed memory or cause the following errors\uff1a\nsrun:First task exited 60s ago\nsrun:StepId=128229.0 task 3:running\nsrun :StepId=128229.0 tasks 0-2:exited\nsrun:launch/slurm:step signal:Terminating StepId=128229.0\nsrun :srun: Job step aborted: Waiting up to 32 seconds for job step to finish.130131\nsrun: error: ion171:task 3:Killed\nwhen calculating the second step. Running the main input file alone can run it. Is there a way to abandon multiple apps and execute it in serial order? Or how to group the training sampler and specify the number of kernels for a single example",
          "url": "https://github.com/idaholab/moose/discussions/26729",
          "updatedAt": "2024-02-04T02:24:35Z",
          "publishedAt": "2024-02-03T16:31:40Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou can limit the number of concurrent apps using the min/max procs per app parameters.\nIf you still want only one processor to run with several assigned, then use a singleRankPartitioner in the Paritioning block to force it back to one processor\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8355893",
                  "updatedAt": "2024-02-03T16:43:05Z",
                  "publishedAt": "2024-02-03T16:43:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "Hello Under which module is it used? I tried using it and it seems to have no effect. Also, the MOOSE on the cluster is an early version, and there was no Reporters module at that time",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8355993",
                          "updatedAt": "2024-02-03T16:58:29Z",
                          "publishedAt": "2024-02-03T16:58:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "Does setting the number of threads have any impact on the task",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8356087",
                          "updatedAt": "2024-02-03T17:16:48Z",
                          "publishedAt": "2024-02-03T17:16:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Using more threads will duplicate most moose objects but overall will cost less memory than distrivuted MPI parallelism because the mesh and the field variables won't be duplicated",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8356119",
                          "updatedAt": "2024-02-03T17:20:20Z",
                          "publishedAt": "2024-02-03T17:20:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "What is the relationship between the number of threads used and the number of kernels?",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8356130",
                          "updatedAt": "2024-02-03T17:22:19Z",
                          "publishedAt": "2024-02-03T17:22:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "I can run a single example in 1 hour with 80 kernels now. I want to run all 256 examples in series using 80 kernels. Do I set mode=batch - reset in the mutiapps module and then set min/max procs per app parameters in the samplers module",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8356155",
                          "updatedAt": "2024-02-03T17:26:24Z",
                          "publishedAt": "2024-02-03T17:26:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "For example, how many threads would be suitable for me to set using 80 kernels?",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8356267",
                          "updatedAt": "2024-02-03T17:50:29Z",
                          "publishedAt": "2024-02-03T17:50:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "min/max procs per app\n\nis in the MultiApps parameters. If you use a FullSolveSamplerMultiApp, you can see the parameters here\nhttps://mooseframework.inl.gov/source/multiapps/SamplerFullSolveMultiApp.html\nI think there's a misunderstanding on what a 'kernel' is here. What does it mean for you?",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8357930",
                          "updatedAt": "2024-02-04T01:13:40Z",
                          "publishedAt": "2024-02-04T01:13:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "grmnptr"
                          },
                          "bodyText": "You have to specify a matching max/min-procs-per-row on the Sampler too.",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8357938",
                          "updatedAt": "2024-02-04T01:15:26Z",
                          "publishedAt": "2024-02-04T01:15:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "Is it like this?\n[Samplers]\n\t[Quadrature]\n\t\ttype = Quadrature\n\t\torder = 2 \n\t\tdistributions = 'K_dist density_dist cp_dist fluxcoff_dist'\n\t\texecute_on- PRE_MULTIAPF_SETUP\n\t\tmin_procs_per_row = 60\n\t[]\n[]\n[MultiApps]\n\t[roshan]\n\t\ttype =SamplerFullSolveMultiApp\n\t\tinput_files = file.i\n\t\tsampler =Quadraturemode - batch-reset\n\t\tmin_procs_per_app = 60\n\t[]\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8358034",
                          "updatedAt": "2024-02-04T01:47:15Z",
                          "publishedAt": "2024-02-04T01:47:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "But the warning displays\uff1a\npc_uniform.i:33: unused parameter 'Samplers/Quadrature/min_procs_per_row'\npc_uniform.i:33: unused parameter 'MultiApps/roshan/min_procs_per_app'",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8358044",
                          "updatedAt": "2024-02-04T01:51:29Z",
                          "publishedAt": "2024-02-04T01:51:28Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "grmnptr"
                  },
                  "bodyText": "Also, make sure you use the batch-reset mode in combination with your min/max procs per app settings. Here is a short description on the gains:\nhttps://mooseframework.inl.gov/modules/stochastic_tools/batch_mode.html\nThe number of kernels would not influence your memory consumption too much unless you take it to the extreme.",
                  "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8355940",
                  "updatedAt": "2024-02-03T16:58:23Z",
                  "publishedAt": "2024-02-03T16:49:11Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "I can run a single example in 1 hour with 80 kernels now. I want to run all 256 examples in series using 80 kernels. Do I set mode=batch - reset in the mutiapps module and then set min/max procs per app parameters in the samplers module",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8356186",
                          "updatedAt": "2024-02-03T17:33:20Z",
                          "publishedAt": "2024-02-03T17:33:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "This version does not have the reprotters module, how to use min/max procs per app parameters",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8357904",
                          "updatedAt": "2024-02-04T00:59:26Z",
                          "publishedAt": "2024-02-04T00:59:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ating24"
                          },
                          "bodyText": "I took a look at the version of SamplerFullSolveMultiApp being used C did not mention min/max_ Procs_ Per_ App and min/max_ Procs_ Per_ Row",
                          "url": "https://github.com/idaholab/moose/discussions/26729#discussioncomment-8358067",
                          "updatedAt": "2024-02-04T02:00:02Z",
                          "publishedAt": "2024-02-04T02:00:02Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Help with understanding how to setup Dynamics Module",
          "author": {
            "login": "richmondodufisan"
          },
          "bodyText": "Hi,\nI read through the Dynamics module page and think I understand it to some extent but I don't think I have it right as I am getting this error message:\n*** ERROR ***\nMooseVariableFE: Old time derivative of solution (`u_dot_old`) is not stored. Please set uDotOldRequested() to true in FEProblemBase before requesting `u_dot_old`.\n\nI am currently trying to implement a simple cantilever beam subjected to a heaviside loading on the free end. First with no damping, and then eventually damping. After that I would like to add more features (plasticity, viscoelasticity, etc). I already implemented the simplest case (no damping) in MATLAB for a 1-D bar so I would like to confirm my results here. I'm using a 2D bar just so I can be able to visualize the traveling wave.\nFor the case with no damping, I attached my input file below:\nno_damp.zip\n[Mesh]\n  type = FileMesh\n  file = beam.msh\n  use_displaced_mesh = false\n[]\n\n[GlobalParams]\n  volumetric_locking_correction = false\n  use_displaced_mesh = false\n  displacements = 'disp_x disp_y'  \n  order = FIRST\n  family = LAGRANGE\n[]\n\n[Variables]\n  [disp_x]\n  []\n  \n  [disp_y]\n  []\n[]\n\n[Kernels]\n  [stress_x]\n\ttype = DynamicStressDivergenceTensors\n    variable = disp_x\n    component = 0\n\tzeta = 0\n  []\n  [stress_y]\n\ttype = DynamicStressDivergenceTensors\n    variable = disp_y\n    component = 1\n\tzeta = 0\n  []\n  [inertia_x]\n    type = InertialForce\n    variable = disp_x\n\tdensity = density\n\teta = 0\n  []\n  [inertia_y]\n    type = InertialForce\n    variable = disp_y\n\tdensity = density\n\teta = 0\n  []\n[]\n\n[BCs]\n  [right_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = fixed_end\n    value = 0.0\n  []\n  [right_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = fixed_end\n    value = 0.0\n  []\n  [left_x]\n\ttype = FunctionNeumannBC\n\tvariable = disp_x\n\tfunction = pressure\n\tboundary = loading_end\n  []\n[]\n\n[Materials]\n  [elast_tensor]\n\ttype = ComputeIsotropicElasticityTensor\n\tyoungs_modulus = 210e3\n\tpoissons_ratio = 0.499\n  []\n  [strain]\n    type = ComputeSmallStrain\n  []\n  [stress]\n    type = ComputeLinearElasticStress\n  []\n  [density]\n    type = GenericConstantMaterial\n    prop_names = 'density'\n    prop_values = '7.85e-6' # in kg/mm3\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = NEWTON\n  nl_rel_tol = 1e-8\n  nl_abs_tol = 1e-8\n  dtmin = 1e-4\n  timestep_tolerance = 1e-6\n  start_time = -0.005\n  end_time = 3750e-05\n  dt = 2.445598573141631e-05\n[]\n\n[Functions]\n  [pressure]\n    type = ADParsedFunction\n    expression = 'if(t<0,0.0,250)' # Heaviside function in MPa i.e N/mm2\n  []\n[]\n\n[Outputs]\n  exodus = true\n  perf_graph = true\n[]\n\n-used DynamicStressDivergenceTensors for the stiffness term and used InertialForce for the Mass term. Also set both zeta and eta to zero explicitly. I am not sure what else is missing/why I am getting this error. I attached the input file and mesh below.\nEDIT:\nI \"fixed\" the issue by adding\n  [TimeIntegrator]\n    type = NewmarkBeta\n    beta = 0.25\n    gamma = 0.5\n  []\n\nTo the executioner block. So apparently I needed to specify it directly. The documentation page says the transient problem is solved with HHT or Newmark which I'm guessing are the optimal ones to use. Would it be possible to use, say, Explicit Euler? That would involve trying to reconstruct the whole formulation, right?\nIn addition to this, I have a few more questions:\n-How can I create an aux variable so I can see the stress distribution along the beam at the end of the simulation?\n-Also, if I want to track the velocity and acceleration, I see that NewmarkAccelAux was used in other input files. How does that work? Does it automatically know how to calculate it through some relationship with DynamicStressDivergenceTensors?\n-In 1-D, I don't need to worry about Poisson's ratio, but here I needed to set it to 0.499 (i.e nearly incompressible) to get the traveling wave solution, otherwise I got a really badly deformed mesh. I noticed while the simulation was running that it needed several iterations to solve. I have read about nearly incompressible solutions usually being problematic and less accurate (resolved through methods like the B-bar method). Is that a problem here? Because the next thing I want to do is to try to adjust the material properties so I can recreate the 1-D MATLAB solution.\nBelow is a screenshot of part of the solution:\n\nthe _disp value travels back and forth exactly like it did in 1D",
          "url": "https://github.com/idaholab/moose/discussions/26724",
          "updatedAt": "2024-02-04T01:22:56Z",
          "publishedAt": "2024-02-02T23:07:19Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIt s possible to use other time integrators but you need to modify them a little bit to request the uDotOld()\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26724#discussioncomment-8352754",
                  "updatedAt": "2024-02-03T05:00:20Z",
                  "publishedAt": "2024-02-03T05:00:20Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "I see, thanks.\nAlso, to get the stress components as an aux variable (based on the loading I really only expect there to be xx stress), would it be:\n  [stress_xx]\n    order = CONSTANT\n    family = MONOMIAL\n  [xx]\n\nin the AuxVariables block and\n  [stress_xx]\n    type = RankTwoAux\n    variable = stress_xx\n    rank_two_tensor = stress\n    index_j = 0\n    index_i = 0\n    execute_on = timestep_end\n  []\n\nin the AuxKernels block? For stresses computed by ComputeLinearElasticStress, will the material property name ALWAYS be \"stress\"? What happens when I include plasticity? Will \"stress\" return the updated value including plasticity effects?\n\nAs for the calculation of velocity and acceleration, which block actually does it? Is it the AuxKernels block i.e\n  [./accel_x]\n    type = NewmarkAccelAux\n    variable = accel_x\n    displacement = disp_x\n    velocity = vel_x\n    beta = 0.422\n    execute_on = timestep_end\n  [../]\n\nor the Kernels block? i.e\n  [./inertia_x]\n    type = InertialForce\n    variable = disp_x\n    velocity = vel_x\n    acceleration = accel_x\n    beta = 0.422\n    gamma = 0.8\n    eta=0.1\n    alpha = -0.3\n  [../]\n\nIf the calculations are done in the AuxKernels block, why do we need to specify it in the Kernels block still? Also, how would I modify the whole script to use HHT Time Integration if I chose that instead?\n\nAre there any checks implemented to prevent it from underpredicting the solution due to locking with poission's ratio = 0.499? Any fixes other than needing an incredibly fine mesh?",
                          "url": "https://github.com/idaholab/moose/discussions/26724#discussioncomment-8353000",
                          "updatedAt": "2024-02-03T06:41:09Z",
                          "publishedAt": "2024-02-03T06:32:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nFor the first one, you can check the name of the material properties defined using Debug / show_material_props\nThe AuxKernel computes the quantity. The kernel consumes the acceleration variable.\nI don't know about HHT. I imagine you ll want to use the same strategy as for Newmark, computing quantities on the side then having custom kernels that form the discretized equation\nFor the last question, I think there s a help page on volumetric locking in the module documentation",
                          "url": "https://github.com/idaholab/moose/discussions/26724#discussioncomment-8355871",
                          "updatedAt": "2024-02-03T16:39:19Z",
                          "publishedAt": "2024-02-03T16:39:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Thank you\nFor the middle question, why does the Kernel need to consume the acceleration & velocity variables if it can solve without it? What is it consuming it for? Those variables can be computed completely with just the displacements at every timestep (which is done in AuxKernel), can't they? So why still send to the Kernel?",
                          "url": "https://github.com/idaholab/moose/discussions/26724#discussioncomment-8357349",
                          "updatedAt": "2024-02-03T21:33:58Z",
                          "publishedAt": "2024-02-03T21:33:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "why does the Kernel need to consume the acceleration & velocity variables if it can solve without it?\n\nwhat makes you think it can solve without it? If you open the InertialForce.C file on your machine you can see what it does with those variables.\nBeing able to compute them everywhere does not mean we should recompute them everywhere. Sometimes it s faster to compute them once and re-use the result in several places",
                          "url": "https://github.com/idaholab/moose/discussions/26724#discussioncomment-8357959",
                          "updatedAt": "2024-02-04T01:22:56Z",
                          "publishedAt": "2024-02-04T01:22:56Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Curvature calculation for surfacetension force.",
          "author": {
            "login": "MashrurShejan"
          },
          "bodyText": "Hello, I am trying to incorporate curvature calculation in My surface tension force kernel which is giving me error currently. Here is the kernel code below-\n#include \"SurfaceTensionForce.h\"\n\nregisterMooseObject(\"NavierStokesApp\", SurfaceTensionForce);\n\nInputParameters SurfaceTensionForce::validParams()\n{\n  InputParameters params = ADKernel::validParams();\n  params.addClassDescription(\"ADKernel for computing surface tension force\");\n  params.addRequiredParam<MaterialPropertyName>(\"sigma\", \"The name of the surface tension coefficient material property.\");\n  params.addRequiredCoupledVar(\"alpha\", \"The name of the volume fraction variable.\");\n  return params;\n}\n\nSurfaceTensionForce::SurfaceTensionForce(const InputParameters & parameters) :\n  ADKernel(parameters),\n  _sigma(getADMaterialProperty<Real>(\"sigma\")),\n  _grad_alpha(adCoupledGradient(\"alpha\"))\n{\n}\n\nADReal SurfaceTensionForce::computeQpResidual()\n{\n  ADReal kappa = computeKappa();\n  ADReal sigma_interpolated = _sigma[_qp];\n  return sigma_interpolated * kappa * _grad_alpha[_qp] * _grad_test[_i][_qp];\n}\n\nADReal SurfaceTensionForce::computeKappa() const\n{\n  ADReal grad_alpha_mag = _grad_alpha[_qp].norm();\n\n  if (grad_alpha_mag > 1e-6) //To avoid deviding by zero\n  {\n    ADRealVectorValue norm_alpha = _grad_alpha[_qp] / grad_alpha_mag;\n\n    \n    ADReal kappa = 0.0;\n    for (unsigned int i = 0; i < LIBMESH_DIM; ++i)\n    {\n      // Placeholder \n      //kappa += _grad_norm_alpha[_qp][i]\n    }\n\n    return kappa;\n  }\n  else\n  {\n    return 0.0;\n  }\n}\n\nThe code compiles fine when the the kappa calculation is commented out. I am not sure what I am doing wrong. Can anyone review the code and let me know the issue? Please note that I am not very proficient with C++ or MOOSE Library.",
          "url": "https://github.com/idaholab/moose/discussions/26717",
          "updatedAt": "2024-02-03T06:20:41Z",
          "publishedAt": "2024-02-02T17:37:47Z",
          "category": {
            "name": "Q&A Modules: Navier-Stokes"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIf looks like it should not be grad_norm_alpha but norm_alpha\nThe variables don't have the right name.\nAlso to access a component of a vector you use (i) not [i]",
                  "url": "https://github.com/idaholab/moose/discussions/26717#discussioncomment-8349736",
                  "updatedAt": "2024-02-02T18:33:50Z",
                  "publishedAt": "2024-02-02T18:33:49Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "MashrurShejan"
                          },
                          "bodyText": "Hi! According to the curvature formula I need to calculate the divergence of the normalized vector alpha denoted by _grad_alpha[_qp] / grad_alpha_mag. I am not sure if kappa += norm_alpha[_qp] (i) is what it is supposed to be.",
                          "url": "https://github.com/idaholab/moose/discussions/26717#discussioncomment-8350771",
                          "updatedAt": "2024-02-02T20:56:11Z",
                          "publishedAt": "2024-02-02T20:56:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nsure but you did not define  _grad_norm_alpha anywhere before using it below\n      //kappa += _grad_norm_alpha[_qp][i]\n\nYou need to define it first.",
                          "url": "https://github.com/idaholab/moose/discussions/26717#discussioncomment-8352791",
                          "updatedAt": "2024-02-03T05:15:52Z",
                          "publishedAt": "2024-02-03T05:15:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "MashrurShejan"
                          },
                          "bodyText": "I was trying to calculate the gradient of norm alpha using _grad_norm_alpha and summing them together for different dimensions using  kappa+=_grad_norm_alpha(i) to calculate the divergence here. But I guess this is not how it will work. In that case,  what should be the expression to define the divergence of  norm_alpha here?",
                          "url": "https://github.com/idaholab/moose/discussions/26717#discussioncomment-8352962",
                          "updatedAt": "2024-02-03T06:20:42Z",
                          "publishedAt": "2024-02-03T06:20:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Not making on the demanded cores",
          "author": {
            "login": "AmbroiseJuston"
          },
          "bodyText": "Goof morning, I am running  program I know works on a new machine (48  physical cores, 96 total virtual cores). When running the make -j80 and then running the executable i get this output below:\n(moose) [moose@lprc-b72-112-fem xenon]$ make -j80\n(moose) [moose@lprc-b72-112-fem xenon]$ ./run_tests -j80\ntest:kernels/simple_diffusion.test ........................................................................ OK\n--------------------------------------------------------------------------------------------------------------\nRan 1 tests in 0.3 seconds. Average test time 0.2 seconds, maximum test time 0.2 seconds.\n1 passed, 0 skipped, 0 pending, 0 failed\n(moose) [moose@lprc-b72-112-fem xenon]$ make -j80\n(moose) [moose@lprc-b72-112-fem xenon]$ cd ~/projects/xenon\n(moose) [moose@lprc-b72-112-fem xenon]$ cd ~/projects/xenon/problems\n(moose) [moose@lprc-b72-112-fem problems]$ ls\nxenon_input.i\n(moose) [moose@lprc-b72-112-fem problems]$ ../xenon-opt -i xenon_input.i\nhwloc/linux: Ignoring PCI device with non-16bit domain.\nPass --enable-32bits-pci-domain to configure to support such devices\n(warning: it would break the library ABI, don't enable unless really needed).\nFramework Information:\nMOOSE Version:           git commit ed55c164c2 on 2024-01-17\nLibMesh Version:         \nPETSc Version:           3.20.1\nSLEPc Version:           3.20.0\nCurrent Time:            Fri Feb  2 10:28:18 2024\nExecutable Timestamp:    Fri Feb  2 10:14:01 2024\n\nParallelism:\n  Num Processors:          1\n  Num Threads:             1\n\nWhat i noticed was that it was specifying that only 1 processor and thread was being used for the program, when I obviously want 80, (which is what i though i specified). Any help on what is happening or if MOOSE differentiates between physical and virtual cores would be veryappreiciated. When running a stress test, i can see the cores being used so i know that they are available.",
          "url": "https://github.com/idaholab/moose/discussions/26719",
          "updatedAt": "2024-02-02T18:11:41Z",
          "publishedAt": "2024-02-02T18:01:18Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThere a few things you did there.\nFirst make -j80 correctly used 80 processes to build the code. It seems it was already built so it just skipped. But next time you start from afresh it will use those resources.\nThen ./run_tests -j80 allows for 80 tests to run in parallel. This is not the same as using 80 parallel processes to run each test. What you did is faster though, but we don't expect much gain between -j10 and -j80 because of limitations in the Python threading library.\nFinally you ran an input file in serial, with one process and one thread.\nIf you want to run that same input file in parallel, you can use\nmpirun -n 80 executable-opt -i inputfile.i",
                  "url": "https://github.com/idaholab/moose/discussions/26719#discussioncomment-8349558",
                  "updatedAt": "2024-02-02T18:11:39Z",
                  "publishedAt": "2024-02-02T18:11:38Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "In order to make your test use 80 cores, you'll need to modify that test to do so:\n[Tests]\n  [test]\n    type = 'Exodiff'\n    input = 'simple_diffusion.i'\n    exodiff = 'simple_diffusion_out.e'\n\n    issues = '#1493'\n    design = 'kernels/Diffusion.md'\n    requirement = 'The system shall run a simple 2D linear diffusion problem with Dirichlet boundary conditions on a regular mesh.'\n    min_parallel = 80\n  []\n[]\n\nmin_parallel = 80\nYou can also force the TestHarness to throw parallel options at your binary for every test with:\n./run_tests  --parallel 8 -v\n<trimmed>\nkernels/simple_diffusion.test:   Num Processors:          8\nkernels/simple_diffusion.test:   Num Threads:             1",
                  "url": "https://github.com/idaholab/moose/discussions/26719#discussioncomment-8349560",
                  "updatedAt": "2024-02-02T18:11:42Z",
                  "publishedAt": "2024-02-02T18:11:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Do _comp and _component mean same thing in INSMomentumBase.C?",
          "author": {
            "login": "Minjiang-Zhu"
          },
          "bodyText": "As title says.",
          "url": "https://github.com/idaholab/moose/discussions/26715",
          "updatedAt": "2024-02-01T22:57:14Z",
          "publishedAt": "2024-02-01T22:09:52Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "No they don't. _component indicates the momentum component we are building the residual/Jacobian rows for. comp is used when specifying which momentum component we are building the Jacobian columns for, so how does the momentum component _component depend on the momentum component comp. So you can think of _component denoting the row, and comp the column in a coupling matrix",
                  "url": "https://github.com/idaholab/moose/discussions/26715#discussioncomment-8340047",
                  "updatedAt": "2024-02-01T22:24:12Z",
                  "publishedAt": "2024-02-01T22:22:32Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Storing information for each element in mesh file",
          "author": {
            "login": "Sina-av"
          },
          "bodyText": "Hi there,\nI want to run a mechanical problem where each element of my mesh has a different Young's modulus.\nI.e., I want to have a mesh that can store the value of Young's modulus for each single element. Later I want to access all those values in my material file in Moose.\nI was wondering if there is a mesh format (or a way in general) to do so.\nI know that while using .e file there is the option for \"extra element integer\", but this option only stores an integer which might not be helpful for storing material parameters in cells.\nThank you so much.",
          "url": "https://github.com/idaholab/moose/discussions/26712",
          "updatedAt": "2024-02-01T19:23:08Z",
          "publishedAt": "2024-02-01T19:03:00Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nHow is this young modulus computed? Do you just have a massive CSV file?\nOr do you have an analytical expression?\nthere's plenty of options here for having an element-wise Young modulus\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26712#discussioncomment-8338584",
                  "updatedAt": "2024-02-01T19:05:57Z",
                  "publishedAt": "2024-02-01T19:05:55Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Sina-av"
                          },
                          "bodyText": "Hi Guillaume,\nAt this stage I want to have random Young's modulus for each element. I was thinking about using tools like GSTOOLs (https://github.com/GeoStat-Framework/GSTools), where an input mesh can be given and then a random field is added to that mesh.\nBut I am also looking into other methods too. So please let me know if you have any ideas in general.\nThank so much again.",
                          "url": "https://github.com/idaholab/moose/discussions/26712#discussioncomment-8338641",
                          "updatedAt": "2024-02-01T19:13:13Z",
                          "publishedAt": "2024-02-01T19:13:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "for a first try you can use a CSV file\nthen a propertyReadFile to read that file (there s a certain format in the csv)\nthen a PiecewiseFunctionFromCSV to get it into a field",
                          "url": "https://github.com/idaholab/moose/discussions/26712#discussioncomment-8338667",
                          "updatedAt": "2024-02-01T19:16:28Z",
                          "publishedAt": "2024-02-01T19:16:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Sina-av"
                          },
                          "bodyText": "thanks so much. I will try this and will keep you updated.\nBest.",
                          "url": "https://github.com/idaholab/moose/discussions/26712#discussioncomment-8338722",
                          "updatedAt": "2024-02-01T19:23:10Z",
                          "publishedAt": "2024-02-01T19:23:08Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Solving for a periodic variable",
          "author": {
            "login": "gj19866"
          },
          "bodyText": "Hi there,\nI am having to reconsider some of the methods I am using in my model.\nInstead of considering a complex variable $\\psi$ as $\\psi = \\psi_R + i \\psi_I$, I would like to consider it as the form $\\psi = |\\psi|e^{i\\chi}$.\nHere I would have to solve for the variables $|\\psi|$, and $\\chi$.\nBefore I dive into this fully, do you think that moose would be able to effectively solve for a variable $\\chi$ that exists in a range $(-\\pi, \\pi]$.\nI am worried about the discontinuity, and that $\\chi = \\chi +2\\pi$ in the solve.\nWhat are your thoughts about this?\nMany thanks,\nGillian",
          "url": "https://github.com/idaholab/moose/discussions/26707",
          "updatedAt": "2024-02-01T17:24:20Z",
          "publishedAt": "2024-02-01T12:46:24Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nI would be worried about two things:\n\nthe singularity near psi = 0 because of the absolute value\nthe need for something to pin the chi between 0 and 2 pi, the value could drift without changing the solution due to periodicity\n\nI would go ahead and implement it and find remedies for both if needed.\nI think the first one we can use the Bounds system\nFor the second one we can probably constrain the average of the variable chi over the domain\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26707#discussioncomment-8336979",
                  "updatedAt": "2024-02-01T16:30:08Z",
                  "publishedAt": "2024-02-01T16:30:06Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "gj19866"
                          },
                          "bodyText": "Amazing, Ill give it a shot and see what happens thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/26707#discussioncomment-8337386",
                          "updatedAt": "2024-02-01T17:02:26Z",
                          "publishedAt": "2024-02-01T17:02:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@cticenhour @aaelmeli if you have experience with working with complex variable in this (module, angle) form please let us know!",
                          "url": "https://github.com/idaholab/moose/discussions/26707#discussioncomment-8337511",
                          "updatedAt": "2024-02-01T17:11:20Z",
                          "publishedAt": "2024-02-01T17:11:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "I do not have experience working with this form of complex numbers in MOOSE. Your suggestions seem reasonable to start, @GiudGiud.",
                          "url": "https://github.com/idaholab/moose/discussions/26707#discussioncomment-8337683",
                          "updatedAt": "2024-02-01T17:24:21Z",
                          "publishedAt": "2024-02-01T17:24:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Hardly/Not converged Picard iteration",
          "author": {
            "login": "Bearunner"
          },
          "bodyText": "Dear all,\nI am using the [MultiApps], [Transfers] and Picard interation to simulate the 1D pipe flow and 3D porous media flow (i.e., porous_flow).\nI set the related information in [Executioner], [MultiApps] and [Transfers].  But the Picard iteration is hardly or not converged as shown in the following log. Even if I increase the fixed_point_rel_tol and fixed_point_abs_tol to 1e-5. It still the same.\n**[Executioner]**\n  type = Transient\n  start_time =  ${starting_time}\n  end_time = ${end_time}\n  dtmax = ${1_day}\n  l_tol = 1e-8\n  nl_rel_tol = 1e-8\n  nl_abs_tol = 1e-8\n  l_max_its = 100\n  nl_max_its = 100\n  solve_type = NEWTON\n  fixed_point_rel_tol = 1e-8\n  fixed_point_max_its = 100\n  fixed_point_abs_tol = 1e-8\n  disable_fixed_point_residual_norm_check = true\n  relaxation_factor = 0.1\n []\n\n**[MultiApps]**\n  [./pipe]\n    type = TransientMultiApp\n    input_files = pipe.i\n    positions = '0 -97.5 0'\n    library_path = /home/projects/pipe/lib\n    app_type = pipeApp\n    catch_up = true\n  [../]\n[]\n\n**[Transfers]**\n  [./to_pipe]\n    type = MultiAppGeneralFieldShapeEvaluationTransfer\n    to_multi_app = pipe\n    source_variable = porepressure\n    variable = pp_res\n    from_blocks = a\n    to_blocks = b\n    greedy_search = true\n    use_nearest_app = true\n    error_on_miss = true\n  [../]\n  [./from_pipe]\n    type = MultiAppGeneralFieldNearestNodeTransfer\n    from_multi_app = pipe\n    source_variable = p\n    variable = pp_pipe\n    from_blocks = c\n    to_blocks = d\n  [../]\n[]\n\nPart of the log:\n\ufffd[36mpipe_sub_inj0: \ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39mTime Step 30, time = 3227.17, dt = 646.235\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 0 Nonlinear |R| = \ufffd[32m1.393859e-01\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 1 Nonlinear |R| = \ufffd[32m1.626299e-04\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 2 Nonlinear |R| = \ufffd[32m1.105107e-04\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 3 Nonlinear |R| = \ufffd[32m1.040351e-07\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 4 Nonlinear |R| = \ufffd[32m1.076632e-09\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m\ufffd[32m Solve Converged!\ufffd[39m\n\n\ufffd[35mFixed point residual norm after TIMESTEP_BEGIN MultiApps: \ufffd[32m1.405005e-08\ufffd[39m\n\ufffd[35m\nMain app solve:\ufffd[39m\n 0 Nonlinear |R| = \ufffd[32m1.405005e-08\ufffd[39m\n 1 Nonlinear |R| = \ufffd[32m4.987454e-11\ufffd[39m\n\ufffd[32m Solve Converged!\ufffd[39m\n\n 0 Picard |R| = \ufffd[32m1.591909e+00\ufffd[39m\n 1 Picard |R| = \ufffd[33m1.591663e+00\ufffd[39m\n 2 Picard |R| = \ufffd[32m4.275689e-01\ufffd[39m\n 3 Picard |R| = \ufffd[32m2.974149e-01\ufffd[39m\n 4 Picard |R| = \ufffd[32m2.178361e-01\ufffd[39m\n 5 Picard |R| = \ufffd[32m1.633035e-01\ufffd[39m\n 6 Picard |R| = \ufffd[32m1.236165e-01\ufffd[39m\n 7 Picard |R| = \ufffd[32m9.395439e-02\ufffd[39m\n 8 Picard |R| = \ufffd[32m7.153569e-02\ufffd[39m\n 9 Picard |R| = \ufffd[32m5.450221e-02\ufffd[39m\n10 Picard |R| = \ufffd[32m4.154714e-02\ufffd[39m\n11 Picard |R| = \ufffd[32m3.167794e-02\ufffd[39m\n12 Picard |R| = \ufffd[32m2.415499e-02\ufffd[39m\n13 Picard |R| = \ufffd[32m1.842036e-02\ufffd[39m\n14 Picard |R| = \ufffd[32m1.404766e-02\ufffd[39m\n15 Picard |R| = \ufffd[32m1.071319e-02\ufffd[39m\n16 Picard |R| = \ufffd[32m8.201140e-03\ufffd[39m\n17 Picard |R| = \ufffd[32m6.254511e-03\ufffd[39m\n18 Picard |R| = \ufffd[32m4.769999e-03\ufffd[39m\n19 Picard |R| = \ufffd[32m3.637849e-03\ufffd[39m\n20 Picard |R| = \ufffd[32m2.774418e-03\ufffd[39m\n21 Picard |R| = \ufffd[32m2.115921e-03\ufffd[39m\n22 Picard |R| = \ufffd[32m1.613717e-03\ufffd[39m\n23 Picard |R| = \ufffd[32m1.230709e-03\ufffd[39m\n24 Picard |R| = \ufffd[32m9.386061e-04\ufffd[39m\n25 Picard |R| = \ufffd[32m7.158326e-04\ufffd[39m\n26 Picard |R| = \ufffd[32m5.459334e-04\ufffd[39m\n27 Picard |R| = \ufffd[32m4.163589e-04\ufffd[39m\n28 Picard |R| = \ufffd[32m3.175383e-04\ufffd[39m\n29 Picard |R| = \ufffd[32m2.421722e-04\ufffd[39m\n30 Picard |R| = \ufffd[32m1.846938e-04\ufffd[39m\n31 Picard |R| = \ufffd[32m1.408577e-04\ufffd[39m\n32 Picard |R| = \ufffd[32m1.074259e-04\ufffd[39m\n33 Picard |R| = \ufffd[32m8.192889e-05\ufffd[39m\n34 Picard |R| = \ufffd[32m6.248350e-05\ufffd[39m\n35 Picard |R| = \ufffd[32m4.765336e-05\ufffd[39m\n36 Picard |R| = \ufffd[32m3.634308e-05\ufffd[39m\n37 Picard |R| = \ufffd[32m2.771724e-05\ufffd[39m\n38 Picard |R| = \ufffd[32m2.113879e-05\ufffd[39m\n39 Picard |R| = \ufffd[32m1.612162e-05\ufffd[39m\n40 Picard |R| = \ufffd[32m1.229524e-05\ufffd[39m\n41 Picard |R| = \ufffd[32m9.377037e-06\ufffd[39m\n42 Picard |R| = \ufffd[32m7.151450e-06\ufffd[39m\n43 Picard |R| = \ufffd[32m5.454094e-06\ufffd[39m\n44 Picard |R| = \ufffd[32m4.159596e-06\ufffd[39m\n45 Picard |R| = \ufffd[32m3.172339e-06\ufffd[39m\n46 Picard |R| = \ufffd[32m2.419403e-06\ufffd[39m\n47 Picard |R| = \ufffd[32m1.844672e-06\ufffd[39m\n48 Picard |R| = \ufffd[32m1.406827e-06\ufffd[39m\n49 Picard |R| = \ufffd[32m1.072903e-06\ufffd[39m\n50 Picard |R| = \ufffd[32m8.182389e-07\ufffd[39m\n51 Picard |R| = \ufffd[32m6.240221e-07\ufffd[39m\n52 Picard |R| = \ufffd[32m4.759045e-07\ufffd[39m\n53 Picard |R| = \ufffd[32m3.629440e-07\ufffd[39m\n54 Picard |R| = \ufffd[32m2.767958e-07\ufffd[39m\n55 Picard |R| = \ufffd[32m2.110956e-07\ufffd[39m\n56 Picard |R| = \ufffd[32m1.609900e-07\ufffd[39m\n57 Picard |R| = \ufffd[32m1.227775e-07\ufffd[39m\n58 Picard |R| = \ufffd[32m9.363509e-08\ufffd[39m\n59 Picard |R| = \ufffd[32m7.140995e-08\ufffd[39m\n60 Picard |R| = \ufffd[32m5.446004e-08\ufffd[39m\n61 Picard |R| = \ufffd[32m4.153347e-08\ufffd[39m\n62 Picard |R| = \ufffd[32m3.167511e-08\ufffd[39m\n63 Picard |R| = \ufffd[32m2.415671e-08\ufffd[39m\n64 Picard |R| = \ufffd[32m1.842289e-08\ufffd[39m\n65 Picard |R| = \ufffd[32m1.405005e-08\ufffd[39m\n\nI also follow the first and third tips in #23102. It still the same.\n\n\nSet All execute_on to default (including which of [MultiApps] block).\nSet disable_fixed_point_residual_norm_check=true , use custom_pp and relax the convergence criteria a little\nSet relaxation_factor of main app (of [Executioner] block) to extremely small (around 0.1)\n\n\nCould you please help me on this? Thanks a lot.\nB",
          "url": "https://github.com/idaholab/moose/discussions/26697",
          "updatedAt": "2024-02-01T16:02:25Z",
          "publishedAt": "2024-01-31T00:32:51Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nSetting the   relaxation_factor to  0.1 is meant to dampen the iteration. It will help getting a solution if the coupling is not stable, but it will slow down convergence.\nWhat results do you get without relaxation?\n65 Picard |R| = \ufffd[32m1.405005e-08\ufffd[39m\n\nthis looks to be converging. Just slowly\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8313012",
                  "updatedAt": "2024-01-31T00:38:53Z",
                  "publishedAt": "2024-01-31T00:38:52Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Bearunner"
                          },
                          "bodyText": "Hello, Guillaume. Thanks for your reply.\nThere is no obvious difference when I don't set the 'relaxation_factor'.\nWith time, the picard iteration needs more than 100 times to converge. The dt is still in low value, not greatly increasing.",
                          "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8313460",
                          "updatedAt": "2024-01-31T12:12:55Z",
                          "publishedAt": "2024-01-31T01:03:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I think you need to specify which variables to transform to use relaxation btw. Use the transformed_variables parameter to do so",
                          "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8313474",
                          "updatedAt": "2024-01-31T01:05:35Z",
                          "publishedAt": "2024-01-31T01:05:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bearunner"
                          },
                          "bodyText": "Thanks for the tip.\nIf I add transformed_variables = 'porepressure' ' and don't add the 'relaxation_factor= 0.1', there is no obvious difference to the mentioned hardly converged Picard iteration.\nIf I set the 'relaxation_factor= 0.1' and 'transformed_variables = 'porepressure'' together, it cannot converged at 2nd step.\npipe:\npipe: Time Step 2, time = 2.25, dt = 1.25\npipe:  0 Nonlinear |R| = 5.005947e+01\npipe:  1 Nonlinear |R| = 5.602494e-01\npipe:  2 Nonlinear |R| = 2.560555e-04\npipe:  3 Nonlinear |R| = 2.643941e-07\npipe:  Solve Converged!\nFixed point residual norm after TIMESTEP_BEGIN MultiApps: 2.051577e-03\nMain app solve:\n0 Nonlinear |R| = 2.051577e-03\n1 Nonlinear |R| = 2.887538e-06\n2 Nonlinear |R| = 4.083398e-08\n3 Nonlinear |R| = 9.518876e-10\nSolve Converged!\n0 Picard |R| = 2.756204e+00\n1 Picard |R| = 9.648449e+00\n2 Picard |R| = 1.006712e+00\n3 Picard |R| = 9.097428e-01\n4 Picard |R| = 8.283195e-01\n5 Picard |R| = 7.541837e-01\n6 Picard |R| = 6.866832e-01\n7 Picard |R| = 6.252241e-01\n8 Picard |R| = 5.692657e-01\n9 Picard |R| = 5.183156e-01\n10 Picard |R| = 4.719257e-01\n11 Picard |R| = 4.296877e-01\n12 Picard |R| = 3.912301e-01\n13 Picard |R| = 3.562146e-01\n14 Picard |R| = 3.243329e-01\n15 Picard |R| = 2.953047e-01\n16 Picard |R| = 2.688746e-01\n17 Picard |R| = 2.448100e-01\n18 Picard |R| = 2.228992e-01\n19 Picard |R| = 2.029495e-01\n20 Picard |R| = 1.847853e-01\n21 Picard |R| = 1.682468e-01\n22 Picard |R| = 1.531886e-01\n23 Picard |R| = 1.394780e-01\n24 Picard |R| = 1.269946e-01\n25 Picard |R| = 1.156285e-01\n26 Picard |R| = 1.052796e-01\n27 Picard |R| = 9.585697e-02\n28 Picard |R| = 8.727768e-02\n29 Picard |R| = 7.946625e-02\n30 Picard |R| = 7.235394e-02\n31 Picard |R| = 6.587820e-02\n32 Picard |R| = 5.998204e-02\n33 Picard |R| = 5.461360e-02\n34 Picard |R| = 4.972564e-02\n35 Picard |R| = 4.527515e-02\n36 Picard |R| = 4.122299e-02\n37 Picard |R| = 3.753349e-02\n38 Picard |R| = 3.417422e-02\n39 Picard |R| = 3.111560e-02\n40 Picard |R| = 2.833073e-02\n41 Picard |R| = 2.579511e-02\n42 Picard |R| = 2.348642e-02\n43 Picard |R| = 2.138437e-02\n44 Picard |R| = 1.947046e-02\n45 Picard |R| = 1.772784e-02\n46 Picard |R| = 1.614118e-02\n47 Picard |R| = 1.469653e-02\n48 Picard |R| = 1.338118e-02\n49 Picard |R| = 1.218356e-02\n50 Picard |R| = 1.109312e-02\n51 Picard |R| = 1.010028e-02\n52 Picard |R| = 9.196299e-03\n53 Picard |R| = 8.373225e-03\n54 Picard |R| = 7.623816e-03\n55 Picard |R| = 6.941479e-03\n56 Picard |R| = 6.320213e-03\n57 Picard |R| = 5.754550e-03\n58 Picard |R| = 5.239514e-03\n59 Picard |R| = 4.770575e-03\n60 Picard |R| = 4.343605e-03\n61 Picard |R| = 3.954850e-03\n62 Picard |R| = 3.600889e-03\n63 Picard |R| = 3.278607e-03\n64 Picard |R| = 2.985170e-03\n65 Picard |R| = 2.717996e-03\n66 Picard |R| = 2.474734e-03\n67 Picard |R| = 2.253244e-03\n68 Picard |R| = 2.051577e-03",
                          "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8319178",
                          "updatedAt": "2024-01-31T12:17:43Z",
                          "publishedAt": "2024-01-31T12:17:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If I add transformed_variables = 'porepressure' ' and don't add the 'relaxation_factor= 0.1', there is no obvious difference to the mentioned hardly converged Picard iteration.\n\nthis is expected. There's no transformation if the relaxation factor is not specified.\n\nIf I set the 'relaxation_factor= 0.1' and 'transformed_variables = 'porepressure'' together, it cannot converged at 2nd step.\n\nit seems to be converging, just very slowly.\nCould you solve the multiapp and the main app in the same simulation? Right now you use Picard to solve the coupled problem. If they are in the same app you can use Newton or PJFNK",
                          "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8323257",
                          "updatedAt": "2024-01-31T16:43:40Z",
                          "publishedAt": "2024-01-31T16:43:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bearunner"
                          },
                          "bodyText": "Actually, we initially  solve the multiapp and the main app in the same simulation by merging 1D pipe flow (N-S eqations with pressure, flowrate \uff08and temperature\uff09 as main variables) and 3D porous media flow (similar to porous_flow module with pressure \uff08and temperature\uff09 as main variables) into one app.\nwe used the  Newton or PJFNK and tries the following [Preconditioning]s which were working well in standalone two apps seperately. However, all of them cannot converged. I guess the the [Preconditioning] cannot working well in this mixture-flow simulation. Could you please help me on this? Thanks.\n3D porous media flow\uff1a\n[Preconditioning]\n[basic]\ntype = SMP\nfull = true\npetsc_options = '-ksp_diagonal_scale -ksp_diagonal_scale_fix'\npetsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type -pc_asm_overlap'\npetsc_options_value = ' asm      lu           NONZERO                   2'\n[]\n[]\n1D pipe flow\uff1a\n[Preconditioning]\n[./basic]\ntype = SMP\nfull = true\npetsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type -snes_type -snes_linesearch_type'\npetsc_options_value = ' bjacobi  ilu          NONZERO                   newtonls   basic               '\n[../]\n[]\n0 Nonlinear |R| = 7.216034e+00\n|residual|_2 of individual variables:\nq: 3.34388\np: 6.3945\n0 Linear |R| = 7.216034e+00\n1 Linear |R| = 7.216034e+00\n2 Linear |R| = 7.216034e+00\n3 Linear |R| = 7.216034e+00\n4 Linear |R| = 7.216034e+00\n5 Linear |R| = 7.216034e+00\n6 Linear |R| = 7.216034e+00\n7 Linear |R| = 7.216034e+00\n8 Linear |R| = 7.216034e+00\n9 Linear |R| = 7.216034e+00\n10 Linear |R| = 7.216034e+00\n11 Linear |R| = 7.216034e+00\n12 Linear |R| = 7.216034e+00\n13 Linear |R| = 7.216034e+00\n14 Linear |R| = 7.216034e+00\n15 Linear |R| = 7.216034e+00\n16 Linear |R| = 7.216034e+00\n17 Linear |R| = 7.216034e+00\n18 Linear |R| = 7.216034e+00\n19 Linear |R| = 7.216034e+00\n20 Linear |R| = 7.216034e+00\n21 Linear |R| = 7.216034e+00\n22 Linear |R| = 7.216034e+00\n23 Linear |R| = 7.216034e+00\n24 Linear |R| = 7.216034e+00\n25 Linear |R| = 7.216034e+00\n26 Linear |R| = 7.216034e+00\n27 Linear |R| = 7.216034e+00\n28 Linear |R| = 7.216033e+00\n29 Linear |R| = 7.216033e+00\n30 Linear |R| = 7.216032e+00\n31 Linear |R| = 7.216032e+00\n32 Linear |R| = 7.216032e+00\n33 Linear |R| = 7.216032e+00\n34 Linear |R| = 7.216032e+00\n35 Linear |R| = 7.216032e+00\n36 Linear |R| = 7.216032e+00\n37 Linear |R| = 7.216032e+00\n38 Linear |R| = 7.216032e+00\n39 Linear |R| = 7.216032e+00\n40 Linear |R| = 7.216032e+00\n41 Linear |R| = 7.216032e+00\n42 Linear |R| = 7.216032e+00\n43 Linear |R| = 7.216032e+00\n44 Linear |R| = 7.216032e+00\n45 Linear |R| = 7.216032e+00\n46 Linear |R| = 7.216032e+00\n47 Linear |R| = 7.216032e+00\n48 Linear |R| = 7.216032e+00\n49 Linear |R| = 7.216032e+00\n50 Linear |R| = 7.216032e+00\nLinear solve did not converge due to DIVERGED_ITS iterations 50\n1 Nonlinear |R| = 7.216032e+00\n|residual|_2 of individual variables:\nq: 3.34388\np: 6.39449\n0 Linear |R| = 7.216032e+00\n1 Linear |R| = 7.216032e+00\n2 Linear |R| = 7.216032e+00\n3 Linear |R| = 7.216032e+00\n4 Linear |R| = 7.216032e+00\n5 Linear |R| = 7.216032e+00\n6 Linear |R| = 7.216032e+00\n7 Linear |R| = 7.216032e+00\n8 Linear |R| = 7.216032e+00\n9 Linear |R| = 7.216032e+00\n10 Linear |R| = 7.216032e+00\n11 Linear |R| = 7.216032e+00\n12 Linear |R| = 7.216032e+00\n13 Linear |R| = 7.216032e+00\n14 Linear |R| = 7.216032e+00\n15 Linear |R| = 7.216032e+00\n16 Linear |R| = 7.216032e+00\n17 Linear |R| = 7.216031e+00\n18 Linear |R| = 7.216031e+00\n19 Linear |R| = 7.216031e+00\n20 Linear |R| = 7.216030e+00\n21 Linear |R| = 7.216027e+00\n22 Linear |R| = 7.216022e+00\n23 Linear |R| = 7.216014e+00\n24 Linear |R| = 7.216005e+00\n25 Linear |R| = 7.215996e+00\n26 Linear |R| = 7.215991e+00\n27 Linear |R| = 7.215986e+00\n28 Linear |R| = 7.215985e+00\n29 Linear |R| = 7.215984e+00",
                          "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8325648",
                          "updatedAt": "2024-01-31T18:50:11Z",
                          "publishedAt": "2024-01-31T18:50:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "So initially you had them 'fully' coupled? with both solves in the same input file?\nWhat you are showing here is two preconditioning blocks for individual solves, as in when using multiapps.\nWhat is the convergence log for ? For the fully coupled single-input? Or for another solve?",
                          "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8329479",
                          "updatedAt": "2024-02-01T01:32:06Z",
                          "publishedAt": "2024-02-01T01:32:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bearunner"
                          },
                          "bodyText": "So initially you had them 'fully' coupled? with both solves in the same input file?\n\nYes, they were 'fully' coupled with both solves in the one input file.\n\nWhat you are showing here is two preconditioning blocks for individual solves, as in when using multiapps.\n\nYes. They worked well both in their standalone apps if there was no coupling. They also worked well in Picard interaction (i.e., multiapps) coupling.\nSo I tried these two [Preconditioning]s in the 'fully' coupled with both solves in the one input file. But all failed.\n\nWhat is the convergence log for ? For the fully coupled single-input? Or for another solve?\n\nConvergence log of 'fully' coupled with both solves in the one input file.  It looks like the [Preconditioning] did not work well.\n0 Nonlinear |R| = 7.216034e+00\n|residual|_2 of individual variables:\nq: 3.34388\np: 6.3945\n0 Linear |R| = 7.216034e+00\n1 Linear |R| = 7.216034e+00\n2 Linear |R| = 7.216034e+00\n3 Linear |R| = 7.216034e+00\n4 Linear |R| = 7.216034e+00\n5 Linear |R| = 7.216034e+00\n6 Linear |R| = 7.216034e+00\n7 Linear |R| = 7.216034e+00\n8 Linear |R| = 7.216034e+00\n9 Linear |R| = 7.216034e+00\n10 Linear |R| = 7.216034e+00\n11 Linear |R| = 7.216034e+00\n12 Linear |R| = 7.216034e+00\n13 Linear |R| = 7.216034e+00\n14 Linear |R| = 7.216034e+00\n15 Linear |R| = 7.216034e+00\n16 Linear |R| = 7.216034e+00\n17 Linear |R| = 7.216034e+00\n18 Linear |R| = 7.216034e+00\n19 Linear |R| = 7.216034e+00\n20 Linear |R| = 7.216034e+00\n21 Linear |R| = 7.216034e+00\n22 Linear |R| = 7.216034e+00\n23 Linear |R| = 7.216034e+00\n24 Linear |R| = 7.216034e+00\n25 Linear |R| = 7.216034e+00\n26 Linear |R| = 7.216034e+00\n27 Linear |R| = 7.216034e+00\n28 Linear |R| = 7.216033e+00\n29 Linear |R| = 7.216033e+00\n30 Linear |R| = 7.216032e+00\n31 Linear |R| = 7.216032e+00\n32 Linear |R| = 7.216032e+00\n33 Linear |R| = 7.216032e+00\n34 Linear |R| = 7.216032e+00\n35 Linear |R| = 7.216032e+00\n36 Linear |R| = 7.216032e+00\n37 Linear |R| = 7.216032e+00\n38 Linear |R| = 7.216032e+00\n39 Linear |R| = 7.216032e+00\n40 Linear |R| = 7.216032e+00\n41 Linear |R| = 7.216032e+00\n42 Linear |R| = 7.216032e+00\n43 Linear |R| = 7.216032e+00\n44 Linear |R| = 7.216032e+00\n45 Linear |R| = 7.216032e+00\n46 Linear |R| = 7.216032e+00\n47 Linear |R| = 7.216032e+00\n48 Linear |R| = 7.216032e+00\n49 Linear |R| = 7.216032e+00\n50 Linear |R| = 7.216032e+00\nLinear solve did not converge due to DIVERGED_ITS iterations 50\n1 Nonlinear |R| = 7.216032e+00\n|residual|_2 of individual variables:\nq: 3.34388\np: 6.39449\n0 Linear |R| = 7.216032e+00\n1 Linear |R| = 7.216032e+00\n2 Linear |R| = 7.216032e+00\n3 Linear |R| = 7.216032e+00\n4 Linear |R| = 7.216032e+00\n5 Linear |R| = 7.216032e+00\n6 Linear |R| = 7.216032e+00\n7 Linear |R| = 7.216032e+00\n8 Linear |R| = 7.216032e+00\n9 Linear |R| = 7.216032e+00\n10 Linear |R| = 7.216032e+00\n11 Linear |R| = 7.216032e+00\n12 Linear |R| = 7.216032e+00\n13 Linear |R| = 7.216032e+00\n14 Linear |R| = 7.216032e+00\n15 Linear |R| = 7.216032e+00\n16 Linear |R| = 7.216032e+00\n17 Linear |R| = 7.216031e+00\n18 Linear |R| = 7.216031e+00\n19 Linear |R| = 7.216031e+00\n20 Linear |R| = 7.216030e+00\n21 Linear |R| = 7.216027e+00\n22 Linear |R| = 7.216022e+00\n23 Linear |R| = 7.216014e+00\n24 Linear |R| = 7.216005e+00\n25 Linear |R| = 7.215996e+00\n26 Linear |R| = 7.215991e+00\n27 Linear |R| = 7.215986e+00\n28 Linear |R| = 7.215985e+00\n29 Linear |R| = 7.215984e+00\nConvergence log of 'Picard iteration/MultiApps' coupled with both solves in their standalone input file\n\ufffd[36mpipe_sub_inj0: \ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39mTime Step 30, time = 3227.17, dt = 646.235\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 0 Nonlinear |R| = \ufffd[32m1.393859e-01\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 1 Nonlinear |R| = \ufffd[32m1.626299e-04\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 2 Nonlinear |R| = \ufffd[32m1.105107e-04\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 3 Nonlinear |R| = \ufffd[32m1.040351e-07\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m 4 Nonlinear |R| = \ufffd[32m1.076632e-09\ufffd[39m\n\ufffd[36mpipe_sub_inj0: \ufffd[39m\ufffd[32m Solve Converged!\ufffd[39m\n\ufffd[35mFixed point residual norm after TIMESTEP_BEGIN MultiApps: \ufffd[32m1.405005e-08\ufffd[39m\n\ufffd[35m\nMain app solve:\ufffd[39m\n0 Nonlinear |R| = \ufffd[32m1.405005e-08\ufffd[39m\n1 Nonlinear |R| = \ufffd[32m4.987454e-11\ufffd[39m\n\ufffd[32m Solve Converged!\ufffd[39m\n0 Picard |R| = \ufffd[32m1.591909e+00\ufffd[39m\n1 Picard |R| = \ufffd[33m1.591663e+00\ufffd[39m\n2 Picard |R| = \ufffd[32m4.275689e-01\ufffd[39m\n3 Picard |R| = \ufffd[32m2.974149e-01\ufffd[39m\n4 Picard |R| = \ufffd[32m2.178361e-01\ufffd[39m\n5 Picard |R| = \ufffd[32m1.633035e-01\ufffd[39m\n6 Picard |R| = \ufffd[32m1.236165e-01\ufffd[39m\n7 Picard |R| = \ufffd[32m9.395439e-02\ufffd[39m\n8 Picard |R| = \ufffd[32m7.153569e-02\ufffd[39m\n9 Picard |R| = \ufffd[32m5.450221e-02\ufffd[39m\n10 Picard |R| = \ufffd[32m4.154714e-02\ufffd[39m\n11 Picard |R| = \ufffd[32m3.167794e-02\ufffd[39m\n12 Picard |R| = \ufffd[32m2.415499e-02\ufffd[39m\n13 Picard |R| = \ufffd[32m1.842036e-02\ufffd[39m\n14 Picard |R| = \ufffd[32m1.404766e-02\ufffd[39m\n15 Picard |R| = \ufffd[32m1.071319e-02\ufffd[39m\n16 Picard |R| = \ufffd[32m8.201140e-03\ufffd[39m\n17 Picard |R| = \ufffd[32m6.254511e-03\ufffd[39m\n18 Picard |R| = \ufffd[32m4.769999e-03\ufffd[39m\n19 Picard |R| = \ufffd[32m3.637849e-03\ufffd[39m\n20 Picard |R| = \ufffd[32m2.774418e-03\ufffd[39m\n21 Picard |R| = \ufffd[32m2.115921e-03\ufffd[39m\n22 Picard |R| = \ufffd[32m1.613717e-03\ufffd[39m\n23 Picard |R| = \ufffd[32m1.230709e-03\ufffd[39m\n24 Picard |R| = \ufffd[32m9.386061e-04\ufffd[39m\n25 Picard |R| = \ufffd[32m7.158326e-04\ufffd[39m\n26 Picard |R| = \ufffd[32m5.459334e-04\ufffd[39m\n27 Picard |R| = \ufffd[32m4.163589e-04\ufffd[39m\n28 Picard |R| = \ufffd[32m3.175383e-04\ufffd[39m\n29 Picard |R| = \ufffd[32m2.421722e-04\ufffd[39m\n30 Picard |R| = \ufffd[32m1.846938e-04\ufffd[39m\n31 Picard |R| = \ufffd[32m1.408577e-04\ufffd[39m\n32 Picard |R| = \ufffd[32m1.074259e-04\ufffd[39m\n33 Picard |R| = \ufffd[32m8.192889e-05\ufffd[39m\n34 Picard |R| = \ufffd[32m6.248350e-05\ufffd[39m\n35 Picard |R| = \ufffd[32m4.765336e-05\ufffd[39m\n36 Picard |R| = \ufffd[32m3.634308e-05\ufffd[39m\n37 Picard |R| = \ufffd[32m2.771724e-05\ufffd[39m\n38 Picard |R| = \ufffd[32m2.113879e-05\ufffd[39m\n39 Picard |R| = \ufffd[32m1.612162e-05\ufffd[39m\n40 Picard |R| = \ufffd[32m1.229524e-05\ufffd[39m\n41 Picard |R| = \ufffd[32m9.377037e-06\ufffd[39m\n42 Picard |R| = \ufffd[32m7.151450e-06\ufffd[39m\n43 Picard |R| = \ufffd[32m5.454094e-06\ufffd[39m\n44 Picard |R| = \ufffd[32m4.159596e-06\ufffd[39m\n45 Picard |R| = \ufffd[32m3.172339e-06\ufffd[39m\n46 Picard |R| = \ufffd[32m2.419403e-06\ufffd[39m\n47 Picard |R| = \ufffd[32m1.844672e-06\ufffd[39m\n48 Picard |R| = \ufffd[32m1.406827e-06\ufffd[39m\n49 Picard |R| = \ufffd[32m1.072903e-06\ufffd[39m\n50 Picard |R| = \ufffd[32m8.182389e-07\ufffd[39m\n51 Picard |R| = \ufffd[32m6.240221e-07\ufffd[39m\n52 Picard |R| = \ufffd[32m4.759045e-07\ufffd[39m\n53 Picard |R| = \ufffd[32m3.629440e-07\ufffd[39m\n54 Picard |R| = \ufffd[32m2.767958e-07\ufffd[39m\n55 Picard |R| = \ufffd[32m2.110956e-07\ufffd[39m\n56 Picard |R| = \ufffd[32m1.609900e-07\ufffd[39m\n57 Picard |R| = \ufffd[32m1.227775e-07\ufffd[39m\n58 Picard |R| = \ufffd[32m9.363509e-08\ufffd[39m\n59 Picard |R| = \ufffd[32m7.140995e-08\ufffd[39m\n60 Picard |R| = \ufffd[32m5.446004e-08\ufffd[39m\n61 Picard |R| = \ufffd[32m4.153347e-08\ufffd[39m\n62 Picard |R| = \ufffd[32m3.167511e-08\ufffd[39m\n63 Picard |R| = \ufffd[32m2.415671e-08\ufffd[39m\n64 Picard |R| = \ufffd[32m1.842289e-08\ufffd[39m\n65 Picard |R| = \ufffd[32m1.405005e-08\ufffd[39m",
                          "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8332406",
                          "updatedAt": "2024-02-01T09:16:18Z",
                          "publishedAt": "2024-02-01T09:12:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Do you still have the fully coupled input file?\nand try this\n[Preconditioning]\n[basic]\ntype = SMP\nfull = true\npetsc_options_iname = '-pc_type -pc_factor_shift_type '\npetsc_options_value = '  lu NONZERO'\n[]\n[]\n\nor some of these that are tuned for porous flow but maybe will work for porous flow + another phyiscs\nhttps://mooseframework.inl.gov/modules/porous_flow/solvers.html",
                          "url": "https://github.com/idaholab/moose/discussions/26697#discussioncomment-8336651",
                          "updatedAt": "2024-02-01T16:02:25Z",
                          "publishedAt": "2024-02-01T16:02:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to make execution sequence of a basic MultiApp become: 1. principal parent.i 2; postprocessors parent.i; 3. principal sub.i; 4. postprocessors sub.i",
          "author": {
            "login": "Wang-Yihu"
          },
          "bodyText": "It seems that in MOOSE basic MultiApp, all the postprocessors objects cannot be executed between parent.i and sub.i..., different from transfers objects.\nI have written a parent.i MOOSE input file and a sub.i MOOSE input file. In parent.i input file, I set\n[MultiApps]\n  [sub]\n    type = TransientMultiApp\n    execute_on = TIMESTEP_END\n    input_files = sub.i\n  []\n[]\n\nHowever, in postprocessors execute_on settings, there are these options:\nADJOINT_TIMESTEP_BEGIN;ADJOINT_TIMESTEP_END;ADJOINT;CUSTOM;FINAL;FORWARD;HOMOGENEOUS_FORWARD;INITIAL;LINEAR;MULTIAPP_FIXED_POINT_BEGIN;MULTIAPP_FIXED_POINT_END;NONE;NONLINEAR;TIMESTEP_BEGIN;TIMESTEP_END\nNone of them can realize my desire execution sequence: 1. principal parent.i 2; postprocessors parent.i; 3. principal sub.i; 4. postprocessors sub.i\nSo how can I realize my desire execution sequence? Make postprocessors objects be executed between parent.i and sub.i like transfers objects...",
          "url": "https://github.com/idaholab/moose/discussions/26661",
          "updatedAt": "2024-02-01T14:33:08Z",
          "publishedAt": "2024-01-28T07:37:42Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nUse Debug/show_execution_order and Executioner/verbose to see the actual execution order.\nI think the order you want is the natural one but it is good to check\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26661#discussioncomment-8272987",
                  "updatedAt": "2024-01-28T16:44:31Z",
                  "publishedAt": "2024-01-28T16:44:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If the order is not as you want for the execution of the postprocessor, MULTIAPP_FIXED_POINT_BEGIN for the execute_on of the postprocessor should be exactly where you want it",
                          "url": "https://github.com/idaholab/moose/discussions/26661#discussioncomment-8276761",
                          "updatedAt": "2024-01-29T04:17:02Z",
                          "publishedAt": "2024-01-29T04:17:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Wang-Yihu"
                          },
                          "bodyText": "Hello Guillaume, I've tried MULTIAPP_FIXED_POINT_BEGIN for the execute_on of the postprocessor, but I still cannot realize my goal. The execution sequence is still: 1. principal parent.i ;2. principal sub.i; 3; postprocessors parent.i; 4. postprocessors sub.i. The postprocessors objects are still not be executed between parent.i and sub.i like transfers objects...\nHere are my input files and terminal returns. Is this problem caused by different versions between my MOOSE code and yours? You can try my input files in your computer...\nparent.i\n[Mesh]\n  [rectangle_mesh]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = 10\n    ny = 10\n    xmax = 10.0\n    ymax = 1.0\n  []\n[]\n[Variables]\n  [./u]\n    initial_condition = 0.01\n  [../]\n[]\n[Kernels]\n  [./diff]\n    type = HeatConduction\n    variable = u\n  [../]\n  [./td]\n    type = TimeDerivative\n    variable = u\n  [../]\n[]\n[BCs]\n  [./left]\n    type = DirichletBC\n    variable = u\n    boundary = left\n    value = 10\n  [../]\n  [./right]\n    type = DirichletBC\n    variable = u\n    boundary = right\n    value = 0\n  [../]\n[]\n[Materials]\n  [coef]\n    type = HeatConductionMaterial\n    thermal_conductivity = 1\n  []\n[]\n[Executioner]\n  type = Transient\n  num_steps = 5\n  dt = 5\n  solve_type = 'PJFNK'\n  petsc_options_iname = '-pc_type -pc_hypre_type'\n  petsc_options_value = 'hypre boomeramg'\n[]\n[Outputs]\n  exodus = true\n[]\n[MultiApps]\n  [sub]\n    type = TransientMultiApp\n    execute_on = timestep_end\n    input_files = sub.i\n  []\n[]\n[Postprocessors]\n  [pps_MULTIAPP_FIXED_POINT_BEGIN]\n    type = SideDiffusiveFluxIntegral\n    variable = u\n    boundary = right\n    diffusivity = thermal_conductivity\n    execute_on = MULTIAPP_FIXED_POINT_BEGIN\n  []\n[]\n\nsub.i\n[Mesh]\n  [rectangle_mesh]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = 10\n    ny = 10\n    xmax = 10.0\n    ymax = 1.0\n  []\n[]\n[Variables]\n  [./diffused]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n[]\n[Kernels]\n  [./diff]\n    type = Diffusion\n    variable = diffused\n  [../]\n[]\n[BCs]\n  [leftbc]\n    type = DirichletBC\n    variable = diffused\n    boundary = left\n    value = 10\n  []\n  [rightbc]\n    type = DirichletBC\n    variable = diffused\n    boundary = right\n    value = 0\n  []\n[]\n[Executioner]\n  type = Transient\n  num_steps = 5\n  dt = 5\n  nl_abs_tol = 1e-6\n  solve_type = 'PJFNK'\n  petsc_options_iname = '-pc_type -pc_hypre_type'\n  petsc_options_value = 'hypre boomeramg'\n[]\n[Outputs]\n  exodus = true\n[]\n\nThe terminal will return\nFramework Information:\nMOOSE Version:           git commit 96aeccd391 on 2023-10-11\nLibMesh Version:         \nPETSc Version:           3.19.3\nSLEPc Version:           3.19.1\nCurrent Time:            Mon Jan 29 23:20:31 2024\nExecutable Timestamp:    Sat Jan 20 13:06:24 2024\n\nParallelism:\n  Num Processors:          1\n  Num Threads:             1\n\nMesh: \n  Parallel Type:           replicated\n  Mesh Dimension:          2\n  Spatial Dimension:       2\n  Nodes:                   121\n  Elems:                   100\n  Num Subdomains:          1\n\nNonlinear System:\n  Num DOFs:                121\n  Num Local DOFs:          121\n  Variables:               \"u\" \n  Finite Element Types:    \"LAGRANGE\" \n  Approximation Orders:    \"FIRST\" \n\nExecution Information:\n  Executioner:             Transient\n  TimeStepper:             ConstantDT\n  TimeIntegrator:          ImplicitEuler\n  Solver Mode:             Preconditioned JFNK\n  PETSc Preconditioner:    hypre boomeramg \n\nsub0: Parallelism:\nsub0:   Num Processors:          1\nsub0:   Num Threads:             1\nsub0: \nsub0: Mesh: \nsub0:   Parallel Type:           replicated\nsub0:   Mesh Dimension:          2\nsub0:   Spatial Dimension:       2\nsub0:   Nodes:                   121\nsub0:   Elems:                   100\nsub0:   Num Subdomains:          1\nsub0: \nsub0: Nonlinear System:\nsub0:   Num DOFs:                121\nsub0:   Num Local DOFs:          121\nsub0:   Variables:               \"diffused\" \nsub0:   Finite Element Types:    \"LAGRANGE\" \nsub0:   Approximation Orders:    \"FIRST\" \nsub0: \nsub0: Execution Information:\nsub0:   Executioner:             Transient\nsub0:   TimeStepper:             ConstantDT\nsub0:   TimeIntegrator:          ImplicitEuler\nsub0:   Solver Mode:             Preconditioned JFNK\nsub0:   PETSc Preconditioner:    hypre boomeramg \nsub0: \nsub0: \nsub0: Time Step 0, time = 0\n\nTime Step 0, time = 0\n\nPostprocessor Values:\n+----------------+--------------------------------+\n| time           | pps_MULTIAPP_FIXED_POINT_BEGIN |\n+----------------+--------------------------------+\n|   0.000000e+00 |                   0.000000e+00 |\n+----------------+--------------------------------+\n\n\nTime Step 1, time = 5, dt = 5\n 0 Nonlinear |R| = 2.976489e+00\n      0 Linear |R| = 2.976489e+00\n      1 Linear |R| = 8.678395e-01\n      2 Linear |R| = 2.276220e-01\n      3 Linear |R| = 1.041954e-01\n      4 Linear |R| = 2.323557e-02\n      5 Linear |R| = 8.932714e-03\n      6 Linear |R| = 1.469299e-03\n      7 Linear |R| = 5.112804e-04\n      8 Linear |R| = 9.046868e-05\n      9 Linear |R| = 2.524437e-05\n 1 Nonlinear |R| = 2.515752e-05\n      0 Linear |R| = 2.515752e-05\n      1 Linear |R| = 1.134176e-05\n      2 Linear |R| = 4.153676e-06\n      3 Linear |R| = 1.059784e-06\n      4 Linear |R| = 2.775753e-07\n      5 Linear |R| = 1.250038e-07\n      6 Linear |R| = 4.036809e-08\n      7 Linear |R| = 9.373929e-09\n      8 Linear |R| = 2.731261e-09\n      9 Linear |R| = 9.684379e-10\n     10 Linear |R| = 3.027080e-10\n     11 Linear |R| = 1.172366e-10\n 2 Nonlinear |R| = 1.192954e-10\n Solve Converged!\nsub0: \nsub0: Time Step 1, time = 5, dt = 5\nsub0:  0 Nonlinear |R| = 3.082207e+00\nsub0:       0 Linear |R| = 3.082207e+00\nsub0:       1 Linear |R| = 4.587452e-01\nsub0:       2 Linear |R| = 1.336525e-01\nsub0:       3 Linear |R| = 2.379502e-02\nsub0:       4 Linear |R| = 1.470892e-02\nsub0:       5 Linear |R| = 2.342500e-03\nsub0:       6 Linear |R| = 7.673881e-04\nsub0:       7 Linear |R| = 2.421086e-04\nsub0:       8 Linear |R| = 6.234554e-05\nsub0:       9 Linear |R| = 2.425057e-05\nsub0:  1 Nonlinear |R| = 2.348556e-05\nsub0:       0 Linear |R| = 2.348556e-05\nsub0:       1 Linear |R| = 1.738484e-05\nsub0:       2 Linear |R| = 4.770847e-06\nsub0:       3 Linear |R| = 2.143374e-06\nsub0:       4 Linear |R| = 4.556795e-07\nsub0:       5 Linear |R| = 1.460167e-07\nsub0:       6 Linear |R| = 6.627679e-08\nsub0:       7 Linear |R| = 1.759526e-08\nsub0:       8 Linear |R| = 4.461972e-09\nsub0:       9 Linear |R| = 1.544348e-09\nsub0:      10 Linear |R| = 7.239115e-10\nsub0:      11 Linear |R| = 1.230347e-10\nsub0:  2 Nonlinear |R| = 6.356498e-10\nsub0:  Solve Converged!\n\nPostprocessor Values:\n+----------------+--------------------------------+\n| time           | pps_MULTIAPP_FIXED_POINT_BEGIN |\n+----------------+--------------------------------+\n|   0.000000e+00 |                   0.000000e+00 |\n|   5.000000e+00 |                   0.000000e+00 |\n+----------------+--------------------------------+\n\n\nTime Step 2, time = 10, dt = 5\n 0 Nonlinear |R| = 5.255087e-01\n      0 Linear |R| = 5.255087e-01\n      1 Linear |R| = 2.134116e-01\n      2 Linear |R| = 8.702496e-02\n      3 Linear |R| = 2.748490e-02\n      4 Linear |R| = 6.583767e-03\n      5 Linear |R| = 2.346716e-03\n      6 Linear |R| = 5.119259e-04\n      7 Linear |R| = 9.869132e-05\n      8 Linear |R| = 2.162195e-05\n      9 Linear |R| = 9.498853e-06\n     10 Linear |R| = 2.851269e-06\n 1 Nonlinear |R| = 4.479992e-06\n      0 Linear |R| = 4.479992e-06\n      1 Linear |R| = 1.257410e-06\n      2 Linear |R| = 4.677313e-07\n      3 Linear |R| = 1.580319e-07\n      4 Linear |R| = 4.340009e-08\n      5 Linear |R| = 1.218637e-08\n      6 Linear |R| = 5.880566e-09\n      7 Linear |R| = 1.758683e-09\n      8 Linear |R| = 4.543855e-10\n      9 Linear |R| = 1.443835e-10\n     10 Linear |R| = 4.588931e-11\n     11 Linear |R| = 1.386184e-11\n 2 Nonlinear |R| = 1.428141e-11\n Solve Converged!\nsub0: \nsub0: Time Step 2, time = 10, dt = 5\nsub0:  0 Nonlinear |R| = 6.356498e-10\nsub0:  Solve Converged!\n\nPostprocessor Values:\n+----------------+--------------------------------+\n| time           | pps_MULTIAPP_FIXED_POINT_BEGIN |\n+----------------+--------------------------------+\n|   0.000000e+00 |                   0.000000e+00 |\n|   5.000000e+00 |                   0.000000e+00 |\n|   1.000000e+01 |                   1.061243e-01 |\n+----------------+--------------------------------+\n\n\nTime Step 3, time = 15, dt = 5\n 0 Nonlinear |R| = 2.185691e-01\n      0 Linear |R| = 2.185691e-01\n      1 Linear |R| = 1.485067e-01\n      2 Linear |R| = 4.815993e-02\n      3 Linear |R| = 1.735658e-02\n      4 Linear |R| = 3.797050e-03\n      5 Linear |R| = 1.118219e-03\n      6 Linear |R| = 2.356297e-04\n      7 Linear |R| = 5.823421e-05\n      8 Linear |R| = 1.533279e-05\n      9 Linear |R| = 5.411445e-06\n     10 Linear |R| = 2.099659e-06\n 1 Nonlinear |R| = 3.708459e-06\n      0 Linear |R| = 3.708459e-06\n      1 Linear |R| = 1.211572e-06\n      2 Linear |R| = 3.690877e-07\n      3 Linear |R| = 1.193817e-07\n      4 Linear |R| = 2.856420e-08\n      5 Linear |R| = 1.237382e-08\n      6 Linear |R| = 4.792531e-09\n      7 Linear |R| = 1.133186e-09\n      8 Linear |R| = 4.216877e-10\n      9 Linear |R| = 1.318355e-10\n     10 Linear |R| = 3.946277e-11\n     11 Linear |R| = 1.256721e-11\n 2 Nonlinear |R| = 1.256378e-11\n Solve Converged!\nsub0: \nsub0: Time Step 3, time = 15, dt = 5\nsub0:  0 Nonlinear |R| = 6.356498e-10\nsub0:  Solve Converged!\n\nPostprocessor Values:\n+----------------+--------------------------------+\n| time           | pps_MULTIAPP_FIXED_POINT_BEGIN |\n+----------------+--------------------------------+\n|   0.000000e+00 |                   0.000000e+00 |\n|   5.000000e+00 |                   0.000000e+00 |\n|   1.000000e+01 |                   1.061243e-01 |\n|   1.500000e+01 |                   2.842361e-01 |\n+----------------+--------------------------------+\n\n\nTime Step 4, time = 20, dt = 5\n 0 Nonlinear |R| = 1.324911e-01\n      0 Linear |R| = 1.324911e-01\n      1 Linear |R| = 9.412384e-02\n      2 Linear |R| = 2.641949e-02\n      3 Linear |R| = 8.728063e-03\n      4 Linear |R| = 1.611333e-03\n      5 Linear |R| = 4.976305e-04\n      6 Linear |R| = 1.063236e-04\n      7 Linear |R| = 3.647481e-05\n      8 Linear |R| = 9.732385e-06\n      9 Linear |R| = 3.124005e-06\n     10 Linear |R| = 1.222690e-06\n 1 Nonlinear |R| = 2.502088e-06\n      0 Linear |R| = 2.502088e-06\n      1 Linear |R| = 7.185495e-07\n      2 Linear |R| = 2.005761e-07\n      3 Linear |R| = 6.191835e-08\n      4 Linear |R| = 1.774005e-08\n      5 Linear |R| = 7.944339e-09\n      6 Linear |R| = 2.595579e-09\n      7 Linear |R| = 7.611138e-10\n      8 Linear |R| = 2.793350e-10\n      9 Linear |R| = 7.237345e-11\n     10 Linear |R| = 1.797982e-11\n 2 Nonlinear |R| = 1.804603e-11\n Solve Converged!\nsub0: \nsub0: Time Step 4, time = 20, dt = 5\nsub0:  0 Nonlinear |R| = 6.356498e-10\nsub0:  Solve Converged!\n\nPostprocessor Values:\n+----------------+--------------------------------+\n| time           | pps_MULTIAPP_FIXED_POINT_BEGIN |\n+----------------+--------------------------------+\n|   0.000000e+00 |                   0.000000e+00 |\n|   5.000000e+00 |                   0.000000e+00 |\n|   1.000000e+01 |                   1.061243e-01 |\n|   1.500000e+01 |                   2.842361e-01 |\n|   2.000000e+01 |                   4.713031e-01 |\n+----------------+--------------------------------+\n\n\nTime Step 5, time = 25, dt = 5\n 0 Nonlinear |R| = 8.652591e-02\n      0 Linear |R| = 8.652591e-02\n      1 Linear |R| = 6.177158e-02\n      2 Linear |R| = 1.632813e-02\n      3 Linear |R| = 5.117503e-03\n      4 Linear |R| = 6.913475e-04\n      5 Linear |R| = 2.601342e-04\n      6 Linear |R| = 6.040914e-05\n      7 Linear |R| = 2.230330e-05\n      8 Linear |R| = 6.151167e-06\n      9 Linear |R| = 1.981397e-06\n     10 Linear |R| = 7.485812e-07\n 1 Nonlinear |R| = 1.458158e-06\n      0 Linear |R| = 1.458158e-06\n      1 Linear |R| = 4.481580e-07\n      2 Linear |R| = 1.200042e-07\n      3 Linear |R| = 4.291735e-08\n      4 Linear |R| = 1.301063e-08\n      5 Linear |R| = 5.220129e-09\n      6 Linear |R| = 1.690922e-09\n      7 Linear |R| = 6.195129e-10\n      8 Linear |R| = 2.129039e-10\n      9 Linear |R| = 4.227435e-11\n     10 Linear |R| = 1.178166e-11\n 2 Nonlinear |R| = 1.182717e-11\n Solve Converged!\nsub0: \nsub0: Time Step 5, time = 25, dt = 5\nsub0:  0 Nonlinear |R| = 6.356498e-10\nsub0:  Solve Converged!\n\nPostprocessor Values:\n+----------------+--------------------------------+\n| time           | pps_MULTIAPP_FIXED_POINT_BEGIN |\n+----------------+--------------------------------+\n|   0.000000e+00 |                   0.000000e+00 |\n|   5.000000e+00 |                   0.000000e+00 |\n|   1.000000e+01 |                   1.061243e-01 |\n|   1.500000e+01 |                   2.842361e-01 |\n|   2.000000e+01 |                   4.713031e-01 |\n|   2.500000e+01 |                   6.275704e-01 |\n+----------------+--------------------------------+",
                          "url": "https://github.com/idaholab/moose/discussions/26661#discussioncomment-8285376",
                          "updatedAt": "2024-01-29T15:42:00Z",
                          "publishedAt": "2024-01-29T15:41:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Are you sure you are not figuring out when it is executed by the output of the table? because this is not right\nthe only thing that tells you when it is executed is the Debug/show_execution_order option.\nThe table is always output in this order (at the end of the parent app timestep)",
                          "url": "https://github.com/idaholab/moose/discussions/26661#discussioncomment-8286509",
                          "updatedAt": "2024-01-29T16:50:26Z",
                          "publishedAt": "2024-01-29T16:50:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Wang-Yihu"
                          },
                          "bodyText": "Oh! I do not know this before, I just think the execution order is the same as the solver output order!....",
                          "url": "https://github.com/idaholab/moose/discussions/26661#discussioncomment-8286862",
                          "updatedAt": "2024-01-29T17:03:28Z",
                          "publishedAt": "2024-01-29T17:03:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Wang-Yihu"
                          },
                          "bodyText": "Guillaume\nI try another time. The true answer is to add execute_on = TIMESTEP_END in the [Postprocessors] of parent.i and add execute_on = TIMESTEP_END in the [MultiApps] of parent.i. The execution sequence will be 1. principal parent.i; 2. postprocessors parent.i; 3. principal sub.i, though the return sequence in the shell is different: 1. principal parent.i; 2. principal sub.i; 3. postprocessors parent.i.\nYour guess MULTIAPP_FIXED_POINT_BEGIN is wrong, it makes the execution sequence: 1. principal parent.i; 2. principal sub.i; 3. postprocessors parent.i. , not my desire.\nYour suggestion is quite valuable. I find the execution sequence and return sequence in the shell might are different for the first time. And I also find we can input\n[Debug]\n  show_execution_order = ALWAYS\n[]\n\nto track the execution sequence.\nI also insert several printf in the MOOSE Postprocessors class code(my MOOSEApp name is lianxi_mesh, and rename my printf-inserted-Receiver.C, Receiver.h into ReceiverReport.C, ReceiverReport.h and rename  my printf-inserted-SideDiffusiveFluxIntegral.C, SideDiffusiveFluxIntegral.h into SideDiffusiveFluxIntegralReport.C, SideDiffusiveFluxIntegralReport.h) to track the execution sequence. My code is here to give examples for learners.\nReceiverReport.C.txt\nReceiverReport.h.txt\nSideDiffusiveFluxIntegralReport.C.txt\nSideDiffusiveFluxIntegralReport.h.txt\nsub.i.txt\nparent.i.txt",
                          "url": "https://github.com/idaholab/moose/discussions/26661#discussioncomment-8335644",
                          "updatedAt": "2024-02-01T14:37:58Z",
                          "publishedAt": "2024-02-01T14:33:05Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "nodal variables do not have gradients",
          "author": {
            "login": "gj19866"
          },
          "bodyText": "Dear MOOSErs,\nI was hoping someone might be able to provide some assistance.\nI would like to write an auxvariable that is calculated by the equation,\n$\\underline{j}=\\psi_R \\nabla \\psi_I- \\psi_I \\nabla \\psi_R -\\nabla \\varphi$, where $\\psi_R$, $\\psi_I$, and $\\varphi$ are variables calculated in my programme. (N.B. $\\underline{j}$ is a vector)\nTo do this I initially wrote an auxkernel using the ADVariables, and it failed to compile with the error:\nerror: cannot convert 'const MetaPhysicL::DualNumber<double, MetaPhysicL::SemiDynamicSparseNumberArray<double, long unsigned int, MetaPhysicL::NWrapper<53> >, true>' to 'double' in assignment\n\nI believed this was due to auxkernels not being able to handle ADvariables, so I rewrote the .C file with normal variables instead.\nThe rewritten file then compiled fine, however when running I get the error:\nnodal variables do not have gradients\nThis confused me slightly as the gradients for these variables are computed and used by some of the other kernels.\nI know that it is possible for me to write a kernel to solve this equation, but as the other variables are all solved for already in the code this doesn't seem like a good solution.\nPlease may you advise:\n\nIf (and then how) you can use ADVectorAuxVariables\nIf not how to go about solving this equation given that they are nodal variables, and these \"do not have gradients\"\n\nI appreciate your patience.\nThank you very much,\nGillian",
          "url": "https://github.com/idaholab/moose/discussions/26693",
          "updatedAt": "2024-01-31T17:42:22Z",
          "publishedAt": "2024-01-30T19:27:56Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nAuxKernels do not support automatic differentiation. The dependency on nonlinear variables can be handled using material properties (or functor material properties if you are using FV, or complex FE schemes).\nNodal variables do indeed not have a single defined value for the gradient at a node. At the node, there is a dicontinuity in the derivative, because for example one of the shape functions (the one non-zero at the node, at least for lagrange)'s support domain is centered on a node and its derivative changes sign at the node.\nYou can compute this quantity as an elemental quantity instead. For the auxvariable, use a variable that is defined on degrees of freedom that are on the element rather than on the node. For example, a constant monomial is defined by its elemental value (which we arbitrarily place at the centroid of the element in moose).\nYou can use this as the auxvariable if the quantity should be a vector computed by a VectorAuxKernel\n[AuxVariables]\n[j]\n  family = MONOMIAL_VEC\n  order = .... (constant to start with?)\n[]\n\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26693#discussioncomment-8311102",
                  "updatedAt": "2024-01-30T20:34:33Z",
                  "publishedAt": "2024-01-30T20:34:32Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "gj19866"
                          },
                          "bodyText": "Amazing, thank you very much Guillaume!\nOut of interest, is there a particular reason why AuxVars does not support AD?\nIs it just something that has not been implemented (or doesn't need to be!), or is there something preventing AuxVars being able to being AD?\nAll the best from the UK,\nGillian",
                          "url": "https://github.com/idaholab/moose/discussions/26693#discussioncomment-8312163",
                          "updatedAt": "2024-01-30T22:53:00Z",
                          "publishedAt": "2024-01-30T22:52:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@lindsayad if you want to comment on why auxvariables do not support AD.\nI would just say it s not needed as you can use material properties. It also does not seem efficient to me. When you compute an auxvariable at a Qp, it depends on the variable value at the dofs all around it. Which would themselves depend on nonlinear variable values around them (especially if the FE families to represent the nonlinear and auxiliary variables are different). So just like that you would get a huge AD stencil, in a very artificial way.",
                          "url": "https://github.com/idaholab/moose/discussions/26693#discussioncomment-8312206",
                          "updatedAt": "2024-01-30T23:07:43Z",
                          "publishedAt": "2024-01-30T22:59:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "The problem with AD in the aux system is how to store the derivatives in the aux system solution vector. You cannot use a PetscVector as it only understands reals or complex numbers. Last time I thought about this I considered a DistributedVector but then stopped for some reason. As @GiudGiud said it has not been a priority since we have other means to compute \"auxiliary\" quantities with derivatives via the material system (and then these can also be auto-output into aux vars for postprocessing/visualization)",
                          "url": "https://github.com/idaholab/moose/discussions/26693#discussioncomment-8313069",
                          "updatedAt": "2024-01-31T00:43:36Z",
                          "publishedAt": "2024-01-31T00:43:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "gj19866"
                          },
                          "bodyText": "That's really interesting. Thank you for taking the time to explain it to me.",
                          "url": "https://github.com/idaholab/moose/discussions/26693#discussioncomment-8320513",
                          "updatedAt": "2024-01-31T13:41:08Z",
                          "publishedAt": "2024-01-31T13:41:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "No problem!\nI think I found the reason I stopped my last half-hearted attempt to do AD with aux, digging in my DM history with @roystgnr. The System class in libMesh holds NumericVector<Number> in its _vectors container, so even if Number was configurable to something other than Real or std::complex<Real> (which it currently can't be), you could not mix storage of vectors holding non-dual-numbers and dual-numbers. I think we could figure out a way to handle this, but it hasn't been worth the gains",
                          "url": "https://github.com/idaholab/moose/discussions/26693#discussioncomment-8324469",
                          "updatedAt": "2024-01-31T17:42:23Z",
                          "publishedAt": "2024-01-31T17:42:22Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}