{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wOC0zMVQxNTowMDo0MS0wNTowMM4AVQoJ"
    },
    "edges": [
      {
        "node": {
          "title": "Decreasing the growth rate of dendrites using DiscreteNucleation material property",
          "author": {
            "login": "vermaprk"
          },
          "bodyText": "Hello\nI have a phase field model depicting electro deposition in a non-uniform way as shown in figure (attached). However, I am trying to stop the growth at the spikes by manipulating the phase-field variable mu (grand potential PFM). I am using DiscreteNucleation material property to increase the magnitude of mu at the interface of the spike so that the growth takes place only at the flat area rather than the spike. This should result in a overall much smoother/flatter profile.\nHowever, I am not able to achieve this using DiscreteNucleation material property which is trying to force mu to -20. A high (-ve) value of mu should result in lower concentration at the peak causing lower growth rate at spike as compared to the flatter region.\nThanks\n@dschwen @laagesen @GiudGiud",
          "url": "https://github.com/idaholab/moose/discussions/25364",
          "updatedAt": "2023-09-05T15:43:49Z",
          "publishedAt": "2023-08-31T08:45:21Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "laagesen"
                  },
                  "bodyText": "Hi, I'm not sure why you're trying to stop the growth at the spikes. It seems like if this a natural feature that emerges from the physics of the model, you should not try to artificially suppress it. Dendrite formation can certainly happen in electrodeposition. Can you explain why you are trying to suppress growth?",
                  "url": "https://github.com/idaholab/moose/discussions/25364#discussioncomment-6888622",
                  "updatedAt": "2023-09-01T18:33:23Z",
                  "publishedAt": "2023-09-01T18:33:23Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "vermaprk"
                  },
                  "bodyText": "Yes, I am trying to model the effect of additives which will preferentially\nbe adsorbed at the spikes because of higher electric field near them.\nFurther, these additives will repel the metal ion and lowering their concentration near the\nspike interface leading to suppressed growth.\n\nI am referring to this article\nhttps://doi.org/10.1021/ja312241y\n\n![Screenshot_20230902-070723-620.png](https://github.com/idaholab/moose/assets/69255835/c9d5e69b-1d46-45d1-82bd-9441512723fb)",
                  "url": "https://github.com/idaholab/moose/discussions/25364#discussioncomment-6890595",
                  "updatedAt": "2023-09-02T02:12:31Z",
                  "publishedAt": "2023-09-02T01:39:14Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "I am not an expert in this area, but I don't think nucleation is the right way to go about suppressing growth here. This is a pretty complicated physical problem. Off the top of my head it seems that you would need to add a coupled electric field and solve Poisson's equation to understand the effect of additive concentration on electric field, and then the resulting field would be what is responsible for pushing the metal ions away. I'd suggest to do some literature search to find an existing phase-field model for this problem and we can try to help you implement that once you find it.",
                          "url": "https://github.com/idaholab/moose/discussions/25364#discussioncomment-6915239",
                          "updatedAt": "2023-09-05T14:28:38Z",
                          "publishedAt": "2023-09-05T14:28:37Z",
                          "isAnswer": true
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "vermaprk"
                          },
                          "bodyText": "OK. Thanks for the help. I will certainly get back to you.",
                          "url": "https://github.com/idaholab/moose/discussions/25364#discussioncomment-6916074",
                          "updatedAt": "2023-09-05T15:43:41Z",
                          "publishedAt": "2023-09-05T15:43:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "minimum_time_interval seems not working for exodus output?",
          "author": {
            "login": "lyyc199586"
          },
          "bodyText": "I found that minimum_time_interval seems to be not working for exodus output\n[exodus]\n    type = Exodus\n    minimum_time_interval = 1e-6\n[]\n\nI still get output in exodus with dt<1e-6.",
          "url": "https://github.com/idaholab/moose/discussions/25337",
          "updatedAt": "2023-09-21T18:02:09Z",
          "publishedAt": "2023-08-29T17:35:34Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "you have to specify interval as well to make that parameter work iirc",
                  "url": "https://github.com/idaholab/moose/discussions/25337#discussioncomment-6855980",
                  "updatedAt": "2023-08-29T17:38:29Z",
                  "publishedAt": "2023-08-29T17:38:28Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lyyc199586"
                          },
                          "bodyText": "oh\nI added interval=1, but it sill gives output with dt<minimum time interval",
                          "url": "https://github.com/idaholab/moose/discussions/25337#discussioncomment-6856542",
                          "updatedAt": "2023-08-29T18:48:30Z",
                          "publishedAt": "2023-08-29T18:48:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "alright can you make a MWE and we ll try to run it when we have time",
                          "url": "https://github.com/idaholab/moose/discussions/25337#discussioncomment-6856957",
                          "updatedAt": "2023-08-29T19:35:45Z",
                          "publishedAt": "2023-08-29T19:35:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lyyc199586"
                          },
                          "bodyText": "Sure, please see below @GiudGiud, where minimal_time_interval works for CSV but not for Exodus\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Mesh]\n  [gmg]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = 20\n    ny = 5\n    xmax = 2\n    ymax = 0.5\n  []\n[]\n\n[Variables]\n  [disp_x]\n  []\n  [disp_y]\n  []\n[]\n\n[Kernels]\n  [solid_x]\n    type = ADStressDivergenceTensors\n    component = 0\n    variable = disp_x\n  []\n  [solid_y]\n    type = ADStressDivergenceTensors\n    component = 1\n    variable = disp_y\n  []\n[]\n\n[BCs]\n  [left]\n    type = ADDirichletBC\n    variable = disp_x\n    boundary = left\n    value = 0\n  []\n  [right]\n    type = ADFunctionDirichletBC\n    variable = disp_x\n    boundary = right\n    function = '0.1*t'\n  []\n[]\n\n[Materials]\n  [elasticity]\n    type = ADComputeIsotropicElasticityTensor\n    youngs_modulus = 0.02735 # MPa\n    poissons_ratio = 0.1\n  []\n  [stress]\n    type = ADComputeLinearElasticStress\n  []\n  [strain]\n    type = ADComputeSmallStrain\n  []\n[]\n\n[Postprocessors]\n  [dt]\n    type = TimestepSize\n    outputs = 'csv'\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = LINEAR\n  end_time = 1\n  dt = 0.1\n[]\n\n[Outputs]\n  [exodus]\n    type = Exodus\n    interval = 1\n    minimum_time_interval = 0.2\n  []\n  [csv]\n    type = CSV\n    interval = 1\n    minimum_time_interval = 0.2\n  []\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/25337#discussioncomment-6864341",
                          "updatedAt": "2023-08-30T14:49:32Z",
                          "publishedAt": "2023-08-30T13:31:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/25337#discussioncomment-6871105",
                          "updatedAt": "2023-08-31T03:55:39Z",
                          "publishedAt": "2023-08-31T03:55:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nI just run this input and got only 5 time steps output for exodus, which is the desired behavior\nCan you double check it works for you?\nWhich moose version are you using? Is it real old?\n\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/25337#discussioncomment-6906439",
                          "updatedAt": "2023-09-04T16:33:55Z",
                          "publishedAt": "2023-09-04T16:33:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lyyc199586"
                          },
                          "bodyText": "ummm\ncommit b38350ebf7e36dd6b1de167c186e268745ad4755 (HEAD)\nMerge: 9564dd9346 5f86f198a5\nAuthor: moosetest <bounces@inl.gov>\nDate:   Fri Aug 4 18:09:04 2023 -0600\n\n    Merge commit '5f86f198a56ed6985d1ebc282b81da3f049af20b",
                          "url": "https://github.com/idaholab/moose/discussions/25337#discussioncomment-6907210",
                          "updatedAt": "2023-09-04T18:47:00Z",
                          "publishedAt": "2023-09-04T18:47:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "have not seen a fix for this in the last month",
                          "url": "https://github.com/idaholab/moose/discussions/25337#discussioncomment-6907248",
                          "updatedAt": "2023-09-04T18:53:24Z",
                          "publishedAt": "2023-09-04T18:53:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Jacobi of the mechanical equilibrium equation.",
          "author": {
            "login": "nuomi68"
          },
          "bodyText": "I really like the formulas in the MOOSE homepage. The form is simple and easy to understand. So I would like to ask MOOSEer, how would you write the Jacobi of the mechanical equilibrium equation.\nThere is a way of writing in Stress Divergence Tensors, feeling that there are still too many symbols. Is there a way to write it without Einstein's notation? I looked up a lot of data, the weak form of the mechanical equation will use a lot of matrix expression, look not concise enough.",
          "url": "https://github.com/idaholab/moose/discussions/25393",
          "updatedAt": "2023-09-04T15:50:20Z",
          "publishedAt": "2023-09-04T15:50:19Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": []
          }
        }
      },
      {
        "node": {
          "title": "Nedelec element formulation",
          "author": {
            "login": "karthichockalingam"
          },
          "bodyText": "Hello,\n@nmnobre and I trying to understand the Nedelec implementation.  In particular feCurlPhi has the return type VariablePhiCurl whereas _vector_fe_shape_data[type]->_curl_phi  is of type VectorVariablePhiCurl as defined here. So shouldn't the return function type be VectorVariablePhiCurl instead?\nBut having the return type VectorVariablePhiCurl would lead to compilation error as it is a specialisation of the template\nI was under the impression Nedelec implementation is for vector-valued problems. Why is the return type of feCurlPhi not VectorVariablePhiCurl?\nMany thanks for the clarification!\nKarthi",
          "url": "https://github.com/idaholab/moose/discussions/25324",
          "updatedAt": "2023-10-25T15:06:58Z",
          "publishedAt": "2023-08-28T18:48:52Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@cticenhour",
                  "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6845809",
                  "updatedAt": "2023-08-28T18:58:26Z",
                  "publishedAt": "2023-08-28T18:58:26Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "feCurlPhi is a template\n  template <typename OutputType>\n  const typename OutputTools<OutputType>::VariablePhiCurl & feCurlPhi(FEType type) const\n  {\n    _need_curl[type] = true;\n    buildFE(type);\n    return _fe_shape_data[type]->_curl_phi;\n  }\nYou can see the vector specialization:\ntemplate <>\nconst typename OutputTools<VectorValue<Real>>::VariablePhiCurl &\nAssembly::feCurlPhi<VectorValue<Real>>(FEType type) const\n{\n  _need_curl[type] = true;\n  buildVectorFE(type);\n  return _vector_fe_shape_data[type]->_curl_phi;\n}\ncurl is not a priori tied to Nedelece elements. You can implement a curl for LAGRANGE_VEC as well for example",
                  "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6845840",
                  "updatedAt": "2023-08-28T19:01:28Z",
                  "publishedAt": "2023-08-28T19:01:27Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Having curl available for scalar finite element types allows us to reduce the number of routine implementations, such as for MooseVariableData::computeValues",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6845854",
                          "updatedAt": "2023-08-28T19:05:22Z",
                          "publishedAt": "2023-08-28T19:04:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "karthichockalingam"
                          },
                          "bodyText": "I can see, you want to avoid code duplication when dealing with scalar-valued problems.\nHowever, I don't see how it works out for vector-valued problems.\nWhy is the return type const typename OutputTools<VectorValue<Real>>::VariablePhiCurl because Phi is clearly a vector. Shouldn't the return type be VectorVariablePhiCurl instead?\nYes, thanks for pointing out to me that curl can be computed in other spaces too.",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6845920",
                          "updatedAt": "2023-08-28T19:14:19Z",
                          "publishedAt": "2023-08-28T19:12:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "However, I don't see how it works out for vector-valued problems.\n\nWhy not? We get to use the exact same computeValues routine for all finite element types. Code duplication was the wrong phrase to use perhaps, which is why I edited the comment to say \"number of routine implementations\". The latter is important to stop cases where a developer implements a new feature or fixes a bug in the general template but forgets to add it in a specialization (which would then be an example of code duplication when they then go and add the feature for the specialization)\n\nWhy is the return type const typename OutputTools<VectorValue>::VariablePhiCurl because Phi is clearly a vector. Shouldn't the return type be VectorVariablePhiCurl instead?\n\nThis is from MooseTypes.h\ntemplate <typename OutputType>\nstruct OutputTools\n{\n  typedef typename TensorTools::IncrementRank<OutputType>::type OutputGradient;\n  typedef typename TensorTools::IncrementRank<OutputGradient>::type OutputSecond;\n  typedef typename TensorTools::DecrementRank<OutputType>::type OutputDivergence;\n\n  typedef MooseArray<OutputType> VariableValue;\n  typedef MooseArray<OutputGradient> VariableGradient;\n  typedef MooseArray<OutputSecond> VariableSecond;\n  typedef MooseArray<OutputType> VariableCurl;\n  typedef MooseArray<OutputDivergence> VariableDivergence;\n\n  typedef typename Moose::ShapeType<OutputType>::type OutputShape;\n  typedef typename TensorTools::IncrementRank<OutputShape>::type OutputShapeGradient;\n  typedef typename TensorTools::IncrementRank<OutputShapeGradient>::type OutputShapeSecond;\n  typedef typename TensorTools::DecrementRank<OutputShape>::type OutputShapeDivergence;\n\n  typedef MooseArray<std::vector<OutputShape>> VariablePhiValue;\n  typedef MooseArray<std::vector<OutputShapeGradient>> VariablePhiGradient;\n  typedef MooseArray<std::vector<OutputShapeSecond>> VariablePhiSecond;\n  typedef MooseArray<std::vector<OutputShape>> VariablePhiCurl;\n  typedef MooseArray<std::vector<OutputShapeDivergence>> VariablePhiDivergence;\n\n  typedef MooseArray<std::vector<OutputShape>> VariableTestValue;\n  typedef MooseArray<std::vector<OutputShapeGradient>> VariableTestGradient;\n  typedef MooseArray<std::vector<OutputShapeSecond>> VariableTestSecond;\n  typedef MooseArray<std::vector<OutputShape>> VariableTestCurl;\n  typedef MooseArray<std::vector<OutputShapeDivergence>> VariableTestDivergence;\n\n  // DoF value type for the template class OutputType                                                                 \n  typedef typename Moose::DOFType<OutputType>::type OutputData;\n  typedef MooseArray<OutputData> DoFValue;\n  typedef OutputType OutputValue;\n};\nInstantiating OutputTools<VectorValue<Real>> results in typename OutputTools<VectorValue<Real>>::VariablePhiCurl corresponding to a MooseArray<std::vector<VectorValue<Real>>> which is the equivalent of the VectorVariablePhiCurl. This is the typedef in MooseTypes.h:\ntypedef typename OutputTools<RealVectorValue>::VariablePhiCurl VectorVariablePhiCurl;",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6846529",
                          "updatedAt": "2023-08-28T20:52:12Z",
                          "publishedAt": "2023-08-28T20:49:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "karthichockalingam"
                          },
                          "bodyText": "Thank you very much for your response. This clarifies my question.\nI got confused because I thought the following type would be instantiated instead:\n typedef typename OutputTools<RealVectorValue>::VariablePhiCurl VectorVariablePhiCurl;\nYou are in way a telling me that types <RealVectorValue> and <VectorValue<Real>> are not the same. I guess that was the whole purpose of the developer all along prototyping a general template.\nWe are looking into implementing RT elements for vector-valued problems. So\nOutputTools<OutputType>::VariablePhiDivergence \nwouldn't through up an error for Real types (or is there divergence for scalar field)? Do templates in general get initiated for all types or only the specific case it is invoked for (i.e. VectorValue)?",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6854684",
                          "updatedAt": "2023-08-29T15:50:33Z",
                          "publishedAt": "2023-08-29T15:33:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "VectorValue<Real> and RealVectorValue are actually the same. I'm sure that causes some confusion and I'm sorry about that!\n\nWe are looking into implementing RT elements for vector-valued problems\n\nThat would be awesome. I would love to see that. Have you looked at libMesh which is our underlying finite element library? That's actually the place where a new basis implementation would go and then it would propagate down into MOOSE\n\nwouldn't through up an error for Real types (or is there divergence for scalar field)?\n\nNo it would not. There is not really a divergence for a scalar field. We could consider doing some refactoring in our variable classes that remove curl and divergence code for all but the vector template instances. However, we just haven't had a need motivating that change.\n\nDo templates in general get initiated for all types or only the specific case it is invoked for (i.e. VectorValue)?\n\nThey are only instantiated when used (implicitly) or when explicitly instantiated. For instance at the bottom of MooseVariableFE.C we perform some explicit instantiations:\ntemplate class MooseVariableFE<Real>;\ntemplate class MooseVariableFE<RealVectorValue>;\ntemplate class MooseVariableFE<RealEigenVector>;",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6855147",
                          "updatedAt": "2023-08-29T16:08:36Z",
                          "publishedAt": "2023-08-29T16:07:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "karthichockalingam"
                          },
                          "bodyText": "Yes, we have completed the RT implementation in libmesh, testing is underway to see if we get the right convergence. We just started looking to port it into MOOSE. I am sure, we will have questions along the way.",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6855220",
                          "updatedAt": "2023-08-29T16:16:56Z",
                          "publishedAt": "2023-08-29T16:16:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Very exciting stuff. Please continue asking questions as needed!",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6855260",
                          "updatedAt": "2023-08-29T16:20:50Z",
                          "publishedAt": "2023-08-29T16:20:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "karthichockalingam"
                          },
                          "bodyText": "Is there a way to print the system matrix before it is sent to the solver?\nIf so, where can I add that line in the framework?",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6905272",
                          "updatedAt": "2023-09-04T14:13:24Z",
                          "publishedAt": "2023-09-04T14:13:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Are you using PJNK or Newton?\n-ksp_view_pmat on the command line will show the preconditioning matrix. In general the petsc command line options can be passed on the command line in moose (or in the executioner block) to get the petsc output",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6905438",
                          "updatedAt": "2023-09-04T14:32:28Z",
                          "publishedAt": "2023-09-04T14:32:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Yes @GiudGiud\u2019s command will print the system matrix. For PJFNK we use the system matrix for P (matrix for creating a preconditioner for the linearized Jacobian A) and for NEWTON we use it for both A and P",
                          "url": "https://github.com/idaholab/moose/discussions/25324#discussioncomment-6905503",
                          "updatedAt": "2023-09-04T14:39:28Z",
                          "publishedAt": "2023-09-04T14:39:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "\"PC failed due to FACTOR_NUMERIC_ZEROPIVOT\" when not using MPI",
          "author": {
            "login": "ChaoyueL"
          },
          "bodyText": "Hello,\nI'm having a problem that when I run my model WITH mpi, the model converges fine. But if I run WITHOUT mpi, it gives error as following:\nTime Step 1, time = 10, dt = 10\n\nPerforming automatic scaling calculation\n\n 0 Nonlinear |R| = 6.461196e+03\n\n      0 Linear |R| = 6.461196e+03\n  Linear solve did not converge due to DIVERGED_PC_FAILED iterations 0\n                 PC failed due to FACTOR_NUMERIC_ZEROPIVOT\nNonlinear solve did not converge due to DIVERGED_LINE_SEARCH iterations 0\n Solve Did NOT Converge!\n  Finished Solving                                                                       [ 89.72 s] [   31 MB]\nAborting as solve did not converge\n\nHere are the settings in my .i file:\n[Mesh]\n  type = FileMesh\n  file = 90cal20_modified_cut.inp\n[]\n\n[Variables]\n  [phis]\n    block = 'Volume1_TET4 Volume2_TET4 Volume3_TET4'\n  []\n  [phil]\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n  []\n  [cl]\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n  []\n  [cs]\n    block = 'Volume2_TET4'\n  []\n[]\n\n[GlobalParams]\n  csmax = '48207'\n  k = '4.38e-11'\n  alpha = '0.5'\n  R = '8.314'\n  F = '96485.33'\n  T = '293.15'\n[]\n\n[Materials]\n  [EeqCurve]\n    type = EeqCurveInterface\n    boundary = 'Surface0_2 Surface1_2 Surface2_4'\n    cs = 'cs'\n    EeqCurve_file = OCV_file.csv\n  []\n  [liquid_param]\n    type = ElectrolyteMat\n    block = 'Volume0_TET4'\n    cl = 'cl'\n    epsilon_input = 1\n    Dl_input = 3e-10\n    sigmal_input = 1\n    fcl_input = 1\n    transpnum_input = 0.3\n    #Dl_file = Dl_file.csv\n    #sigmal_file = sigmal_file.csv\n    #fcl_file = fcl_file.csv\n    #transpnum_file = transpnum_file.csv\n  []\n  [sep_param]\n    type = ElectrolyteMat\n    block = 'Volume4_TET4'\n    cl = 'cl'\n    epsilon_input = 0.5\n    Dl_input = 3e-10\n    sigmal_input = 1\n    fcl_input = 1\n    transpnum_input = 0.3\n    #Dl_file = Dl_file.csv\n    #sigmal_file = sigmal_file.csv\n    #fcl_file = fcl_file.csv\n    #transpnum_file = transpnum_file.csv\n  []\n  [solid_param]\n    type = SolidMat\n    block = 'Volume2_TET4'\n    cs = 'cs'\n    Ds_input = 1e-14\n    sigmas_input = 0.1\n    #Ds_file = Ds_file.csv\n    #sigmas_file = sigmas_file.csv\n  []\n  [cc_param]\n    type = GenericConstantMaterial\n    block = 'Volume3_TET4'\n    prop_names = 'sigmas_cc'\n    prop_values = '3.5e7'\n  []\n  [CBD_param]\n    type = CBDMat\n    block = 'Volume1_TET4'\n    epsilon_input = 0.1\n    sigmas_input = 0.1\n    cl = 'cl'\n    Dl_input = 3e-10\n    sigmal_input = 1\n    fcl_input = 1\n    transpnum_input = 0.3\n    #Dl_file = Dl_file.csv\n    #sigmal_file = sigmal_file.csv\n    #fcl_file = fcl_file.csv\n    #transpnum_file = transpnum_file.csv\n  []\n[]\n\n[Kernels]\n  [ohm_s]\n    type = MyADDiffusion\n    block = 'Volume1_TET4 Volume2_TET4'\n    variable = phis\n    diffusivity = sigmas\n  []\n  [ohm_cc]\n    type = ADMatDiffusion\n    block = 'Volume3_TET4'\n    variable = phis\n    diffusivity = 3.5\n  []\n  [ohm_l]\n    type = MyADDiffusion\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n    variable = phil\n    diffusivity = sigmal\n  []\n  [ConcCurrent_l]\n    type = ConcentratedPartCurrent\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n    variable = phil\n    cl = 'cl'\n  []\n  [diff_l]\n    type = MyADDiffusion\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n    variable = cl\n    diffusivity = Dl\n  []\n  [time_l]\n    type = ADMatTimeDerivative\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n    variable = cl\n  []\n  [iPartFlux_l]\n    type = iPartFlux\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n    variable = cl\n    phil = 'phil'\n  []\n  [diff_s]\n    type = MyADDiffusion\n    block = 'Volume2_TET4'\n    variable = cs\n    diffusivity = Ds\n  []\n  [time_s]\n    type = TimeDerivative\n    block = 'Volume2_TET4'\n    variable = cs\n  []\n[]\n\n[ICs]\n  [phis_ic]\n    type = ConstantIC\n    variable = phis\n    value = 4.07\n    block = 'Volume1_TET4 Volume2_TET4 Volume3_TET4'\n  []\n  [phil_ic]\n    type = ConstantIC\n    variable = phil\n    value = 0\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n  []\n  [cl_ic]\n    type = ConstantIC\n    variable = cl\n    value = 1000\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n  []\n  [cs_ic]\n    type = ConstantIC\n    variable = cs\n    value = 22139\n    block = 'Volume2_TET4'\n  []\n[]\n\n[BCs]\n  [phis_neumann]\n    type = ADNeumannBC\n    boundary = 'Surface3_6'\n    variable = phis\n    value = -14.8728\n  []\n  [phil_dirichlet]\n    type = ADDirichletBC\n    boundary = 'Surface4_5'\n    variable = phil\n    value = 0\n  []\n  #[cl_dirichlet]\n   # type = ADDirichletBC\n    #boundary = 'Surface4_5'\n   # variable = cl\n    #value = 1000\n # []\n  [cl_neumann]\n    type=ADNeumannBC\n    boundary='Surface4_5'\n    variable=cl\n    value= 15.4145e-5\n  []\n[]\n\n[InterfaceKernels]\n  [BV]\n    type = BV\n    variable = phis\n    neighbor_var = 'phil'\n    cs = 'cs'\n    cl = 'cl'\n    boundary = 'Surface0_2 Surface1_2 Surface2_4'\n  []\n  [BVFLUX]\n    type = BVFLUX\n    variable = cs\n    neighbor_var = 'cl'\n    phis = 'phis'\n    phil = 'phil'\n    boundary = 'Surface0_2 Surface1_2 Surface2_4'\n  []\n[]\n\n[Functions]\n  [dts]\n    type = PiecewiseLinear\n    y = '0.1 0.1 0.5 10 100'\n    x = '0 1 5 10 100'\n  []\n[]\n\n[Executioner]\n  # petsc_options = '-snes_ksp_ew -help'\n  nl_abs_tol = 1e-30\n  l_abs_tol = 1e-50\n  l_tol = 1e-10\n  nl_rel_tol = 1e-10\n  nl_max_its=10\n  l_max_its = 20\n  type = Transient\n  end_time = 4000\n  automatic_scaling = true\n  solve_type = NEWTON\n  #petsc_options = '-snes_ksp_ew -help'\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n  [TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 10\n    optimal_iterations = 5\n  []\n[]\n\n[Postprocessors]\n  [terminal_potential]\n    type = SideAverageValue\n    variable = 'phis'\n    boundary = 'Surface3_6'\n  []\n  [cs_integral]\n    type = ElementIntegralVariablePostprocessor\n    variable = 'cs'\n    block = 'Volume2_TET4'\n  []\n  [cl_integral]\n    type = ElementIntegralVariablePostprocessor\n    variable = 'cl'\n    block = 'Volume0_TET4 Volume1_TET4 Volume4_TET4'\n  []\n[]\n\n[UserObjects]\n  [terminate_potential]\n    type = Terminator\n    expression = 'terminal_potential<=3.32'\n  []\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n[]\n\nWhat is the reason causing the error? Thank you!",
          "url": "https://github.com/idaholab/moose/discussions/25392",
          "updatedAt": "2023-09-04T13:55:37Z",
          "publishedAt": "2023-09-04T09:00:15Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThe LU solver you are using is hitting a zero pivot in serial, and its parallel version is avoiding it somehow\nto fix this you ll want to have in your [Executioner] block:\n  petsc_options_iname = '-pc_type -pc_factor_shift_type'\n  petsc_options_value = 'lu NONZERO'\n\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25392#discussioncomment-6905135",
                  "updatedAt": "2023-09-04T13:55:38Z",
                  "publishedAt": "2023-09-04T13:55:37Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Dynamic CSV file as BC",
          "author": {
            "login": "Ghazipo"
          },
          "bodyText": "Dear MOOSE community\nI'm encountering an issue with setting a dynamic CSV file as a Boundary Condition (BC) in my application. In my setup, I'm using a FunctionDirichletBC that calls a function of type PiecewiseConstant, and this function is configured to read data from a CSV file. The challenge is that this CSV file is expected to change over time during the simulation.\nI've implemented an external code that appends a new row to the CSV file after each time step of the MOOSE simulation. However, my MOOSE-based application appears to only consider the initial version of the CSV file before the start of the simulation and doesn't recognize its dynamic changes during the simulation.\nI attempted to address this issue by changing the execute_on option in the Function block to \"TIMESTEP_BEGIN,\" with the expectation that MOOSE would recognize the updated CSV file at the beginning of each timestep. Surprisingly, this approach didn't yield the desired results. Strangely, the behavior remains the same whether the execute_on option is set to \"none\" or \"TIMESTEP_BEGIN.\"\nTo be clear and concise, I would greatly appreciate any insights into how MOOSE handles the variations in a CSV file during a simulation. What methods or configurations can be used to ensure that MOOSE recognizes and applies the changes in the CSV file during the simulation?\nThank you for your assistance.",
          "url": "https://github.com/idaholab/moose/discussions/25390",
          "updatedAt": "2023-09-03T16:22:33Z",
          "publishedAt": "2023-09-03T13:39:51Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThe file is read at construction in that function\nhttps://github.com/idaholab/moose/blob/next/framework/src/functions/PiecewiseTabularBase.C\nIf you want it to be read on every step, you ll need to copy this code to a timestepSetup routine to get the file to be read again\n  // determine data source and check parameter consistency\n  if (isParamValid(\"data_file\") && !isParamValid(\"x\") && !isParamValid(\"y\") &&\n      !isParamValid(\"xy_data\"))\n    buildFromFile();\n  else if (!isParamValid(\"data_file\") && isParamValid(\"x\") && isParamValid(\"y\") &&\n           !isParamValid(\"xy_data\"))\n    buildFromXandY();\n  else if (!isParamValid(\"data_file\") && !isParamValid(\"x\") && !isParamValid(\"y\") &&\n           isParamValid(\"xy_data\"))\n    buildFromXY();\n  else\n    mooseError(\"In \",\n               _name,\n               \": Either 'data_file', 'x' and 'y', or 'xy_data' must be specified exclusively.\");\n\nYou really just need to move the buildFromFile() routine\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25390#discussioncomment-6897750",
                  "updatedAt": "2023-09-03T16:22:31Z",
                  "publishedAt": "2023-09-03T16:22:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThe file is read at construction in that function\nhttps://github.com/idaholab/moose/blob/next/framework/src/functions/PiecewiseTabularBase.C\nIf you want it to be read on every step, you ll need to copy this code to a timestepSetup routine to get the file to be read again\n  // determine data source and check parameter consistency\n  if (isParamValid(\"data_file\") && !isParamValid(\"x\") && !isParamValid(\"y\") &&\n      !isParamValid(\"xy_data\"))\n    buildFromFile();\n  else if (!isParamValid(\"data_file\") && isParamValid(\"x\") && isParamValid(\"y\") &&\n           !isParamValid(\"xy_data\"))\n    buildFromXandY();\n  else if (!isParamValid(\"data_file\") && !isParamValid(\"x\") && !isParamValid(\"y\") &&\n           isParamValid(\"xy_data\"))\n    buildFromXY();\n  else\n    mooseError(\"In \",\n               _name,\n               \": Either 'data_file', 'x' and 'y', or 'xy_data' must be specified exclusively.\");\n\nYou really just need to move the buildFromFile() routine\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25390#discussioncomment-6897751",
                  "updatedAt": "2023-09-03T16:22:34Z",
                  "publishedAt": "2023-09-03T16:22:33Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Class definition for 'ComputeCrystalPlasticityThermalEigenstrain'",
          "author": {
            "login": "abarun22"
          },
          "bodyText": "Could someone please direct me to the page where i can find the class definition for the above class?",
          "url": "https://github.com/idaholab/moose/discussions/25389",
          "updatedAt": "2023-09-03T15:30:59Z",
          "publishedAt": "2023-09-03T13:28:58Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "This page?\nhttps://mooseframework.inl.gov/source/materials/crystal_plasticity/ComputeCrystalPlasticityThermalEigenstrain.html\nor the doxygen one?\nI cant find the doxygen but this one is the base class and most of the APIs shared\nhttps://mooseframework.inl.gov/docs/doxygen/modules/classComputeCrystalPlasticityEigenstrainBase.html",
                  "url": "https://github.com/idaholab/moose/discussions/25389#discussioncomment-6897143",
                  "updatedAt": "2023-09-03T14:14:43Z",
                  "publishedAt": "2023-09-03T14:14:43Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Yeah, I was looking for the first one. Thank you.\n\u2026\nOn Sun, Sep 3, 2023 at 3:14\u202fPM Guillaume Giudicelli < ***@***.***> wrote:\n This page?\n\n https://mooseframework.inl.gov/source/materials/crystal_plasticity/ComputeCrystalPlasticityThermalEigenstrain.html\n or the doxygen one?\n I cant find the doxygen but this one is the base class and most of the\n APIs shared\n\n https://mooseframework.inl.gov/docs/doxygen/modules/classComputeCrystalPlasticityEigenstrainBase.html\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#25389 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJSA254JA7T5U2DNMWQBD4TXYSGF3ANCNFSM6AAAAAA4JKKMVU>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/25389#discussioncomment-6897483",
                  "updatedAt": "2023-09-03T15:30:59Z",
                  "publishedAt": "2023-09-03T15:30:58Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Accessing initial values of variable",
          "author": {
            "login": "adigc"
          },
          "bodyText": "For a class I am implementing, when calcuating the residual,  I need the initial values of a variable.\nIs there a function similar to _var.dofValues() with which I can access the initial values of a DOF ?\nIf not how can I get the intial values set from the input file.\nNOTE: I am setting inital value to the variable in the input file.",
          "url": "https://github.com/idaholab/moose/discussions/25365",
          "updatedAt": "2023-09-15T08:47:42Z",
          "publishedAt": "2023-08-31T08:56:50Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nSo you want the initial value later on, as in you cannot rely on the current value of the variable?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25365#discussioncomment-6877148",
                  "updatedAt": "2023-08-31T15:33:19Z",
                  "publishedAt": "2023-08-31T15:33:18Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I would just create an auxiliary variable with the same initial value and not updating it",
                          "url": "https://github.com/idaholab/moose/discussions/25365#discussioncomment-6877151",
                          "updatedAt": "2023-08-31T15:33:40Z",
                          "publishedAt": "2023-08-31T15:33:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "adigc"
                          },
                          "bodyText": "sounds good ... I will follow this approach. But then my question is, this approach will consume more memory no ?",
                          "url": "https://github.com/idaholab/moose/discussions/25365#discussioncomment-6882684",
                          "updatedAt": "2023-09-01T07:04:49Z",
                          "publishedAt": "2023-09-01T07:04:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Sure. But it s not that much. Any time you add a new field you want to look at with paraview, you also incur that kind of cost.\nIf you change the time integration scheme it ll store even more fields, there's plenty of ways to add an extra field that will cost you memory",
                          "url": "https://github.com/idaholab/moose/discussions/25365#discussioncomment-6886917",
                          "updatedAt": "2023-09-01T15:11:01Z",
                          "publishedAt": "2023-09-01T15:11:00Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Where is the functions computeQpResidual and computeQpjacobian are called?",
          "author": {
            "login": "Wolke926"
          },
          "bodyText": "I have some trouble understanding the structure of how MOOSE works. In the kernel files, the functions computeQpResidual and computeQpjacobian can be defined but where or in which file are they called?",
          "url": "https://github.com/idaholab/moose/discussions/25382",
          "updatedAt": "2023-09-01T15:18:06Z",
          "publishedAt": "2023-09-01T03:32:05Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThey are called in the residual and Jacobian loops respectively, see framework/include/loops/ComputeResidualThread.h for example\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25382#discussioncomment-6881732",
                  "updatedAt": "2023-09-01T03:41:12Z",
                  "publishedAt": "2023-09-01T03:41:11Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Using Test Harness with MPI on HPC",
          "author": {
            "login": "Vandenbg"
          },
          "bodyText": "Hello,\nI am attempting to take advantage of the Test Harness and MPI. I have a test file which contains ~1000 parameterizations for a simulation. Here is my PBS script:\n#PBS -N gtvb_job\n#PBS -l select=5:ncpus=48:mpiprocs=48\n#PBS -l walltime=24:00:00\n#PBS -P moose\n\nJOB_NUM=${PBS_JOBID%%\\.*}\n\ninput_file=input_test\n\ncd /home/vandgavi/input\n\nmodule load use.moose moose-dev\n\n'/home/vandgavi/projects/moose/./run_tests' -i $input_file -C $PWD -x -t --include-input-file -j 240 -p 24\n\nBefore simulation start the 'Active Jobs' tab states that I have requested 5 nodes and 240 cpu. Once the simulation starts, this changes to 1 node and 48 cpu. Each simulation then runs with 'Num Processors:1, Num Threads:1', which is drastically slower than using the mpiexec command for 1 parameterization using 1 node and 24 cpu.\nI have this same issue with a test file with several 'sampler, type = quadrature' calculations. These initially state the use of 1 node and 24 cpu, however, each calculation by the quadrature sample is run with 'Num Processors:1, Num Threads:1'.\nIs this a bug or a feature? Is there a way to use the Test Harness with MPI?",
          "url": "https://github.com/idaholab/moose/discussions/25342",
          "updatedAt": "2023-08-31T20:00:41Z",
          "publishedAt": "2023-08-29T22:30:41Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nWhat you have would be fine if you had a single node with 240 CPUs essentially.\nIf you do ./run_tests --help you should see all the options for using PBS instead of relying on the python distribution of tests\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6857978",
                  "updatedAt": "2023-08-29T22:34:46Z",
                  "publishedAt": "2023-08-29T22:34:46Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Vandenbg"
                          },
                          "bodyText": "Hello, Guillaume. These are the options you're talking about.\n  Options controlling which queue manager to use\n  --pbs name               Launch tests using PBS as your scheduler. You must supply a name to identify this session with\n  --pbs-pre-source    Source specified file before launching tests\n  --pbs-project           Identify your job(s) with this project (default: moose)\n  --pbs-queue             Submit jobs to the specified queue\n  --pbs-node-cpus       CPUS Per Node. The default (no setting), will always use only one node\n  --pbs-cleanup name    Clean up files generated by supplied --pbs name\n\nThe only one I understand in this is '--pbs-node-cpus 48'. I tacked it onto the ./run_tests command within my PBS script and I am still getting the same problem.\nI sort of understand your reply, could you explain the second sentence further for me?",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6858088",
                          "updatedAt": "2023-08-29T23:01:04Z",
                          "publishedAt": "2023-08-29T23:01:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I dont think you should be including this in your script.\nI think you just run run_tests on the command line with the pbs options to start pbs jobs",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6859388",
                          "updatedAt": "2023-08-30T03:45:59Z",
                          "publishedAt": "2023-08-30T03:45:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "GiudGiud is correct. You use the TestHarness to launch PBS jobs, and not PBS to launch the TestHarness. Er... this isn't 100% accurate. The TestHarness does invoke the TestHarness again. But in a very controlled way. The fact remains this is not supposed to be the responsibility of the user.\nWhat you should do instead (but there may be caveats in your case):\nhead_node> ./run_tests --pbs my_run \\\n  --spec-file path/to/1000_job_test_spec_file \\\n  --pbs-node-cpus 48 \\\n  --pbs-project gtvb_job \\\n  --pbs-pre-source /path/to/env_profile # a bash/sh profile that performs `module load use.moose moose-dev`\nThis will create a PBS Script, and launch it via qsub.\nThe TestHarness will figure out how many nodes it needs based on the maximum discovered core count requirement among all jobs in a single test spec file (min_parallel = ) divided by supplied --pbs-node-cpus argument. So in order for the TestHarness to create the same PBS submission script you supplied, somewhere among your 1000 job test spec file you will need a job asking for a minimum of 240:\n[Tests]\n  [test_1000]\n    min_parallel = 240\n  [../]\n[]\n\nWhere the TestHarness isn't cool... is if among your 1000 jobs you have jobs only asking for 1 core, or 2, or anything less than 240, weird 'stuff' is going to happen:\nThe TestHarness being Python based, is only scalable to about 12 simultaneous jobs. So however you end up divvying out jobs, if the TestHarness begins to track the goings-on of more than this, you'll start to see a performance drop.\nBasically, if the TestHarness ends up running -j 240, and in turn actually launches 240 single jobs, I cannot say how efficient that will be.\nThe bottom line, I would have each job in your 1000 job test spec file contain min_parallel = 240. This of course will mean each job (all 1000 of them) will run serially.\nI hope this made some sense. I'd be happy to go into more detail!",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6864811",
                          "updatedAt": "2023-08-30T14:07:37Z",
                          "publishedAt": "2023-08-30T14:06:37Z",
                          "isAnswer": true
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Vandenbg"
                          },
                          "bodyText": "Thanks for the suggestion Guillaume, and the explanation Jason.\nI have been tinkering with the ./run_tests command and I have a confirmation and a clarifying question.\n\nmin_parallel is the number of processors used by a single simulation and to reduce errors I should set it to the same value for all simulations? If I set min_parallel = 480 and --pbs-node-cpus 48, then it will do 10 simulations at once with 48 cpu? Can I place it outside a simulation block? Like so:\n\n[Tests]\n  min_parallel = 240\n  [test_1000]\n     ...\n  []\n[]\n\n\nFor --pbs-pre-source, I should put the path to my .bash_profile which contains these lines? or would it be better to just make a short bash script with just the command?\n\n# start moose every time\nmodule load use.moose moose-dev\n\nThanks for the help!!!",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6866846",
                          "updatedAt": "2023-08-30T17:05:10Z",
                          "publishedAt": "2023-08-30T17:05:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I've never put min_parallel here, always in individual tests.Does it work?",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6866901",
                          "updatedAt": "2023-08-30T17:10:12Z",
                          "publishedAt": "2023-08-30T17:10:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "min_parallel = 480 and --pbs-node-cpus 48, then it will do 10 simulations at once with 48 cpu?\n\nThis will instruct the TestHarness to request the following resources in the PBS select statement:\n#PBS -l select=10:ncpus=48:mpiprocs=48\n\nAs for what the TestHarness will do when it encounters your tests, if you supply the following in your test spec file:\n[Tests]\n  min_parallel = 480\n  [test_1000]\n     ...\n  []\n[]\n\nThe 'second' TestHarness will receive the following -j argument while on a node:\n./run_tests -j 480\nWhich will execute your MOOSE based application thusly:\nmpiexec -n 480 moose_app-opt -i some_input_file.i\nAnd because min_parallel is set as such for every test in the test spec file, the TestHarness will understand that it only has enough resources granted to it, to execute one job at a time.\nEDIT: forgot to answer your bash/profile Q:\nI would try to limit what the TestHarness's sources to an absolute minimum. But thats me! I see no harm in asking the TestHarness to source your ~/.bashrc. Complicated things might get in the way I suppose (Like, if your ~/.bashrc also sources/loads Conda, etc)",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6867060",
                          "updatedAt": "2023-08-30T17:30:55Z",
                          "publishedAt": "2023-08-30T17:23:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Honestly I've never placed min_parallel in a top-level block as well. I am not sure if this works. But I also don't see why that wouldn't work.",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6867064",
                          "updatedAt": "2023-08-30T17:31:41Z",
                          "publishedAt": "2023-08-30T17:24:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "To tell you the truth, the TestHarness was never intended to facilitate large jobs. Just a bunch of smaller ones. Its there to \"test\" the capabilities of MOOSE, not really to be your problem solver/manager. I applaud your attempt! I am curious how it works out for you.",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6867097",
                          "updatedAt": "2023-08-30T17:28:03Z",
                          "publishedAt": "2023-08-30T17:28:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Vandenbg"
                          },
                          "bodyText": "I appreciate the discussion and conversion to the scripts I know, this is very helpful. Placing it on top gives me a fatal error (\"QSUB Group Failure\"), that may be caused by something else, I'll need to trouble shoot. I have been told several times that the test harness isn't meant for this, however, it has an amazing capability that I want to take advantage of. I will place it within each block! I'll make a post later if I find anything.\nCheers!",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6867139",
                          "updatedAt": "2023-08-30T17:32:52Z",
                          "publishedAt": "2023-08-30T17:32:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I like the idea of supporting min_parallel at the top. I will look into adding this!",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6867148",
                          "updatedAt": "2023-08-30T17:33:40Z",
                          "publishedAt": "2023-08-30T17:33:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Vandenbg"
                  },
                  "bodyText": "Okay, I've had a bit of time to mess with the ./run_tests PBS options. I found a few things while trouble shooting.\nThis is what I submitted:\n'/home/vandgavi/projects/bison/./run_tests' --pbs testing --spec-file /home/vandgavi/inputs/fp/di/testing_3 --pbs-node-cpus 48 --pbs-project ne_ldrd --pbs-pre-source /home/vandgavi/.bash_profile\n\n--pbs $name this will create an output file where your run tests file is $name.\n--pbs-project $project required for project.\n--spec-file this is where the disconnect begins. I tell ./run_tests to run my testing_3 spec file and it creates a $test_name.qsub script within the specified folder:\n#!/bin/bash\n#PBS -N params-di_1\n#PBS -l select=1:ncpus=24\n#PBS -l walltime=00:06:00\n#PBS -P ne_ldrd\n\n#PBS -j oe\n#PBS -o /home/vandgavi/inputs/fp/di/qsub.output\n#PBS -l place=free\nsource /home/vandgavi/.bash_profile || exit 1\nJOB_NUM=${PBS_JOBID%\\.*}\n\nexport MV2_ENABLE_AFFINITY=0\n\ncd /home/vandgavi/projects/bison\n/home/vandgavi/projects/bison/run_tests -j 24 --pbs-node-cpus 48 --pbs-project ne_ldrd --pbs-pre-source /home/vandgavi/.bash_profile --spec-file /home/vandgavi/inputs/fp/di/tests -o /home/vandgavi/inputs/fp/di --sep-files\n\nThere are a couple things to point out here and other tests I tried.\nThe walltime requested is a sum of each max_time in the script, for example, my testing spec has 3 tests each with 120s max_time. Results in walltime=00:06:00.  The sum of max_time's within the spec can NOT exceed the max wall time of 168 hours, it will request over maximum. @milljm this may be something you could fix in the same issue, I am not sure.\nThe next is that if the outputs of this script are present, it will automatically skip all the tests. if you wish to re-run this same test, all outputs must be deleted, i.e. the $test_name.qsub and $name files in their respective folders mentioned earlier.\nThis is something I call a bug, however, it may be a feature. The created .qsub file contains the line:\n--spec-file /home/vandgavi/inputs/fp/di/tests\nThis is wrong, even though my initial ./run_tests command includes the name of my spec as testing_3 it instead ran my tests spec which only checks input syntax. For now you must name the spec file you want to run, tests. Which throws a wrench in my plans as I have roughly 70 spec files with different names I'd like to use.\nThe first test in the script will be the name that shows up within the \"Active Jobs\" tab. This will show that only one job is running, however, the output shows that all three are running at one after another.\nWithin each test block in the spec file min_parallel = # must be specified or it will run on one cpu. The min_parallel=# divided by --pbs-node-cpus # will show up in the -j # of the $test_name.qsub file above. It will ask for the right number in the $test_name.qsub, however, when it trys to cue the job with those resources, it says there are \"insufficient slots\". For example, it will cue 2 nodes 96 cpu, the tests with min_parallel=96 will fail and it will say insufficient slots. Right now it seems like it can only use 1 node 48 cpu.\nAt this point, I'm not sure I can use Test Harness with MPI on the HPC.",
                  "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6868719",
                  "updatedAt": "2023-08-30T20:45:32Z",
                  "publishedAt": "2023-08-30T20:45:31Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Can you post the contents of /home/vandgavi/inputs/fp/di/testing_3? I'd like to understand why the TestHarness decided to call it something different later on in the PBS script. I recall we do some separation work, but we don't create new spec files as far as I know. IIRC -i and --spec-file are supposed to be treated as precisely that: this test file.\n\nThe walltime requested is a sum of each max_time in the script, for example, my testing spec has 3 tests each with 120s max_time. Results in walltime=00:06:00. The sum of max_time's within the spec can NOT exceed the max wall time of 168 hours, it will request over maximum. @milljm this may be something you could fix in the same issue, I am not sure.\n\nThis will be interesting to tackle. I am wondering if the TestHarness is hard-coded to run one test at a time. I don't think that is the case, but now it's got me wondering. @gambka, is this the issue you are concerned with (the additional questions about parallel_scheduler = true?\nI am heading home for the day, and will address the rest tomorrow!",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6868836",
                          "updatedAt": "2023-08-30T21:05:08Z",
                          "publishedAt": "2023-08-30T21:05:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Vandenbg"
                          },
                          "bodyText": "I love the test harness capability. Making it easier to use would be awesome! I know others that use it who would also appreciate this work.\nHere are the contents of that file. To make things a little more complicated, this script calls on a sampler input. Something else you could look at is the fact that the sampler will cue each individual simulation within it using 1 node 1 cpu no matter what. MPI on those would be very helpful... I have to change things up a little bit because I am technically working with KP-Bison, but here it is:\n[Tests]\n  parallel_scheduling = True\n  [test1]\n    type = 'RunApp'\n    input = '../direct_integration.i'\n    cli_args = 'input_1d=c1/t_1d.i\n                input_cracking=c1/t_cracking.i\n                input_asphericity=c1/t_asphericity.i\n                output=output/test_out/TEST1'\n    max_time = 60\n    min_parallel = 48\n  []\n  [test2]\n    type = 'RunApp'\n    input = '../direct_integration.i'\n    cli_args = 'input_1d=c1/t_1d.i\n                input_cracking=c1/t_cracking.i\n                input_asphericity=c1/t_asphericity.i\n                output=output/test_out/TEST2'\n    max_time = 60\n    min_parallel = 48\n  []\n  [test3]\n    type = 'RunApp'\n    input = '../direct_integration.i'\n    cli_args = 'input_1d=c1/t_1d.i\n                input_cracking=c1/t_cracking.i\n                input_asphericity=c1/t_asphericity.i\n                output=output/test_out/TEST3'\n    max_time = 60\n    min_parallel = 48\n  []\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6868934",
                          "updatedAt": "2023-08-30T21:19:24Z",
                          "publishedAt": "2023-08-30T21:19:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Something else you could look at is the fact that the sampler will cue each individual simulation within it using 1 node 1 cpu no matter what. MPI on those would be very helpful...\n\nI'm afraid I have no idea what you are talking about here. Is this some script in MOOSE somewhere?\nI think the only way to handle the wall time is going to be through an additional parameter. A global one:\n[Tests]\n  parallel_scheduling = True\n  walltime = 5:00:00\n  min_parallel = 240\n  [test1]\n    max_time = 60\n    min_parallel = 48\n  []\n  [test2]\n    max_time = 60\n    min_parallel = 48\n  []\n  [test3]\n    max_time = 60\n    min_parallel = 48\n  []\n[]\n\nIf what I want to implement, is implemented, the idea is that 5 jobs would run simultaneously (-j 240 / 48 = 5), and all the jobs contained in this spec file need to finish within 5 hours. Does this sound right?",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6875545",
                          "updatedAt": "2023-08-31T13:01:44Z",
                          "publishedAt": "2023-08-31T13:01:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Vandenbg"
                          },
                          "bodyText": "One of these simulations uses the Quadrature Sampler. This will run many simulations within it, all with 1 node and 1 cpu.\nYes that sounds right.",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6877423",
                          "updatedAt": "2023-08-31T15:58:07Z",
                          "publishedAt": "2023-08-31T15:58:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Oh, I see. I should let you know that I am not a C++ developer :) So the TestHarness is about it as far as my involvement goes.",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6878110",
                          "updatedAt": "2023-08-31T17:09:01Z",
                          "publishedAt": "2023-08-31T17:09:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "First step could be implementing those cool things in run_tests logic. Then maybe we would want to factor out the scheduling to a proper \"scheduler.py\" scripts so people's workflow doesnt involve run_tests",
                          "url": "https://github.com/idaholab/moose/discussions/25342#discussioncomment-6879493",
                          "updatedAt": "2023-08-31T20:00:42Z",
                          "publishedAt": "2023-08-31T20:00:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}