{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wNy0yN1QwMTozODoxMS0wNTowMM4AUupX"
    },
    "edges": [
      {
        "node": {
          "title": "Use several different custom interpolation functions simultaneously",
          "author": {
            "login": "aaaaaaqing"
          },
          "bodyText": "In the finite element, the interpolation function (test function) is very important, especially in some algorithms, the selection of the interpolation function directly affects the results of the algorithm. At present, I want to use multiple interpolation functions in the same formula. So how do you use multiple interpolating functions in an equation at the same time (even scalar interpolating functions and vector interpolating functions at the same time)? If there is a way or a similar example, I would appreciate your help",
          "url": "https://github.com/idaholab/moose/discussions/25046",
          "updatedAt": "2023-07-31T17:55:24Z",
          "publishedAt": "2023-07-27T06:55:55Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "Can you be more specific about what kind of discretization you want to perform? We do things like Streamline-Upwind Petrov-Galerkin in several places in the repository",
                  "url": "https://github.com/idaholab/moose/discussions/25046#discussioncomment-6567386",
                  "updatedAt": "2023-07-27T16:17:35Z",
                  "publishedAt": "2023-07-27T16:17:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "Thanks for your reply. I had some difficulties with moose's interpolation functions, first I wanted to use both vector and scalar functions to dediscretize equations, and to custom-build Jacobian matrices and residuals (both interpolation functions are used). But I don't know how to use this algorithm in moose, so I'd like to check if there are any good suggestions",
                          "url": "https://github.com/idaholab/moose/discussions/25046#discussioncomment-6567771",
                          "updatedAt": "2023-07-27T17:00:50Z",
                          "publishedAt": "2023-07-27T17:00:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "That's not very clear to me. Do you have multiple physics? What are they?",
                          "url": "https://github.com/idaholab/moose/discussions/25046#discussioncomment-6569267",
                          "updatedAt": "2023-07-27T20:10:12Z",
                          "publishedAt": "2023-07-27T20:10:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "This is because the weak form I use comes from an algorithm:\nI have a current continuity equation: \u2207 jn-\u2202 n/\u2202t-Rn=0, where Jn is the current density, n is the carrier concentration, and Rn is the net recombination velocity. In the steady state case, this equation becomes \u2207 Jn-Rn =0, then use Jn = W (j) *Jn (j), where W is the vector interpolation function, then i use Ni as the scalar test function, then the equation becomes: N (i) *\u2207 (W (j) *Jn (j)) -Ni *Rn =0. Therefore, I want to use both interpolation functions at the same time, and whether moose is capable of handling this situation",
                          "url": "https://github.com/idaholab/moose/discussions/25046#discussioncomment-6586369",
                          "updatedAt": "2023-07-30T08:44:33Z",
                          "publishedAt": "2023-07-30T08:44:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "This is just an example that I'm giving you to show you my discretization, you don't have to think about the rationality of this algorithm, just to show why I want two different kinds of interpolation functions to be used at the same time, right",
                          "url": "https://github.com/idaholab/moose/discussions/25046#discussioncomment-6586386",
                          "updatedAt": "2023-07-30T08:48:03Z",
                          "publishedAt": "2023-07-30T08:48:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "I'll think about the rationality of it as I see fit for answering the questions you are asking",
                          "url": "https://github.com/idaholab/moose/discussions/25046#discussioncomment-6596803",
                          "updatedAt": "2023-07-31T15:54:40Z",
                          "publishedAt": "2023-07-31T15:53:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "By \"two different kinds of interpolation functions\" do you mean different Finite Element spaces?  You can certainly have both a vector-valued and a scalar-valued variable coupled into the same system of equations (though vector-valued spaces are tricky enough that in general users tend to construct them from tensor products of scalar spaces unless they have some special needs like Nedelec elements where the space can't be decomposed that way).\nOr are you trying to talk about Petrov-Galerkin methods?  Your question here is really hard to parse.",
                          "url": "https://github.com/idaholab/moose/discussions/25046#discussioncomment-6597846",
                          "updatedAt": "2023-07-31T17:53:12Z",
                          "publishedAt": "2023-07-31T17:53:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "To add on to @roystgnr's comment, we do have examples of multiple-variable systems with multiple finite element types (one being scalar, and another being vector). One example is here: https://github.com/idaholab/moose/blob/next/test/tests/kernels/vector_fe/coupled_scalar_vector.i",
                          "url": "https://github.com/idaholab/moose/discussions/25046#discussioncomment-6597867",
                          "updatedAt": "2023-07-31T17:55:24Z",
                          "publishedAt": "2023-07-31T17:55:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PETSc was configured with MPICH but now appears to be compiling using a non-MPICH mpi.h",
          "author": {
            "login": "richman2024"
          },
          "bodyText": "Hi,\nWhen I ran \"make -j 6\" for test after installed MOOSE following the Build and Test MOOSE, I got 2 problems as below:\n/public/home/ghfund3_a47/mambaforge3/envs/moose/include/petscsys.h:235:6: error: #error \"PETSc was configured with MPICH but now appears to be compiling using a non-MPICH mpi.h\"`\n`/opt/rh/devtoolset-7/root/usr/include/c++/7/bits/stl_algobase.h:378:57: error: no type named 'value_type' in 'struct std::iterator_traits<infix_ostream_iterator<std::basic_string<char> > >'`\n\nWhat should I do to fix them?\nThank you  for your help!",
          "url": "https://github.com/idaholab/moose/discussions/25047",
          "updatedAt": "2023-07-29T22:41:20Z",
          "publishedAt": "2023-07-27T08:14:25Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nfor the first one it seems you are mixing mpi distributions. Can you paste the output of the diagnostics script in moose/scripts? So we can check how the mixing is happening\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25047#discussioncomment-6562702",
                  "updatedAt": "2023-07-27T08:28:56Z",
                  "publishedAt": "2023-07-27T08:28:55Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "richman2024"
                          },
                          "bodyText": "Hi GiudGiud,\nThe output after ran diagnostics.sh as below:\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: NFSServer Description: NFS Server release 3.2 (RTM1) Release: 3.2 Codename: RTM1\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 128\n\nMemory Free: 174553.957 MB\n\n$CC not set\n\nMPICC:\nwhich mpicc:\n        /opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin/mpicc\nmpicc -show:\n        gcc -I/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include -I/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include/openmpi -I/opt/hpc/software/mpi/hwloc/include -pthread -L/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib -L/opt/hpc/software/mpi/hwloc//lib -L/usr//lib -Wl,-rpath -Wl,/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib -Wl,-rpath -Wl,/opt/hpc/software/mpi/hwloc//lib -Wl,-rpath -Wl,/usr//lib -Wl,--enable-new-dtags -lmpi\n\nCOMPILER gcc:\ngcc (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\nCopyright (C) 2017 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n        /public/home/ghfund3_a47/mambaforge3/bin/python\n        Python 3.10.6\n\nMODULES:\nCurrently Loaded Modulefiles:\n  1) compiler/devtoolset/7.3.1   3) compiler/dtk/22.10.1\n  2) mpi/hpcx/2.11.0/gcc-7.3.1\n\nPETSC_DIR not set\n\nENVIRONMENT:\nCPLUS_INCLUDE_PATH=/public/software/compiler/dtk-22.10.1/include:/public/software/compiler/dtk-22.10.1/llvm/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux\nMANPATH=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/share/man:/opt/rh/devtoolset-7/root/usr/share/man:/opt/gridview/slurm/share/man:/opt/gridview/munge/share/man:\nLDFLAGS=-Wl,-rpath=/opt/rh/devtoolset-7/root/usr/lib64 -Wl,-rpath=/opt/rh/devtoolset-7/root/usr/lib\nXDG_SESSION_ID=1012616\nSHMEM_HOME=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1\nHOSTNAME=login01\nAMDGPU_TARGETS=gfx906\nHPCX_MPI_DIR=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1\nOMPI_MCA_btl_openib_allow_ib=1\nSHELL=/bin/bash\nTERM=xterm\nHISTSIZE=1000\nSSH_CLIENT=10.10.2.2 55752 22\nPERL5LIB=/opt/rh/devtoolset-7/root/usr/lib64/perl5/vendor_perl:/opt/rh/devtoolset-7/root/usr/share/perl5/vendor_perl:/public/home/ghfund3_a47/perl5/lib/perl5:\nLIBRARY_PATH=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib\nOMPI_MCA_coll_hcoll_enable=0\nCONDA_SHLVL=1\nHSA_PATH=/opt/rocm/hsa\nUCX_MAX_EAGER_LANES=4\nCONDA_PROMPT_MODIFIER=(base) \nQTDIR=/usr/lib64/qt-3.3\nUCX_MAX_RNDV_LANES=4\nPERL_MB_OPT=--install_base /public/home/ghfund3_a47/perl5\nQTINC=/usr/lib64/qt-3.3/include\nUCX_ZCOPY_THRESH=16384\nSSH_TTY=/dev/pts/4\nCLUSCONF_HOME=/opt/clusconf\nOMPI_MCA_pml=ucx\nQT_GRAPHICSSYSTEM_CHECKED=1\nUSER=ghfund3_a47\nHISTORY_FILE=/var/log/user-log/user.log\nHPCX_DIR=/opt/hpc/software/mpi/hpcx/v2.11.0\nOPAL_PREFIX=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:\nLD_LIBRARY_PATH=/public/software/compiler/dtk-22.10.1/lib:/public/software/compiler/dtk-22.10.1/lib64:/public/software/compiler/dtk-22.10.1/hip/lib:/public/software/compiler/dtk-22.10.1/llvm/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/lib:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/gridview/slurm/lib:/opt/gridview/slurm/lib64:/opt/gridview/munge/lib\nUCX_RNDV_THRESH=16384\nUCX_IB_PCI_BW=mlx5_0:50Gbs,mlx5_1:50Gbs,mlx5_2:50Gbs,mlx5_3:50Gbs\nCONDA_EXE=/public/home/ghfund3_a47/mambaforge3/bin/conda\nCPATH=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include\nHPCX_HOME=/opt/hpc/software/mpi/hpcx/v2.11.0\nUCX_IB_ADDR_TYPE=ib_global\n_CE_CONDA=\nHPCX_SHARP_DIR=/opt/hpc/software/mpi/hpcx/v2.11.0/sharp\nUCX_NET_DEVICES=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1\nPATH=/public/home/ghfund3_a47/mambaforge3/bin:/public/home/ghfund3_a47/mambaforge3/condabin:/public/software/compiler/dtk-22.10.1/bin:/public/software/compiler/dtk-22.10.1/llvm/bin:/public/software/compiler/dtk-22.10.1/hip/bin:/public/software/compiler/dtk-22.10.1/hip/bin/hipify:/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/bin:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/public/home/ghfund3_a47/perl5/bin:/opt/rocm/bin:/opt/rocm/profiler/bin:/opt/rocm/opencl/bin/x86_64:/opt/rocm/hcc/bin:/opt/rocm/hip/bin:/opt/gridview/slurm/bin:/opt/gridview/slurm/sbin:/opt/gridview/munge/bin:/opt/gridview/munge/sbin:/opt/clusconf/sbin:/opt/clusconf/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/sbin:/public/home/ghfund3_a47/.local/bin:/public/home/ghfund3_a47/bin\nMAIL=/var/spool/mail/ghfund3_a47\nNFSCONF=/opt/clusconf/etc/nfs.cfg\nROCM_PATH=/public/software/compiler/dtk-22.10.1\nHPCX_HCOLL_DIR=/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll\nC_INCLUDE_PATH=/public/software/compiler/dtk-22.10.1/include:/public/software/compiler/dtk-22.10.1/llvm/include:/opt/rh/devtoolset-7/root/usr/include/c++/7:/opt/rh/devtoolset-7/root/usr/include/c++/7/x86_64-redhat-linux:/opt/gridview/slurm/include:/opt/gridview/munge/include\nCONDA_PREFIX=/public/home/ghfund3_a47/mambaforge3\nPWD=/public/home/ghfund3_a47/projects/moose-next/scripts\nOSHMEM_HOME=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1\n_LMFILES_=/opt/hpc/software/modules/compiler/devtoolset/7.3.1:/opt/hpc/software/modules/mpi/hpcx/2.11.0/gcc-7.3.1:/public/software/modules/compiler/dtk/22.10.1\nIPMICONF=/opt/clusconf/etc/ipmi.cfg\nLANG=en_US.UTF-8\nLOG_HOME=/var/log/user-log\nHPCX_UCX_DIR=/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm\nMODULEPATH=/public/software/modules:/opt/hpc/software/modules\nAUTOCLUSCONF=/opt/clusconf/etc/autoconf.cfg\nKDEDIRS=/usr\nLOADEDMODULES=compiler/devtoolset/7.3.1:mpi/hpcx/2.11.0/gcc-7.3.1:compiler/dtk/22.10.1\nGRIDVIEW_HOME=/opt/gridview\nOMPI_MCA_btl_openib_warn_default_gid_prefix=0\nHCC_HOME=/opt/rocm/hcc\nSLURM_HOME=/opt/gridview/slurm\nOMPI_HOME=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1\nHISTCONTROL=ignoredups\n_CE_M=\nHOME=/public/home/ghfund3_a47\nSHLVL=2\nHPCX_OSHMEM_DIR=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1\nUCX_RC_MLX5_TIMEOUT=1000ms\nOMPI_MCA_btl_openib_warn_no_device_params_found=0\nUCX_DC_MLX5_TIMEOUT=1000ms\nUSER_ACTION=ghfund3_a47(10.10.2.2)\nPERL_LOCAL_LIB_ROOT=:/public/home/ghfund3_a47/perl5\nLOGNAME=ghfund3_a47\nCONDA_PYTHON_EXE=/public/home/ghfund3_a47/mambaforge3/bin/python\nSTARTWAITTIME=300\nQTLIB=/usr/lib64/qt-3.3/lib\nSSH_CONNECTION=10.10.2.2 55752 10.10.2.1 22\nXDG_DATA_DIRS=/public/home/ghfund3_a47/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\nMODULESHOME=/usr/share/Modules\nMPI_HOME=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1\nOMPI_MCA_coll_hcoll_np=0\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nCONDA_DEFAULT_ENV=base\nPROMPT_COMMAND={ date \"+[%F %T `hostname` `whoami`] `history 1 | { read x cmd; echo \" -> $USER_ACTION   $cmd\";}`\"; }>> $HISTORY_FILE\nINFOPATH=/opt/rh/devtoolset-7/root/usr/share/info\nXDG_RUNTIME_DIR=/run/user/3777\nMUNGE_HOME=/opt/gridview/munge\nHIP_PATH=/public/software/compiler/dtk-22.10.1/hip\nQT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins\nINCLUDE=/opt/hpc/software/mpi/hpcx/v2.11.0/gcc-7.3.1/include:/opt/hpc/software/mpi/hpcx/v2.11.0/ucx_without_rocm/include:/opt/hpc/software/mpi/hpcx/v2.11.0/sharp/include:/opt/hpc/software/mpi/hpcx/v2.11.0/hcoll/include\nPERL_MM_OPT=INSTALL_BASE=/public/home/ghfund3_a47/perl5\nOMPI_MCA_btl=^uct\nBASH_FUNC_module()=() {  eval `/usr/bin/modulecmd bash $*`\n}\n_=/usr/bin/env",
                          "url": "https://github.com/idaholab/moose/discussions/25047#discussioncomment-6562775",
                          "updatedAt": "2023-07-29T22:41:06Z",
                          "publishedAt": "2023-07-27T08:36:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You have mamba and a local MPI compiler loaded.\nI think you are on a HPC machine so you should not use mamba. You should be following our HPC instructions",
                          "url": "https://github.com/idaholab/moose/discussions/25047#discussioncomment-6563534",
                          "updatedAt": "2023-07-27T09:57:06Z",
                          "publishedAt": "2023-07-27T09:57:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richman2024"
                          },
                          "bodyText": "You are right!\nI tried again following HPC Cluster, when I was runing 'update_and_rebuild_petsc.sh' got below report:\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\npython3: can't open file '/public/home/ghfund3_a47/projects/moose-next/petsc/./configure': [Errno 2] No such file or directory\nThere was an error. Exiting...\n\nIt seems without HDF5 on the HPC, but I haven't permission to install HDF5.\nAny other ways can I use to install MOOSE?",
                          "url": "https://github.com/idaholab/moose/discussions/25047#discussioncomment-6565746",
                          "updatedAt": "2023-07-29T22:40:45Z",
                          "publishedAt": "2023-07-27T14:06:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "If you have write access to your home directory, and access to the internet from that account: then you have permission to install everything you need.\nThe error in question may stem from an earlier error. Perhaps failing to download HDF5? The modules I am seeing loaded, as well as the version of GCC in use, is below the minimum version we require (MOOSE requires GCC 7.5.0 or newer). So you will need to take care of that before continuing (PETSc may build fine, then libMesh... but then MOOSE will fail, wasting your time!).\nIt's most likely that you have newer versions or GCC available via modules. My advice is to run: module avail, peruse over that list, and choose modules which satisfy MOOSE's requirements. They need to fall into:\n\n\nPython: >=3.8 <3.11\n\n3.10.x being the stable standard at the moment.\n\n\n\nGCC >= 7.5.0 < 11.x\n\nTry to stick with a version in between the range above. It's safer to stay away from bleeding edge compilers. Most of our testing is done on GCC 8.x - 10.x.\nIntel compilers are not supported.\n\n\n\nMPI\n\nThis can be OpenMPI, MPICH, MVAPICH. You'll likely see all three available on your cluster. However, it may be best to ask your HPC admins which MPI stack to use. Clusters are usually tailor made, and therefor have a very optimized MPI stack. This is why we ask users to avoid using our Conda packages, when operating on an HPC cluster.\n\n\n\nBasically, I am asking you to remove Conda from the equation, and start over, by figuring out what \"modules\" are available on your cluster.",
                          "url": "https://github.com/idaholab/moose/discussions/25047#discussioncomment-6566434",
                          "updatedAt": "2023-07-27T14:58:49Z",
                          "publishedAt": "2023-07-27T14:58:49Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Element Integral Variable user object",
          "author": {
            "login": "gabburgio"
          },
          "bodyText": "Hello everybody\nI wanted to ask a question about a detail I don't understand in the implementation of the Element Integral / Element Integral Variable user object.\nThe virtual Real getValue(); method in the Element Integral user object is meant to provide the value of the postprocessor for external use, if I understand correctly.\nHowever, if I try to do something like:\n_uo(getUserObject<ElementIntegralVariableUserObject>(\"user_object\")),\nto retrieve the user object, and then call uo.getValue(), for instance in some method of the class I'm working on, I get an error of the type:\npassing 'const ElementIntegralVariableUserObject' as 'this' argument discards qualifiers [-fpermissive]\nThis , to my understanding, is due to the fact that getValue() is not a constant method, therefore it cannot be called on a constant reference like the one returned by getUserObject.\nMy question is then, why is getValue() not a constant method?\nI see for instance that the public return methods of LayeredBase are constant:\n virtual Real integralValue(Point p) const;\n\n  /**\n   * Get the value for a given layer\n   * @param layer The layer index\n   * @return The value for the given layer\n   */\n  virtual Real getLayerValue(unsigned int layer) const;\n\n  /**\n   * Helper function to return the layer the point lies in.\n   * @param p The point.\n   * @return The layer the Point is found in.\n   */\n  virtual unsigned int getLayer(Point p) const;\n\nand the same is true for NearestPointBase\n/**\n  * Given a Point return the integral value associated with the layer\n  * that point falls in for the layered average closest to that\n  * point.\n  *\n  * @param p The point to look for in the layers.\n  */\n virtual Real spatialValue(const Point & p) const override;\n\n /**\n  * Get the points at which the nearest operation is performed\n  * @return points\n  */\n virtual const std::vector<Point> & getPoints() const { return _points; }\n\n\nAnd InterfaceQpUserobjectBase\n\n  /**\n   * method returning the quadrature point value\n   **/\n  Real getQpValue(const dof_id_type elem, const unsigned int side, unsigned int qp) const;\n  /**\n   * function returning the element side average value\n   **/\n  Real getSideAverageValue(const dof_id_type elem, const unsigned int side) const;\n\nand really just about any user object I see.\nAm I misunderstanding how I should retrieve the value of the Element Integral Variable user object?\nThanks in advance",
          "url": "https://github.com/idaholab/moose/discussions/25055",
          "updatedAt": "2023-07-29T09:04:50Z",
          "publishedAt": "2023-07-28T17:55:39Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nwe have an ongoing maintenance task to make this routine const.\nIt was not because it used to be performing r\u00e9duction across processes but that got cleared recently\nFor now you ll want to const cast away the constness to be able to use getValue\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25055#discussioncomment-6580852",
                  "updatedAt": "2023-07-29T03:25:04Z",
                  "publishedAt": "2023-07-29T03:25:03Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "gabburgio"
                          },
                          "bodyText": "Thank you!",
                          "url": "https://github.com/idaholab/moose/discussions/25055#discussioncomment-6581793",
                          "updatedAt": "2023-07-29T09:04:51Z",
                          "publishedAt": "2023-07-29T09:04:50Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "\"add_output\" is not a registered task",
          "author": {
            "login": "elbert5770"
          },
          "bodyText": "I have moose installed using python on an HPC cluster.  Moose works when just running directly on a node, but I'm having trouble getting it to run with sbatch.  The .slurm file is:\n#!/bin/bash\n#SBATCH --job-name=MOOSE1\n#SBATCH --account=elbert\n#SBATCH --partition=compute-hugemem\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=40\n#SBATCH --mem=700G\n#SBATCH --time=24:00:00 # Max runtime in DD-HH:MM:SS format.\n#SBATCH --chdir=/gscratch/elbert/projects/cahn_hilliard2\n#SBATCH --export=ALL\n\nmamba init\nsource ~/.bashrc\nmamba activate moose\nmpiexec -np 38 ./cahn_hilliard2-opt -i s4_mobility_cross_KM5.i\n\nThis leads to an error that I can't track down:\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/bin/conda\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/bin/conda-env\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/bin/activate\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/bin/deactivate\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/etc/profile.d/conda.sh\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/etc/fish/conf.d/conda.fish\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/shell/condabin/Conda.psm1\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/shell/condabin/conda-hook.ps1\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/lib/python3.10/site-packages/xontrib/conda.xsh\nno change     /mmfs1/gscratch/elbert/python/mambaforge3/etc/profile.d/conda.csh\nmodified      /mmfs1/home/elbertdl/.bashrc\n\n==> For changes to take effect, close and re-open your current shell. <==\n\nAdded mamba to /mmfs1/home/elbertdl/.bashrc\n\n==> For changes to take effect, close and re-open your current shell. <==\n\n\n\n*** ERROR ***\n\"add_output\" is not a registered task.\n\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0```",
          "url": "https://github.com/idaholab/moose/discussions/25049",
          "updatedAt": "2023-07-28T21:57:51Z",
          "publishedAt": "2023-07-27T15:11:32Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "This is an input problem. Can you please share your input file?",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6566648",
                  "updatedAt": "2023-07-27T15:14:43Z",
                  "publishedAt": "2023-07-27T15:14:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Separate problem - but there is not a need to call mamba init more than once.",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6566668",
                  "updatedAt": "2023-07-27T15:16:02Z",
                  "publishedAt": "2023-07-27T15:16:01Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "elbert5770"
                  },
                  "bodyText": "#\n# Example simulation of an iron-chromium alloy at 500 C. Equilibrium\n# concentrations are at 23.6 and 82.3 mol% Cr. Kappa value, free energy equation,\n# and mobility equation were provided by Lars Hoglund. Solved using the split\n# form of the Cahn-Hilliard equation.\n#\n\n[Mesh]\n  type = GeneratedMesh\n  dim = 2\n  elem_type = QUAD4\n  nx = 128\n  ny = 128\n  nz = 0\n  xmin = 0\n  xmax = 1\n  ymin = 0\n  ymax = 1\n  zmin = 0\n  zmax = 0\n  uniform_refine = 2\n[]\n\n[Variables]\n  [./c]   # Mole fraction of Cr (unitless)\n    order = FIRST\n    family = LAGRANGE\n  [../]\n  [./w]   # Chemical potential (eV/mol)\n    order = FIRST\n    family = LAGRANGE\n  [../]\n[]\n\n[Variables]\n  [./c]   # Mole fraction of Cr (unitless)\n    order = FIRST\n    family = LAGRANGE\n  [../]\n  [./w]   # Chemical potential (eV/mol)\n    order = FIRST\n    family = LAGRANGE\n  [../]\n[]\n\n[ICs]\n  [./concentrationIC]   # 46.774 mol% Cr with variations\n    type = CrossIC\n    variable = c\n    x1 = 0.25\n    x2 = 0.75\n    y1 = 0.25\n    y2 = 0.75\n    average = 0.45\n    amplitude = 0.4\n  [../]\n[]\n\n[BCs]\n  [./Periodic]\n    [./c_bcs]\n      auto_direction = 'x y'\n    [../]\n  [../]\n[]\n\n[Kernels]\n  [./w_dot]\n    variable = w\n    v = c\n    type = CoupledTimeDerivative\n  [../]\n  [./coupled_res]\n    variable = w\n    type = SplitCHWRes\n    mob_name = M\n  [../]\n  [./coupled_parsed]\n    variable = c\n    type = SplitCHParsed\n    f_name = f_loc\n    kappa_name = kappa_c\n    w = w\n  [../]\n[]\n\n[Materials]\n  # d is a scaling factor that makes it easier for the solution to converge\n  # without changing the results. It is defined in each of the first three\n  # materials and must have the same value in each one.\n  [./constants]\n    # Define constant values kappa_c and M. Eventually M will be replaced with\n    # an equation rather than a constant.\n    type = GenericFunctionMaterial\n    prop_names = 'kappa_c'\n    prop_values = '0.1'\n                   # kappa_c*eV_J*nm_m^2*d\n\n  [../]\n [./mobility]               # Mobility (nm^2 mol/eV/s)\n    # NOTE: This is a fitted equation, so only 'Conv' has units\n    type = DerivativeParsedMaterial\n    property_name = M\n    coupled_variables = c\n    constant_names =       'Acr nm_m   eV_J   d'\n    constant_expressions = '1.0  1e+09      6.24150934e+18          1e-27'\n    expression = 'Acr*c*(1-c)'\n    derivative_order = 1\n    outputs = exodus\n  [../]\n  [./local_energy]           # Local free energy function (eV/mol)\n    type = DerivativeParsedMaterial\n    property_name = f_loc\n    coupled_variables = c\n    expression = '6*27000*(1/3*(c*log(c)+(1-c)*log(1-c))+c*(1-c))'\n    derivative_order = 2\n  [../]\n[]\n\n[Postprocessors]\n  [./step_size]             # Size of the time step\n    type = TimestepSize\n  [../]\n  [./iterations]            # Number of iterations needed to converge timestep\n    type = NumNonlinearIterations\n  [../]\n  [./nodes]                 # Number of nodes in mesh\n    type = NumNodes\n  [../]\n  [./evaluations]           # Cumulative residual calculations for simulation\n    type = NumResidualEvaluations\n  [../]\n  [./active_time]           # Time computer spent on simulation\n    type = PerfGraphData\n    section_name = \"Root\"\n    data_type = total\n  [../]\n[]\n\n[Preconditioning]\n  [./coupled]\n    type = SMP\n    full = true\n  [../]\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = NEWTON\n  l_max_its = 30\n  l_tol = 1e-9\n  nl_max_its = 50\n  nl_abs_tol = 1e-9\n  dtmin = 1e-15\n  end_time = 1   # 7 days\n  petsc_options_iname = '-pc_type -ksp_gmres_restart -sub_ksp_type\n                         -sub_pc_type -pc_asm_overlap'\n  petsc_options_value = 'asm      31                  preonly\n                         ilu          1'\n  [./TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 1e-12\n    cutback_factor = 0.8\n    growth_factor = 1.5\n    optimal_iterations = 7\n  [../]\n  [./Adaptivity]\n    coarsen_fraction = 0.1\n    refine_fraction = 0.7\n    max_h_level = 4\n    cycles_per_step = 2\n  [../]\n[]\n\n[Debug]\n  show_var_residual_norms = true\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n  [out]\n    type = Checkpoint\n    interval = 3\n    num_files = 2\n  []\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6566731",
                  "updatedAt": "2023-07-27T15:20:47Z",
                  "publishedAt": "2023-07-27T15:20:47Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "elbert5770"
                  },
                  "bodyText": "The  mpiexec... command works from within a node.  Running the code for a while and then trying again to run from sbatch with --restore gives the same error, so it doesn't seem to be an issue with creating .e, .csv or checkpoint files/folder.\nRunning sbatch with 'which python' and 'mamba list' give the expected outputs.",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6567600",
                  "updatedAt": "2023-07-27T16:40:32Z",
                  "publishedAt": "2023-07-27T16:40:31Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "elbert5770"
                  },
                  "bodyText": "Removing checkpointing from output (with a new file/filename) does not fix the problem.  It seems it can't parse the .i file when launched through sbatch to find the output block?  Weird.",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6568177",
                  "updatedAt": "2023-07-27T17:44:39Z",
                  "publishedAt": "2023-07-27T17:44:39Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Can you try simplifying a bit? Just do a syntax check without MPI:\n#!/bin/bash\n#SBATCH --job-name=MOOSE1\n#SBATCH --account=elbert\n#SBATCH --partition=compute-hugemem\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=40\n#SBATCH --mem=700G\n#SBATCH --time=00:05:00 # Max runtime in DD-HH:MM:SS format.\n#SBATCH --chdir=/gscratch/elbert/projects/cahn_hilliard2\n#SBATCH --export=ALL\n\nsource ~/.bashrc\nmamba activate moose\n./cahn_hilliard2-opt -i s4_mobility_cross_KM5.i --check-input",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6577892",
                  "updatedAt": "2023-07-28T16:30:35Z",
                  "publishedAt": "2023-07-28T16:30:25Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "elbert5770"
                          },
                          "bodyText": "/mmfs1/gscratch/elbert/python/mambaforge3/envs/moose/bin/python\nSetting Up\n  Finished Uniformly Refining                                                            [^[[33m  1.42 s^[[39m] [^[[33m  204 MB^[[39m]\n^[[36mThe following total 2 aux variables:\n  M\n  dM/dc\nare added for automatic output by MaterialOutputAction.^[[39m\n  Initializing\n    Finished Initializing Equation Systems                                               [^[[33m  3.23 s^[[39m] [^[[33m  566 MB^[[39m]\n  Finished Initializing                                                                  [^[[33m  3.69 s^[[39m] [^[[33m  566 MB^[[39m]\nFinished Setting Up                                                                      [^[[33m  5.47 s^[[39m] [^[[33m  566 MB^[[39m]\nSyntax OK",
                          "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6577966",
                          "updatedAt": "2023-07-28T16:41:52Z",
                          "publishedAt": "2023-07-28T16:41:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "elbert5770"
                          },
                          "bodyText": "With MPI (mpiexec -np 38 ./cahn_hilliard2-opt -i s4_mobility_cross_KM5.i --check-input):\n/mmfs1/gscratch/elbert/python/mambaforge3/envs/moose/bin/python\n\n\n*** ERROR ***\n\"add_output\" is not a registered task.\n\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0",
                          "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6578007",
                          "updatedAt": "2023-07-28T16:48:27Z",
                          "publishedAt": "2023-07-28T16:48:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "loganharbour"
                          },
                          "bodyText": "I'm going to assume that you get the same with two processes as well.\nDoes which mpirun give you the one from within the mamba environment?",
                          "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6578060",
                          "updatedAt": "2023-07-28T16:56:58Z",
                          "publishedAt": "2023-07-28T16:56:57Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "elbert5770"
                  },
                  "bodyText": "source ~/.bashrc\nmamba activate moose\nwhich python\nwhich mpicc\nwhich gcc\n#./cahn_hilliard2-opt -i s4_mobility_cross_KM5.i --check-input\nmpiexec -np 38 ./cahn_hilliard2-opt -i s4_mobility_cross_KM5.i --check-input\n\n/mmfs1/gscratch/elbert/python/mambaforge3/envs/moose/bin/python\n/mmfs1/gscratch/elbert/python/mambaforge3/envs/moose/bin/mpicc\n/usr/bin/gcc\n\n\n*** ERROR ***\n\"add_output\" is not a registered task.\n\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6578086",
                  "updatedAt": "2023-07-28T17:00:55Z",
                  "publishedAt": "2023-07-28T17:00:55Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "elbert5770"
                          },
                          "bodyText": "source ~/.bashrc\nmamba activate moose\nwhich python\nwhich mpicc\nwhich mpirun\nwhich gcc\n#./cahn_hilliard2-opt -i s4_mobility_cross_KM5.i --check-input\nmpiexec -np 2 ./cahn_hilliard2-opt -i s4_mobility_cross_KM5.i --check-input\n\n/mmfs1/gscratch/elbert/python/mambaforge3/envs/moose/bin/python\n/mmfs1/gscratch/elbert/python/mambaforge3/envs/moose/bin/mpicc\n/mmfs1/gscratch/elbert/python/mambaforge3/envs/moose/bin/mpirun\n/usr/bin/gcc\nSetting Up\n  Uniformly Refining                                                                     [^[[33m  6.37 s^[[39m] [^[[33m  225 MB^[[39m]\n^[[36mThe following total 2 aux variables:\n  M\n  dM/dc\nare added for automatic output by MaterialOutputAction.^[[39m\n  Initializing\n    Initializing Equation Systems.                                                       [^[[33m 11.16 s^[[39m] [^[[33m  416 MB^[[39m]\n  Finished Initializing                                                                  [^[[33m 12.51 s^[[39m] [^[[33m  416 MB^[[39m]\nFinished Setting Up                                                                      [^[[33m 21.44 s^[[39m] [^[[33m  416 MB^[[39m]\nSyntax OK\nSyntax OK",
                          "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6578114",
                          "updatedAt": "2023-07-28T17:04:25Z",
                          "publishedAt": "2023-07-28T17:04:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "elbert5770"
                          },
                          "bodyText": "2 processes okay, 8 processes okay, 18 or 38 processes not okay.",
                          "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6578145",
                          "updatedAt": "2023-07-28T17:08:45Z",
                          "publishedAt": "2023-07-28T17:08:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "elbert5770"
                          },
                          "bodyText": "10 and 14 processes okay, 16 processes not okay",
                          "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6578210",
                          "updatedAt": "2023-07-28T17:17:28Z",
                          "publishedAt": "2023-07-28T17:17:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "elbert5770"
                  },
                  "bodyText": "And the simulation is running fine with 14 processes.  Interestingly, the number of local nodes is 19033, so close to optimal for this problem.",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6578456",
                  "updatedAt": "2023-07-28T17:58:05Z",
                  "publishedAt": "2023-07-28T17:58:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "elbert5770"
                          },
                          "bodyText": "I take it back, it is running, but about 100 times slower than just running mpiexec from a node with 14 processes.",
                          "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6578686",
                          "updatedAt": "2023-07-28T18:29:35Z",
                          "publishedAt": "2023-07-28T18:29:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "elbert5770"
                  },
                  "bodyText": "Oops, should be #SBATCH --cpus-per-task=40, not #SBATCH --ntasks-per-node=40\nWorks now.",
                  "url": "https://github.com/idaholab/moose/discussions/25049#discussioncomment-6579906",
                  "updatedAt": "2023-07-28T21:57:51Z",
                  "publishedAt": "2023-07-28T21:57:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "adGradSln and adGradSlnNeighbor",
          "author": {
            "login": "heinono1"
          },
          "bodyText": "Hi. I need to evaluate gradients of two variables on element faces in an FV interface kernel code. AFAIK, var1().adSlnGrad(*_face_info,determineState()) will evaluate the gradient of var1 on the element face, but I am a little hazy on how to evaluate var2 on the same face when var2 lives in an element on the other side of the face. I have tried var2().adGradSln(_face_info->neighborPtr(),determineState()) but I am not sure this is correct (problems converging). There is also adGradSlnNeighbor but var2().adGradSlnNeighbor() does not work (it seems like adGradSlnNeighbor() does not want any arguments but all sorts of errors are generated by libMesh). Any comments, help, or suggestion are gratefully accepted.\nCheers,\nOlle",
          "url": "https://github.com/idaholab/moose/discussions/25048",
          "updatedAt": "2023-07-27T16:36:51Z",
          "publishedAt": "2023-07-27T14:52:45Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "For var2 have you tried var2().adSlnGrad(*_face_info,determineState())?",
                  "url": "https://github.com/idaholab/moose/discussions/25048#discussioncomment-6567467",
                  "updatedAt": "2023-07-27T16:26:43Z",
                  "publishedAt": "2023-07-27T16:26:42Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Hi Alex. Yes, now I have. It  compiles but also gives convergence problems - to be honest, the convergence problems could be due to something else and not this particular issue.\nThanks,\nOlle",
                  "url": "https://github.com/idaholab/moose/discussions/25048#discussioncomment-6567564",
                  "updatedAt": "2023-07-27T16:36:47Z",
                  "publishedAt": "2023-07-27T16:36:46Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Unable to compile MOOSE due to PETSc error in Linux",
          "author": {
            "login": "sb50nemo"
          },
          "bodyText": "I am trying to install MOOSE on my linux machine by following the procedure in MOOSE Framework - Getting started.. However, I am unable to compile MOOSE because of the following error:\n/home/user/miniconda3/envs/moose/include/petscsys.h:211:6: error: #error \"PETSc was configured with MPICH but now appears to be compiling using a non-MPICH mpi.h\"\n 211 | #    error \"PETSc was configured with MPICH but now appears to be compiling using a non-MPICH mpi.h\" |      ^~~~~\nHow can I deal with this error? Any suggestions would be much appreciated. Thank you.",
          "url": "https://github.com/idaholab/moose/discussions/17228",
          "updatedAt": "2023-07-27T08:17:24Z",
          "publishedAt": "2021-03-04T11:27:48Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "There appears to be multiple MPI wrappers available within your PATH, and make is choosing the wrong one.\nCan you tell us more about the machine you are operating on? Linux? What Distro/Flavor? Also, a report of what your environment looks like would be very helpful (these sorts of issues are almost always environment related):\nenv\nDo you use modules? Any loaded (env above would tell us)?\nmodule list\nWhat does your PATH look like (env above would tell us):\necho $PATH\nCheers!",
                  "url": "https://github.com/idaholab/moose/discussions/17228#discussioncomment-429334",
                  "updatedAt": "2022-07-19T07:19:13Z",
                  "publishedAt": "2021-03-04T14:10:08Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "sb50nemo"
                          },
                          "bodyText": "Thanks Jason!\nI figured out that I needed to install mpich module, and then I was able to install MOOSE.\nThanks a lot for your help!",
                          "url": "https://github.com/idaholab/moose/discussions/17228#discussioncomment-429496",
                          "updatedAt": "2022-07-19T07:37:32Z",
                          "publishedAt": "2021-03-04T14:42:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richman2024"
                          },
                          "bodyText": "Hi, I found multiple MPI wrappers are available in my PATH, how could I choose the correct one.\nThank you for your help!",
                          "url": "https://github.com/idaholab/moose/discussions/17228#discussioncomment-6558140",
                          "updatedAt": "2023-07-27T03:44:05Z",
                          "publishedAt": "2023-07-27T03:44:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Please post in a new discussions thread.\nWe ll need to know your installation method and platform",
                          "url": "https://github.com/idaholab/moose/discussions/17228#discussioncomment-6562507",
                          "updatedAt": "2023-07-27T08:06:02Z",
                          "publishedAt": "2023-07-27T08:06:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richman2024"
                          },
                          "bodyText": "I post a new discusion as #25047",
                          "url": "https://github.com/idaholab/moose/discussions/17228#discussioncomment-6562603",
                          "updatedAt": "2023-07-27T08:17:24Z",
                          "publishedAt": "2023-07-27T08:17:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Enabling opacity is vanishing my image on Paraview",
          "author": {
            "login": "mdmuntaha"
          },
          "bodyText": "Hi, Can anyone tell why enabling opacity vanishing the image on paraview? Please see the attached image.",
          "url": "https://github.com/idaholab/moose/discussions/25042",
          "updatedAt": "2023-07-27T08:09:19Z",
          "publishedAt": "2023-07-26T19:58:55Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nyou probably can ask this on the paraview forum to get an answer quicker.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25042#discussioncomment-6562539",
                  "updatedAt": "2023-07-27T08:09:19Z",
                  "publishedAt": "2023-07-27T08:09:19Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PETSc was configured with MPICH but now appears to be compiling using a non-MPICH mpi.h",
          "author": {
            "login": "KangChenRui"
          },
          "bodyText": "Hi everyone,\nyesterday\uff0cI used \"make\",  the report was \"PETSc was configured with MPICH but now appears to be compiling using a non-MPICH mpi.h\".\nwhat should I do ?\nThanks for your help!",
          "url": "https://github.com/idaholab/moose/discussions/22145",
          "updatedAt": "2023-07-27T08:04:59Z",
          "publishedAt": "2022-09-20T09:06:27Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou need to be consistent between MPI distributions when building MOOSE and its dependencies. Typically this can happen if you loaded an HPC module when configuring, then loaded another when (re-)building\nOr if you have both a conda-installed MPI distribution and a non-conda MPI already on your machine\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22145#discussioncomment-3689503",
                  "updatedAt": "2022-09-20T11:35:37Z",
                  "publishedAt": "2022-09-20T11:35:36Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Adairle"
                          },
                          "bodyText": "thanks for your help!\nbut I don\u2018t how to operate it.",
                          "url": "https://github.com/idaholab/moose/discussions/22145#discussioncomment-3866795",
                          "updatedAt": "2023-06-05T14:43:10Z",
                          "publishedAt": "2022-10-13T06:39:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "what is the output of the diagnostic script in moose/scripts",
                          "url": "https://github.com/idaholab/moose/discussions/22145#discussioncomment-3870155",
                          "updatedAt": "2022-10-13T13:32:20Z",
                          "publishedAt": "2022-10-13T13:32:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richman2024"
                          },
                          "bodyText": "I got this problem too, besides another one is \"no type named 'value_type' in 'struct std::iterator_traits<infix_ostream_iterator<std::basic_string > >\", what shoud I do?\nThank you for your help!",
                          "url": "https://github.com/idaholab/moose/discussions/22145#discussioncomment-6557959",
                          "updatedAt": "2023-07-27T03:08:39Z",
                          "publishedAt": "2023-07-27T03:08:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nplease post in a new discussions thanks",
                          "url": "https://github.com/idaholab/moose/discussions/22145#discussioncomment-6562488",
                          "updatedAt": "2023-07-27T08:04:59Z",
                          "publishedAt": "2023-07-27T08:04:59Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "error while updating mamba",
          "author": {
            "login": "harveyzhengxh"
          },
          "bodyText": "Hi\nI installed moose in 2022/04 and updated it first time yesterday.\nafter Update MOOSE repository by:\ncd ~/projects/moose\ngit fetch origin\ngit rebase origin/master\nI check the website https://mooseframework.inl.gov/getting_started/new_users.html and update by mamba:\nmamba activate moose\nmamba update --all\nbut the following error comes up while compiling my app:\n/home/nsi/projects/moose/framework/build/header_symlinks/MathFVUtils.h:196:44: error: 'TensorTraits' is not a member of 'libMesh::TensorTools' 196 |   else if constexpr (libMesh::TensorTools::TensorTraits<T1>::rank == 1) |                                            ^~~~~~~~~~~~ /home/nsi/projects/moose/framework/build/header_symlinks/MathFVUtils.h:196:59: error: expected primary-expression before '>' token 196 |   else if constexpr (libMesh::TensorTools::TensorTraits<T1>::rank == 1) |                                                           ^ /home/nsi/projects/moose/framework/build/header_symlinks/MathFVUtils.h:196:62: error: '::rank' has not been declared; did you mean 'std::rank'? 196 |   else if constexpr (libMesh::TensorTools::TensorTraits<T1>::rank == 1) |                                                              ^~~~ |                                                              std::rank \nThen I update conda by:\nconda activate moose\nconda update --all\nwhich  didn't solve the error\nwhen I execute mamba update --all again comes the error:\nTraceback (most recent call last): File \"/home/nsi/mambaforge3/bin/mamba\", line 7, in <module> from mamba.mamba import main File \"/home/nsi/mambaforge3/lib/python3.9/site-packages/mamba/mamba.py\", line 51, in <module> from mamba import repoquery as repoquery_api File \"/home/nsi/mambaforge3/lib/python3.9/site-packages/mamba/repoquery.py\", line 12, in <module> def _repoquery(query_type, q, pool, fmt=api.QueryFormat.JSON): AttributeError: module 'libmambapy' has no attribute 'QueryFormat' \nthe error also comes when executing mamba env remove -n moose and mamba init\nI am not sure whether the error has something to do with \"conda update --all\" or not and I don't know how to deal with the error, which makes me fail to compile my app and moose/test.\nThanks very much",
          "url": "https://github.com/idaholab/moose/discussions/24971",
          "updatedAt": "2023-07-27T07:36:13Z",
          "publishedAt": "2023-07-15T12:51:26Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nif mamba update fails, this probably means the environment is borked.\nDelete the moose mamba environment then re install it following the original instructions getting started with moose on Linux/mac\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/24971#discussioncomment-6461939",
                  "updatedAt": "2023-07-16T19:16:10Z",
                  "publishedAt": "2023-07-16T19:16:09Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Well @harveyzhengxh is trying to delete the moose environment mamba env remove -n moose but that also causes the error. When this happens, the correct thing to do, would be to reinstall Mambaforge all together.\nrm -rf /home/nsi/mambaforge3\nClose any opened terminals, open them again, and follow through the installation procedures from the beginning. If the problem persists, this could mean there is something \"python\" related in your bash or other shell profiles sourcing \"something\" which interferes with Mambaforge \"somehow\". We can try to diagnose that, once you've reinstalled Mambaforge, and if you do get the same errors...",
                          "url": "https://github.com/idaholab/moose/discussions/24971#discussioncomment-6467933",
                          "updatedAt": "2023-07-17T13:40:59Z",
                          "publishedAt": "2023-07-17T13:40:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "harveyzhengxh"
                          },
                          "bodyText": "thanks for your answer,i solve the error by reinstall mamba first.\nconda install -c conda-forge mamba\nthen reinstall the moose mamba environment by:\nmamba env remove -n moose\nmamba create -n moose moose-dev\nI am capable of compiling moose and my own app now",
                          "url": "https://github.com/idaholab/moose/discussions/24971#discussioncomment-6562160",
                          "updatedAt": "2023-07-27T07:36:13Z",
                          "publishedAt": "2023-07-27T07:36:13Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Moose interprets all grid points with the information from the first point in the CSV",
          "author": {
            "login": "aaaaaaqing"
          },
          "bodyText": "I encountered an issue when using PiecewiseConstantFromCSV and PropertyReadFile to read doping information from a CSV file. The problem is that the software seems to only read the data of the first point in the CSV file, meaning that Moose interprets all grid points with the information from the first point in the CSV.\nFirstly, let me introduce the format of the CSV file that I want to read. The first two columns represent the X and Y coordinates, respectively, and the third column contains the doping information.\nSecondly, let me explain how I used PiecewiseConstantFromCSV and PropertyReadFile in the input file: [Provide the relevant configuration and settings in the input file].\n[UserObjects]\n  [reader_node_n]\n  type = PropertyReadFile\n  prop_file_name = 'rec_n2_3.csv'\n  read_type = 'voronoi'\n  nprop = 2\n  nvoronoi = 4152 # My CSV file has a total of 4152 points\n  []\n[]\n\n[Functions]\n  [node_n]\n    type = PiecewiseConstantFromCSV\n    read_prop_user_object = 'reader_node_n'\n    read_type = 'voronoi'\n    column_number = '2'\n  []\n[]\n\nI am confused as to why such an error occurs, I would appreciate your help",
          "url": "https://github.com/idaholab/moose/discussions/25022",
          "updatedAt": "2023-07-27T06:41:11Z",
          "publishedAt": "2023-07-24T06:43:04Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "I've seen issues with this object when the CSV file has windows line endings. The file is not properly parsed anymore.\nYou can use to clean the file:\ndos2unix file_name.csv\nor\nsed -e 's/\\r//g' file_name.csv &> file_name_clean.csv",
                  "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6526132",
                  "updatedAt": "2023-07-24T07:58:55Z",
                  "publishedAt": "2023-07-24T07:41:46Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "First of all, thank you for your answer. You mean that there is a problem with my CSV file format, because moose cannot work correctly because of the end of Windows. However, after I used 'dos2unix file_name.csv' to correct my csv file, the problem still hasn't been solved, is there a problem elsewhere",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6526540",
                          "updatedAt": "2023-07-24T08:20:18Z",
                          "publishedAt": "2023-07-24T08:20:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Do you mind pasting a few lines from the CSV file?\nIs this a 3D simulation with 2D points in the CSV file?",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6526653",
                          "updatedAt": "2023-07-24T08:28:43Z",
                          "publishedAt": "2023-07-24T08:28:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "The sections of my csv file are as follows\uff1a\n0 0 1.52E+09\n1.00E-06 0 1.51E+09\n1.00E-06 1.00E-06 1.64E+09\nThe first column is the coordinates of x, the second column is the coordinates of y, and the third column is the doping information.When I use 'PiecewiseConstantFromCSV' and PropertyReadFile 'to read, all of the dot doping is' 1.52E+09'.",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6526711",
                          "updatedAt": "2023-07-24T08:34:06Z",
                          "publishedAt": "2023-07-24T08:34:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "2D\u6587\u4ef6",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6526719",
                          "updatedAt": "2023-07-24T08:34:28Z",
                          "publishedAt": "2023-07-24T08:34:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "I am using\n'  for( int a = 1; a < 5; a = a + 1 )\n{\ncout << \"p \u7684\u503c\uff1a\" << _node_n.value(a) << endl;\ncout << \"i \u7684\u503c\uff1a\" << a << endl;\n} '\nto check whether the doping information is correctly read",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6526751",
                          "updatedAt": "2023-07-24T08:38:18Z",
                          "publishedAt": "2023-07-24T08:38:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Can you use Exodus output instead to check? I d rely on that",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6527213",
                          "updatedAt": "2023-07-24T09:26:12Z",
                          "publishedAt": "2023-07-24T09:26:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "Are '.e 'files output by\n' [Outputs]\nexodus = true\n[]\u2018viewed through paraview\uff1f\nDoes this'Output 'need any additional modification",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6527796",
                          "updatedAt": "2023-07-24T10:18:05Z",
                          "publishedAt": "2023-07-24T10:18:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "I can't view the data set of the csv file in '.e' through 'paraview'",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6527819",
                          "updatedAt": "2023-07-24T10:20:31Z",
                          "publishedAt": "2023-07-24T10:20:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaaaaaqing"
                          },
                          "bodyText": "I will not get an 'Exodus output' containing csv data, what do I do",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6529024",
                          "updatedAt": "2023-07-24T12:28:23Z",
                          "publishedAt": "2023-07-24T12:28:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You wont get a csv from exodus output, but you get a visual representation of the fields and you  can select a certain location in paraview to look at the value there.\nYou ll need to output to a variable, use a FunctionAux auxkernel for that",
                          "url": "https://github.com/idaholab/moose/discussions/25022#discussioncomment-6533541",
                          "updatedAt": "2023-07-24T20:36:21Z",
                          "publishedAt": "2023-07-24T20:36:21Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}