{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wOC0zMFQwMjo0OTo1My0wNTowMM4AbEV9"
    },
    "edges": [
      {
        "node": {
          "title": "Question about coupling external codes in MOOSE",
          "author": {
            "login": "Ethan-xj"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nHello everyone, I'm doing some coupling works based on MOOSE. In detail, I'm coupling two external codes and they are solved by their own solvers. If I want to use MOOSE to control the coupling and solving of external programs, which features of MOOSE should I utilize? Could you provide some recommendations? Thank you very much.\nEthan",
          "url": "https://github.com/idaholab/moose/discussions/28287",
          "updatedAt": "2024-09-02T11:54:26Z",
          "publishedAt": "2024-08-02T08:24:00Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nOne of the ways to do this is to use the ExternalProblem class. There are not very many examples outside of moose.\nI think Cardinal used to use an external problem",
                  "url": "https://github.com/idaholab/moose/discussions/28287#discussioncomment-10222286",
                  "updatedAt": "2024-08-02T10:24:55Z",
                  "publishedAt": "2024-08-02T10:24:54Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Ethan-xj"
                          },
                          "bodyText": "OK Giud. But Cardinal doesn't have many documents about the code structure. So it's a bit hard for me to understand the coupling strategy of Cardinal.",
                          "url": "https://github.com/idaholab/moose/discussions/28287#discussioncomment-10228431",
                          "updatedAt": "2024-08-03T01:53:55Z",
                          "publishedAt": "2024-08-03T01:53:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The main idea is that it is creating a new Problem class to order the solve steps\nhttps://github.com/neams-th-coe/cardinal/blob/devel/src/base/CardinalProblem.C\nThen there is custom data mappings between Nek/OpenMC data and MOOSE, which is passed between codes using user objects\nhttps://github.com/neams-th-coe/cardinal/blob/devel/src/userobjects/NekBinnedSideIntegral.C\nAn alternate example is the Petsc external problem. This is a lot more hard-coded for a specific PETSc solve (which is arguably not the best practice)\nThere too a new Problem class orders the solve\nhttps://github.com/idaholab/moose/blob/next/modules/external_petsc_solver/src/problems/ExternalPETScProblem.C",
                          "url": "https://github.com/idaholab/moose/discussions/28287#discussioncomment-10230736",
                          "updatedAt": "2024-08-03T13:54:11Z",
                          "publishedAt": "2024-08-03T13:48:01Z",
                          "isAnswer": true
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ethan-xj"
                          },
                          "bodyText": "Thanks and I will take it as refer!",
                          "url": "https://github.com/idaholab/moose/discussions/28287#discussioncomment-10472247",
                          "updatedAt": "2024-08-28T08:27:49Z",
                          "publishedAt": "2024-08-28T08:27:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ethan-xj"
                          },
                          "bodyText": "@GiudGiud Hi Giud. I have some other questions. Can MOOSE run an external program through a static library, or must it be done through a dynamic library? Currently, I have two external programs that need to run, but they have different compilation environments, and I'm not sure how to resolve this issue.\nEthan",
                          "url": "https://github.com/idaholab/moose/discussions/28287#discussioncomment-10515771",
                          "updatedAt": "2024-09-02T02:40:59Z",
                          "publishedAt": "2024-09-02T02:40:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Both external problems need to couple in memory? You can't call them in a script?\nI think both are options. Dynamic is definitely easier if they are moose apps",
                          "url": "https://github.com/idaholab/moose/discussions/28287#discussioncomment-10515915",
                          "updatedAt": "2024-09-02T03:10:12Z",
                          "publishedAt": "2024-09-02T03:10:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ethan-xj"
                          },
                          "bodyText": "Yes in memory. I want to use MOOSE to control the transient solution for two programs.\nAnd they are not moose apps. So, I am wondering if static libraries are less dependent on the environment at runtime, which might make them easier to use.",
                          "url": "https://github.com/idaholab/moose/discussions/28287#discussioncomment-10516039",
                          "updatedAt": "2024-09-02T03:29:56Z",
                          "publishedAt": "2024-09-02T03:29:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Static libraries will grow the executable. I'm not sure if there are much differences with regards to depending on the environment. Do you mean you plan to compile on one machine and run on another?\nI think standard practice in moose is to build and link dynamic libraries. You will find more examples for dynamic libraries",
                          "url": "https://github.com/idaholab/moose/discussions/28287#discussioncomment-10520113",
                          "updatedAt": "2024-09-02T11:54:27Z",
                          "publishedAt": "2024-09-02T11:54:26Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Raytracing for powder bed absorptivity calculations",
          "author": {
            "login": "B09di"
          },
          "bodyText": "Hello MOOSE community.\nI'm interested in implementing raytracing in MOOSE to calculate Absorptivity of powder bed in the context of Laser Powder Bed Fusion/Selective Laser Sintering  for some configuration/microstructure I have which uses diffuse interface. If anyone has a clue how to implement it in MOOSE, please share the idea or an input file for a similar problem. Any help is highly appreciated.",
          "url": "https://github.com/idaholab/moose/discussions/20098",
          "updatedAt": "2024-09-01T20:53:46Z",
          "publishedAt": "2022-01-21T12:41:38Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nHow is the powder going to be represented in the model exactly?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20098#discussioncomment-2016416",
                  "updatedAt": "2022-09-14T03:55:27Z",
                  "publishedAt": "2022-01-21T15:37:30Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "B09di"
                          },
                          "bodyText": "Hello Guillaume. Thank you for your response.\nFirst of all, we generated powder bed using GeoDict allowing for point contact between the particles. We then ran PhaseField sintering simulations on it to allow for some neck growth between the particles. we have the resultant microstructure for which we want to determine the absorptivity. We want to represent the resultant microstructure/powder bed using unstructured finite element mesh in the model.",
                          "url": "https://github.com/idaholab/moose/discussions/20098#discussioncomment-2017156",
                          "updatedAt": "2022-09-14T03:55:29Z",
                          "publishedAt": "2022-01-21T17:07:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so you have an explicitly heterogeneous geometry?\nand you have volumetric properties for laser absorption, and reflectivity for the surfaces?\nhow big is the laser compared to the particles?",
                          "url": "https://github.com/idaholab/moose/discussions/20098#discussioncomment-2017712",
                          "updatedAt": "2022-09-14T03:55:30Z",
                          "publishedAt": "2022-01-21T18:15:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@B09di any news on this?",
                          "url": "https://github.com/idaholab/moose/discussions/20098#discussioncomment-2281711",
                          "updatedAt": "2022-09-14T03:55:31Z",
                          "publishedAt": "2022-03-02T16:25:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "liang-tech"
                          },
                          "bodyText": "I have the same idea as B09di. The powder bed has many particles/spheres that can absorb  the energy of laser and reflect the laser. In phase field, we can use an order parameter c to represent solid/particle (c =1) and gas (c = 0). Therefore, the laser can transport in the gas and be absorbed by the solid surface. For the question \"how big is the laser compared to the particles?\", the particles and the laser have the same radius. The laser beam should be splited into many parts due to heterogeneity power.",
                          "url": "https://github.com/idaholab/moose/discussions/20098#discussioncomment-10500007",
                          "updatedAt": "2024-08-30T14:37:46Z",
                          "publishedAt": "2024-08-30T14:37:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so you are envisioning this as a ray tracing problem? Or should you be capturing diffraction?",
                          "url": "https://github.com/idaholab/moose/discussions/20098#discussioncomment-10508163",
                          "updatedAt": "2024-08-31T17:49:32Z",
                          "publishedAt": "2024-08-31T17:49:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "liang-tech"
                          },
                          "bodyText": "I think it is more like a problem about geometrical optics. We need to track the path of laser beam and find the area absorbing the energy from laser. (Reference:  Zhou Xu, Wang Ze-kun, Liu Peng Hu Mou-bin. Discrepancies between Gaussian surface heat source model and ray tracing heat source model for numerical simulation of selective laser melting. Comput Mech 2022. Doi: 10.1007/s00466-022-02235-1.)",
                          "url": "https://github.com/idaholab/moose/discussions/20098#discussioncomment-10511006",
                          "updatedAt": "2024-09-01T08:47:20Z",
                          "publishedAt": "2024-09-01T08:47:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If you can make it work with ray tracing or finite elements feel free to try. Otherwise I think moose won't be the right tool",
                          "url": "https://github.com/idaholab/moose/discussions/20098#discussioncomment-10514533",
                          "updatedAt": "2024-09-01T20:53:47Z",
                          "publishedAt": "2024-09-01T20:53:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "make a MooseApp use another MooseApp",
          "author": {
            "login": "jmeier"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n I have consulted the Posting Guidelines.\n I have searched the Discussions Forum and MOOSE Framework Troubleshooting and have not found what I was looking for\n Q&A Getting Started is the most appropriate category for my question (trouble installing, beginner user, ...)\n\nIssue or question about MOOSE\nDear Moose-Community,\nAfter creating my moose-app, I can select the modules to be used in the Makefile. Now I like to add my own module. But I want to create my own module not below moose/modules but in a own \"library-\"MooseApp. Then my regular MooseApp should use the official Moose folder and pull in my \"library-\"MooseApp.\nBackground: I'd like to avoid to copy&paste the C++ code of my \"library-\"MooseApp to all of my regular MooseApps.\nI think I'm missing the right words and terms here - but I hope you can guess my desire.\nIs there an \"official\" way to have my MooseApp using the regular Moose in its own untouched folder side by side with an \"library-\"MooseApp?\nJ\u00f6rg\n(Optional) code in question / simulation log / errors\nNo response\nEncountering Errors? Please include diagnostic output\nNo response",
          "url": "https://github.com/idaholab/moose/discussions/28445",
          "updatedAt": "2024-09-01T20:52:50Z",
          "publishedAt": "2024-08-21T12:53:36Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou can link moose apps together. There are examples in Cardinal and in FENIX\nIt's mostly modifications to the Makefile and to the XYZApp file in base/\nCasey is doing this in this PR\nidaholab/fenix#53\nThere are also examples in Cardinal for linking in Griffin, bison etc",
                  "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10409142",
                  "updatedAt": "2024-08-21T15:01:17Z",
                  "publishedAt": "2024-08-21T15:01:16Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jmeier"
                          },
                          "bodyText": "Dear Guillaume,\nThanks for the links and references. It looks promising, what they are doing.\nWhile working on this I realized I should change my question a bit:\nFor most of my use-cases here I do not need a full grown libraray-MooseApp on the other end. It would be sufficient if I only have a folder (e.g. /contrib/bla/) containing the source files in the subfolders /contrib/bla/src and /contrib/bla/include to be also considered when compiling my moose app. Now the idea is to provide /contrib/bla/ as a git-submodule. The modifications to the users MooseApp should be as minimal as possibe (beside adding a git-submodule in /contrib).\nTherefore, I'd like to rephrase my question: How to make Moose consider the files in /contrib/bla/src and /contrib/bla/include in addition to the files located in /src and /include of my MooseApp when compiling?\nMy current progress is as follows:\n\n\nI have a github repository to be added as a git submodule to a user-MooseApp under /contrib to be found here:\nhttps://github.com/jmeier/moose-code-plugin\n\n\nIn addition to clone my repo as a submodule, the user has to add the following lines to the end of his MooseApp-Makefile\ncontrib_mk := $(shell find $(CURDIR)/contrib/ -name \"*.mk\")\ninclude $(contrib_mk)\n\n\n\nThe user is also asked to provide a AppName.h file containing his app name. Using this const I can provide functions, userobjectes, etc. registered to his app (e.g. registerMooseObject(APPNAME, StepFunction);). By the way: this should be standard for MooseApps to have its own name as a const. The content of AppName.h should be something like:\n#pragma once\ninline constexpr char APPNAME[] = \"NameOfYourApp\";\n\n\n\nMy submodule brings its own Makefile and should tell the MooseApp to also consider the submodule source files when compiling.\n\n\nThe last step does not seem to work. Currently, moose-code-plugin/codeplugin.mk looks like:\n# what is the path of the dir we are in?\nmkfile_path := $(abspath $(lastword $(MAKEFILE_LIST)))\ncurrent_dir := $(shell dirname $(mkfile_path))\n\nADDITIONAL_CPPFLAGS += $(foreach i, $(shell find $(current_dir)/include -type d), -I $(i))\n\nfragment_srcfiles := $(shell find $(current_dir)/src -name \"*.C\")\nfragment_objs := $(patsubst %.C, %.$(obj-suffix), $(fragment_srcfiles))\ninclude $(fragment_objs)\n\nADDITIONAL_APP_OBJECTS += $(fragment_objs)\n\nWhen compiling the users MooseApp, the source file names are mentioned on the console output (Compiling C++ ...) and the MooseApp executable is created. But when I try to use the functions/userobjects/etc. defined inside  /contrib/bla/src and /contrib/bla/include in an Moose-input-file, Moose tells me the objects are not registerd.\nMy assumption is: I'm compiling the files, but they dont get linked.\nOne more question: I've seen that Moose is compiling the users code in MooseApp/src and MooseApp/include in a \"unity\" build. My codeplugin.mk seems not to do a unity build. Is this also part of my problem?\n\n  \n    \n      moose/framework/app.mk\n    \n    \n        Lines 70 to 113\n      in\n      307442a\n    \n  \n  \n    \n\n        \n          \n           ### Unity Build ### \n        \n\n        \n          \n           ifeq ($(MOOSE_UNITY),true) \n        \n\n        \n          \n            \n        \n\n        \n          \n           unity_src_dir := $(APPLICATION_DIR)/build/unity_src \n        \n\n        \n          \n            \n        \n\n        \n          \n           # Build unity buiild directory \n        \n\n        \n          \n           $(eval $(call unity_dir_rule, $(unity_src_dir))) \n        \n\n        \n          \n            \n        \n\n        \n          \n           # Exclude .libs... but also: exclude unity building src. \n        \n\n        \n          \n           # The idea here is that if all they have is src then it's a big jumble of stuff \n        \n\n        \n          \n           # that won't benefit from unity building \n        \n\n        \n          \n           # Also, exclude the base directory by default because it's another big jumble \n        \n\n        \n          \n           # of unrelated stuff. \n        \n\n        \n          \n           non_unity_dirs := %.libs %/src $(app_non_unity_dirs) \n        \n\n        \n          \n            \n        \n\n        \n          \n           # Find all of the top-level subdirectories in our src folder(s) \n        \n\n        \n          \n           # We will create a Unity file for each individual subdirectory \n        \n\n        \n          \n           # The idea is that files grouped withing a subdirectory are closely related \n        \n\n        \n          \n           # and will benefit from a Unity build \n        \n\n        \n          \n           srcsubdirs := $(shell find $(APPLICATION_DIR)/src -maxdepth 1 -type d -not -path '*/.libs*') \n        \n\n        \n          \n           allsrcsubdirs := $(shell find $(APPLICATION_DIR)/src -type d -not -path '*/.libs*') \n        \n\n        \n          \n            \n        \n\n        \n          \n           # Filter out the paths we don't want to Unity build \n        \n\n        \n          \n           unity_srcsubdirs := $(filter-out $(non_unity_dirs), $(srcsubdirs)) \n        \n\n        \n          \n           non_unity_srcsubdirs := $(filter $(non_unity_dirs), $(allsrcsubdirs)) \n        \n\n        \n          \n            \n        \n\n        \n          \n           # This is a biggie \n        \n\n        \n          \n           # Loop over the subdirectories, creating a rule to create the Unity source file \n        \n\n        \n          \n           # for each subdirectory.  To do that we need to create a unique name using the \n        \n\n        \n          \n           # full hierarchy of the path underneath src \n        \n\n        \n          \n           $(foreach srcsubdir,$(unity_srcsubdirs),$(eval $(call unity_file_rule,$(call unity_unique_name,$(unity_src_dir),$(APPLICATION_DIR),$(srcsubdir)),$(shell find $(srcsubdir) \\( -type f -o -type l \\) -regex \"[^\\#~]*\\.C\"),$(srcsubdir),$(unity_src_dir)))) \n        \n\n        \n          \n            \n        \n\n        \n          \n           # This creates the whole list of Unity source files so we can use it as a dependency \n        \n\n        \n          \n           app_unity_srcfiles := $(foreach srcsubdir,$(unity_srcsubdirs),$(call unity_unique_name,$(unity_src_dir),$(APPLICATION_DIR),$(srcsubdir))) \n        \n\n        \n          \n            \n        \n\n        \n          \n           # Add to the global list of unity source files \n        \n\n        \n          \n           unity_srcfiles += $(app_unity_srcfiles) \n        \n\n        \n          \n            \n        \n\n        \n          \n           # Pick up all of the additional files in the src directory since we're not unity building those \n        \n\n        \n          \n           app_non_unity_srcfiles := $(shell find $(non_unity_srcsubdirs) -maxdepth 1 \\( -type f -o -type l \\) -regex \"[^\\#~]*\\.C\" $(find_excludes)) \n        \n\n        \n          \n            \n        \n\n        \n          \n           # Override srcfiles \n        \n\n        \n          \n           srcfiles    := $(app_unity_srcfiles) $(app_non_unity_srcfiles) \n        \n\n        \n          \n           endif",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10426233",
                          "updatedAt": "2024-08-23T05:33:57Z",
                          "publishedAt": "2024-08-23T05:33:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If you do a syntax dump with  --json &> dump\ndo you find your objects?\nDid you remember to re-register the contrib source files? By default they are registered to the app in the contrib I imagine, not the app you are using. So you would need to add some code in MainApp.C to register the contrib app. See Cardinal or FENIX again for examples of doing that",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10431479",
                          "updatedAt": "2024-08-23T14:35:47Z",
                          "publishedAt": "2024-08-23T14:35:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jmeier"
                          },
                          "bodyText": "If you do a syntax dump with --json &> dump\ndo you find your objects?\n\nI did some testing:\n\n\nFirst I have all my code in MyApp/include and MyApp/src and I compiled my app. This is the test-case without the desired usage of the contrib folder\n\n\nDoing now --dump MyObjectName shows my objects. And the objects can be used in my Moose simulation.\n\n\nNow I move my code into MyApp/contrib/MyFragment/include and MyApp/contrib/MyFragment/src and having my codeplugin.mk in place.\n\n\nNow I compile my app using make -j 32 (please note: without -B): make does not realize my missing source files and quits immediately and tells me its done. --> this alone is not good\n\n\nDoing now --dump MyObjectName still shows my object. And the objects can be used in my Moose simulation.\n\n\nNow I compile my app using make -j 32 -B (please note: with -B): make does all the compiling and linking sucessfully.\nIn the output I can see Compiling C++ (in opt mode) /home/USERNAME/projects/MyApp/contrib/MyFragment/src/MyObjectName.C... as one of the first files to be compiled. Before all the stuff in moose/framework\n\n\nNow I cannot use my objects anymore in a Moose simulation. Doing now --dump MyObjectName does NOT find my object and shows this error:\n*** ERROR ***\n[json.exception.type_error.305] cannot use operator[] with a string argument with null\n\n\n\n\nDid you remember to re-register the contrib source files?\n\nAs written above, the stuff in contrib is not a fully grown moose app. Its just a bunch of *.C and *.h files.\nRegistering of the objects is done for the user's app (APPNAME is a inline constexpr char):\nregisterMooseObject(APPNAME, OpalinusEigenstrainFromInitialStress);",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10432301",
                          "updatedAt": "2024-08-23T15:52:21Z",
                          "publishedAt": "2024-08-23T15:50:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Thanks this is plenty of information.\nYou are probably on the right track that they are not being linked.\nUsing make -n (makes it very verbose) what do you see for the linker commands?",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10432688",
                          "updatedAt": "2024-08-23T16:32:08Z",
                          "publishedAt": "2024-08-23T16:32:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jmeier"
                          },
                          "bodyText": "The parameter -n yields 36 calls to libtool with --mode=link.\nNone of them contains the filename of one of my source files in /home/USERNAME/projects/MyApp/contrib/MyFragment\nOther code files located in MyApp/src show up in the 33rd \"linking\" ending with libmodule_loader_with_cr_em_eps_fp_fet_gc_ls_ray_rdg_rct_rich_st_ht_sp_sm_ns_con_fsi_misc_opt_pd_pf_pflow_th_st_xfem_comb-opt.la (what in turn is then used in the subsequent \"linkings\")\nMy obvious next question is now: How do I get my files into the 33rd linking?",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10433799",
                          "updatedAt": "2024-08-23T19:00:23Z",
                          "publishedAt": "2024-08-23T19:00:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "It s got to be through one of these _src_files variables in the Makefile.\nI'm on travel. I won't sleuth through the Makefile until Monday.\nWhat you are doing is pretty unconventional imo. You might as well nest a contrib src folder under src/ and a contrib include folder under include/. Even though that won't leverage git submodules it should let the app see the code as its own",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10434417",
                          "updatedAt": "2024-08-23T20:40:16Z",
                          "publishedAt": "2024-08-23T20:40:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jmeier"
                          },
                          "bodyText": "It would be great if you could check the makefiles next week. Thanks in advance.\nAs a further test I tried to create a symlinks to the contrib-folder (what does also not seem to work):\n\n\nIn MyApp/src to /contrib/MyFragment/src (while in MyApp/src):\n ls ../contrib/MyFragment/src MyFragment\n\n\nAnd the same for the include folder to create a symlink in MyApp/include to /contrib/MyFragment/include (while in MyApp/include):\n ls ../contrib/MyFragment/include MyFragment\n\n\nAfter compiling, my MooseApp still does not know anything about my UserObjects in /contrib/MyFragment. Make seems to ignore (and not to follow) my symlinked directory. So not an option either (and a bit hacky anyway).\nBTW: I'd also like to avoid sparse checkouts with git.",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10437329",
                          "updatedAt": "2024-08-24T11:30:24Z",
                          "publishedAt": "2024-08-24T11:30:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I should have time tomorrow to find how to do this",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10481487",
                          "updatedAt": "2024-08-29T02:00:19Z",
                          "publishedAt": "2024-08-29T02:00:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jmeier"
                          },
                          "bodyText": "Thats great!\nWhile we are on this: Why dont we have -DMOOSEAPPNAME=$(APPLICATION_NAME)? Wouldn't that allow us to do something like this:\nregisterMooseObject(MOOSEAPPNAME, MyFancyMooseObject);",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10482726",
                          "updatedAt": "2024-08-29T05:47:23Z",
                          "publishedAt": "2024-08-29T05:47:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "even when we have\nregisterMooseObject(SomeOtherApp, MyFancyMooseObject);\n\nwe can still register all the objects of SomeOtherApp to your current app, so it's not really needed.\nBut you can do this locally in your app, this is what the ADDITIONAL_CPPFLAGS variable in the Makefile is for",
                          "url": "https://github.com/idaholab/moose/discussions/28445#discussioncomment-10485418",
                          "updatedAt": "2024-08-29T10:02:25Z",
                          "publishedAt": "2024-08-29T10:02:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Jacobian Analyzer Error",
          "author": {
            "login": "pj131611"
          },
          "bodyText": "Greetings!\nI just build and compiled Moose on a new laptop and everything works out well untill I tried \"analyzejacobian.py\". As far as I remember, all I need to do is \"analyzejacobian.py file.i\", then it will automatically output a json file that contains the jacobian. But it failed. The best I can get is when I do:\nanalyzejacobian.py -e main-opt -i main.i\nand I got:\nError executing moose based application to gather DOF map.\nI also tried main-opt -i main.i --ksp_view_mat with solve_type = NEWTON, and I got\n\nWARNING! There are options you set that were not used!\nWARNING! could be spelling mistake, etc!\nThere is one unused database option. It is:\nOption left: name:--ksp_view_mat (no value) source: command line\n\nSo I'm pretty stuck at this point. Can anyone help me out? Thanks in advance.",
          "url": "https://github.com/idaholab/moose/discussions/28520",
          "updatedAt": "2024-09-02T12:23:57Z",
          "publishedAt": "2024-08-31T09:25:15Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIn main.i you can add\n[Executioner]\n  petsc_options = '-ksp_view_mat'\n[]\n\nyou might also be able to pass -ksp_view_mat on the command line, with a single dash",
                  "url": "https://github.com/idaholab/moose/discussions/28520#discussioncomment-10507484",
                  "updatedAt": "2024-08-31T15:02:06Z",
                  "publishedAt": "2024-08-31T15:02:05Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "pj131611"
                          },
                          "bodyText": "I see, thanks! What about residuals vector?",
                          "url": "https://github.com/idaholab/moose/discussions/28520#discussioncomment-10509812",
                          "updatedAt": "2024-09-01T02:45:12Z",
                          "publishedAt": "2024-09-01T02:45:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You can print those with either a DebugResidualAux auxkernel\nOr there is an option in the Debug block iirc",
                          "url": "https://github.com/idaholab/moose/discussions/28520#discussioncomment-10509980",
                          "updatedAt": "2024-09-01T03:43:12Z",
                          "publishedAt": "2024-09-01T03:43:11Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Subdomain modification",
          "author": {
            "login": "PEI0214"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nHello\nWhen I modified the subdomain using TimedSubdomainModifier, the displacement continued to increase at each time step. When I use CoupledVarThresholdElementSubdomainModifier modify domain, this will not happen. What causes this? Do you have any good suggestions? The input file and the results are as follows:\n\nTest1.i\n[Problem]\n  kernel_coverage_check = false\n  material_coverage_check = false\n[]\n\nBox1_inactive_id = '2'\ninactive_domain_block_ids = '${Box1_inactive_id}'\n\n[Mesh]\n  [BaseMesh]\n    type = GeneratedMeshGenerator\n    dim = 3\n    nx = 10\n    ny = 10\n    nz = 30\n    xmin = -0.5\n    xmax = +0.5\n    ymin = -0.5\n    ymax = +0.5\n    zmin = 0\n    zmax = +3\n  []\n\n  add_subdomain_ids = ${inactive_domain_block_ids}\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n  use_displaced_mesh = false\n[]\n\n[Variables]\n  [disp_x]\n    block = '0 ${inactive_domain_block_ids}'\n  []\n  [disp_y]\n    block = '0 ${inactive_domain_block_ids}'\n  []\n  [disp_z]\n    block = '0 ${inactive_domain_block_ids}'\n  []\n[]\n\n[Physics]\n  [SolidMechanics]\n    [QuasiStatic]\n      [all]\n        strain = finite\n        add_variables = true\n        generate_output = 'stress_xx stress_yy stress_zz stress_xz strain_xx strain_yy strain_zz'\n        block = ${inactive_domain_block_ids}\n      []\n    []\n  []\n[]\n\n# ===== Gravity =====\n[Kernels]\n  [Gravity1]\n    type = Gravity\n    block = ${inactive_domain_block_ids}\n    use_displaced_mesh = false\n    variable = disp_z\n    value = -10\n  []\n[]\n\n# ===== Boundary Conditions =====\n[BCs]\n  [archor_x]\n    type = DirichletBC\n    boundary = 'back'\n    variable = disp_x\n    value = 0\n  []\n\n  [archor_y]\n    type = DirichletBC\n    boundary = 'back'\n    variable = disp_y\n    value = 0\n  []\n\n  [archor_z]\n    type = DirichletBC\n    boundary = 'back'\n    variable = disp_z\n    value = 0\n  []\n[]\n\n# ===== Materials (linear-elastic to keep it simple) =====\n[Materials]\n  [elasticity_tensor1]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 50E6 # 50 MPa\n    poissons_ratio = 0.3\n    block = ${inactive_domain_block_ids}\n  []\n\n  [stress]\n    type = ComputeFiniteStrainElasticStress\n    block = ${inactive_domain_block_ids}\n  []\n\n  [density1]\n    type = GenericConstantMaterial\n    prop_names = density\n    prop_values = 2000\n    block = ${inactive_domain_block_ids}\n  []\n[]\n\n[UserObjects]\n  [GlobalSubdomainModifier]\n    type = TimedSubdomainModifier\n    times = '2'\n    blocks_from = '0'\n    blocks_to = '2'\n    execute_on = 'INITIAL TIMESTEP_BEGIN'\n  []\n[]\n\n# ===== Executioner =====\n[Executioner]\n  type = Transient\n  # automatic_scaling = true\n\n  end_time = 5\n  dt = 1\n\n  solve_type = 'PJFNK'\n  petsc_options = '-snes_converged_reason'\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n  petsc_options_value = ' lu       mumps'\n\n  nl_abs_tol = 1E-3\n  nl_max_its = 400\n\n  l_tol = 1E-3\n  l_max_its = 200\n[]\n\n[Outputs]\n  exodus = true\n[]\n\n[Debug]\n  show_var_residual_norms = true\n[]\n\n\n  \n    \n    \n\n    test1.mp4\n    \n  \n\n  \n\n  \n\n\n\n\nTest2.i\n[Problem]\n  kernel_coverage_check = false\n  material_coverage_check = false\n[]\n\nBox1_inactive_id = '2'\ninactive_domain_block_ids = '${Box1_inactive_id}'\n\n[Mesh]\n  [BaseMesh]\n    type = GeneratedMeshGenerator\n    dim = 3\n    nx = 10\n    ny = 10\n    nz = 30\n    xmin = -0.5\n    xmax = +0.5\n    ymin = -0.5\n    ymax = +0.5\n    zmin = 0\n    zmax = +3\n  []\n\n  add_subdomain_ids = ${inactive_domain_block_ids}\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n  use_displaced_mesh = false\n[]\n\n[Variables]\n  [disp_x]\n    block = '0 ${inactive_domain_block_ids}'\n  []\n  [disp_y]\n    block = '0 ${inactive_domain_block_ids}'\n  []\n  [disp_z]\n    block = '0 ${inactive_domain_block_ids}'\n  []\n[]\n\n[Physics]\n  [SolidMechanics]\n    [QuasiStatic]\n      [all]\n        strain = finite\n        add_variables = true\n        generate_output = 'stress_xx stress_yy stress_zz stress_xz strain_xx strain_yy strain_zz'\n        block = ${inactive_domain_block_ids}\n      []\n    []\n  []\n[]\n\n# ===== Gravity =====\n[Kernels]\n  [Gravity1]\n    type = Gravity\n    block = ${inactive_domain_block_ids}\n    use_displaced_mesh = false\n    variable = disp_z\n    value = -10\n  []\n[]\n\n# ===== Boundary Conditions =====\n[BCs]\n  [archor_x]\n    type = DirichletBC\n    boundary = 'back'\n    variable = disp_x\n    value = 0\n  []\n\n  [archor_y]\n    type = DirichletBC\n    boundary = 'back'\n    variable = disp_y\n    value = 0\n  []\n\n  [archor_z]\n    type = DirichletBC\n    boundary = 'back'\n    variable = disp_z\n    value = 0\n  []\n[]\n\n# ===== Materials (linear-elastic to keep it simple) =====\n[Materials]\n  [elasticity_tensor1]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 50E6 # 50 MPa\n    poissons_ratio = 0.3\n    block = ${inactive_domain_block_ids}\n  []\n\n  [stress]\n    type = ComputeFiniteStrainElasticStress\n    block = ${inactive_domain_block_ids}\n  []\n\n  [density1]\n    type = GenericConstantMaterial\n    prop_names = density\n    prop_values = 2000\n    block = ${inactive_domain_block_ids}\n  []\n[]\n\n\n# ===== Block Activation/Deactivation =====\n[Functions]\n  [Box1_ActivationStateFunction]\n    type = PiecewiseConstant\n    direction = left\n    xy_data = '0  1\n               1  1\n               2  0\n               5  0'\n  []\n[]\n\n[AuxVariables]\n  [Box1_Activation_AuxVariable]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n[]\n\n[AuxKernels]\n  [Box1_Activation_FunctionAux]\n    type = FunctionAux\n    variable = 'Box1_Activation_AuxVariable'\n    function = 'Box1_ActivationStateFunction'\n    execute_on = 'INITIAL TIMESTEP_BEGIN'\n  []\n[]\n\n[UserObjects]\n  [Box1_SubdomainModifier]\n    type = CoupledVarThresholdElementSubdomainModifier\n    coupled_var = 'Box1_Activation_AuxVariable'\n    criterion_type = ABOVE\n    threshold = 0.5\n    block = 0\n    subdomain_id = 0\n    complement_subdomain_id = ${Box1_inactive_id}\n    execute_on = 'INITIAL TIMESTEP_BEGIN'\n  []\n[]\n\n# ===== Executioner =====\n[Executioner]\n  type = Transient\n  # automatic_scaling = true\n\n  end_time = 5\n  dt = 1\n\n  solve_type = 'PJFNK'\n  petsc_options = '-snes_converged_reason'\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n  petsc_options_value = ' lu       mumps'\n\n  nl_abs_tol = 1E-3\n  nl_max_its = 400\n\n  l_tol = 1E-3\n  l_max_its = 200\n[]\n\n[Outputs]\n  exodus = true\n[]\n\n[Debug]\n  show_var_residual_norms = true\n[]\n\n\n  \n    \n    \n\n    test2.mp4",
          "url": "https://github.com/idaholab/moose/discussions/28519",
          "updatedAt": "2024-08-31T15:00:39Z",
          "publishedAt": "2024-08-31T07:36:16Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Duplicate post of #28521",
                  "url": "https://github.com/idaholab/moose/discussions/28519#discussioncomment-10507478",
                  "updatedAt": "2024-08-31T15:00:40Z",
                  "publishedAt": "2024-08-31T15:00:39Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Thermal hydraulic model long initialisation times",
          "author": {
            "login": "jvwilliams23"
          },
          "bodyText": "Hi,\nI am running a thermal hydraulic analysis of a coolant system. The graph has about 808 FlowChannels and 557 VolumeJunctions to connect them.\nThe issue I am having is when I begin to run the simulation (using the 'combined' module solver) it takes a long time at the setup stage of \"Initializing Equation Systems\". To do this, it takes 5000 seconds and uses 814 MB memory. The mesh only has 3232 nodes and 2424 elements. I have noticed this time increases when I use more elements and also more VolumeJunctions (if I use JunctionOneToOne1Phase for all junctions with 1 inlet and 1 outlet, it takes like 300 seconds to initialise, which is still quite long).\nIs there any way to avoid this? It seems there is a variable created for every \"junction\" (also these show up in paraview as individual variables), which must cause the issue. I have attached a log file below.\nBest wishes,\nJosh\n--------------------------------------------\nSetting Up\n  Finished Setting Up Options                                                            [  5.94 s] [  186 MB]\n\n\n*** Info ***\nProjection lowers order, please expect a loss of accuracy\nStill Setting Up......\n  Initializing\n    Initializing Equation Systems........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... [5027.27 s] [  814 MB]\n  Finished Initializing                                                                  [5030.18 s] [ 2804 MB]\nFinished Setting Up                                                                      [5079.07 s] [ 2341 MB]\n\nFramework Information:\nMOOSE Version:           git commit a643d739e2 on 2023-06-26\nLibMesh Version:\nPETSc Version:           3.16.6\nSLEPc Version:           3.16.2\nCurrent Time:            Fri Sep 22 11:07:09 2023\nExecutable Timestamp:    Tue Sep 12 15:36:10 2023\n\nParallelism:\n  Num Processors:          1\n  Num Threads:             1\n\nMesh:\n  Parallel Type:           replicated\n  Mesh Dimension:          1\n  Spatial Dimension:       3\n  Nodes:                   3232\n  Elems:                   2424\n  Num Subdomains:          808\n\nNonlinear System:\n  Num DOFs:                10057\n  Num Local DOFs:          10057\n  Variables:               { \"junction-0:rhoEV\" \"junction-0:rhoV\" \"junction-0:rhouV\" \"junction-0:rhovV\"\n                             \"junction-0:rhowV\" ... \"junction-99:rhoEV\" \"junction-99:rhoV\" \"junction-99:rhouV\"\n                             \"junction-99:rhovV\" \"junction-99:rhowV\" } { \"rhoA\" \"rhoEA\" \"rhouA\" }\n  Finite Element Types:    \"SCALAR\" \"MONOMIAL\"\n  Approximation Orders:    \"FIRST\" \"CONSTANT\"\n\nAuxiliary System:\n  Num DOFs:                34799\n  Num Local DOFs:          34799\n  Variables:               \"A\" \"A_linear\" { \"H\" \"P_hf\" \"T\" } \"T_wall\" \"e\" { \"junction-0:T\" \"junction-0:p\"\n                             \"junction-0:vel\" \"junction-100:T\" \"junction-100:p\" ... \"junction-97:p\" \"junction-97:vel\"\n                             \"junction-99:T\" \"junction-99:p\" \"junction-99:vel\" } { \"p\" \"rho\" \"v\" \"vel_x\"\n                             \"vel_y\" \"vel_z\" }\n  Finite Element Types:    \"MONOMIAL\" \"LAGRANGE\" \"MONOMIAL\" \"LAGRANGE\" \"MONOMIAL\" \"SCALAR\" \"MONOMIAL\"\n\n  Approximation Orders:    \"CONSTANT\" \"FIRST\" \"CONSTANT\" \"FIRST\" \"CONSTANT\" \"FIRST\" \"CONSTANT\"\n\nExecution Information:\n  Executioner:             Transient\n  TimeStepper:             ConstantDT\n  TimeIntegrator:          ImplicitEuler\n  Solver Mode:             NEWTON\n  MOOSE Preconditioner:    SMP (auto)",
          "url": "https://github.com/idaholab/moose/discussions/25555",
          "updatedAt": "2024-11-21T16:35:23Z",
          "publishedAt": "2023-09-22T14:54:02Z",
          "category": {
            "name": "Q&A Modules: Thermal Hydraulics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\n@joshuahansel will have more insights but I think the prime way to accelerate these things is to move from having one object per flow channel and junction to a single vectorized object adding all these components. This will take a little bit of reworking the code, and we typically only do that if we have a need for a faster simulation.\nThere might also be trivial slow downs of the code. If you profile the code we may just be able to fix them. Please see this page for profiling techniques:\nhttps://mooseframework.inl.gov/application_development/profiling.html\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7085720",
                  "updatedAt": "2023-09-22T20:55:34Z",
                  "publishedAt": "2023-09-22T20:55:33Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jvwilliams23"
                          },
                          "bodyText": "Hi\nI tried PerfGraph to profile, it just said the whole time is on EquationSystems::Init. I found this pretty much just calls libMesh. I then profiled using gperftools. Showing the top 12 most time consuming actions below:\n\nJosh",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7102521",
                          "updatedAt": "2023-09-25T14:27:04Z",
                          "publishedAt": "2023-09-25T14:27:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@lindsayad knows this code better than I do.\nIs there a way to bypass the API and provide the sparsity pattern?",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7102561",
                          "updatedAt": "2023-09-25T14:31:11Z",
                          "publishedAt": "2023-09-25T14:31:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "It would be helpful to see the call graph as well which you can create with svg > foo.svg or png > foo.png. We spent quite a bit of time trying to improve this for finite volume simulations in libMesh/libmesh#3422. However, the scalar variables here are another wrinkle as they are not conceptually covered by our GhostingFunctor::map_type whose keys are elements. Off the top of my head I do not know at what point we handle the sparsity from scalar variables but @roystgnr probably would. It would be nice to see an annotation of lines in handle_vi_vj to know what lines are the expensive ones in there. I'm guessing it's just sort_row at the end of the method that is taking all the time",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7104281",
                          "updatedAt": "2023-09-25T17:03:13Z",
                          "publishedAt": "2023-09-25T17:03:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jvwilliams23"
                          },
                          "bodyText": "Hi, here is the png (had issues with svg in github).",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7114116",
                          "updatedAt": "2023-09-26T15:14:00Z",
                          "publishedAt": "2023-09-26T15:12:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Can you run\nlist handle_vi_vj",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7114549",
                          "updatedAt": "2023-09-26T15:55:55Z",
                          "publishedAt": "2023-09-26T15:55:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "A SCALAR variable, for data at a pipe junction?  Is that really how we do it?  A SCALAR variable is one whose value is meaningful on every element in your mesh (or at least every element in the subdomain on which it's defined).  Five variables for each of a hundred junctions means, for instance, that an EDGE2 connected to one junction wouldn't have an 11x11 element matrix (3 Lagrange variables on each of 2 nodes, plus 5 for the neighboring junction), it would have a 506x506 element matrix.\nIf that's really what we want to do, there's probably still room to optimize it (we could handle the 6x6 and 500x6 and 6x500 blocks separately on each element, then only work on the 500x500 block once at the end?), but I'm skeptical that this is really what we want to do.  I'd have thought we'd want a NodeElem for each junction's variables, to get the sparsity pattern right.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7115180",
                          "updatedAt": "2023-09-26T17:04:44Z",
                          "publishedAt": "2023-09-26T17:04:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jvwilliams23"
                          },
                          "bodyText": "@lindsayad  I get:\n(pprof) list handle_vi_vj\nTotal: 111.61s\nNo source information for libMesh::SparsityPattern::Build::handle_vi_vj\n\nsorry not much help",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7115329",
                          "updatedAt": "2023-09-26T17:31:15Z",
                          "publishedAt": "2023-09-26T17:23:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jvwilliams23"
                          },
                          "bodyText": "@roystgnr Yeah I do not understand why a SCALAR variable is needed for each junction.\nI am working on MOOSE for a nuclear-related project, but my background is biological flows. I would like to do 1D modelling of flow in biomedical geometries in a future project, but the number of junctions will be over 10,000 easily (maybe approaching 100,000). So with the current VolumeJunction this will not be possible.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7115380",
                          "updatedAt": "2023-09-26T17:30:05Z",
                          "publishedAt": "2023-09-26T17:30:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "That's ok. You've already gone above and beyond with doing any profiling! It's much appreciated. It might be helpful to have your input file so we can do our own profiling, but I think we can also make significant progress by changing the variable type for the junction. @joshuahansel do you want me to create an issue for this, or do you want to?",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7115389",
                          "updatedAt": "2023-09-26T17:31:04Z",
                          "publishedAt": "2023-09-26T17:31:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joshuahansel"
                          },
                          "bodyText": "do you want me to create an issue for this, or do you want to?\n\nI'd like you to have that honor.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7116910",
                          "updatedAt": "2023-09-26T20:51:02Z",
                          "publishedAt": "2023-09-26T20:51:01Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joshuahansel"
                  },
                  "bodyText": "@licharlot You have some experience with large numbers of components. We've already applied that dependency resolution fix, right?",
                  "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7101583",
                  "updatedAt": "2023-09-25T13:04:13Z",
                  "publishedAt": "2023-09-25T13:04:12Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "licharlot"
                          },
                          "bodyText": "Yes, the dependency resolution fix is in (otherwise, that problem would take several days or weeks to initialize). I did use large numbers of channels, but never with a very large number of volume junctions. The increased number of variables is probably slowing things down.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7102236",
                          "updatedAt": "2023-09-25T14:04:22Z",
                          "publishedAt": "2023-09-25T14:04:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jvwilliams23"
                          },
                          "bodyText": "Hi @licharlot. Yes definitely the number of variables. What did you use to connect the channels if not VolumeJunctions? I need to impose some source term at the connections for form loss. The ElbowPipe1Phase is not really suitable on geometries with complex bends.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7102466",
                          "updatedAt": "2023-09-25T14:22:38Z",
                          "publishedAt": "2023-09-25T14:22:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "licharlot"
                          },
                          "bodyText": "I was modeling a full reactor core, so I had a large number of parallel channels connected together at each end using a volume junction. So large number of channels, but only 2 huge volume junctions. Could you use the FormLossFromFunction1Phase component to apply the form losses? Another possible way would be to use a fancy friction factor.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7102844",
                          "updatedAt": "2023-09-25T14:55:03Z",
                          "publishedAt": "2023-09-25T14:55:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jvwilliams23"
                          },
                          "bodyText": "Thanks! For one-to-one junctions I will just use a higher friction factor on the adjacent pipes as a form loss for now (will move to FormLossFromFunction1Phase later).\nSo the number of junctions is now approximately 68, and initialisation takes 100 seconds.\nWhen I increase the number of elements per flowchannel from 3 to 20, it takes around 600 seconds.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7103276",
                          "updatedAt": "2023-09-25T15:29:58Z",
                          "publishedAt": "2023-09-25T15:29:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joshuahansel"
                          },
                          "bodyText": "And to be clear, JunctionParallelChannels1Phase has a form loss option, so that can be used for any case where the connected channels are parallel.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-7103767",
                          "updatedAt": "2023-09-25T16:10:05Z",
                          "publishedAt": "2023-09-25T16:10:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "NorthMagic"
                          },
                          "bodyText": "I have a similar problem with THM initialization out of time especailly with multi-cpu cores/threads parallel computing. I tried to simulate a full core with hundreds of parallel 1D flow channels and without junctions, it takes more than 300 s to finish the initialization. Moreover, I found that the initialization time increased greatly with the number of used cpu cores and threads, which was unacceptable for doing simulations.\nFrom my understanding to the mentioned answers above, a long-time initialization was cuased by too many variables which did not exist in my simulation.  More impotantly, the initialization time increased with used cpu cores. Could you please give me some suggestions?",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-9801139",
                          "updatedAt": "2024-06-18T03:39:03Z",
                          "publishedAt": "2024-06-18T03:34:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joshuahansel"
                          },
                          "bodyText": "Ok so you're saying your simulation does not have junctions, so you shouldn't have the same issue. Can you also profile your simulation? https://mooseframework.inl.gov/application_development/profiling.html",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-9805740",
                          "updatedAt": "2024-06-18T12:19:30Z",
                          "publishedAt": "2024-06-18T12:19:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "NorthMagic"
                          },
                          "bodyText": "Sorry, I have never used this profiling tool. I need more time to install and profile my simulation. I put some new testing outputs in Discussion #27941 and the input file for running THM was posted.",
                          "url": "https://github.com/idaholab/moose/discussions/25555#discussioncomment-9813531",
                          "updatedAt": "2024-06-19T05:58:50Z",
                          "publishedAt": "2024-06-19T05:58:50Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Moose installation without sudo permissions _glibc older versions",
          "author": {
            "login": "vermaprk"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nHello\nI recently tried to upgrade moose on my university server (without sudo permissions).\n\nCreating conda environment using conda create -n moose moose-dev=2024.08.26 mpich\ngives me error to update to update __glibc to 2.28 or higher as follow:\n\nChannels:\n - https://conda.software.inl.gov/public\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: failed\n\nLibMambaUnsatisfiableError: Encountered problems while solving:\n  - nothing provides __glibc >=2.28,<3.0.a0 needed by c-ares-1.33.0-ha66036c_0\n\nCould not solve for environment specs\nThe following package could not be installed\n\u2514\u2500 moose-dev 2024.08.26**  is not installable because it requires\n   \u2514\u2500 moose-peacock 2024.08.12.* , which requires\n      \u2514\u2500 moose-mpi 2024.08.12.* , which requires\n         \u2514\u2500 c-ares 1.33.0 ha66036c_0, which requires\n            \u2514\u2500 __glibc >=2.28,<3.0.a0 , which is missing on the system.\n\nUpgrading __glibc requires me to update a number of packages that is a tedious task as most of the gnu softwares installed in /bin at server is outdated.\nInstead of doing this:\nI created conda environment using\nconda create -n moose moose mpich\nSo, I get a warning during moose installation as:\nWARNING: (CondaVersionMismatch(...), \"Conda package 'moose-dev' is currently at version '2024.07.19' and the required version is '2024.08.26'.\\nThe correct version can be installed via:\\n    conda install moose-dev=2024.08.26\")\nI have ignored this warning for time being as I dont want to upgrade the __glibc right now.\nBut doing this makes me not able to run the mpiexec command for utilizing parallel processing. The error is as follow:\n\nSLURM_CLUSTER_NAME = param-shakti\nSLURM_JOB_ACCOUNT = sreerajes\nSLURM_JOB_ID = 1537885\nSLURM_JOB_NAME = tiny\nSLURM_JOB_NODELIST = cn[039-040]\nSLURM_JOB_USER = 19es92r03\nSLURM_JOB_UID = 6905\nSLURM_JOB_PARTITION = shared\nSLURM_TASK_PID = 49164\nSLURM_SUBMIT_DIR = /scratch/19es92r03/O4_progress\nSLURM_CPUS_ON_NODE = 35\nSLURM_NTASKS = 36\nSLURM_TASK_PID = 49164\n==========================================\n\n^[[33m\n*** Warning, This code is deprecated and will be removed in future versions:\n/scratch/19es92r03/O4_progress/input_original_updated.i:350: (Outputs/other/interval):\n'interval' has been deprecated and will be removed on 02/01/2025. Please use 'time_step_interval' instead.\n^[[39m\nAutoDiff exception: Unknown serialization file version (this message will only be shown once per process)\n\n^[[31m\n*** ERROR ***\nFailed to take order 1 derivative in material Mobility_coefficient^[[39m\n\nAbort(1) on node 0 (rank 0 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n\nI am in a hurry, so should I downgrade the moose version to conda compatible, or is there any other way out? Also is this parallel processing error solely due to conda and moose mismatch or some other issue ?\nThe older version of moose worked fine on multiple processing on server.\nThanks",
          "url": "https://github.com/idaholab/moose/discussions/28513",
          "updatedAt": "2024-09-01T05:16:13Z",
          "publishedAt": "2024-08-30T18:08:01Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "The only thing you can do in this case, is building the entire stack on your own, without the use of our Conda packages. Indeed, when running on HPC Clusters, it is more desirable not using Conda packages.\nSo in essence, you'd be doing the right thing by utilizing your cluster's native module(s) system.",
                  "url": "https://github.com/idaholab/moose/discussions/28513#discussioncomment-10502469",
                  "updatedAt": "2024-08-30T18:22:31Z",
                  "publishedAt": "2024-08-30T18:22:29Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Restarting Simulations with Different Processor Counts in MOOSE",
          "author": {
            "login": "shrituntunroy"
          },
          "bodyText": "Hi MOOSE Team,\nI\u2019m a new MOOSE user currently learning how to work with the framework.\nI have a question about restarting a simulation in a MOOSE-based application. Specifically, I\u2019d like to know if it\u2019s possible to restart a simulation using a different number of processors than the original run.\nI\u2019m aware of the --recover option using checkpoints, which works well when the number of processors remains the same across both runs. However, I\u2019ve encountered issues when trying to restart with a different number of processors. Could you provide guidance on how to handle this situation?\nThank you for your help.",
          "url": "https://github.com/idaholab/moose/discussions/28506",
          "updatedAt": "2024-08-30T12:51:13Z",
          "publishedAt": "2024-08-30T08:10:39Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nWe do not support this using checkpoint.\nYou can attempt N-to-M restarts using exodus files (either as the main mesh file, or using a SolutionUserObject), but there are a number of items (stateful properties, older variable values) that will not get restarted.\nhttps://mooseframework.inl.gov/application_usage/restart_recover.html\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/28506#discussioncomment-10498766",
                  "updatedAt": "2024-08-30T12:51:13Z",
                  "publishedAt": "2024-08-30T12:51:13Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Tests failing after new installation on Mac",
          "author": {
            "login": "hsheldon"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n I have consulted the Posting Guidelines.\n I have searched the Discussions Forum and MOOSE Framework Troubleshooting and have not found what I was looking for\n Q&A Getting Started is the most appropriate category for my question (trouble installing, beginner user, ...)\n\nIssue or question about MOOSE\nI have removed all previous installations of Conda and MOOSE from my MacBook (M1 chip), reinstalled and built the tests. I am getting lots of failed tests, see attached log file. I have also attached the build output, which is showing some warnings. I have been through the troubleshooting page and can't find a solution. Please help!\nbuild_test_warnings.txt\ntests_failed.txt\n(Optional) code in question / simulation log / errors\nNo response\nEncountering Errors? Please include diagnostic output\n(moose) she341@COTTON-BM test % ../scripts/diagnostics.sh \n####################################################################################################\nInfluential Environment Variables\n\nCC=mpicc\nCC_FOR_BUILD=/Users/she341/miniforge/envs/moose/bin/arm64-apple-darwin20.0.0-clang\nCFLAGS=-ftree-vectorize -fPIC -fstack-protector-strong -O2 -pipe -isystem /Users/she341/miniforge/envs/moose/include\nCONDA_CHANNEL=https://conda.software.inl.gov/public\nCONDA_DEFAULT_ENV=moose\nCONDA_EXE=/Users/she341/miniforge/bin/conda\nCONDA_PREFIX=/Users/she341/miniforge/envs/moose\nCONDA_PREFIX_1=/Users/she341/miniforge\nCONDA_PROMPT_MODIFIER=(moose) \nCONDA_PYTHON_EXE=/Users/she341/miniforge/bin/python\nCONDA_SHLVL=2\nCONDA_TOOLCHAIN_BUILD=arm64-apple-darwin20.0.0\nCONDA_TOOLCHAIN_HOST=arm64-apple-darwin20.0.0\nCPPFLAGS=-D_FORTIFY_SOURCE=2 -isystem /Users/she341/miniforge/envs/moose/include\nCURL_CA_BUNDLE=\nCXX=mpicxx\nCXXFLAGS=-ftree-vectorize -fPIC -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -fmessage-length=0 -isystem /Users/she341/miniforge/envs/moose/include -std=c++17\nCXX_FOR_BUILD=/Users/she341/miniforge/envs/moose/bin/arm64-apple-darwin20.0.0-clang++\nF77=mpif77\nF90=mpif90\nF95=/Users/she341/miniforge/envs/moose/bin/arm64-apple-darwin20.0.0-gfortran\nFC=mpif90\nFC_FOR_BUILD=/Users/she341/miniforge/envs/moose/bin/arm64-apple-darwin20.0.0-gfortran\nFFLAGS=-march=armv8.3-a -ftree-vectorize -fPIC -fno-stack-protector -O2 -pipe -isystem /Users/she341/miniforge/envs/moose/include\nFI_PROVIDER=tcp\nHDF5_DIR=/Users/she341/miniforge/envs/moose\nLD=arm64-apple-darwin20.0.0-ld\nLDFLAGS=-Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/Users/she341/miniforge/envs/moose/lib -L/Users/she341/miniforge/envs/moose/lib\nLDFLAGS_LD=-headerpad_max_install_names -dead_strip_dylibs -rpath /Users/she341/miniforge/envs/moose/lib -L/Users/she341/miniforge/envs/moose/lib\nLIBMESH_DIR=/Users/she341/miniforge/envs/moose/libmesh\nMOOSEPF=/Users/she341/projects/moose/modules/porous_flow/porous_flow-opt\nMOOSE_JOBS=6\nMOOSE_NO_CODESIGN=true\nPATH=/Users/she341/.local/bin:/Users/she341/miniforge/envs/moose/bin:/Users/she341/miniforge/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/laps:/usr/local/munki:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/she341/miniforge/envs/moose/wasp/bin\nPETSC_DIR=/Users/she341/miniforge/envs/moose/petsc\nREQUESTS_CA_BUNDLE=\nSSL_CERT_FILE=\nWASP_DIR=/Users/she341/miniforge/envs/moose/wasp\n####################################################################################################\nCompiler(s) (CC CXX FC F77 F90):\n\nwhich $CC; /Users/she341/miniforge/envs/moose/bin/mpicc\n$CC --version; clang version 16.0.6\n$CC -show; arm64-apple-darwin20.0.0-clang -I/Users/she341/miniforge/envs/moose/include -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -Wl,-rpath,/Users/she341/miniforge/envs/moose/lib -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -lmpi -lpmpi\n\nwhich $CXX; /Users/she341/miniforge/envs/moose/bin/mpicxx\n$CXX --version; clang version 16.0.6\n$CXX -show; arm64-apple-darwin20.0.0-clang++ -I/Users/she341/miniforge/envs/moose/include -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -Wl,-rpath,/Users/she341/miniforge/envs/moose/lib -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -lmpicxx -lmpi -lpmpi\n\nwhich $FC; /Users/she341/miniforge/envs/moose/bin/mpif90\n$FC --version; GNU Fortran (GCC) 12.3.0\n$FC -show; arm64-apple-darwin20.0.0-gfortran -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -Wl,-rpath,/Users/she341/miniforge/envs/moose/lib -I/Users/she341/miniforge/envs/moose/include -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -lmpifort -lmpi -lpmpi\n\nwhich $F77; /Users/she341/miniforge/envs/moose/bin/mpif77\n$F77 --version; GNU Fortran (GCC) 12.3.0\n$F77 -show; arm64-apple-darwin20.0.0-gfortran -I/Users/she341/miniforge/envs/moose/include -fallow-argument-mismatch -L/Users/she341/miniforge/envs/moose/lib -Wl,-rpath,/Users/she341/miniforge/envs/moose/lib -I/Users/she341/miniforge/envs/moose/include -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -lmpifort -lmpi -lpmpi\n\nwhich $F90; /Users/she341/miniforge/envs/moose/bin/mpif90\n$F90 --version; GNU Fortran (GCC) 12.3.0\n$F90 -show; arm64-apple-darwin20.0.0-gfortran -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -Wl,-rpath,/Users/she341/miniforge/envs/moose/lib -I/Users/she341/miniforge/envs/moose/include -I/Users/she341/miniforge/envs/moose/include -L/Users/she341/miniforge/envs/moose/lib -lmpifort -lmpi -lpmpi\n\nOK\n####################################################################################################\nPython Sanity Checks\n\n/usr/bin/env python3 --version; (reporting as: Python 3.11.8) matches\nwhich python3 python;\n\n/Users/she341/miniforge/envs/moose/bin/python3 --version; == Python 3.11.8\n/Users/she341/miniforge/envs/moose/bin/python --version; == Python 3.11.8\n\nOK\n####################################################################################################\nPython Modules (TestHarness, run-ability)\n\nOK\n####################################################################################################\nMOOSE Repository/Conda Version Checks\n\n              Installed     Required\n\nOK\n\nchecks PASSED\n(moose) she341@COTTON-BM test %",
          "url": "https://github.com/idaholab/moose/discussions/28505",
          "updatedAt": "2024-09-01T22:25:02Z",
          "publishedAt": "2024-08-30T00:37:47Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThis is a known failure. it is solved in the moose/next branch already.\nThe python package deepdiff, which we rely on for the test suite for the failing tests, was updated and performs checks differently.\nYou can safely ignore all the SCHEMADIFF failures\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/28505#discussioncomment-10492879",
                  "updatedAt": "2024-08-30T01:05:53Z",
                  "publishedAt": "2024-08-30T01:05:52Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Also, I've pinned this issue on Discussions, with a workaround: #28494",
                          "url": "https://github.com/idaholab/moose/discussions/28505#discussioncomment-10498315",
                          "updatedAt": "2024-08-30T12:06:51Z",
                          "publishedAt": "2024-08-30T12:06:50Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Help for building MOOSE in a offline HPC",
          "author": {
            "login": "shrituntunroy"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n I have consulted the Posting Guidelines.\n I have searched the Discussions Forum and MOOSE Framework Troubleshooting and have not found what I was looking for\n Q&A Getting Started is the most appropriate category for my question (trouble installing, beginner user, ...)\n\nIssue or question about MOOSE\nHi Moose Team,\nI am a new MOOSE user and currently learning to use MOOSE. I am installing moose in a offline HPC (on which I dont have sudo access)  following this method - https://mooseframework.inl.gov/getting_started/installation/offline_installation.html\nI have already installed these offline packages in my system and they are working correctly.\nBut in the HPC, I am getting following error related to clang compiler:\n/home/lus03/vikramr/projects/moose/scripts\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\n=============================================================================================\n                         Configuring PETSc to compile on your system\n=============================================================================================\nTESTING: checkCCompiler from config.setCompilers(config/BuildSystem/config/setCompilers.py:1427)\n*********************************************************************************************\n           UNABLE to CONFIGURE with GIVEN OPTIONS (see configure.log for details):\n---------------------------------------------------------------------------------------------\n             C compiler you provided with -CC= cannot be found or does not work.\n*********************************************************************************************\n\nThere was an error. Exiting...\n[vikramr@atulya335 scripts]$ module load openmpi-gcc/4.0.1\n[vikramr@atulya335 scripts]$ ./update_and_rebuild_petsc.sh --CC=$CC --CXX=$CXX --FC=$FC --with-packages-download-dir=~/projects/downloads\n/home/lus03/vikramr/projects/moose/scripts\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\n=============================================================================================\n                         Configuring PETSc to compile on your system\n=============================================================================================\nTESTING: checkCCompiler from config.setCompilers(config/BuildSystem/config/setCompilers.py:1427)\n*********************************************************************************************\n           UNABLE to CONFIGURE with GIVEN OPTIONS (see configure.log for details):\n---------------------------------------------------------------------------------------------\n             C compiler you provided with -CC= cannot be found or does not work.\n*********************************************************************************************\n\nThere was an error. Exiting...\n[vikramr@atulya335 scripts]$ export CC=mpicc CXX=mpicxx FC=mpif90 F90=mpif90 F77=mpif77\n[vikramr@atulya335 scripts]$ ./update_and_rebuild_petsc.sh --CC=$CC --CXX=$CXX --FC=$FC --with-packages-download-dir=~/projects/downloads\n/home/lus03/vikramr/projects/moose/scripts\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\n=============================================================================================\n                         Configuring PETSc to compile on your system\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n    Found environment variable: CC=mpicc. Ignoring it, since its also set on command line\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n   Found environment variable: CXX=mpicxx. Ignoring it, since its also set on command line\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n    Found environment variable: FC=mpif90. Ignoring it, since its also set on command line\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: F77=mpif77. Ignoring it! Use \"./configure F77=$F77\" if you\n  really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: F90=mpif90. Ignoring it! Use \"./configure F90=$F90\" if you\n  really want to use this value\n=============================================================================================\nTESTING: checkCxxDialect from config.setCompilers(config/BuildSystem/config/setCompilers.py:773)\n*********************************************************************************************\n           UNABLE to CONFIGURE with GIVEN OPTIONS (see configure.log for details):\n---------------------------------------------------------------------------------------------\n  Using CUDA dialect C++11 as lower bound due to package(s):\n  - hypre\n  - strumpack\n  - SuperLU_DIST\n  But CUDA compiler (clang) appears non-compliant with C++11 or didn't accept:\n  - -std=gnu++20\n  - -std=c++20\n  - -std=gnu++17\n  - -std=c++17\n  - -std=gnu++14\n  - -std=c++14\n  - -std=gnu++11\n  - -std=c++11\n*********************************************************************************************\n\nThere was an error. Exiting...\n\nI have tried hiding the clang compiler by exporting the following line - export clang=0 in bashrc but still I m getting the same error.\nIn my pc I didnt had clang but still moose installed correctly\nKindly help me for resolving this error.\n(Optional) code in question / simulation log / errors\nNo response\nEncountering Errors? Please include diagnostic output\n##################################################################################################\nInfluential Environment Variables\n\nCC=mpicc\nCONDA_CHANNEL=https://conda.software.inl.gov/public\nCONDA_SHLVL=0\nCURL_CA_BUNDLE=\nCXX=mpicxx\nF77=mpif77\nF90=mpif90\nFC=mpif90\nLD_LIBRARY_PATH=/dlocal/sysapps/openmpi-gcc-4.0.1/lib64:/dlocal/sysapps/gcc-9.2.0/lib64:/dlocal/sysapps/gcc-9.2.0/lib:/home/lus03/vikramr/app/gcc/gcc1230/lib64:/home/lus03/vikramr/app/gcc/gcc1230/lib:/home/lus03/vikramr/app/gcc/mpc121/lib:/home/lus03/vikramr/app/gcc/mpfr410/lib:/home/lus03/vikramr/app/gcc/gmp621/lib:/home/lus03/vikramr/app/gcc/gcc1230/lib:/home/lus03/vikramr/app/gcc/mpc121/lib:/home/lus03/vikramr/app/gcc/mpfr410/lib:/home/lus03/vikramr/app/gcc/gmp621/lib:/home/lus03/vikramr/app/gcc/gcc1230/lib64:/home/lus03/vikramr/app/gcc/gcc1230/lib:/home/lus03/vikramr/app/gcc/mpc121/lib:/home/lus03/vikramr/app/gcc/mpfr410/lib:/home/lus03/vikramr/app/gcc/gmp621/lib:/home/lus03/vikramr/app/gcc/gcc1230/lib:/home/lus03/vikramr/app/gcc/mpc121/lib:/home/lus03/vikramr/app/gcc/mpfr410/lib:/home/lus03/vikramr/app/gcc/gmp621/lib:\nMODULESHOME=/usr/share/Modules\nMOOSE_JOBS=6\nPATH=/home/lus03/vikramr/.python3/3.7.4/bin:/dlocal/sysapps/anaconda3/2019.10/bin/:/dlocal/sysapps/openmpi-gcc-4.0.1/bin/:/dlocal/sysapps/gcc-9.2.0/bin:/home/lus03/vikramr/app/python/Python-3.8.0:/home/lus03/vikramr/app/cmake/cmake-3.28.3/bin:/home/lus03/vikramr/app/python/Python-3.8.0:/home/lus03/vikramr/app/gcc/gcc1230/bin:/home/lus03/vikramr/app/cmake/cmake-3.28.3/bin:/usr/bin:/usr/sbin:/usr/lib64/qt-3.3/bin:/usr/condabin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/local/anupam:/opt/VirtualGL/bin:/opt/ibutils/bin:.:/home/lus03/vikramr/app/lammps2023/lammps-2Aug2023/src:/home/lus03/vikramr/.local/bin:/home/lus03/vikramr/bin:/home/lus03/vikramr/app/lammps2023/lammps-2Aug2023/src\nREQUESTS_CA_BUNDLE=\nSSL_CERT_FILE=\n\n##################################################################################################\nCompiler(s) (CC CXX FC F77 F90):\n\nCC=/dlocal/sysapps/openmpi-gcc-4.0.1/bin/mpicc\nCC -show:\ngcc -I/dlocal/sysapps/openmpi-gcc-4.0.1/include/openmpi -I/dlocal/sysapps/openmpi-gcc-4.0.1/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -I/dlocal/sysapps/openmpi-gcc-4.0.1/include/openmpi/opal/mca/event/libevent2022/libevent -I/dlocal/sysapps/openmpi-gcc-4.0.1/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/dlocal/sysapps/openmpi-gcc-4.0.1/include -pthread -Wl,-rpath -Wl,/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -Wl,--enable-new-dtags -L/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -lmpi\nCC version:     gcc (GCC) 9.2.0\n\nCXX=/dlocal/sysapps/openmpi-gcc-4.0.1/bin/mpicxx\nCXX -show:\ng++ -I/dlocal/sysapps/openmpi-gcc-4.0.1/include/openmpi -I/dlocal/sysapps/openmpi-gcc-4.0.1/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include -I/dlocal/sysapps/openmpi-gcc-4.0.1/include/openmpi/opal/mca/event/libevent2022/libevent -I/dlocal/sysapps/openmpi-gcc-4.0.1/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/dlocal/sysapps/openmpi-gcc-4.0.1/include -pthread -Wl,-rpath -Wl,/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -Wl,--enable-new-dtags -L/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -lmpi\nCXX version:    g++ (GCC) 9.2.0\n\nFC=/dlocal/sysapps/openmpi-gcc-4.0.1/bin/mpif90\nFC -show:\ngfortran -I/dlocal/sysapps/openmpi-gcc-4.0.1/include -pthread -I/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -Wl,-rpath -Wl,/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -Wl,--enable-new-dtags -L/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -lmpi_usempi -lmpi_mpifh -lmpi\nFC version:     GNU Fortran (GCC) 9.2.0\n\nF77=/dlocal/sysapps/openmpi-gcc-4.0.1/bin/mpif77\nF77 -show:\ngfortran -I/dlocal/sysapps/openmpi-gcc-4.0.1/include -pthread -I/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -Wl,-rpath -Wl,/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -Wl,--enable-new-dtags -L/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -lmpi_usempi -lmpi_mpifh -lmpi\nF77 version:    GNU Fortran (GCC) 9.2.0\n\nF90=/dlocal/sysapps/openmpi-gcc-4.0.1/bin/mpif90\nF90 -show:\ngfortran -I/dlocal/sysapps/openmpi-gcc-4.0.1/include -pthread -I/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -Wl,-rpath -Wl,/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -Wl,--enable-new-dtags -L/dlocal/sysapps/openmpi-gcc-4.0.1/lib64 -lmpi_usempi -lmpi_mpifh -lmpi\nF90 version:    GNU Fortran (GCC) 9.2.0\n\nOK\n\n##################################################################################################\nPython Sanity Checks\n\nVerify `/usr/bin/env python3 --version` (reporting as: Python 3.7.4),\nmatches versions for: `which python3 && which python`\n\nOK\n\n##################################################################################################\nPython Modules (TestHarness, run-ability)\n\nOK",
          "url": "https://github.com/idaholab/moose/discussions/28466",
          "updatedAt": "2024-08-30T12:08:45Z",
          "publishedAt": "2024-08-26T07:56:02Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "With the mention of Cuda, I wouldn't be surprised if GCC 9.x is too old. I hope I am wrong. Tagging @grmnptr for possible confirmation...\nAlso, would it be possible for you to attach the configure.log it mentions? It should reside:\n/home/lus03/vikramr/projects/moose/petsc/configure.log",
                  "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10451498",
                  "updatedAt": "2024-08-26T12:44:36Z",
                  "publishedAt": "2024-08-26T12:44:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "shrituntunroy"
                  },
                  "bodyText": "Hi Jason,\nThankyou for your reply.\nPlease find the configure.log file.\nI have also tried gcc 12.3.0 but still I am getting the same error. Is it possible to build moose without cuda so as to avoid this error\nconfigure.log -\nconfigure.log",
                  "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10458956",
                  "updatedAt": "2024-08-27T05:45:45Z",
                  "publishedAt": "2024-08-27T05:45:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "it is still finding clang when looking for a compiler for cuda code.\nExecuting: clang --version\nstdout:\nclang version 3.4.2 (tags/RELEASE_34/dot2-final)\nTarget: x86_64-redhat-linux-gnu\nThread model: posix\n\nlet's try to find clang (which clang) then remove it from your PATH\nwe certainly can compile without cuda support",
                          "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10461507",
                          "updatedAt": "2024-08-27T10:11:04Z",
                          "publishedAt": "2024-08-27T10:11:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Looks like its in /usr/bin according to the configure.log. Not a PATH we can easily remove.",
                          "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10462778",
                          "updatedAt": "2024-08-27T12:26:07Z",
                          "publishedAt": "2024-08-27T12:26:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "shrituntunroy"
                          },
                          "bodyText": "Hi Giudicelli ,\nThanks for your reply\nWhich clang returns the following -\n[vikramr@atulya335 ~]$ which clang\n/usr/bin/clang\nI have tried removing the usr/bin/ from the path but I am unable to run the script as shown below\n[vikramr@atulya335 scripts]$ which clang\n/usr/bin/clang\n[vikramr@atulya335 scripts]$ export PATH=$(echo $PATH | sed -e 's|:/usr/bin||g' -e 's|/usr/bin:||g')\n[vikramr@atulya335 scripts]$ which clang\n/usr/bin/which: no clang in (/home/lus03/vikramr/.python3/3.7.4/bin:/dlocal/sysapps/anaconda3/2019.10/bin/:/dlocal/sysapps/openmpi-gcc-4.0.1/bin/:/home/lus03/vikramr/app/python/Python-3.8.0:/home/lus03/vikramr/app/gcc/gcc1230/bin:/home/lus03/vikramr/app/cmake/cmake-3.28.3/bin:/usr/mpi/gcc/openmpi-4.0.2rc3/bin/:/usr/sbin:/usr/lib64/qt-3.3/bin:/usr/condabin:/usr/local/bin:/usr/local/sbin:/usr/sbin:/usr/local/anupam:/opt/VirtualGL/bin:/opt/ibutils/bin:.:/home/lus03/vikramr/app/lammps2023/lammps-2Aug2023/src:/home/lus03/vikramr/.local/bin:/home/lus03/vikramr/bin)\n[vikramr@atulya335 scripts]$ ./update_and_rebuild_petsc.sh --skip-submodule-update --with-packages-download-dir=~/projects/downloads\n/usr/bin/env: bash: No such file or directory\nI also tried an alias\nalias clang='echo \"clang is disabled\"'\n\nbut still it results in same error as in configure.log.\nCan you suggest some method so as to compile petsc without using clang or cuda",
                          "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10462989",
                          "updatedAt": "2024-08-27T12:45:37Z",
                          "publishedAt": "2024-08-27T12:45:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "yeah, you can't remove /usr/bin from PATH. Nothing will work afterwards. I am surprised that which worked to be honest.",
                          "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10463722",
                          "updatedAt": "2024-08-27T13:53:04Z",
                          "publishedAt": "2024-08-27T13:52:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Jason is right we wont be able to do that.\nI'm not seeing --with-cuda in the configure command.\nI am seeing this at the top of the configure log though\nPython version:\n3.7.4 (default, Aug 13 2019, 20:35:49) \n[GCC 7.3.0]",
                          "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10463817",
                          "updatedAt": "2024-08-27T14:03:57Z",
                          "publishedAt": "2024-08-27T14:00:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so let's clean this PATH still. Make sure the conda python is found first, and the newer GCC too.\nPATH is read from beginning (left) to the end",
                          "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10463837",
                          "updatedAt": "2024-08-27T14:01:54Z",
                          "publishedAt": "2024-08-27T14:01:53Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "shrituntunroy"
                  },
                  "bodyText": "Thank you Jason and Guillaume,\nFor your replies and support.\nI successfully compiled and installed moose in the HPC system by installing and compiling a newer version of llvm and clang.",
                  "url": "https://github.com/idaholab/moose/discussions/28466#discussioncomment-10495437",
                  "updatedAt": "2024-08-30T07:49:54Z",
                  "publishedAt": "2024-08-30T07:49:53Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}