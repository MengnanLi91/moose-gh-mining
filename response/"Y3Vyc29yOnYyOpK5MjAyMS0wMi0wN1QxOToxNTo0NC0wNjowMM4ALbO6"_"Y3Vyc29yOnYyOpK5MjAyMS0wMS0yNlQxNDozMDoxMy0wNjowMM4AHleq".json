{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wMS0yNlQxNDozMDoxMy0wNjowMM4AHleq"
    },
    "edges": [
      {
        "node": {
          "title": "Memory usage in Contact modules",
          "author": {
            "login": "makeclean"
          },
          "bodyText": "Quick question, Ive got a 'toy' problem which resembles our children's favourite Danish building blocks. There are two blocks next to each other, along the long face as seen below.\n\nIn total there are 1583 nodes on each surface (4928 edges and 3346 triangles if that matters - dont think it does) and trying to get this problem setup and running on my desktop, requires more than 128 Gb of memory ?!?! That seems excessive to me, any suggestions here would be appreciated.",
          "url": "https://github.com/idaholab/moose/discussions/16773",
          "updatedAt": "2022-09-26T01:50:38Z",
          "publishedAt": "2021-01-21T09:45:56Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "makeclean"
                  },
                  "bodyText": "So memory usage is around 18.2 Gb until computing initial stateful property values, and then it rockets up fairly quickly to 128 Gb and then is killed.",
                  "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-298541",
                  "updatedAt": "2022-09-26T01:50:38Z",
                  "publishedAt": "2021-01-21T09:59:20Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "What type of properties are you using that are stateful? Saving the state requires that values of the material properties are stored for every quadrature point in your mesh. It is common to have a large memory demand with stateful properties.",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-303804",
                          "updatedAt": "2022-09-26T01:50:38Z",
                          "publishedAt": "2021-01-22T23:47:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "nothing of note which makes it weird",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-309018",
                          "updatedAt": "2022-09-26T01:50:38Z",
                          "publishedAt": "2021-01-25T16:18:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "With your dock_slippy.i input I am not seeing memory usage above about 300 MB",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-318834",
                          "updatedAt": "2022-09-26T01:50:38Z",
                          "publishedAt": "2021-01-28T20:12:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Replying on this thread so I can leave the other one to talk about convergence. Congrats @makeclean you've unearthed two large memory issues. Here is this ticket: #16836",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-319384",
                          "updatedAt": "2022-09-26T01:50:38Z",
                          "publishedAt": "2021-01-29T00:20:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Ok after #16845, for your lego input, I go from this graph:\n\nwhich was taken from the middle of initing stateful props, and shortly before I canceled the simulation so my computer wouldn't run out of memory to this graph:\n\nwhich was taken after initing stateful properties...because now we can get to that point without running out of memory. So 84 GB for stateful properties (and that number was still growing) to 4GB. I'm a big fan. Thanks for your helpful inputs @makeclean !",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-329220",
                          "updatedAt": "2022-09-26T01:50:38Z",
                          "publishedAt": "2021-02-01T23:34:32Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "makeclean"
                  },
                  "bodyText": "Some progress, it seems the two objects in contact must be separate mesh blocks, is that true MOOSE folks?",
                  "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-300765",
                  "updatedAt": "2022-09-26T01:50:39Z",
                  "publishedAt": "2021-01-21T21:29:09Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "If two surfaces were part of a contact group, and those surfaces also had nodes that were part of a fixed 0 displacement boundary, what would happen?",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-300857",
                          "updatedAt": "2021-01-21T22:19:47Z",
                          "publishedAt": "2021-01-21T22:19:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "@lindsayad Can you answer this contact question?",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-303805",
                          "updatedAt": "2021-01-22T23:48:51Z",
                          "publishedAt": "2021-01-22T23:48:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Ill try and formulate a minimum viable problem that shows the issue.",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-309044",
                          "updatedAt": "2021-01-25T16:27:35Z",
                          "publishedAt": "2021-01-25T16:27:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "What contact formulation are you using? With respect to contact vs. dirichlet condition: the dirichlet condition would win because it runs after constraints. A MWE would be great yea, thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-309053",
                          "updatedAt": "2022-10-04T14:17:28Z",
                          "publishedAt": "2021-01-25T16:30:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Ok, I can't reproduce the problem with my simple block example, where should I put the mesh from the example above?",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312561",
                          "updatedAt": "2022-10-04T14:17:27Z",
                          "publishedAt": "2021-01-26T20:50:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Its regular contact not dirichlet",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312562",
                          "updatedAt": "2022-10-04T14:17:28Z",
                          "publishedAt": "2021-01-26T20:51:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "If two surfaces were part of a contact group, and those surfaces also had nodes that were part of a fixed 0 displacement boundary, what would happen?\n\nRight here you asked what would happen if you had a surface that was simultaneously designated for contact and was part of a fixed 0 displacement boundary, which is presumably a Dirichlet BC.",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312681",
                          "updatedAt": "2022-10-04T14:17:27Z",
                          "publishedAt": "2021-01-26T21:38:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "oh i see, sorry, bit slow tonight :)",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312722",
                          "updatedAt": "2022-10-04T14:17:28Z",
                          "publishedAt": "2021-01-26T21:46:36Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "makeclean"
                  },
                  "bodyText": "Also, further weirdness, I have what I'm calling a 'docking' problem - but basically I have a 'rubber' hollow sphere (as shown in the image) and a stainless steel 'sphere' I am forcing the displacement as a function of time so that they start seperate and then get into contact later. In this case gravity is acting along the +ve z axis, there is a 0 displacement condition on the flat bottom of the rubber hollow sphere. I also constrain the x,y movement of the sphere with 0 displacement condition, such that the sphere only moves along the z axis. There are 3 contact surfaces; the outside part of the stainless sphere (a), the entry part into the hollow sphere (b) and the inside of the hollow sphere (c) and I have frictional contacts setup for ac and bc. All is great, contact is made the rubber deforms, but eventually we get a convergence failure at the minimum timestep.\n\nI've tried turning on AMR, but this leads to segfaults and also some weird deformations that you'll see in the next image, and some oddly connected mesh elements.\n\nSo my questions are manifold, but\n\nshould contact and AMR work concurrently?\nIve tried taking both to second order but this leads to both PETSC segfault-ing\nshould I try just a finer mesh?\n\nHappy to share inputs, can I attach them here?",
                  "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312589",
                  "updatedAt": "2022-10-04T14:17:27Z",
                  "publishedAt": "2021-01-26T21:04:00Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Happy to share inputs, can I attach them here?\n\nIf they are MOOSE-only",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312684",
                          "updatedAt": "2023-01-27T23:17:46Z",
                          "publishedAt": "2021-01-26T21:39:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "no, exodus meshes from Cubit im afraid",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312721",
                          "updatedAt": "2023-01-27T23:17:46Z",
                          "publishedAt": "2021-01-26T21:46:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Oh I just meant MOOSE-only objects in the input file. How big are the meshes?",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312903",
                          "updatedAt": "2023-01-27T23:17:46Z",
                          "publishedAt": "2021-01-26T23:00:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Not huge, only 50k ish elements - runs in around 30 mins to failure",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312911",
                          "updatedAt": "2023-01-27T23:17:47Z",
                          "publishedAt": "2021-01-26T23:05:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "on one core",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312912",
                          "updatedAt": "2023-01-27T23:17:46Z",
                          "publishedAt": "2021-01-26T23:05:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "If they are not ginormous, you can send them to alexander.lindsay@inl.gov. (I don't think you can attach them here on github...if you can, then just do that)",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-312915",
                          "updatedAt": "2023-01-27T23:17:47Z",
                          "publishedAt": "2021-01-26T23:06:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "The dock problem should also show the memory usage problem right? I  see that you are using finite strain",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-318829",
                          "updatedAt": "2023-01-27T23:17:46Z",
                          "publishedAt": "2021-01-28T20:09:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Dock slippy has no memory problems and runs fine. Only the original lego problem had that issue, which I can't reproduce in a small problem. I'll forward on the original file, its not that huge either, hence the original question re: memory usage. The issue with dock_slippy.i is eventually a convergence fail, and the fact that going to 2nd order is an immediate segfault.",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-318989",
                          "updatedAt": "2023-01-27T23:17:47Z",
                          "publishedAt": "2021-01-28T20:40:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Ok yes I would like to have the really bad memory input. FWIW even dock-slippy, though it only resulted in 300 MB of peak memory usage, showed us a potential bug that I am investigating a fix for right now. Once I am done investigating the memory usage, I will think about the other issues.",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-319087",
                          "updatedAt": "2023-01-27T23:18:05Z",
                          "publishedAt": "2021-01-28T21:21:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Oh is that bug the one about the stress in the moving part comes and goes away?",
                          "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-319106",
                          "updatedAt": "2023-01-27T23:18:16Z",
                          "publishedAt": "2021-01-28T21:31:39Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "makeclean"
                  },
                  "bodyText": "No, that problem runs fine. I cant'reproduce the contact memory  usage\nproblem, except in the original geometry.\n\u2026\nOn Thu, 2021-01-28 at 12:09 -0800, Alex Lindsay wrote:\n The dock problem should also show the memory usage problem right? I\n see that you are using finite strain\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub, or unsubscribe.",
                  "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-318987",
                  "updatedAt": "2022-10-04T14:17:13Z",
                  "publishedAt": "2021-01-28T20:38:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "bwspenc"
                  },
                  "bodyText": "I'm jumping into this pretty late. It sounds like some of the memory issues have been resolved. I'd just add here that the patch_size parameter in the Mesh block can affect memory usage. That controls the size of the set of nodes on the primary face nearest to the secondary node that contains the faces that are considered for contact with that node. Back in the day, you had to set this really large when you had large sliding between the surfaces, because that set of nodes was static, but now that we have the patch_update_strategy=iteration option, that is dynamically updated, so you can get by with a pretty small patch. If I remember right, the set of candidate faces consists of all of the faces that contain the nodes that are found in the patch. That means that you can probably set it to something close to the number of nodes on a face. Maybe you can get by with a patch size of 10 or less.",
                  "url": "https://github.com/idaholab/moose/discussions/16773#discussioncomment-343425",
                  "updatedAt": "2022-10-04T13:44:44Z",
                  "publishedAt": "2021-02-06T01:07:31Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Incoming Network Connection on macOS",
          "author": {
            "login": "dogrady2"
          },
          "bodyText": "Hi All,\nWhenever I run sam on my macOS Catalina laptop I get the following pop-up\n\nThe pop-up occurs regardless of the model that is being run, it also occurs when running --dump. Other sam developer/users do not report getting the pop-up. I have not been able to traceback what is causing the network connection. I am not sure if the cause of the pop-up is in the compilation environment, sam level or moose level. Does anyone have any suggestions for disabling the pop-up?\nThanks,\nDan O'Grady",
          "url": "https://github.com/idaholab/moose/discussions/16876",
          "updatedAt": "2022-09-21T21:45:01Z",
          "publishedAt": "2021-02-03T17:41:25Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "permcody"
                  },
                  "bodyText": "Hi Dan, To the best of my knowledge there are two ways around this:\n\nTurn off your firewall - easy to do, but maybe not the best way since it could open you up to potential security issues.\nSign your binary with an Apple Developer certificate - This of course means you have to register and become an Apple Developer. If you have a signed binary, it won't be flagged by Mac OS's firewall. The issue with this is that you have to do this every time you build. We already have a line in our Makefile that makes MacOS binaries debuggable. It would be easy to add a line if we could do this generically for everyone, but I'm not aware of a solution that will work for everyone since you (as the end-developer) are creating the binary as you so you have to be the one that signs it each time it is created.",
                  "url": "https://github.com/idaholab/moose/discussions/16876#discussioncomment-335699",
                  "updatedAt": "2022-09-21T21:45:02Z",
                  "publishedAt": "2021-02-03T18:44:11Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "travismui"
                          },
                          "bodyText": "Hi @permcody, thanks for your notes on this. Just to add onto @dogrady2, I've also started experiencing this pop-up issue this week (I think he noticed this back in November 2020), so I've tried some troubleshooting.\n\nThis seems to affect any Moose app, I noticed this with Bison and also ex01-opt in the Moose examples, so I'm curious if others in the Moose community have noticed the pop-ups recently.\nWe suspect it's rooted in the latest macOS Security Update 2021-001 from Feb 1 (see Apple) that has triggered this 'bug'.\nThe incessant pop-ups occur only when the macOS firewall is on and the first time after you build your executable. After a restart, the pop-ups don't show up but they do after you re-build.\n\nA couple details on what I checked to diagnose the error:\n\nOur work machines are on macOS Catalina, but I reproduced this issue on Mojave too (see below)\nTurning off the firewall fixes the issue\u2013 but this may not be an option for some users.\nWe are not enrolled in the Apple Developer program so I don't think we can sign with a certificate. It does seem that Sam is not signed when I checked with codesign.\nOn a personal machine using Mojave, before applying the Security Update I did not experience the pop-up dialogue. After applying the Security Update, I get the pop-up issue as described above. Unfortunately, I don't know of any easy way to unroll a macOS Security Update without restoring to a backup image.",
                          "url": "https://github.com/idaholab/moose/discussions/16876#discussioncomment-339853",
                          "updatedAt": "2022-09-21T21:45:03Z",
                          "publishedAt": "2021-02-05T00:02:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Memory error: Falcon Moose",
          "author": {
            "login": "pranayasai"
          },
          "bodyText": "Hello Everyone,\nI am trying to run a big mesh (4Million cells) simulation using Falcon/Kerstel (MOOSE) on Sawtooth (at INL HPC) but the simulation keeps crashing.\nSometimes it crashes without finishing any iterations and sometimes it runs one or two iterations and then crashes.\nI have tried running them on the compute node and also using PBS script but the error still persists.\nThe errors and number of processors used are listed below.\nI have tried running the simulation in multiple combinations for the number of processors requested/used:\n7200 requested: 3600 used (did not work, crashes with Error 1 (see below))\n1000 requested: 500 used (did not work, crashes with Error 1)\n1000 requested: 100 used (did not work, crashes with Error 1)\n500 requested: 100 used (did not work crashes with Error 1)\n500 requested: 75 used (works up to one iteration and then crashes with Error 2(see below))\n144 requested: 50 used (works up to one iteration and then crashes with Error 3(see below))\nError 1:\n[asaipran][~/Desktop/Forge/DFN/full2]> mpirun -np 150 ~/projects/kestrel/kestrel-opt -i fullDFN_PA_tran_PT.i\nBuilding mesh ..........................                                                   [ 27.39 s]\nCaching mesh information ....                                                              [  5.20 s]\nInitializing equation system ...........                                                   [ 12.94 s]\nCaching mesh information ....                                                              [  5.62 s]\nCopying variables from Exodus ...                                                          [  4.82 s]\nRunning App: main\nFramework Information:\nMOOSE Version:           git commit e3fb9a7 on 2020-08-10\nLibMesh Version:         8d10d4aff7dc56cb100ab27450120fd522449f40\nPETSc Version:           3.12.5\nSLEPc Version:           3.12.1\nCurrent Time:            Wed Feb  3 09:37:41 2021\nExecutable Timestamp:    Thu Sep 24 16:05:15 2020\nParallelism:\nNum Processors:          150\nNum Threads:             1\nMesh:\nParallel Type:           replicated\nMesh Dimension:          3\nSpatial Dimension:       3\nNodes:\nTotal:                 2050401\nLocal:                 15289\nElems:\nTotal:                 2000000\nLocal:                 13315\nNum Subdomains:          1\nNum Partitions:          150\nPartitioner:             metis\nNonlinear System:\nNum DOFs:                4100802\nNum Local DOFs:          30578\nVariables:               { \"pressure\" \"temperature\" }\nFinite Element Types:    \"LAGRANGE\"\nApproximation Orders:    \"FIRST\"\nAuxiliary System:\nNum DOFs:                18000000\nNum Local DOFs:          119835\nVariables:               { \"vel_x\" \"vel_y\" \"vel_z\" \"fluid_density\" \"viscosity\" \"porosity\" \"perm_x\"\n\"perm_y\" \"perm_z\" }\nFinite Element Types:    \"MONOMIAL\"\nApproximation Orders:    \"CONSTANT\"\nExecution Information:\nExecutioner:             Transient\nTimeStepper:             SolutionTimeAdaptiveDT\nSolver Mode:             NEWTON\nPETSc Preconditioner:    asm\nMOOSE Preconditioner:    SMP\nLEGACY MODES ENABLED:\nThis application uses the legacy material output option: material properties are output only on TIMESTEP_END, not INITIAL. To remove this message, set 'use_legacy_material_output' to false in this application. If there are gold output files that contain material property output for which output occurs on INITIAL, then these will generate diffs due to zero values being stored, and these tests should be re-golded.\nCaching mesh information ....                                                              [  5.33 s]\n[cli_131]: aborting job:\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 131\n[asaipran][~/Desktop/Forge/DFN/full2]>\nError 2:\nComputing initial residual .....                                                           [  6.09 s]\n0 Nonlinear |R| = 1.826232e+09\n===================================================================================\n=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES\n=   PID 125496 RUNNING AT r6i2n19.ib0.sawtooth.inl.gov\n=   EXIT CODE: 9\n=   CLEANING UP REMAINING PROCESSES\n=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES\n[proxy:0:1@r6i2n22] HYD_pmcd_pmip_control_cmd_cb (pm/pmiserv/pmip_cb.c:911): assert (!closed) failed\n[proxy:0:1@r6i2n22] HYDT_dmxu_poll_wait_for_event (tools/demux/demux_poll.c:76): callback returned error status\n[proxy:0:1@r6i2n22] main (pm/pmiserv/pmip.c:202): demux engine error waiting for event\n[proxy:0:2@r6i2n20] HYD_pmcd_pmip_control_cmd_cb (pm/pmiserv/pmip_cb.c:911): assert (!closed) failed\n[proxy:0:2@r6i2n20] HYDT_dmxu_poll_wait_for_event (tools/demux/demux_poll.c:76): callback returned error status\n[proxy:0:2@r6i2n20] main (pm/pmiserv/pmip.c:202): demux engine error waiting for event\n[mpiexec@r6i2n19] HYDT_bscu_wait_for_completion (tools/bootstrap/utils/bscu_wait.c:75): one of the processes terminated badly; aborting\n[mpiexec@r6i2n19] HYDT_bsci_wait_for_completion (tools/bootstrap/src/bsci_wait.c:23): launcher returned error waiting for completion\n[mpiexec@r6i2n19] HYD_pmci_wait_for_completion (pm/pmiserv/pmiserv_pmci.c:218): launcher returned error waiting for completion\n[mpiexec@r6i2n19] main (ui/mpich/mpiexec.c:340): process manager error waiting for completion\nError 3:\nComputing initial residual ......                                                          [  7.05 s]\n0 Nonlinear |R| = 6.311744e-01\n[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Petsc has generated inconsistent data\n[0]PETSC ERROR: is not a subset of lis\n[0]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[0]PETSC ERROR: Petsc Release Version 3.12.5, Mar, 29, 2020\n[0]PETSC ERROR: /home/asaipran/projects/kestrel/kestrel-opt on a  named r2i3n26 by asaipran Tue Feb  2 22:58:16 2021\n[0]PETSC ERROR: Configure options --COPTFLAGS=-O3 --CXXOPTFLAGS=-O3 --FOPTFLAGS=-O3 --with-x=0 --with-mpi=1 --with-ssl=0 --with-openmp=1 --with-debugging=0 --with-64-bit-indices=1 --with-cxx-dialect=C++11 --with-shared-libraries=1 --with-fortran-bindings=0 --with-cxxlib-autodetect=0 --download-hypre=1 --download-metis=1 --download-slepc=1 --download-ptscotch=1 --download-parmetis=1 --download-scalapack=1 --download-fblaslapack=1 --download-superlu_dist=1 AR=ar CC=mpicc CXX=mpicxx FC=mpifort FC=mpifort FC=mpifort --prefix=/apps/moose/stack/petsc-3.12.5_mvapich2-2.3.3_gcc-9.2.0\n[0]PETSC ERROR: #1 PCSetUp_ASM() line 378 in /tmp/tmp.yuC9vViIq5/petsc-3.12.5/src/ksp/pc/impls/asm/asm.c\n[0]PETSC ERROR: #2 PCSetUp() line 894 in /tmp/tmp.yuC9vViIq5/petsc-3.12.5/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #3 KSPSetUp() line 376 in /tmp/tmp.yuC9vViIq5/petsc-3.12.5/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #4 KSPSolve() line 703 in /tmp/tmp.yuC9vViIq5/petsc-3.12.5/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #5 SNESSolve_NEWTONLS() line 225 in /tmp/tmp.yuC9vViIq5/petsc-3.12.5/src/snes/impls/ls/ls.c\n[0]PETSC ERROR: #6 SNESSolve() line 4482 in /tmp/tmp.yuC9vViIq5/petsc-3.12.5/src/snes/interface/snes.c\n[cli_0]: aborting job:\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\nA similar input deck runs perfectly fine with a smaller mesh size.\nI am not entirely sure where does the problem lie.\nCan you please provide me any guidance to fix or to be able to run this simulation?\nThank you\nPranay",
          "url": "https://github.com/idaholab/moose/discussions/16878",
          "updatedAt": "2022-05-31T15:07:50Z",
          "publishedAt": "2021-02-03T17:53:41Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hi Pranay\nYou should request a number of nodes directly, and specify the maximum amount of memory available per node.\nCan you paste your PBS script here? The part where you are requesting ressources especially.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/16878#discussioncomment-335646",
                  "updatedAt": "2022-07-28T13:54:19Z",
                  "publishedAt": "2021-02-03T18:17:22Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "pranayasai"
                          },
                          "bodyText": "Hello Guillaume,\nI believe the available memory per node is around 4GB\nHere is the PBS script.\n#!/bin/bash\n#PBS -N full5\n#PBS -l select=5:ncpus=48:mpiprocs=48\n#PBS -l walltime=48:00:00\n#PBS -j oe\nmodule load use.moose PETSc\necho\necho PBS_NODEFILE is $PBS_NODEFILE\necho PBS_QUEUE is $PBS_QUEUE\necho\nhostname\nNPROCS=wc -l < $PBS_NODEFILE\necho\necho NPROCS: $NPROCS\necho\ncd /home/asaipran/Desktop/Forge/DFN/full5/\nmpirun -np 150 ~/projects/kestrel/kestrel-opt -i fullDFN_PA_tran_PT.i\n-Pranay",
                          "url": "https://github.com/idaholab/moose/discussions/16878#discussioncomment-335815",
                          "updatedAt": "2022-07-28T13:54:19Z",
                          "publishedAt": "2021-02-03T19:29:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Falcon nodes should have closer to 120 GB on-node ( so close to 4GB per core ). I dont know the exact number.\nAnd they have 2 x 18 cores per node, with no hyper-threading I think.\nWe don't recommend using more MPI ranks than CPUs on Falcon.\nSo to run 150 MPI processes, I would recommend asking for 5 nodes, and using 30 cores on each node.\nSo #PBS -l select=5:ncpus=180:mpiprocs=150:mem=115gb\n180 (=5 x 36) cores is to book the nodes entirely. You can also indicate you want exclusivity with another flag (saw it in discussions earlier on, dont use it myself) or by requesting all the memory.\nNow an issue that may come up is that the MPI processes are not placed equitably on each node. To help with that, you have to pass a hostfile to mpirun / mpiexec to tell it which hosts to spawn processes on.\nIf your job is too memory intensive, you should first update to the latest moose since there was a memory reducing PR lately, and you can try booking more and more nodes to spread your problem over.",
                          "url": "https://github.com/idaholab/moose/discussions/16878#discussioncomment-335880",
                          "updatedAt": "2022-07-28T13:54:25Z",
                          "publishedAt": "2021-02-03T19:56:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "pranayasai"
                          },
                          "bodyText": "I just checked, Sawtooth has 192GB on the node with 48 processors. No hyper-threading.",
                          "url": "https://github.com/idaholab/moose/discussions/16878#discussioncomment-335927",
                          "updatedAt": "2022-07-28T13:54:27Z",
                          "publishedAt": "2021-02-03T20:09:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "oh my bad, sorry I thought you meant Falcon the cluster! Ok Same advice, just adapt the numbers.",
                          "url": "https://github.com/idaholab/moose/discussions/16878#discussioncomment-336010",
                          "updatedAt": "2022-07-28T13:54:30Z",
                          "publishedAt": "2021-02-03T20:44:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Reduce the time to simulate grain growth",
          "author": {
            "login": "PengWei97"
          },
          "bodyText": "Dear MOOSE experts,\nI am trying to simulate a grain growth simulation of 1000 grains. I set end_time = 10000, but it took 58 hours. How can I optimize the input file to reduce the running time?\nThe following are the basic parameters and related files used in my simulation:\nNumber of cpu cores: 20\nNumber of initial grids: 409600\nGrid adaptation and time adaptation are used\nInput file: grain_growth_2D_graintracker_1000_output.txt\nPart of the information output to the terminal:\ngrain_growth_2D_graintracker_1000_output.log\nAfter adapting the grid, the initial model diagram\uff1a\n\nAny suggestions or recommendations to fix these problems would be greatly appreciated.\nThank you\nWei Peng",
          "url": "https://github.com/idaholab/moose/discussions/16592",
          "updatedAt": "2022-06-28T05:24:16Z",
          "publishedAt": "2020-12-26T04:50:20Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "laagesen"
                  },
                  "bodyText": "A couple of days is not unreasonable for a 1000-grain simulation. There are a couple of things you could try:\n-increase optimal_iterations to 8 or 10 in the TimeStepper block. This will likely give you larger time steps. You should check to make sure the results are consistent with the initial run you did with that parameter set to 6\n-Add a NumDOFs postprocessor so you know how many DOFs are in your problem as a function of time. You generally want about 10,000 to 20,000 DOF per core, so you can choose your # of cores to try to maintain this. When you say Number of initial grids: 409600 I am not sure if you mean number of elements in the mesh or number of DOFs. If it is # of elements in the mesh, the number of DOFs is probably significantly larger, so you are probably much larger that 20,000 DOF/core since you are using 20 cores. So you would need more cores.",
                  "url": "https://github.com/idaholab/moose/discussions/16592#discussioncomment-261165",
                  "updatedAt": "2022-06-28T05:24:17Z",
                  "publishedAt": "2021-01-04T20:57:38Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Hi laagesen,\nThank you very much for your response. Yes, Number of initial grids: 409,600 means number of elements. And the number of DOFs: 4,108,810, which is displayed in the log file at the end. According to your suggestion, the number of cores used is about 205, which is a very large number!\nThe question I want to ask is, is 205 the optimal number of cores to run this example?\nThanks again for your time. Finally, do you have some suggestions to optimize the input file to shorten the simulation time for a beginner like me? For example, whether to use a distributed mesh or a replicated mesh, etc.\nKind regards,\nWei Peng\nDetails are as follows,\nFramework Information:\nMOOSE Version:           git commit 54c00d0775 on 2020-12-17\nLibMesh Version:         c176405e538983b35afc4f6827e29e155141e42e\nPETSc Version:           3.14.2\nSLEPc Version:           3.14.0\nCurrent Time:            Wed Dec 23 22:33:21 2020\nExecutable Timestamp:    Sun Dec 20 22:02:49 2020\n\nParallelism:\n  Num Processors:          10\n  Num Threads:             1\n\nMesh: \n  Parallel Type:           replicated\n  Mesh Dimension:          2\n  Spatial Dimension:       2\n  Nodes:                   \n    Total:                 410881\n    Local:                 41401\n  Elems:                   \n    Total:                 409600\n    Local:                 40973\n  Num Subdomains:          1\n  Num Partitions:          10\n  Partitioner:             metis\n\nNonlinear System:\n  Num DOFs:                4108810\n  Num Local DOFs:          414010\n  Variables:               { \"gr0\" \"gr1\" \"gr2\" \"gr3\" \"gr4\" \"gr5\" \"gr6\" \"gr7\" \"disp_x\" \"disp_y\" } \n  Finite Element Types:    \"LAGRANGE\" \n  Approximation Orders:    \"FIRST\" \n\nAuxiliary System:\n  Num DOFs:                3687681\n  Num Local DOFs:          369185\n  Variables:               \"bnds\" { \"elastic_strain11\" \"elastic_strain22\" \"elastic_strain12\" \"unique_grains\" \n                             \"var_indices\" \"vonmises_stress\" \"C1111\" \"euler_angle\" } \n  Finite Element Types:    \"LAGRANGE\" \"MONOMIAL\" \n  Approximation Orders:    \"FIRST\" \"CONSTANT\" \n\nExecution Information:\n  Executioner:             Transient\n  TimeStepper:             IterationAdaptiveDT\n  Solver Mode:             Preconditioned JFNK\n  MOOSE Preconditioner:    SMP",
                          "url": "https://github.com/idaholab/moose/discussions/16592#discussioncomment-262161",
                          "updatedAt": "2022-06-28T05:24:17Z",
                          "publishedAt": "2021-01-05T09:34:44Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "friedmud"
                  },
                  "bodyText": "I agree with @laagesen : the best way to speed this up is to use more processors.  As he mentioned, you could easily use 200 procs with 4M dofs.\nAs for \"optimal\" - that really depends.  We recommend about 20k dofs/proc in order to keep parallel efficiency high.  You may be able to go well beyond that and still get speedup (I would expect to see speedup on this problem out to 500 cores or so)... but your parallel efficiency will start to drop off (the amount of speed increase won't be commensurate with the number of procs you're using).\nAnother thing you may want to try is to use the DT2 timestepper... you will have to play with the parameters to see what works for your problem - but there is a possibility of greatly increasing the timestep size.",
                  "url": "https://github.com/idaholab/moose/discussions/16592#discussioncomment-264985",
                  "updatedAt": "2022-06-28T05:24:18Z",
                  "publishedAt": "2021-01-06T15:22:16Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Thank friedmud for your information. Regarding the second suggestion you gave, I don\u2019t quite understand it. I don't quite understand your second suggestion. You mean to add the TimeStepper module to increase the size of the time step, right?\nFinally, if I change the Mesh type to a distributed mesh, will this help reduce simulation time? And, in MOOSE, which aspects of the calculation are affected by the mesh type?",
                          "url": "https://github.com/idaholab/moose/discussions/16592#discussioncomment-266024",
                          "updatedAt": "2022-06-28T05:24:18Z",
                          "publishedAt": "2021-01-07T01:11:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The DT2 time stepper is an adaptive time stepper that will increase the time step through the simulation, based on comparing the solution with time steps of dt and dt/2. This is usually more effective than trying to uniformly increase the time step.\nUsing a distributed mesh is typically done to reduce the memory footprint, as it avoids duplicating the mesh for every process. I don't foresee a large impact on simulation time.\nEDIT: Maybe with the mesh adaptivity, there could be an impact\nYou could try running your simulation with the --timing flag (EDIT add PerfGraphOutput) to see which parts of the simulation are the most computationally expensive.",
                          "url": "https://github.com/idaholab/moose/discussions/16592#discussioncomment-266710",
                          "updatedAt": "2022-06-28T05:24:18Z",
                          "publishedAt": "2021-01-07T11:06:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "After adding PerfGraphOutput to the output, it displays the proportion of each calculation part on the terminal as follows,\n\n\nAfter adding PerfGraphOutput to the output, it displays the proportion of each calculation part on the terminal as follows,\nIn this regard, would you have any good suggestions for moose beginners like me?",
                          "url": "https://github.com/idaholab/moose/discussions/16592#discussioncomment-266831",
                          "updatedAt": "2022-06-28T05:24:18Z",
                          "publishedAt": "2021-01-07T12:18:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The previous advice given by @laagesen and @friedmud still applies.\nIn addition, it seems to spend a significant amount of time in mesh adaptivity. Maybe using a distributed mesh is a good idea? I am not sure how the modifications to the mesh are spread across processes.\nSince the problem is converging with no issues, you could use the skip_exception_check parameter in your executioner to gain 5% runtime here.\nEDIT: It's probably not really that function causing the 5% overhead, it's probably just work imbalance between nodes that's felt at this synchronization step",
                          "url": "https://github.com/idaholab/moose/discussions/16592#discussioncomment-266864",
                          "updatedAt": "2022-06-28T05:24:21Z",
                          "publishedAt": "2021-01-07T12:38:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Thank you for your suggestion. I will try to use a distributed mesh and hope it will help reduce the calculation time.",
                          "url": "https://github.com/idaholab/moose/discussions/16592#discussioncomment-326393",
                          "updatedAt": "2022-06-28T05:24:21Z",
                          "publishedAt": "2021-02-01T05:49:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to add App to \"External App tests\"",
          "author": {
            "login": "WilkAndy"
          },
          "bodyText": "Hi,\n@josebastiase and i have made a small MOOSE app called RHEA, https://github.com/josebastiase/RHEA.   It has two Materials and two tests associated with it.   How can we add it to the \"External App tests\" ?\na",
          "url": "https://github.com/idaholab/moose/discussions/16837",
          "updatedAt": "2022-07-06T07:21:02Z",
          "publishedAt": "2021-01-29T04:12:33Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Go ahead and shoot me an email, @WilkAndy, and we'll discuss the process of setting this up.\nFor anyone else curious about having your applications tested: Providing that your application is in active development and you're willing to contribute to keeping it up to date, we're glad to add it to testing!\nNote that there are other requirements for approval, but those will be discussed one-on-one when making the request.",
                  "url": "https://github.com/idaholab/moose/discussions/16837#discussioncomment-319641",
                  "updatedAt": "2022-07-06T07:21:01Z",
                  "publishedAt": "2021-01-29T04:19:42Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Segfault while executing a test input file",
          "author": {
            "login": "HeejaeJu"
          },
          "bodyText": "Hello Moose team.\nI'was following the tutorial for app development tutorial and contact a segmentation fault error message while executing simple_diffusion.i using peacock.\nThe message is as bellow.\nFound executable: /Users/heejaeju/Moose/tutorial/tutorial-opt\n[1]    13124 segmentation fault  ~/Moose/moose/python/peacock/peacock -i simple_diffusion.i\nHow can I fix it?\nThanks in advance for any help!",
          "url": "https://github.com/idaholab/moose/discussions/16827",
          "updatedAt": "2022-07-04T08:37:46Z",
          "publishedAt": "2021-01-28T08:28:33Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nCan you please give us more details on your environment? Is this a Linux machine or Windows Subsystem for Linux?\nDoes it work when you run the input file without peacock ? (/Users/heejaeju/Moose/tutorial/tutorial-opt -i simple_diffusion.i)\nDoes it work when you start peacock without an input file and load the input file through the GUI ?\nThis is simple_diffusion.i from the tests for step 1 right?\nHopefully this will give some insights.\nTagging @aeslaughter since he is the most knowledgeable about peacock.\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/16827#discussioncomment-317870",
                  "updatedAt": "2022-07-04T08:37:46Z",
                  "publishedAt": "2021-01-28T14:33:41Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "@GiudGiud is correct, we need some information on your system and environment. Also, if you just run Peacock without any arguments, does it still fail?",
                          "url": "https://github.com/idaholab/moose/discussions/16827#discussioncomment-317948",
                          "updatedAt": "2022-07-04T08:37:46Z",
                          "publishedAt": "2021-01-28T15:05:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "HeejaeJu"
                          },
                          "bodyText": "Thank you for the reply.\nYes, this is the simple_diffuion.i.\nMy environment is Macintosh(OS : BigSur). Execution without peacock works well.\nAnd I found that the same error message is printed when I try to start peacock.exe.\nIn addition, double clicking the peacock.exe file shows me the terminal window as bellow.\n\nIs there any solution for this?\nThanks.",
                          "url": "https://github.com/idaholab/moose/discussions/16827#discussioncomment-318015",
                          "updatedAt": "2022-07-04T08:37:46Z",
                          "publishedAt": "2021-01-28T15:27:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "I wonder if this is the problem: https://keremkoseoglu.com/2020/11/14/pyqt5-problem-on-macos-big-sur-solved\nDid you install the MOOSE environment with conda or manually?\nIf you are using conda, try searching for the available options and install one of the older ones. I am running 5.12.\nconda search pyqt\nconda install pyqt=5.12 # or whatever version you want to try out",
                          "url": "https://github.com/idaholab/moose/discussions/16827#discussioncomment-318108",
                          "updatedAt": "2022-07-04T08:37:47Z",
                          "publishedAt": "2021-01-28T15:59:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "It seems that Peacock is not working on BigSur, I have two other reports of failure. I will need to investigate the problem, it will be next week at the earliest before I get to this.",
                          "url": "https://github.com/idaholab/moose/discussions/16827#discussioncomment-318178",
                          "updatedAt": "2022-07-04T08:37:44Z",
                          "publishedAt": "2021-01-28T16:24:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "HeejaeJu"
                          },
                          "bodyText": "Thanks for your help. I will wait for solution.\nHave a nice weekend!",
                          "url": "https://github.com/idaholab/moose/discussions/16827#discussioncomment-319475",
                          "updatedAt": "2022-07-04T08:37:45Z",
                          "publishedAt": "2021-01-29T01:19:54Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Registering Objects",
          "author": {
            "login": "Leni-Yeo"
          },
          "bodyText": "Hello, I am a student and new user for MOOSE. I was doing the workshop and trying the registration for the \"DarcyPressure\" kernel. The step do not specifically tell where and how to specifically register. It is initially registered under \"DarcyThermoMechApp\" in the workshop section, but I keep getting a \" 'DarcyPressure' is not a registered object \" error. I tried registering under \"MooseApp\" and got the same trouble. I was just trying to run the tutorial input file step2.i . I was told that I do not need to create a  new application to run it (Which I am not sure either). It would be helpful if you could provide locations where to save the .h and .C files or the general steps to do in the case I need to save/register a new object. Thank you.",
          "url": "https://github.com/idaholab/moose/discussions/16833",
          "updatedAt": "2022-06-10T18:27:29Z",
          "publishedAt": "2021-01-28T22:26:08Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "We have detailed instructions that might help.\nhttps://mooseframework.inl.gov/getting_started/examples_and_tutorials/tutorial01_app_development/index.html",
                  "url": "https://github.com/idaholab/moose/discussions/16833#discussioncomment-319285",
                  "updatedAt": "2022-06-10T18:27:29Z",
                  "publishedAt": "2021-01-28T23:09:03Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "I recommend going through each step carefully. You will find some help specific to your question in Step 5 of this new tutorial.",
                          "url": "https://github.com/idaholab/moose/discussions/16833#discussioncomment-319290",
                          "updatedAt": "2022-06-10T18:27:31Z",
                          "publishedAt": "2021-01-28T23:10:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Leni-Yeo"
                          },
                          "bodyText": "Thank you, that worked!",
                          "url": "https://github.com/idaholab/moose/discussions/16833#discussioncomment-319309",
                          "updatedAt": "2022-06-10T18:27:31Z",
                          "publishedAt": "2021-01-28T23:23:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Hide (some) postprocessors from console?",
          "author": {
            "login": "aprilnovak"
          },
          "bodyText": "Hi all,\nIs it possible to hide individual postprocessors from being output to the screen? For instance, if all I care about is the value of LinearCombinationPostprocessor, that combines multiple other postprocessors into something meaningful, is it possible to hide the postprocessors that just went into the calculation of the LinearCombinationPostprocessor? This would make it easier to monitor the solution progression so I don't have loads of screen output.\nI know I can hide them from the Exodus file with hide- is there something similar for the screen output?\nThanks!\n-April",
          "url": "https://github.com/idaholab/moose/discussions/16815",
          "updatedAt": "2023-10-12T15:33:16Z",
          "publishedAt": "2021-01-26T21:41:19Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Does this work?\n[Outputs]\n  [foo]\n    type = Console\n    hide = 'bar baz'\n  []\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/16815#discussioncomment-312720",
                  "updatedAt": "2023-10-12T15:33:16Z",
                  "publishedAt": "2021-01-26T21:45:50Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "Yes! Wonderful, thank you :)",
                          "url": "https://github.com/idaholab/moose/discussions/16815#discussioncomment-312746",
                          "updatedAt": "2023-10-12T15:33:30Z",
                          "publishedAt": "2021-01-26T21:48:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "You can also put \"outputs = none\" in the PP/VPP block as well.",
                          "url": "https://github.com/idaholab/moose/discussions/16815#discussioncomment-312805",
                          "updatedAt": "2023-10-12T15:33:30Z",
                          "publishedAt": "2021-01-26T22:10:53Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Cholesky decomposition error for boundary restricted AuxKernel",
          "author": {
            "login": "aprilnovak"
          },
          "bodyText": "Hi all,\nI'd like to apply the DiffusionFluxAux auxkernel to a MONOMIAL auxvariable of FIRST order on a boundary. However, when I try this, I get the following error:\nTime Step 1, time = 1, dt = 1\nError! Can only use Cholesky decomposition with symmetric positive definite matrices.\nStack frames: 26\n0: libMesh::print_trace(std::ostream&)\n1: libMesh::MacroFunctions::report_error(char const*, int, char const*, char const*)\n2: libMesh::DenseMatrix<double>::_cholesky_decompose()\n3: void libMesh::DenseMatrix<double>::cholesky_solve<double>(libMesh::DenseVector<double> const&, libMesh::DenseVector<double>&)\n4: AuxKernelTempl<double>::compute()\n5: ComputeElemAuxBcsThread<AuxKernelTempl<double> >::operator()(libMesh::StoredRange<MooseMesh::const_bnd_elem_iterator, BndElement const*> const&)\n6: void libMesh::Threads::parallel_reduce<libMesh::StoredRange<MooseMesh::const_bnd_elem_iterator, BndElement const*>, ComputeElemAuxBcsThread<AuxKernelTempl<double> > >(libMesh::StoredRange<MooseMesh::const_bnd_elem_iterator, BndElement const*> const&, ComputeElemAuxBcsThread<AuxKernelTempl<double> >&)\n7: void AuxiliarySystem::computeElementalVarsHelper<AuxKernelTempl<double> >(MooseObjectWarehouse<AuxKernelTempl<double> > const&, std::vector<std::vector<MooseVariableFieldBase*, std::allocator<MooseVariableFieldBase*> >, std::allocator<std::vector<MooseVariableFieldBase*, std::allocator<MooseVariableFieldBase*> > > > const&, unsigned int)\n8: AuxiliarySystem::computeElementalVars(MooseEnumItem)\n9: AuxiliarySystem::compute(MooseEnumItem)\n10: FEProblemBase::computeResidualTags(std::set<unsigned int, std::less<unsigned int>, std::allocator<unsigned int> > const&)\n11: FEProblemBase::computeResidualInternal(libMesh::NumericVector<double> const&, libMesh::NumericVector<double>&, std::set<unsigned int, std::less<unsigned int>, std::allocator<unsigned int> > const&)\n12: FEProblemBase::computeResidualSys(libMesh::NonlinearImplicitSystem&, libMesh::NumericVector<double> const&, libMesh::NumericVector<double>&)\n13: NonlinearSystem::solve()\n14: FEProblemBase::solve()\n15: FEProblemSolve::solve()\n16: PicardSolve::solveStep(double, double&, double, double&, bool, std::set<unsigned int, std::less<unsigned int>, std::allocator<unsigned int> > const&)\n17: PicardSolve::solve()\n18: TimeStepper::step()\n19: Transient::takeStep(double)\n20: Transient::execute()\n21: MooseApp::executeExecutioner()\n22: MooseApp::run()\n23: ../../../../cardinal-opt(+0x5eaf) [0x56135c9a5eaf]\n24: __libc_start_main\n25: ../../../../cardinal-opt(+0x60ae) [0x56135c9a60ae]\n[0] ./include/libmesh/dense_matrix_impl.h, line 1001, compiled Dec 18 2020 at 12:35:12\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\n\nAfter playing around a bit, I've found two different settings that affect whether or not I see this error:\n\nIf I change the order to order = CONSTANT, but keep the boundary restriction (boundary = '1'), I don't see the error.\nIf I remove the boundary restriction (remove boundary='1'), I don't see the error for either order = CONSTANT or order = FIRST.\n\nI'm having a hard time trying to reason out whether if I just shouldn't be trying to apply this AuxKernel in this manner, or if there's a bug here. I would have thought that any order elemental auxvariable might complain about a boundary-restricted auxkernel, but order = CONSTANT is fine. If I'm doing something erroneous here, I think a more narrowly-focused error message would be helpful.\nI've attached a minimal example that shows this error. The error disappears if you (A) change the avg_flux order to CONSTANT, or (B) remove boundary='1' for the DiffusionFluxAux kernel.\nThanks,\n-April",
          "url": "https://github.com/idaholab/moose/discussions/16562",
          "updatedAt": "2022-08-08T02:52:01Z",
          "publishedAt": "2020-12-21T22:01:11Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aprilnovak"
                  },
                  "bodyText": "example.txt",
                  "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-232114",
                  "updatedAt": "2022-08-08T02:52:03Z",
                  "publishedAt": "2020-12-21T22:02:14Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "This simply means the matrix is not symmetric anymore when the DiffusionFluxAux kernel is restricted on boundary.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-237927",
                          "updatedAt": "2022-08-08T02:52:03Z",
                          "publishedAt": "2020-12-23T18:23:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "Gotcha. Is that error supposed to kill the program though? I would have thought that there's not really any solves going on with the auxvariables - just compute them from other variables.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-237935",
                          "updatedAt": "2022-08-08T02:52:03Z",
                          "publishedAt": "2020-12-23T18:29:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Here is the code:\n  else /* high-order */\n    {\n      _local_re.resize(_n_local_dofs);\n      _local_re.zero();\n      _local_ke.resize(_n_local_dofs, _n_local_dofs);\n      _local_ke.zero();\n\n      // assemble the local mass matrix and the load\n      for (unsigned int i = 0; i < _test.size(); i++)\n        for (_qp = 0; _qp < _qrule->n_points(); _qp++)\n        {\n          ComputeValueType t = _JxW[_qp] * _coord[_qp] * _test[i][_qp];\n          _local_re(i) += t * computeValue();\n          for (unsigned int j = 0; j < _test.size(); j++)\n            _local_ke(i, j) += t * _test[j][_qp];\n        }\n\n      // mass matrix is always SPD\n      _local_sol.resize(_n_local_dofs);\n      _local_ke.cholesky_solve(_local_re, _local_sol);\n\n      _var.setDofValues(_local_sol);\n    }\n\nIt has nothing to do with PETSc solver for the nonlinear system. It is trying to inverse a small element matrix system to get RHS. If this fails, the computation can not keep going.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-237975",
                          "updatedAt": "2022-08-08T02:52:03Z",
                          "publishedAt": "2020-12-23T18:41:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "Would an error like the following always catch when this happens? I guess I am wishing that the error message were more instructive.\n// in AuxKernel constructor?\nif (order > CONSTANT && isParamValid(\"boundary\"))\n  mooseError(\"Auxvariable orders higher than CONSTANT are not supported with boundary-restricted AuxKernels\");",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-238363",
                          "updatedAt": "2020-12-23T20:00:55Z",
                          "publishedAt": "2020-12-23T20:00:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "You can make it a paramError(\"boundary\", \"AuxVariable orders...\") and it will report a line number in the error report.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-240259",
                          "updatedAt": "2020-12-24T16:11:05Z",
                          "publishedAt": "2020-12-24T16:11:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Would an error like the following always catch when this happens? I guess I am wishing that the error message were more instructive.\n\n\n// in AuxKernel constructor?\nif (order > CONSTANT && isParamValid(\"boundary\"))\nmooseError(\"Auxvariable orders higher than CONSTANT are not supported with boundary-restricted AuxKernels\");\n\nIt is a great idea to add these checks. At the same time, I am still trying to understand why the mass matrix is not symmetric when it is restricted on a boundary.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-240276",
                          "updatedAt": "2020-12-24T16:21:20Z",
                          "publishedAt": "2020-12-24T16:21:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "@fdkong, any update on why the mass matrix wouldn't be symmetric on a boundary?\nFor Cardinal, we're currently restricted to using a CONSTANT MONOMIAL to transfer heat flux into nekRS (because nodal auxvariables don't support gradients, and the Cholesky error is preventing me from using FIRST MONOMIAL). Our BISON mesh for the fuel pins are generally much coarser than the nekRS meshes for the surrounding fluid. Being restricted to a constant heat flux in each BISON element is distorting the heat flux going into nekRS so that we're needing to use really fine BISON meshes (for the solid) just to match the results of a standalone reference nekRS solution (i.e. nekRS solving for both the fluid and solid).\nMaybe there's another approach I haven't thought of to retain that spatial resolution of the flux coming from MOOSE?",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-302999",
                          "updatedAt": "2021-01-22T17:45:30Z",
                          "publishedAt": "2021-01-22T17:45:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "@aprilnovak If you could send me an example, I will look into this.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-309745",
                          "updatedAt": "2021-01-25T20:23:08Z",
                          "publishedAt": "2021-01-25T20:23:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "YaqiWang"
                          },
                          "bodyText": "Do we have any test covering this case in MOOSE? The matrix could be rank deficient because we are only assembling it with the quadrature points on the boundary side. We may need the pseudo inverse for this. But I am surprised we do not see any error before if this is the case.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-310035",
                          "updatedAt": "2021-01-25T23:54:15Z",
                          "publishedAt": "2021-01-25T22:29:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "Sure, thanks a bunch! I've attached an example here. This will show you that changing from CONSTANT MONOMIAL to FIRST MONOMIAL throws the error.\nAfter thinking about this more, I realize that I have a second question. It relates to my original question, so I thought I'd ask it here, but if it'd be better as a new discussion let me know! I'll illustrate my question with an example:\nThe Problem and Reference Solution\nI have a 2-region cylinder heat conduction problem. In the center cylinder, I have a super asymmetrical heat source (to exaggerate the results), while in the outer cylinder there is no heat source. Other than that, there is just heat conduction. To get a reference solution with a single application, I ran two different meshes (I actually made new meshes for each, and didn't just use uniform_refine):\nCoarse mesh, with 9128 elements:\n\nFine mesh, with 235420 elements:\n\nThe reference temperature distribution on the super fine mesh looks like:\n\nThe temperature on the finer reference mesh is visually identical, so I don't repeat it. On the interface between the two cylinders, the maximum and minimum temperatures for the reference cases are:\n\n\n\nMesh\nNumber of elements\nMax temp\nMin temp\n\n\n\n\nCoarse reference\n9128\n1959.62\n1340.72\n\n\nFine reference\n235420\n1961.22\n1341.78\n\n\n\nMy point in showing two reference solutions is to prove that the results I'm trying to replicate are converged. And it's important that the coarse mesh of ~9000 elements is very close to the fine result, so for my intent I consider the 9000 element mesh to be converged.\nSolving as Two Regions\nSo, suppose that I want to match those max/min temperatures for the same physics, but I'm solving each cylinder with a different App. This approximates what I'm doing with the nekRS wrapping - one App solves for the inner cylinder, while a different App solves for the outer cylinder. For instance, here are two meshes - the top one would be solved by the master app (where a MatchedValueBC is set on the outer surface), while the bottom one would be solved by the sub app (where a CoupledVarNeumannBC is set on the inner surface). Same physics, and with iteration, I should match the reference two-region problem.\nApp 1 domain and solution:\n\nApp 2 domain and solution:\n\nThe Problem:\nThe problem is that, even when using meshes of similar resolution as the reference meshes, the fact that I'm limited to a CONSTANT MONOMIAL heat flux going in to the outer cylinder means that I need a super fine mesh just to represent the data transfer. This table shows examples of what I'm talking about: I gradually refine the meshes in the nek_master.i and nek.i input files (I always match them perfectly at the interface to avoid anything weird with needing to normalize incoming flux values). When the number of elements is similar to the number used in the reference mesh, errors are on the order of 25 K. Even when using meshes with 3x as many elements as the reference mesh, errors are still on the order of 8 K.\n\n\n\nMesh\nNumber of elements\nMax temp ERROR\nMin temp ERROR\n\n\n\n\n2 coupled apps\n9000\n23.5\n6.1\n\n\n2 coupled apps\n12000\n13.0\n3.3\n\n\n2 coupled apps\n18000\n9.2\n2.3\n\n\n2 coupled apps\n24000\n8.0\n1.6\n\n\n\nHere's the finest mesh I'm using:\n\nThis is way finer than the coarse mesh I'm comparing against (i.e. picture 1), so I would expect good agreement.\nThe Question\nHow can I solve two coupled Apps and get similar results as the fully-coupled two-region problem, without needing > 3x finer meshes in the coupled App case?\nIs it impossible? From my original question, my motivation was getting around this super fine mesh issue, which is really the root of what I'm interested in addressing.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-310042",
                          "updatedAt": "2021-01-25T22:50:49Z",
                          "publishedAt": "2021-01-25T22:34:55Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@fdkong Can you help with this?",
                  "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-232426",
                  "updatedAt": "2022-08-08T02:52:09Z",
                  "publishedAt": "2020-12-21T22:50:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "YaqiWang"
                  },
                  "bodyText": "This change can produce the error on my Mac:\ndiff --git a/test/tests/auxkernels/element_aux_boundary/element_aux_boundary.i b/test/tests/auxkernels/element_aux_boundary/element_aux_boundary.i\nindex 3a8bbd227a..de6b47fe36 100644\n--- a/test/tests/auxkernels/element_aux_boundary/element_aux_boundary.i\n+++ b/test/tests/auxkernels/element_aux_boundary/element_aux_boundary.i\n@@ -13,7 +13,7 @@\n [AuxVariables]\n   [./real_property]\n     family = MONOMIAL\n-    order = CONSTANT\n+    order = SECOND\n   [../]\n []\n\nSupposedly with order=FIRST we should see the same error, but I check the code in libMesh,\n              libmesh_error_msg_if(A(i,j) <= 0.0,\n                                   \"Error! Can only use Cholesky decomposition with symmetric positive definite matrices.\");\n\npossibly it is very sensitive to numerical round-off comparing with 0, thus we did not see the error with the first order. Possibly due to the same reason, you did not see errors in some of your calculations. I checked several moose tests, they are all using constant monomial unfortunately. The fix will be using different lapack function to find a special solution for the singular mass matrix. For example, replace\n_local_ke.cholesky_solve(_local_re, _local_sol);\n\nin AuxKernel.C with\n_local_ke.svd_solve(_local_re, _local_sol);\n\ncan temporarily solve the problem. We choose cholesky_solve because we think mass matrix is SPD and Cholesky is more efficient.",
                  "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-310466",
                  "updatedAt": "2022-08-08T02:52:10Z",
                  "publishedAt": "2021-01-26T04:14:05Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "thus we did not see the error with the first order\n\nI actually did see the error with FIRST order - but it's interesting that you didn't! That's definitely a good insight. In any case, it would probably be a good idea to include some non-CONSTANT monomial tests in MOOSE.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-310482",
                          "updatedAt": "2022-08-08T02:52:12Z",
                          "publishedAt": "2021-01-26T04:26:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "YaqiWang"
                          },
                          "bodyText": "Good to hear that. You can try svd_solve for now if your task is urgent.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-310487",
                          "updatedAt": "2022-09-21T13:14:36Z",
                          "publishedAt": "2021-01-26T04:29:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "YaqiWang"
                          },
                          "bodyText": "Also April, not sure if you noticed my warning in my previous post. I did not check your model, but be sure you do not hit the situation I described in #5061.",
                          "url": "https://github.com/idaholab/moose/discussions/16562#discussioncomment-310496",
                          "updatedAt": "2022-09-21T13:14:37Z",
                          "publishedAt": "2021-01-26T04:36:15Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Issue compiling project on HPC",
          "author": {
            "login": "ajsummers"
          },
          "bodyText": "I previously posted an installation problem here: #16711\nFollowing from this, I have run into an error while running make for the first time on a project:\nCompiling C++ (in opt mode) /home/ajs0201/projects/moose/framework/build/unity_src/outputs_Unity.C...\n/usr/bin/ld: BFD version 2.20.51.0.2-5.48.el6_10.1 20100205 internal error, aborting at reloc.c line 443 in bfd_get_reloc_size\n\n/usr/bin/ld: Please report this bug.\n\ncollect2: error: ld returned 1 exit status\nmake: *** [/home/ajs0201/projects/moose/modules/module_loader/lib/libmodule_loader_with-opt.la] Error 1\nmake: *** Waiting for unfinished jobs....\n/usr/bin/ld: warning: libgfortran.so.4, needed by /home/ajs0201/miniconda3/envs/moose/lib/libpetsc.so, not found (try using -rpath or -rpath-link)\n\nThe operating system of the HPC is CentOS 6.10.\nI have the following modules loaded into the environment:\n\ngcc/8.1.0\nmpich/3.2.1\ncmake/gcc/3.12.3\nmpc/1.0.3\n\nThanks again in advance for any help!",
          "url": "https://github.com/idaholab/moose/discussions/16802",
          "updatedAt": "2022-11-17T18:40:12Z",
          "publishedAt": "2021-01-24T00:02:50Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "Dear @ajsummers\nTry the manual installation instructions:\nhttps://mooseframework.inl.gov/getting_started/installation/manual_installation_gcc.html\nIt works better for us on CentOS.\nBest Regards,\nNicol\u00f2 Grilli\nNational University of Singapore",
                  "url": "https://github.com/idaholab/moose/discussions/16802#discussioncomment-305654",
                  "updatedAt": "2022-11-17T18:40:16Z",
                  "publishedAt": "2021-01-24T02:25:13Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "Going over your old post and this one, it looks like you did both a Conda and a Manual Install:\nI started off using the standard Linux installation method. Installed Miniconda and the Moose environment, then cloned in the repository. After this, I followed the GCC/MPICH manual installation method.\nThe error in question is from using Conda's moose-petsc, in conflict with compilers provided by your system:\n    gcc/8.1.0\n    mpich/3.2.1\n    cmake/gcc/3.12.3\n    mpc/1.0.3\n\nThese will conflict, as moose-petsc (10 days ago) was built with GCC 7.3.0 (Conda's compilers at the time: libgfortran.so.4). Today if you were to start over, you would see that moose-petsc is now using Conda's latest GCC compilers. -but that is neither here nor there. However, there would still be a conflict...\nWhen deciding which set of instructions to follow, it is important to follow only one set. This is because:\n\nWhen using Conda (and specifically the moose packages moose-libmesh, or moose-petsc), there is no reason to build PETSc (or libMesh).\nWhen following the Manual Installation method, you have to build everything manually (possibly gcc and mpich. Certainly PETSc, libMesh).\n\nBecause it looks like you were able to successfully build PETSc back in the old thread, I would recommend uninstalling (or simply not using) the moose environment you installed with Conda. If you followed the Conda instructions to the letter, that would mean you should do:\nconda deactivate\nconda env remove -n moose\n\nNext, we should re-build libMesh, MOOSE and your application cleanly.\nClose your terminals (for good measure). Re-open them, and clean moose and your application (note! be sure to commit your changes or this will delete them!)\ncd /home/ajs0201/projects/moose\ngit clean -xfd\ncd yourapp #<-- where ever this may be\ngit clean -xfd     # Note: again, be sure to commit any changes. This command deletes un-tracked files.\n\nThe above will purge any stale libraries lingering around, possibly still built with Conda's compilers.\nNext, re-build libMesh, MOOSE and your app using your system compilers:\nmodule purge\nmodule load gcc/8.1.0 mpich/3.2.1 cmake/gcc/3.12.3 mpc/1.0.3\ncd /home/ajs0201/projects/moose/scripts\n./update_and_rebuild_libmesh.sh\ncd yourapp\nmake",
                  "url": "https://github.com/idaholab/moose/discussions/16802#discussioncomment-308667",
                  "updatedAt": "2022-11-17T18:42:36Z",
                  "publishedAt": "2021-01-25T14:32:44Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ajsummers"
                          },
                          "bodyText": "Thank you for your response. I will try this and update the thread. Should I recompile petsc as well?",
                          "url": "https://github.com/idaholab/moose/discussions/16802#discussioncomment-310055",
                          "updatedAt": "2023-08-04T02:29:37Z",
                          "publishedAt": "2021-01-25T22:38:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ajsummers"
                          },
                          "bodyText": "I followed the directions in this post and got the following message while performing make in the test directory:\nThere appears to be a libMesh update in progress for\nthe branch of MOOSE you are operating on (only the\nmaster branch contains publicly available moose-libmesh\npackages).\n\nYou must provide your own libMesh either by:\n\n    1. Uninstall moose-libmesh and build it manually via\n       moose/scripts/update_and_rebuild_libmesh.sh\nor\n    2. Use `conda build` to build your own moose-libmesh\n       package.\n\nI've decided to remove Anaconda entirely and restart the GCC/MPICH installation from scratch.",
                          "url": "https://github.com/idaholab/moose/discussions/16802#discussioncomment-310277",
                          "updatedAt": "2023-08-04T02:29:47Z",
                          "publishedAt": "2021-01-26T01:09:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "should i recompile petsc\nNo need to rebuild PETSc, as I think you have that built correctly.\nIt may be that you have conda-build available. Honestly that little check is causing more issues than it solves... Is there a way you can create a new vanilla conda environment, so as not to have conda-build available?\nconda create -n bare -q -y\nconda activate bare\n\nShould be pretty blank. Unless someone added some defaults that get installed with new environment created...\nSorry for the delayed response!",
                          "url": "https://github.com/idaholab/moose/discussions/16802#discussioncomment-312360",
                          "updatedAt": "2023-08-04T02:29:49Z",
                          "publishedAt": "2021-01-26T19:10:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Oh! I didn't see this little tidbit here:\nI've decided to remove Anaconda entirely and restart the GCC/MPICH installation from scratch.\nThat's fine, sorry I wasn't able to respond in a timely manner. If you do happen to run into that \"There appears to be a libMesh update...\", please try the above conda bare environment trick (however, without Anaconda even installed, I foresee no issues). If that doesn't work, I know exactly where that little hiccup is happening in MOOSE, and we can avoid it (line 8 or ~10 in moose/framework/moose.mk). You will see it calling a conda verify script in moose/scripts. That is our culprit.\nIt is normally only triggered if someone has conda-build installed in their conda environment (what we use to build moose-libmesh). And such, it sometimes gets in the way of legitimate builders trying to manually build the libraries such as yourself.",
                          "url": "https://github.com/idaholab/moose/discussions/16802#discussioncomment-312515",
                          "updatedAt": "2023-08-04T02:29:51Z",
                          "publishedAt": "2021-01-26T20:30:13Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}