{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wNC0xN1QxMDo1NTozNi0wNTowMM4ATcT4"
    },
    "edges": [
      {
        "node": {
          "title": "Stress-strain curve",
          "author": {
            "login": "xchengood"
          },
          "bodyText": "Hello,\nI am studying CrystalPlasticityHCPDislocationSlipBeyerleinUpdate class and performing example files like update_method_hcp_no_substructure.i. The question is that there is only an elastic part and no yield occurs in the stress-strain curve below.  What parameters should I change?\n\nWhen I change end_time from 2.5 to 250, I got the error below. Any ideas about this?\n\nThank you.",
          "url": "https://github.com/idaholab/moose/discussions/24113",
          "updatedAt": "2023-04-19T16:32:11Z",
          "publishedAt": "2023-04-18T16:20:51Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "sapitts"
                  },
                  "bodyText": "Hi @xchengood,\nThis particular input file is designed to artificially test a corner case in the code, and I wouldn't expect this set of input parameters to produce realistic results, see the comment in the input file here:\n\n  \n    \n      moose/modules/tensor_mechanics/test/tests/crystal_plasticity/hcp_single_crystal/update_method_hcp_no_substructure.i\n    \n    \n         Line 127\n      in\n      d295aad\n    \n  \n  \n    \n\n        \n          \n               substructure_rate_coefficient_per_mode = '-355 -0.4' #artifical, non-physical values for testing purposes \n        \n    \n  \n\n\nTo help guide your efforts in finding realistic vs code regression testing input files, you can look at the requirement and detail lines in the tests file. And the published papers cited in the class documentation\nHope this helps,\nStephanie",
                  "url": "https://github.com/idaholab/moose/discussions/24113#discussioncomment-5651477",
                  "updatedAt": "2023-04-18T16:44:18Z",
                  "publishedAt": "2023-04-18T16:44:17Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MOOSE INSTALLATION FOR HPC",
          "author": {
            "login": "412120052"
          },
          "bodyText": "i am very new to Moose and HPC...while installing moose in our HPC i am facing the following issues..kindly help me with this.....Thanks in advance\n[theormet@kalam ~]$ sudo yum install gcc\n[sudo] password for theormet: \nLoaded plugins: fastestmirror, langpacks\nLoading mirror speeds from cached hostfile\nPackage gcc-4.8.5-16.el7.x86_64 already installed and latest version\nNothing to do\n[theormet@kalam ~]$ \n[theormet@kalam /]$ cd share/apps/\n[theormet@kalam apps]$ \n[theormet@kalam apps]$ \n[theormet@kalam apps]$ \n[theormet@kalam apps]$ echo $CC $CXX $FC $F90 $F77\n\n[theormet@kalam apps]$ export CC=mpicc CXX=mpicxx FC=mpif90 F90=mpif90 F77=mpif77\n[theormet@kalam apps]$ echo $0\nbash\n[theormet@kalam apps]$ mkdir -p ~/projects\n[theormet@kalam apps]$ cd ~/projects\n[theormet@kalam projects]$ ls\n[theormet@kalam projects]$ cd ..\n[theormet@kalam ~]$ ls\nDesktop  Documents  Downloads  Music  Pictures  projects  Public  Templates  Videos\n[theormet@kalam ~]$ cd ~/projects\n[theormet@kalam projects]$ \n[theormet@kalam projects]$ \n[theormet@kalam projects]$ git clone https://github.com/idaholab/moose.git\nCloning into 'moose'...\nremote: Enumerating objects: 659712, done.\nremote: Counting objects: 100% (621/621), done.\nremote: Compressing objects: 100% (247/247), done.\nReceiving objects:  95% (630865/659712), 484.18 MiB | 611.00 KiB/s   \nMessage from syslogd@kalam at Apr 18 18:18:55 ...\n kernel:do_IRQ: 7.231 No irq handler for vector (irq -1)\nremote: Total 659712 (delta 387), reused 505 (delta 372), pack-reused 659091\nReceiving objects: 100% (659712/659712), 498.52 MiB | 658.00 KiB/s, done.\nResolving deltas: 100% (499403/499403), done.\nChecking out files: 100% (36481/36481), done.\n[theormet@kalam projects]$ \n[theormet@kalam projects]$ \n[theormet@kalam projects]$ \n[theormet@kalam projects]$ cd moose\n[theormet@kalam moose]$ git checkout master\nBranch master set up to track remote branch master from origin.\nSwitched to a new branch 'master'\n[theormet@kalam moose]$ \n[theormet@kalam moose]$ \n[theormet@kalam moose]$ cd ~/projects/moose/scripts\n[theormet@kalam scripts]$ \n[theormet@kalam scripts]$ \n[theormet@kalam scripts]$ export MOOSE_JOBS=6 METHODS=opt\n[theormet@kalam scripts]$ \n[theormet@kalam scripts]$ \n\n\n./update_and_rebuild_petsc.sh\n/home/theormet/projects/moose/scripts\nSubmodule 'petsc' (https://gitlab.com/petsc/petsc.git) registered for path 'petsc'\nCloning into 'petsc'...\nremote: Enumerating objects: 1084142, done.\nremote: Counting objects: 100% (441/441), done.\nremote: Compressing objects: 100% (221/221), done.\nremote: Total 1084142 (delta 255), reused 345 (delta 216), pack-reused 1083701\nReceiving objects: 100% (1084142/1084142), 337.35 MiB | 8.12 MiB/s, done.\nResolving deltas: 100% (830497/830497), done.\nSubmodule path 'petsc': checked out '477e44bbb558b1357d86363677accbb4bcdfaabc'\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nThe version of PETSc you are using is out-of-date, we recommend updating to the new release\n Available Version: 3.19   Installed Version: 3.16.6\nhttps://petsc.org/release/download/\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n=============================================================================================\n                      Configuring PETSc to compile on your system                            \n=============================================================================================\n=============================================================================================                  ***** WARNING: CC (set to mpicc) found in environment variables - ignoring                                use ./configure CC=$CC if you really want to use that value ******                                =============================================================================================            =============================================================================================                  ***** WARNING: CXX (set to mpicxx) found in environment variables - ignoring                              use ./configure CXX=$CXX if you really want to use that value ******                              =============================================================================================            =============================================================================================                  ***** WARNING: FC (set to mpif90) found in environment variables - ignoring                               use ./configure FC=$FC if you really want to use that value ******                                =============================================================================================            =============================================================================================                  ***** WARNING: F77 (set to mpif77) found in environment variables - ignoring                              use ./configure F77=$F77 if you really want to use that value ******                              =============================================================================================            =============================================================================================                  ***** WARNING: F90 (set to mpif90) found in environment variables - ignoring                              use ./configure F90=$F90 if you really want to use that value ******                              =============================================================================================            =============================================================================================                  ***** WARNING: Using default optimization C flags -g -O                                                  You might consider manually setting optimal optimization flags for your system with                       COPTFLAGS=\"optimization flags\" see config/examples/arch-*-opt.py for examples                     =============================================================================================            =============================================================================================                  ***** WARNING: Using default Cxx optimization flags -g -O                                                You might consider manually setting optimal optimization flags for your system with                       CXXOPTFLAGS=\"optimization flags\" see config/examples/arch-*-opt.py for examples                   =============================================================================================            =============================================================================================                  ***** WARNING: Using default FORTRAN optimization flags -g -O                                            You might consider manually setting optimal optimization flags for your system with                       FOPTFLAGS=\"optimization flags\" see config/examples/arch-*-opt.py for examples                     ======================================================================================================================================================================================================                  Compiling PTScotch; this may take several minutes                                                  =============================================================================================                                *******************************************************************************\n         UNABLE to CONFIGURE with GIVEN OPTIONS    (see configure.log for details):\n-------------------------------------------------------------------------------\nError running make on PTScotch: Could not execute \"['cd /home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src && make clean ptesmumps esmumps']\":\n/usr/bin/mkdir -p ../bin\n/usr/bin/mkdir -p ../include\n/usr/bin/mkdir -p ../lib\n(cd libscotch ;      make clean)\nmake[1]: Entering directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/libscotch'\nrm -f *~ *.o lib*.a parser_yy.c parser_ly.h parser_ll.c *scotch.h *scotchf.h y.output *dummysizes\nmake[1]: Leaving directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/libscotch'\n(cd scotch ;         make clean)\nmake[1]: Entering directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/scotch'\nrm -f *~ *.o acpl amk_ccc amk_fft2 amk_grf amk_hy amk_m2 amk_p2 atst gbase gcv gdump *ggath *gmap gmk_hy gmk_m2 gmk_m3 gmk_msh gmk_ub2 gmtst *gord gotst gout *gpart *gscat *gtst mcv mmk_m2 mmk_m3 mord mtst\nmake[1]: Leaving directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/scotch'\n(cd libscotchmetis ; make clean)\nmake[1]: Entering directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/libscotchmetis'\nrm -f *~ *.o lib*.a metis.h metisf.h parmetis.h\nmake[1]: Leaving directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/libscotchmetis'\n(cd check ;          make clean)\nmake[1]: Entering directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/check'\nrm -f *~ *.o\nmake[1]: Leaving directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/check'\n(cd esmumps ;        make clean)\nmake[1]: Entering directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/esmumps'\nrm -f *~ common.h *.o lib*.a main_esmumps\nmake[1]: Leaving directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/esmumps'\n(cd libscotch ;      make VERSION=6 RELEASE=1 PATCHLEVEL=1 scotch && make install)\nmake[1]: Entering directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/libscotch'\nmake \t\t\t\t\t\\\n\t\t\t\tCC=\"mpicc\"\t\t\t\t\t\\\n\t\t\t\tCCD=\"mpicc\"\t\t\t\t\t\\\n\t\t\t\tscotch.h\t\t\t\t\t\\\n\t\t\t\tscotchf.h\t\t\t\t\t\\\n\t\t\t\tlibscotch.a\t\t\t\t\t\\\n\t\t\t\tlibscotcherr.a\t\t\t\t\\\n\t\t\t\tlibscotcherrexit.a\nmake[2]: Entering directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/libscotch'\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch.c -o arch.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1 dummysizes.c -o dummysizes -lm -lrt\n./dummysizes \"-s\" library.h scotch.h\n./dummysizes \"-s\" library_f.h scotchf.h\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_build.c -o arch_build.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_build2.c -o arch_build2.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_cmplt.c -o arch_cmplt.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_cmpltw.c -o arch_cmpltw.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_deco.c -o arch_deco.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_deco2.c -o arch_deco2.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_dist.c -o arch_dist.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_hcub.c -o arch_hcub.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_mesh.c -o arch_mesh.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_sub.c -o arch_sub.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_tleaf.c -o arch_tleaf.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_torus.c -o arch_torus.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_vcmplt.c -o arch_vcmplt.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c arch_vhcub.c -o arch_vhcub.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph.c -o bgraph.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_bd.c -o bgraph_bipart_bd.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_df.c -o bgraph_bipart_df.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_ex.c -o bgraph_bipart_ex.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_fm.c -o bgraph_bipart_fm.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_gg.c -o bgraph_bipart_gg.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_gp.c -o bgraph_bipart_gp.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_ml.c -o bgraph_bipart_ml.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_st.c -o bgraph_bipart_st.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_bipart_zr.c -o bgraph_bipart_zr.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_check.c -o bgraph_check.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c bgraph_store.c -o bgraph_store.o -DSCOTCH_VERSION_NUM=6 -DSCOTCH_RELEASE_NUM=1 -DSCOTCH_PATCHLEVEL_NUM=1\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c common.c -DSCOTCH_COMMON_RENAME -o common.o\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c common_file.c -DSCOTCH_COMMON_RENAME -o common_file.o\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c common_file_compress.c -DSCOTCH_COMMON_RENAME -o common_file_compress.o\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c common_file_decompress.c -DSCOTCH_COMMON_RENAME -o common_file_decompress.o\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c common_integer.c -DSCOTCH_COMMON_RENAME -o common_integer.o\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c common_memory.c -DSCOTCH_COMMON_RENAME -o common_memory.o\nmpicc -fPIC -fstack-protector -g -O -std=c99  -DCOMMON_RANDOM_FIXED_SEED -DSCOTCH_RENAME -Drestrict=\"__restrict\" -DINTSIZE64 -DSCOTCH_METIS_PREFIX  -c common_string.c -DSCOTCH_COMMON_RENAME -o common_string.o\nmake[2]: Leaving directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/libscotch'\nmake[1]: Leaving directory `/home/theormet/projects/moose/petsc/arch-moose/externalpackages/git.ptscotch/src/libscotch'In file included from /usr/include/sys/wait.h:30:0,\n                 from common.h:132,\n                 from common_string.c:57:\n/usr/include/signal.h:156:1: error: unknown type name \u2018siginfo_t\u2019\n extern void psiginfo (const siginfo_t *__pinfo, const char *__s);\n ^\nmake[2]: *** [common_string.o] Error 1\nmake[1]: *** [scotch] Error 2\nmake: *** [libscotch] Error 2",
          "url": "https://github.com/idaholab/moose/discussions/24110",
          "updatedAt": "2023-04-18T13:40:57Z",
          "publishedAt": "2023-04-18T13:18:19Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nwhat does gcc --version return?\nPackage gcc-4.8.5-16.el7.x86_64 already installed and latest version makes me think you GCC is too old.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/24110#discussioncomment-5649061",
                  "updatedAt": "2023-04-18T13:22:25Z",
                  "publishedAt": "2023-04-18T13:22:24Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "412120052"
                  },
                  "bodyText": "Thank you..can you help me with how to update the GCC version.. am very new to linux also",
                  "url": "https://github.com/idaholab/moose/discussions/24110#discussioncomment-5649119",
                  "updatedAt": "2023-04-18T13:29:23Z",
                  "publishedAt": "2023-04-18T13:29:22Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "what does gcc --version return?",
                          "url": "https://github.com/idaholab/moose/discussions/24110#discussioncomment-5649197",
                          "updatedAt": "2023-04-18T13:34:38Z",
                          "publishedAt": "2023-04-18T13:34:38Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "412120052"
                  },
                  "bodyText": "gcc --version\ngcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.",
                  "url": "https://github.com/idaholab/moose/discussions/24110#discussioncomment-5649229",
                  "updatedAt": "2023-04-18T13:36:26Z",
                  "publishedAt": "2023-04-18T13:36:25Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Definitely too old.\nhttps://mooseframework.inl.gov/sqa/minimum_requirements.html\nOk so there are a few ways from here:\n\nyou use the conda install. It works reasonably well on clusters:  https://mooseframework.inl.gov/moose/getting_started/installation/conda.html\nsince this is an HPC cluster, you ask your cluster admins to install an MPI distribution for you\n\nSince you are new to linux, I would not recommend installing gcc on your own",
                          "url": "https://github.com/idaholab/moose/discussions/24110#discussioncomment-5649269",
                          "updatedAt": "2023-04-18T13:39:48Z",
                          "publishedAt": "2023-04-18T13:39:47Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "dt suddenly reduces to 1e-13 and program stops running. How to fix this issue?",
          "author": {
            "login": "K-Komal-98"
          },
          "bodyText": "At time step 1552, it was running. Suddenly dt kept on decreasing from say 0.0001 to 1e-13 and program stopped working. I got this message on terminal. What should I do to fix this error?\nTime Step 1553, time = 66.6014, dt = 1e-13\n 0 Nonlinear |R| = ^[[32m2.183326e+10^[[39m\n      0 Linear |R| = ^[[32m2.183326e+10^[[39m\n  Linear solve did not converge due to DIVERGED_PC_FAILED iterations 0\n                 PC failed due to FACTOR_NUMERIC_ZEROPIVOT\nNonlinear solve did not converge due to DIVERGED_LINE_SEARCH iterations 0\n^[[31m Solve Did NOT Converge!^[[39m\nAborting as solve did not converge",
          "url": "https://github.com/idaholab/moose/discussions/24109",
          "updatedAt": "2023-04-29T02:57:10Z",
          "publishedAt": "2023-04-18T10:35:53Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Start from here: https://mooseframework.inl.gov/moose/application_usage/failed_solves.html",
                  "url": "https://github.com/idaholab/moose/discussions/24109#discussioncomment-5648693",
                  "updatedAt": "2023-04-18T12:52:00Z",
                  "publishedAt": "2023-04-18T12:51:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to apply Robin BCs?",
          "author": {
            "login": "K-Komal-98"
          },
          "bodyText": "I am modeling a battery problem. There are 3 blocks : anode, electrolyte and cathode. There are also robin BCs in the problem like d^y_1/dx + y_1^2 = g.   Is their any block corresponding to this kind of BC?",
          "url": "https://github.com/idaholab/moose/discussions/24108",
          "updatedAt": "2023-04-29T02:57:03Z",
          "publishedAt": "2023-04-18T10:02:04Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Use a MatNeumannBC in conjunction with a ParsedMaterial. Below is an example of imposing heat convection BC:\n[BCs]\n  [hconv]\n    type = ADMatNeumannBC\n    variable = T\n    boundary = 'insul_od insul_top_bot'\n    value = -1\n    boundary_material = qconv\n  []\n[]\n\n[Materials]\n  [qconv]\n    type = ADParsedMaterial\n    property_name = qconv\n    expression = 'htc*(T-T_inf)'\n    coupled_variables = 'T'\n    constant_names = 'htc T_inf'\n    constant_expressions = '${htc} ${T_inf}'\n    boundary = 'insul_od insul_top_bot'\n  []\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/24108#discussioncomment-5648628",
                  "updatedAt": "2023-04-18T12:46:42Z",
                  "publishedAt": "2023-04-18T12:46:41Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Why is effective_creep_strain negative?",
          "author": {
            "login": "zong-yanyun"
          },
          "bodyText": "In my tensor_mechanics module, creep_ rate is not a negative number in computeResidual, but effective_ creep_ Strain decreases. Could you tell me if this is reasonable in moose? And why? I personally verified that the absolute value should be correct, perhaps the symbol is incorrect?",
          "url": "https://github.com/idaholab/moose/discussions/24062",
          "updatedAt": "2023-04-29T02:56:58Z",
          "publishedAt": "2023-04-13T13:57:52Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "I am not sure about the answer off the top of my head, but I think it might be possible. Which creep model are you using?",
                  "url": "https://github.com/idaholab/moose/discussions/24062#discussioncomment-5609856",
                  "updatedAt": "2023-04-13T22:31:06Z",
                  "publishedAt": "2023-04-13T22:31:05Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "zong-yanyun"
                          },
                          "bodyText": "The creep model was developed by ourselves. And I verified that my creep rate is not negative.",
                          "url": "https://github.com/idaholab/moose/discussions/24062#discussioncomment-5633104",
                          "updatedAt": "2023-04-17T06:27:41Z",
                          "publishedAt": "2023-04-17T06:27:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "zong-yanyun"
                          },
                          "bodyText": "I have added some detailed descriptions. Could you please take a look again.",
                          "url": "https://github.com/idaholab/moose/discussions/24062#discussioncomment-5637724",
                          "updatedAt": "2023-04-17T14:10:30Z",
                          "publishedAt": "2023-04-17T14:10:29Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "zong-yanyun"
                  },
                  "bodyText": "This is the effective creep strain result based on our own creep model. In our creep model, the effective creep strain rate is computed based on an empirical correlation, which is guaranteed to be positive. In our understanding,\n\nSo if the creep rate is positive, the effective creep strain increment must be positive. But the effective strain decreases in our result ...\nIs our understanding wrong? Or there are some BUGs in MOOSE?",
                  "url": "https://github.com/idaholab/moose/discussions/24062#discussioncomment-5637693",
                  "updatedAt": "2023-04-17T14:08:18Z",
                  "publishedAt": "2023-04-17T14:08:16Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "You need to do proper code verification. You can integrate your in-house creep model using your favorite script, say python or matlab, with a constant stress load. Then compare the integrated effective creep strain with a single element test in moose. That's the usual way of benchmarking proprietary code.",
                          "url": "https://github.com/idaholab/moose/discussions/24062#discussioncomment-5638062",
                          "updatedAt": "2023-04-17T14:38:17Z",
                          "publishedAt": "2023-04-17T14:38:16Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "zong-yanyun"
                  },
                  "bodyText": "I conducted the test according to what you said, and the results were no problem.\n\nI really can't find the reason for the decline of effective creep strain. There are some BUGs in MOOSE?",
                  "url": "https://github.com/idaholab/moose/discussions/24062#discussioncomment-5646862",
                  "updatedAt": "2023-04-18T09:27:36Z",
                  "publishedAt": "2023-04-18T09:27:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "It's very difficult for us to guess what you are doing exactly. At this point, it might be easier to set up the non-proprietary part of your model in a git repository and share with us. With a minimal non-working example, it will be much easier to debug, if there's any.",
                          "url": "https://github.com/idaholab/moose/discussions/24062#discussioncomment-5648581",
                          "updatedAt": "2023-04-18T12:43:01Z",
                          "publishedAt": "2023-04-18T12:42:59Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Slowdown when adding MaterialRankTwoTensorAux auxillary kernel in a peridynamic simulation",
          "author": {
            "login": "andrisfreimanis"
          },
          "bodyText": "Dear All,\nI was running a simple PD simulation and noticed that adding an auxiliary kernel to output stress slows down the simulation from 38s to 72s. As I understand stress variable is already calculated and MaterialRankTwoTensorAux just accesses the calculated material property, and writes it in the output file. Is something else going on (like some extra calculations)?\nThe simulation is a simple elastic simulation. 500 nodes and ran on a single core. I attached both input files. They are the same except  that the slower one has\n  [stress_yy] type = MaterialRankTwoTensorAux property = stress variable = stress_yy i = 1 j = 1 []\nadded to aux kernel block.\nNOSB_elastic.zip",
          "url": "https://github.com/idaholab/moose/discussions/24086",
          "updatedAt": "2023-04-18T07:16:04Z",
          "publishedAt": "2023-04-17T07:36:57Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Well the moose material models are evaluated on-thy-fly, with the very fundamental assumption that the material models are cheap. In other words, every time we compute aux vars, the materials are \"re-evaluated\" for all the elements.\nSo if your material models are expensive, for example in a PD simulation, you will want to avoid excessive aux var evaluations as well. Setting execute_on = TIMESTEP_END in the auxkernel should make the timing look better. The default execute_on for auxkernels is LINEAR TIMESTEP_END.",
                  "url": "https://github.com/idaholab/moose/discussions/24086#discussioncomment-5638103",
                  "updatedAt": "2023-04-17T14:41:58Z",
                  "publishedAt": "2023-04-17T14:41:57Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "andrisfreimanis"
                          },
                          "bodyText": "Yea, that makes sense. I guess it's the extra evaluation that adds the time.\nThanks",
                          "url": "https://github.com/idaholab/moose/discussions/24086#discussioncomment-5645523",
                          "updatedAt": "2023-04-18T07:16:03Z",
                          "publishedAt": "2023-04-18T07:16:03Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "General question on the memory usage, taking mortar thermal contact problem as example",
          "author": {
            "login": "BoZeng1997"
          },
          "bodyText": "Hi,\nI am running a thermal mechanical problem with heat flux between blocks computed by mortar method (no mechanical contact). The whole problem is demanding highly in memory because the mesh size required to coupled with a phase-field subapp is dense. Several questions rise when I am trying to reduce the memory usage.\n\nIs there a general way to estimate required memory before we hit out_of_memory error?\nAny recommendation on reducing memory required for each cpu on the cluster? Is --distributed-mesh going to help me while one rank still has to hold the entire mesh for exodus output?\nHow to interpret PostProcessors/MemoryUsage/total, the last column Mem(MB) of Outputs\\PerfGraphOutput, and the actually memory used on a cluster? From a few simulations I have ran, the first two are way less than the last one. For Outputs\\PerfGraphOutput's last column, I got Mem(replicated mesh) < Mem(distributed mesh), not sure if this makes sense.\nFull nonlinear Newton algorithm with direct linear solver (lu) does not seem to be efficient in resource for a 3D problem with DOF of millions. I will post another discussion for solver option.  the discussion is #24020\n\nBelow is the input deck for our discussion. The model is a simplified version of mine but includes all aspects. The real problem has more blocks and contact pairs, and the majority of the DOF is in the interior.\nlink to the mesh file https://duke.box.com/s/rrg20ri0xi2c4p43k9u6kwoy2i8ll3p4\nstress_free_temperature = 300\nthermal_expansion_coeff = 6.66e-6\n\n[Problem]\n  type = FEProblem\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n  temperature = T_K\n[]\n\n[Mesh]\n  patch_update_strategy = iteration\n  use_displaced_mesh = true\n  patch_size = 40\n  [ori]\n    type = FileMeshGenerator\n    file = 'pellets_mm_h0.2.msh'\n  []\n  [refine]\n    type = RefineBlockGenerator\n    input = ori\n    refinement = 1\n    block = 'pellet_outer pellet_inner '\n  []\n  [lm_pellet_1]\n    input = refine\n    type = LowerDBlockFromSidesetGenerator\n    sidesets = 'void_pellet_1' \n    new_block_id = '10001'\n    new_block_name = 'pellet_secondary_subdomain'\n  []\n  [lm_pellet_0]\n    input = lm_pellet_1\n    type = LowerDBlockFromSidesetGenerator\n    sidesets = 'void_pellet_0' \n    new_block_id = '10000'\n    new_block_name = 'pellet_primary_subdomain'\n  []\n[]\n\n[Variables]\n  [disp_x]\n    block = 'pellet_inner pellet_outer '\n  []\n  [disp_y]\n    block = 'pellet_inner pellet_outer '\n  []\n  [disp_z]\n    block = 'pellet_inner pellet_outer '\n  []\n  [T_K]\n    [InitialCondition]\n      type = ConstantIC\n      value = 300.0\n    []\n  []\n  [lm_pellet]\n    block = 'pellet_secondary_subdomain'\n    use_dual = true\n  []\n[]\n\n[Kernels]\n  [solid_x]\n    type = ADStressDivergenceTensors\n    variable = disp_x\n    component = 0\n    block = 'pellet_inner pellet_outer'\n    use_displaced_mesh = false\n  []\n  [solid_y]\n    type = ADStressDivergenceTensors\n    variable = disp_y\n    component = 1\n    block = 'pellet_inner pellet_outer '\n    use_displaced_mesh = false\n  []\n  [solid_z]\n    type = ADStressDivergenceTensors\n    variable = disp_z\n    component = 2\n    block = 'pellet_inner pellet_outer '\n    use_displaced_mesh = false\n  []\n  [timeder]\n    type = ADHeatConductionTimeDerivative\n    variable = 'T_K'\n    density_name = density\n    specific_heat = specific_heat\n    block = 'pellet_inner pellet_outer '\n    use_displaced_mesh = true\n  []\n  [diff]\n    type = ADHeatConduction\n    variable = 'T_K'\n    thermal_conductivity = thermal_conductivity\n    block = 'pellet_inner pellet_outer '\n    use_displaced_mesh = true\n  []\n  [heatsource]\n    type = ADMatHeatSource\n    variable = 'T_K'\n    material_property = radial_source\n    block = 'pellet_inner pellet_outer '\n    use_displaced_mesh = true\n  []\n[]\n\n\n[Debug]\n  show_var_residual_norms = TRUE\n[]\n\n\n[BCs]\n  [mirror_z]\n    type = ADDirichletBC\n    variable = disp_z\n    boundary = 'mirror_innerp mirror_outerp  '\n    value = 0\n  []\n  [mirror_x]\n    type = ADDirichletBC\n    variable = disp_x\n    boundary = 'mirror_innerp mirror_outerp  '\n    value = 0\n  []\n  [mirror_y]\n    type = ADDirichletBC\n    variable = disp_y\n    boundary = 'mirror_innerp mirror_outerp  '\n    value = 0\n  []\n[]\n\n\n[Materials]\n  [pellet_properties]\n    type = ADGenericConstantMaterial\n    prop_names = 'density  thermal_conductivity specific_heat'\n    prop_values = '3.3112e-3  34e-3 1.2217e3' \n    block = 'pellet_inner pellet_outer'\n  []\n  [pulse_shape_linear]\n    type = ADGenericFunctionMaterial\n    prop_values = '5e-2*max(11455*(t)/7,1e-9)'\n    prop_names = 'radial_source'\n    output_properties = 'radial_source'\n    block = 'pellet_inner pellet_outer'\n    use_displaced_mesh = false\n  []\n  [strain]\n    type = ADComputeSmallStrain\n    displacements = 'disp_x disp_y disp_z'\n    eigenstrain_names = eigenstrain #nameS!\n    block = 'pellet_inner pellet_outer '\n  []\n  [thermal_strain]\n    type = ADComputeThermalExpansionEigenstrain\n    stress_free_temperature = ${stress_free_temperature} \n    thermal_expansion_coeff = ${thermal_expansion_coeff} \n    eigenstrain_name = eigenstrain\n    block = 'pellet_inner pellet_outer '\n  []\n  [elasticity]\n    type = ADComputeIsotropicElasticityTensor\n    youngs_modulus = 3.306e5\n    poissons_ratio = 0.329\n  []\n  [stress]\n    type = ADComputeLinearElasticStress\n    block = 'pellet_inner pellet_outer '\n  []\n[]\n\n[UserObjects]\n  [conduction]\n    type = GapFluxModelConduction\n    temperature = T_K\n    boundary = 'void_pellet_0 void_pellet_1    '\n    gap_conductivity = 0.4\n    use_displaced_mesh = true\n  []\n  [rad_pellet]\n    type = GapFluxModelRadiation\n    temperature = T_K\n    boundary = void_pellet_0\n    primary_emissivity = 0.37\n    secondary_emissivity = 0.37\n    use_displaced_mesh = true\n  []\n[]\n\n[Constraints]\n  [gap_pellet]\n    type = ModularGapConductanceConstraint\n    variable = lm_pellet\n    secondary_variable = T_K\n    primary_boundary = 'void_pellet_0'\n    primary_subdomain = pellet_primary_subdomain\n    secondary_boundary = 'void_pellet_1'\n    secondary_subdomain = pellet_secondary_subdomain\n    gap_flux_models = 'conduction rad_pellet' #closed_pellet  \n    gap_geometry_type = 'CYLINDER'\n    cylinder_axis_point_1 = '0 0 0'\n    cylinder_axis_point_2 = '0 0 1'\n    use_displaced_mesh = true\n    quadrature = SECOND\n  []\n[]\n\n[Postprocessors]\n  [Tmax_outerp]\n    type = NodalExtremeValue\n    variable = T_K\n    value_type = max\n    block = pellet_outer\n  []\n  [memtotal]\n    type = MemoryUsage\n    mem_type = physical_memory\n    mem_units = megabytes\n    value_type = total\n    execute_on = 'INITIAL TIMESTEP_END'\n  []\n  [memmax]\n    type = MemoryUsage\n    mem_type = physical_memory\n    mem_units = megabytes\n    value_type = max_process\n    execute_on = 'INITIAL TIMESTEP_END'\n  []\n  [memavg]\n    type = MemoryUsage\n    mem_type = physical_memory\n    mem_units = megabytes\n    value_type = average\n    execute_on = 'INITIAL TIMESTEP_END'\n  []\n  [total_time]\n    type = PerfGraphData\n    execute_on = 'INITIAL TIMESTEP_END'\n    data_type = 'TOTAL'\n    section_name = 'Root'\n  []\n  [run_time]\n    type = ChangeOverTimePostprocessor\n    postprocessor = total_time\n    execute_on = 'INITIAL TIMESTEP_END'\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = NEWTON\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package -pc_factor_shift_type -pc_factor_shift_amount '\n  petsc_options_value = 'lu   superlu_dist NONZERO 1e-14' #hypre boomeramg lu   superlu_dist\n  automatic_scaling = true\n  line_search = none\n  ignore_variables_for_autoscaling = 'lm_pellet ' \n  compute_scaling_once = true \n  scaling_group_variables = 'disp_x disp_y disp_z; T_K' \n  nl_rel_tol = 1e-50\n  nl_abs_tol = 1e-8\n  nl_max_its = 20\n  dtmin = 1\n  dt = 1\n  start_time = 0\n  end_time =7\n[]\n\n[Outputs]\n  [exodus]\n    type = Exodus\n    show = 'T_K disp_x disp_y disp_z'\n  []\n  [csv]\n    type = CSV\n  []\n  [pgraph]\n    type = PerfGraphOutput\n    execute_on = 'initial TIMESTEP_END' \n  []\n  print_linear_residuals = false\n  file_base = test\n[]\n\nhere is my framework information\nFramework Information:\nMOOSE Version:           git commit b35f3b36c9 on 2023-03-23\nLibMesh Version:         \nPETSc Version:           3.16.6\nSLEPc Version:           3.16.2",
          "url": "https://github.com/idaholab/moose/discussions/24019",
          "updatedAt": "2023-04-18T04:14:03Z",
          "publishedAt": "2023-04-10T18:55:51Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Is there a general way to estimate required memory before we hit out_of_memory error?\n\nWatch the perf graph output on the right side, or use a MemoryUsage VPP if you reach the solve part.\n\nAny recommendation on reducing memory required for each cpu on the cluster? Is --distributed-mesh going to help me while one rank still has to hold the entire mesh for exodus output?\n\nUse nemesis output if exodus is limiting. It will avoid the serializing.\nReduce the number of auxiliary variables to a minimum\nDo not use LU. Use an iterative solver\n\nHow to interpret PostProcessors/MemoryUsage/total, the last column Mem(MB) of Outputs\\PerfGraphOutput, and the actually memory used on a cluster? From a few simulations I have ran, the first two are way less than the last one. For Outputs\\PerfGraphOutput's last column, I got Mem(replicated mesh) < Mem(distributed mesh), not sure if this makes sense.\n\nIf the mesh is not a big part of the consumption, Mem(replicated mesh) and Mem(distributed mesh) being very close is normal.\nDistributed bigger is not expected, we could have a look\n\nFull nonlinear Newton algorithm with direct linear solver (lu) does not seem to be efficient in resource for a 3D problem with DOF of millions. I will post another discussion for solver option.\n\nYes. Try to use something iterative. Maybe a field split?",
                  "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5573469",
                  "updatedAt": "2023-04-10T19:18:43Z",
                  "publishedAt": "2023-04-10T19:18:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "I got this warning when I am using --distributed-mesh but also exodus output. Does it mean this performance graph is only meaningful when rank 0 has the entire mesh? So that when --distributed-mesh with Nemesis output, perf graph is not useless?\nPerformance Graph:\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|                                Section                               | Calls |   Self(s)  |   Avg(s)   |    %   | Mem(MB) |  Total(s)  |   Avg(s)   |    %   | Mem(MB) |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| raccoonTestApp (main)                                                |     1 |      0.011 |      0.011 |   0.10 |       3 |     10.845 |     10.845 | 100.00 |     140 |\n|   Action::SetupMeshAction::Mesh::SetupMeshAction::act::setup_mesh    |     1 |      0.001 |      0.001 |   0.01 |       0 |      0.001 |      0.001 |   0.01 |       0 |\n|   Action::SetupMeshAction::Mesh::SetupMeshAction::act::set_mesh_base |     2 |      0.055 |      0.028 |   0.51 |       6 |      0.062 |      0.031 |   0.57 |       7 |\n|   MeshGeneratorSystem::createMeshGeneratorOrder                      |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|   FEProblem::computeUserObjects                                      |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|   FEProblem::computeUserObjects                                      |     3 |      0.001 |      0.000 |   0.01 |       0 |      0.001 |      0.000 |   0.01 |       0 |\n|   FEProblem::executeControls                                         |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|   FEProblem::outputStep                                              |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.001 |      0.001 |   0.01 |       0 |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nWarning:  This MeshOutput subclass only supports meshes which have been serialized!\nWarning:  This MeshOutput subclass only supports meshes which have been serialized!",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5574434",
                          "updatedAt": "2023-04-10T22:11:08Z",
                          "publishedAt": "2023-04-10T22:11:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "You are getting that warning because you have Exodus output in a distributed mesh simulation. Exodus does a serialization on process 0, and that is what you are being warned about",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5574664",
                          "updatedAt": "2023-04-10T23:12:40Z",
                          "publishedAt": "2023-04-10T23:12:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "got it. It is for exodus output, not for per graph. Thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5574850",
                          "updatedAt": "2023-04-10T23:58:23Z",
                          "publishedAt": "2023-04-10T23:58:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "BoZeng1997"
                  },
                  "bodyText": "Looks like --distributed-mesh is not working well on my workstation. When I run mpiexec -n 4 app-opt -i input.i --distributed-mesh I got the following error message\nWe caught a libMesh::LogicError: No index 24645 in ghosted vector.\nVector contains [0,22707)\nAnd ghost array {42108,42107, ...(a very long list of numbers)\n\nI got the same error when trying to visualize VectorMemoryUsage by VectorPostprocessorVisualizationAux\nShould --distributed-mesh work fine on ubuntu workstation? It runs on my cluster (but do have the issue that Mem(replicated mesh) < Mem(distributed mesh)",
                  "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5574422",
                  "updatedAt": "2023-04-10T22:06:55Z",
                  "publishedAt": "2023-04-10T22:06:54Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "yes distributed mesh should run on all architectures. Do you want to share your input file?",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5574669",
                          "updatedAt": "2023-04-10T23:13:42Z",
                          "publishedAt": "2023-04-10T23:13:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "the input deck and the mesh file I shared in my first comment is enough to get me the error with --distribute-mesh on my workstation in mamba environment.",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5574833",
                          "updatedAt": "2023-04-10T23:54:33Z",
                          "publishedAt": "2023-04-10T23:53:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Looks like a mortar ghosting issue with distributed meshes. Thanks for reporting it, we ll try to take a look",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5574872",
                          "updatedAt": "2023-04-11T00:03:30Z",
                          "publishedAt": "2023-04-11T00:03:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@BoZeng1997 I'm investigating this more, but can you try running with your RefineBlockGenerator going last? We likely need to add some new handling in the lower-D mesh generators to handle non-level-0 elements",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5586506",
                          "updatedAt": "2023-04-11T23:43:27Z",
                          "publishedAt": "2023-04-11T23:43:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "No issue when RefineBlockGenerator going last.\nI see why the simulation on the cluster runs fine. There the input file for the original problem is reading a file mesh with LowerD mesh, not generating them. I forgot the difference when running this simplified problem. It is not an issue due to different architectures.\nBtw, Mem(replicated mesh) < Mem(distributed mesh) only occurs in the first few steps. After that, memory usage falls back to Mem(replicated mesh) > Mem(distributed mesh)",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5586649",
                          "updatedAt": "2023-04-11T23:59:19Z",
                          "publishedAt": "2023-04-11T23:59:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "This does not help with the question of estimating memory usage ahead of time, but if you want to understand what might be consuming a lot of memory, you can look into the MOOSE profiling pages",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5586849",
                          "updatedAt": "2023-04-12T00:32:40Z",
                          "publishedAt": "2023-04-12T00:32:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "BoZeng1997"
                  },
                  "bodyText": "I did the following modification and it greatly reduced the memory usage\n\nreduce auxiliary variables to minimum.\nuse --distributed-mesh and nemesis output\nuse this solver option combination (this is the best I can get when no split preconditioner is using. The discussion about FSP failure is at #24020 and no one has commented on it yet. I would appreciate if someone attend to that discussion)\n\nsolve_type = PJFNK\npetsc_options_iname = '-pc_type -ksp_type -ksp_grmres_restart -sub_ksp_type -sub_pc_type -pc_asm_overlap -sub_pc_factor_shift_type -sub_pc_factor_shift_amount ' \npetsc_options_value = 'asm      gmres     200                preonly       lu           2  NONZERO 1e-14  '\n\nHowever, with use_displaced_mesh=true, patch_update_strategy=iteration, and patch_update_strategy=iteration, a lot of time is spent on\n 2 Nonlinear |R| = 3.081837e-01\n    |residual|_2 of individual variables:\n                  disp_x:    6.52829e-05\n                  disp_y:    8.11888e-05\n                  disp_z:    0.000121236\n                  T_K:       0.308184\n                  lm_pellet: 3.8352e-06\n                  lm_nb:     1.68717e-06\n                  lm_ss:     2.2316e-08\n    Computing Jacobian....                                                               [ 29.87 s] [ 1959 MB]\n    Computing Residual\n      Updating Mortar Mesh                                                               [ 10.05 s] [ 1959 MB]\n    Still Computing Residual.                                                            [ 20.83 s] [ 1959 MB]\n    Computing Residual\n      Updating Mortar Mesh                                                               [ 10.54 s] [ 1959 MB]\n    Still Computing Residual.                                                            [ 21.25 s] [ 1959 MB]\n(a very very long list of computing residual and updating mesh)\n\nincreasing number of processors did not enhance the speed much.\n\nis distributed mesh that does not allow fast geometry access the major cause? I read it on one of MOOSE's page,\n\n\nBoth the \"replicated\" and \"distributed\" mesh formats are parallel with respect to the execution of the finite element assembly and solve. In both types the solution data is distributed, which is the portion of the simulation that usually dominates memory demands.\n\nbut I am not sure if that is also the case when updating mesh at each iteration\n\nis it because my choice of solver options sacrificed too much accuracy for low memory usage and eventually led to too many iterations?\nis no-split preconditioner petsc settings by design an inefficient solver option for a large problem with mortar method by PJFNK algorithm? (I mean, with split, the small part for contact will be solved by direct solver and the large buck interior by iterative solver, which seems to be more efficient. please, again, no one has attended to #24020 yet)\n\nMy understanding of the framework when parallel computing communication, updating mesh at each iteration, iterative solver combined is very limited. My guesses of the cause of low efficiency could be totally wrong. The issue switched from extremely memory consuming to extremely slow. My priority is to find a middle ground that runs the simulation well. But I would also appreciate any guidance on understanding the framework.",
                  "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5607092",
                  "updatedAt": "2023-04-13T17:05:52Z",
                  "publishedAt": "2023-04-13T17:05:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "Here is a typical performance graph for my original problem with the solver options I mentioned above\n+----------------+----------------+----------------+\n| time           | run_time       | total_time     |\n+----------------+----------------+----------------+\n|   4.800000e+01 |   0.000000e+00 |   2.079186e+02 |\n|   4.850000e+01 |   7.446035e+03 |   7.653953e+03 |\n|   4.900000e+01 |   3.282288e+03 |   1.093624e+04 |\n|   4.950000e+01 |   3.323142e+03 |   1.425938e+04 |\n+----------------+----------------+----------------+\n\n\n\nPerformance Graph:\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|                                   Section                                  | Calls |   Self(s)  |   Avg(s)   |    %   | Mem(MB) |  Total(s)  |   Avg(s)   |    %   | Mem(MB) |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| raccoonTestApp (main)                                                      |     1 |      0.020 |      0.020 |   0.00 |       2 |  14296.747 |  14296.747 | 100.00 |    1542 |\n|   RankMap::construct                                                       |     1 |      0.061 |      0.061 |   0.00 |       0 |      0.061 |      0.061 |   0.00 |       0 |\n|   MooseApp::run                                                            |     1 |      0.000 |      0.000 |   0.00 |       0 |  14296.666 |  14296.666 | 100.00 |    1540 |\n|     MooseApp::setup                                                        |     1 |      0.000 |      0.000 |   0.00 |       0 |    201.142 |    201.142 |   1.41 |    1390 |\n|       MooseApp::runInputFile                                               |     1 |      0.069 |      0.069 |   0.00 |       0 |    200.936 |    200.936 |   1.41 |    1382 |\n|         MultiApp::fracture::init                                           |     1 |      0.008 |      0.008 |   0.00 |       0 |      0.008 |      0.008 |   0.00 |       0 |\n|         MultiApp::fracture::createApps                                     |     1 |     16.688 |     16.688 |   0.12 |      88 |     16.688 |     16.688 |   0.12 |      88 |\n|         Action::SetupMeshAction::Mesh::SetupMeshAction::act::setup_mesh    |     1 |      0.001 |      0.001 |   0.00 |       0 |      0.001 |      0.001 |   0.00 |       0 |\n|         Action::SetupMeshAction::Mesh::SetupMeshAction::act::set_mesh_base |     2 |      6.407 |      3.204 |   0.04 |     205 |      6.443 |      3.221 |   0.05 |     205 |\n|           MeshGeneratorMesh::init                                          |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           MeshGeneratorMesh::GhostGhostedBoundaries                        |     2 |      0.035 |      0.018 |   0.00 |       0 |      0.035 |      0.018 |   0.00 |       0 |\n|         Action::SetupMeshCompleteAction::Mesh::completeSetupUndisplaced    |     2 |      0.000 |      0.000 |   0.00 |       0 |      1.870 |      0.935 |   0.01 |     112 |\n|           MeshGeneratorMesh::prepare                                       |     2 |      0.158 |      0.079 |   0.00 |       0 |      1.870 |      0.935 |   0.01 |     112 |\n|             MeshGeneratorMesh::update                                      |     1 |      0.148 |      0.148 |   0.00 |       0 |      1.508 |      1.508 |   0.01 |     112 |\n|               MeshGeneratorMesh::cacheInfo                                 |     1 |      1.212 |      1.212 |   0.01 |     112 |      1.212 |      1.212 |   0.01 |     112 |\n|         Action::SetupMeshCompleteAction::Mesh::completeSetupDisplaced      |     2 |      0.000 |      0.000 |   0.00 |       0 |      1.754 |      0.877 |   0.01 |     121 |\n|           MeshGeneratorMesh::prepare                                       |     2 |      0.138 |      0.069 |   0.00 |       2 |      1.754 |      0.877 |   0.01 |     121 |\n|             MeshGeneratorMesh::update                                      |     1 |      0.134 |      0.134 |   0.00 |       0 |      1.419 |      1.419 |   0.01 |     119 |\n|               MeshGeneratorMesh::cacheInfo                                 |     1 |      1.132 |      1.132 |   0.01 |     112 |      1.132 |      1.132 |   0.01 |     112 |\n|         Action::SetupMeshCompleteAction::Mesh::uniformRefine               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         Action::SetupMeshCompleteAction::Mesh::deleteRemoteElems           |     1 |     12.018 |     12.018 |   0.08 |       6 |     23.925 |     23.925 |   0.17 |      77 |\n|         MeshGeneratorSystem::createMeshGeneratorOrder                      |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         FEProblem::init                                                    |     1 |      0.234 |      0.234 |   0.00 |       0 |     15.433 |     15.433 |   0.11 |      15 |\n|           FEProblem::fillCouplingMatrix                                    |     1 |      0.001 |      0.001 |   0.00 |       0 |      0.001 |      0.001 |   0.00 |       0 |\n|           FEProblem::ghostGhostedBoundaries                                |     1 |      0.005 |      0.005 |   0.00 |       0 |      0.028 |      0.028 |   0.00 |       0 |\n|             MeshGeneratorMesh::GhostGhostedBoundaries                      |     2 |      0.023 |      0.012 |   0.00 |       0 |      0.023 |      0.012 |   0.00 |       0 |\n|           MeshGeneratorMesh::meshChanged                                   |     3 |      0.004 |      0.001 |   0.00 |       0 |      3.549 |      1.183 |   0.02 |       0 |\n|             MeshGeneratorMesh::update                                      |     3 |      1.981 |      0.660 |   0.01 |       0 |      3.433 |      1.144 |   0.02 |       0 |\n|               MeshGeneratorMesh::cacheInfo                                 |     3 |      0.872 |      0.291 |   0.01 |       0 |      0.872 |      0.291 |   0.01 |       0 |\n|           FEProblem::EquationSystems::Init                                 |     1 |      6.872 |      6.872 |   0.05 |       0 |      6.970 |      6.970 |   0.05 |       0 |\n|             FEProblem::updateGeometricSearch                               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               DisplacedProblem::updateGeometricSearch                      |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           DisplacedProblem::eq::init                                       |     1 |      4.651 |      4.651 |   0.03 |      15 |      4.651 |      4.651 |   0.03 |      15 |\n|     MooseApp::execute                                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |  14095.524 |  14095.524 |  98.59 |     150 |\n|       MooseApp::executeExecutioner                                         |     1 |      0.102 |      0.102 |   0.00 |       0 |  14095.524 |  14095.524 |  98.59 |     150 |\n|         FEProblem::computeUserObjects                                      |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         AuxiliarySystem::computeNodalVecVars                               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         AuxiliarySystem::computeNodalVars                                  |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         AuxiliarySystem::computeMortarNodalVars                            |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         AuxiliarySystem::computeElementalVecVars                           |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         AuxiliarySystem::computeElementalVars                              |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         FEProblem::initialSetup                                            |     1 |      0.120 |      0.120 |   0.00 |      15 |      6.705 |      6.705 |   0.05 |      74 |\n|           FEProblem::updateGeometricSearch                                 |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|             DisplacedProblem::updateGeometricSearch                        |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           FEProblem::computeUserObjects                                    |     3 |      0.002 |      0.001 |   0.00 |       0 |      0.002 |      0.001 |   0.00 |       0 |\n|           AuxiliarySystem::computeNodalVecVars                             |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::computeNodalVars                                |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::computeMortarNodalVars                          |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::computeElementalVecVars                         |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::computeElementalVars                            |     1 |      1.354 |      1.354 |   0.01 |       1 |      1.668 |      1.668 |   0.01 |       1 |\n|           FEProblem::computingMaxDofs                                      |     1 |      0.009 |      0.009 |   0.00 |       0 |      0.009 |      0.009 |   0.00 |       0 |\n|           FEProblem::reinitScalars                                         |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           FEProblem::reinitScalars                                         |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           FEProblem::projectSolution                                       |     1 |      0.868 |      0.868 |   0.01 |       1 |      0.868 |      0.868 |   0.01 |       1 |\n|           FEProblem::materialInitialSetup                                  |     1 |      0.001 |      0.001 |   0.00 |       0 |      0.570 |      0.570 |   0.00 |       0 |\n|             FEProblem::computingInitialStatefulProps                       |     1 |      0.569 |      0.569 |   0.00 |       0 |      0.569 |      0.569 |   0.00 |       0 |\n|           FEProblem::copySolutionsBackwards                                |     1 |      0.411 |      0.411 |   0.00 |      13 |      0.411 |      0.411 |   0.00 |      13 |\n|           NonlinearSystemBase::nlInitialSetup                              |     1 |      0.080 |      0.080 |   0.00 |      11 |      0.080 |      0.080 |   0.00 |      11 |\n|             NonlinearSystemBase::kernelsInitialSetup                       |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|             NonlinearSystemBase::mortarSetup                               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::initialSetup                                    |     1 |      0.038 |      0.038 |   0.00 |       3 |      0.038 |      0.038 |   0.00 |       3 |\n|           DisplacedProblem::updateMesh                                     |     2 |      0.764 |      0.382 |   0.01 |      16 |      0.979 |      0.489 |   0.01 |      27 |\n|           FEProblem::reinitBecauseOfGhostingOrNewGeomObjects               |     1 |      0.005 |      0.005 |   0.00 |       0 |      0.005 |      0.005 |   0.00 |       0 |\n|           FEProblem::initialSetupMultiApps                                 |     1 |      1.307 |      1.307 |   0.01 |       1 |      1.307 |      1.307 |   0.01 |       1 |\n|           FEProblem::initialSetupTransfers                                 |     1 |      0.094 |      0.094 |   0.00 |       0 |      0.094 |      0.094 |   0.00 |       0 |\n|           FEProblem::executeControls                                       |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         FEProblem::outputStep                                              |     4 |      0.244 |      0.061 |   0.00 |       0 |     24.669 |      6.167 |   0.17 |       6 |\n|           Console::outputStep                                              |    14 |     24.356 |      1.740 |   0.17 |       6 |     24.356 |      1.740 |   0.17 |       6 |\n|         FEProblem::onTimestepBegin                                         |     3 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         Transient::PicardSolve                                             |     3 |      0.306 |      0.102 |   0.00 |       3 |  14063.972 |   4687.991 |  98.37 |      70 |\n|           FEProblem::computeUserObjects                                    |    18 |      0.055 |      0.003 |   0.00 |       0 |      0.055 |      0.003 |   0.00 |       0 |\n|           AuxiliarySystem::computeNodalVecVars                             |     9 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::computeNodalVars                                |     9 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::computeMortarNodalVars                          |     9 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::computeElementalVecVars                         |     9 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           AuxiliarySystem::computeElementalVars                            |     9 |      7.962 |      0.885 |   0.06 |       0 |     10.621 |      1.180 |   0.07 |       0 |\n|           FEProblem::outputStep                                            |     6 |      0.163 |      0.027 |   0.00 |       0 |      0.258 |      0.043 |   0.00 |       0 |\n|             Console::outputStep                                            |     3 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           FEProblem::executeControls                                       |     3 |      0.001 |      0.000 |   0.00 |       0 |      0.001 |      0.000 |   0.00 |       0 |\n|           MultiApp::fracture::backup                                       |     3 |      0.060 |      0.020 |   0.00 |       0 |      0.060 |      0.020 |   0.00 |       0 |\n|           FEProblem::computeResidualL2Norm                                 |     6 |      0.027 |      0.004 |   0.00 |       0 |    185.041 |     30.840 |   1.29 |      57 |\n|             FEProblem::computeResidualInternal                             |     6 |      0.000 |      0.000 |   0.00 |       0 |    185.014 |     30.836 |   1.29 |      57 |\n|               FEProblem::computeUserObjects                                |    12 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeNodalVecVars                         |    12 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeNodalVars                            |    12 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeMortarNodalVars                      |    12 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeElementalVecVars                     |    12 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeElementalVars                        |    12 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               FEProblem::reinitScalars                                     |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               DisplacedProblem::updateMesh                                 |     6 |      2.570 |      0.428 |   0.02 |      36 |      2.760 |      0.460 |   0.02 |      37 |\n|               NonlinearSystemBase::computeResidualInternal                 |     6 |      7.050 |      1.175 |   0.05 |       5 |    118.823 |     19.804 |   0.83 |       5 |\n|                 FEProblem::reinitScalars                                   |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 NonlinearSystemBase::residualSetup                         |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 NonlinearSystemBase::Kernels                               |     6 |     19.678 |      3.280 |   0.14 |       0 |     19.678 |      3.280 |   0.14 |       0 |\n|               NonlinearSystemBase::NodalBCs                                |     6 |      0.066 |      0.011 |   0.00 |       1 |      0.066 |      0.011 |   0.00 |       1 |\n|           FEProblem::solve                                                 |     3 |   1358.247 |    452.749 |   9.50 |     -52 |  13851.414 |   4617.138 |  96.89 |      -4 |\n|             Console::outputStep                                            |    26 |      2.969 |      0.114 |   0.02 |       2 |      2.969 |      0.114 |   0.02 |       2 |\n|             FEProblem::reinitBecauseOfGhostingOrNewGeomObjects             |     3 |      0.017 |      0.006 |   0.00 |       0 |      0.017 |      0.006 |   0.00 |       0 |\n|             NonlinearSystemBase::nlInitialResidual                         |     3 |      0.015 |      0.005 |   0.00 |       0 |     53.042 |     17.681 |   0.37 |      21 |\n|               FEProblem::computeResidualInternal                           |     3 |      0.000 |      0.000 |   0.00 |       0 |     53.027 |     17.676 |   0.37 |      21 |\n|                 FEProblem::computeUserObjects                              |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeNodalVecVars                       |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeNodalVars                          |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeMortarNodalVars                    |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeElementalVecVars                   |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeElementalVars                      |     6 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 FEProblem::reinitScalars                                   |     3 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 DisplacedProblem::updateMesh                               |     3 |      1.285 |      0.428 |   0.01 |      16 |      1.397 |      0.466 |   0.01 |      16 |\n|                 NonlinearSystemBase::computeResidualInternal               |     3 |      1.563 |      0.521 |   0.01 |       1 |     21.146 |      7.049 |   0.15 |       1 |\n|                   FEProblem::reinitScalars                                 |     3 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                   NonlinearSystemBase::residualSetup                       |     3 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                   NonlinearSystemBase::Kernels                             |     3 |      1.299 |      0.433 |   0.01 |       0 |      1.299 |      0.433 |   0.01 |       0 |\n|                 NonlinearSystemBase::NodalBCs                              |     3 |      0.030 |      0.010 |   0.00 |       0 |      0.030 |      0.010 |   0.00 |       0 |\n|             FEProblem::computeResidualInternal                             |   653 |      0.030 |      0.000 |   0.00 |       0 |  12200.315 |     18.683 |  85.34 |      -2 |\n|               FEProblem::computeUserObjects                                |  1306 |      0.024 |      0.000 |   0.00 |       0 |      0.024 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeNodalVecVars                         |  1306 |      0.003 |      0.000 |   0.00 |       1 |      0.003 |      0.000 |   0.00 |       1 |\n|               AuxiliarySystem::computeNodalVars                            |  1306 |      0.002 |      0.000 |   0.00 |       0 |      0.002 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeMortarNodalVars                      |  1306 |      0.002 |      0.000 |   0.00 |       0 |      0.002 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeElementalVecVars                     |  1306 |      0.002 |      0.000 |   0.00 |       0 |      0.002 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeElementalVars                        |  1306 |      0.002 |      0.000 |   0.00 |       0 |      0.002 |      0.000 |   0.00 |       0 |\n|               FEProblem::reinitScalars                                     |   653 |      0.001 |      0.000 |   0.00 |       0 |      0.001 |      0.000 |   0.00 |       0 |\n|               DisplacedProblem::updateMesh                                 |   653 |    280.244 |      0.429 |   1.96 |      11 |    303.970 |      0.465 |   2.13 |      14 |\n|               NonlinearSystemBase::computeResidualInternal                 |   653 |    374.441 |      0.573 |   2.62 |       5 |   5031.115 |      7.705 |  35.19 |     -36 |\n|                 FEProblem::reinitScalars                                   |   653 |      0.002 |      0.000 |   0.00 |       0 |      0.002 |      0.000 |   0.00 |       0 |\n|                 NonlinearSystemBase::residualSetup                         |   653 |      0.016 |      0.000 |   0.00 |       1 |      0.016 |      0.000 |   0.00 |       1 |\n|                 NonlinearSystemBase::Kernels                               |   653 |    286.712 |      0.439 |   2.01 |       4 |    286.712 |      0.439 |   2.01 |       4 |\n|               NonlinearSystemBase::NodalBCs                                |   653 |      5.169 |      0.008 |   0.04 |       2 |      5.169 |      0.008 |   0.04 |       2 |\n|             NonlinearSystemBase::initialBCs                                |     3 |      0.012 |      0.004 |   0.00 |       0 |      0.012 |      0.004 |   0.00 |       0 |\n|             NonlinearSystemBase::computeScaling                            |     1 |      0.420 |      0.420 |   0.00 |       4 |     15.989 |     15.989 |   0.11 |      23 |\n|               FEProblem::computeJacobianInternal                           |     1 |      0.000 |      0.000 |   0.00 |       0 |     15.569 |     15.569 |   0.11 |      19 |\n|                 FEProblem::computeUserObjects                              |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeNodalVecVars                       |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeNodalVars                          |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeMortarNodalVars                    |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeElementalVecVars                   |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 AuxiliarySystem::computeElementalVars                      |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 FEProblem::reinitScalars                                   |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|                 DisplacedProblem::updateMesh                               |     1 |      0.414 |      0.414 |   0.00 |      16 |      0.441 |      0.441 |   0.00 |      17 |\n|                 NonlinearSystemBase::computeJacobianInternal               |     1 |      3.611 |      3.611 |   0.03 |       1 |     14.994 |     14.994 |   0.10 |       1 |\n|                   FEProblem::reinitScalars                                 |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|             FEProblem::computeJacobianInternal                             |    10 |      0.000 |      0.000 |   0.00 |       0 |    220.727 |     22.073 |   1.54 |       3 |\n|               FEProblem::computeUserObjects                                |    20 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeNodalVecVars                         |    20 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeNodalVars                            |    20 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeMortarNodalVars                      |    20 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeElementalVecVars                     |    20 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               AuxiliarySystem::computeElementalVars                        |    20 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               FEProblem::reinitScalars                                     |    10 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|               DisplacedProblem::updateMesh                                 |    10 |      4.229 |      0.423 |   0.03 |       0 |      4.573 |      0.457 |   0.03 |       1 |\n|               NonlinearSystemBase::computeJacobianInternal                 |    10 |     49.736 |      4.974 |   0.35 |       1 |    215.387 |     21.539 |   1.51 |       1 |\n|                 FEProblem::reinitScalars                                   |    20 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|           FEProblem::execMultiAppTransfers                                 |     6 |     11.508 |      1.918 |   0.08 |       6 |     11.508 |      1.918 |   0.08 |       6 |\n|           FEProblem::execMultiApps                                         |     3 |      0.002 |      0.001 |   0.00 |       0 |      4.698 |      1.566 |   0.03 |       7 |\n|             MultiApp::fracture::solveStep                                  |     3 |      4.695 |      1.565 |   0.03 |       7 |      4.695 |      1.565 |   0.03 |       7 |\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nThe amount of time spent on computing residual is amazing high in solve. Is that healthy for iterative solver in PJFNK algorithm? (so far I am not able to extract more info from this graph. welcome guidance on what else I should pay attention to on the perf graph for my kind of problem.)",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5607167",
                          "updatedAt": "2023-04-13T17:13:28Z",
                          "publishedAt": "2023-04-13T17:13:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "There is no self % that reports all that high in the perf graph so it's very difficult to guess what is slow. I would suggest trying gperftools (the link I provided earlier for profiling) in order to dig into this. I don't currently have time to do the detailed diagnostics on this myself",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5608577",
                          "updatedAt": "2023-04-13T19:40:04Z",
                          "publishedAt": "2023-04-13T19:40:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "The best answer I can think of to this question is that PJFNK by design does not work efficiently on big problems with mortar elements. When we are evaluating jacobian implicitly\n$$J\\overrightarrow{v} \\approx \\frac{\\overrightarrow{R}(\\overrightarrow{u}+\\epsilon\\overrightarrow{v})-\\overrightarrow{R}(\\overrightarrow{u})}{\\epsilon}$$\nthe small amount of difference $\\epsilon$ updating across processor and at every iteration is time consuming for large problem.\nGoing back to Newton algorithm is favored in this case.",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5642444",
                          "updatedAt": "2023-04-17T22:41:01Z",
                          "publishedAt": "2023-04-17T22:41:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "PJFNK can definitely be expensive if you're taking a lot of linear iterations and doing a lot of function/residual evaluations. Was that the case for you?",
                          "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5644295",
                          "updatedAt": "2023-04-18T04:14:04Z",
                          "publishedAt": "2023-04-18T04:14:03Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "BoZeng1997"
                  },
                  "bodyText": "Sums up the best settings to balance memory usage and computing speed that I have for now:\n\nreduce auxiliary variables to minimum.\nuse --distributed-mesh and nemesis output.\nit is ok to scale thermal LM (since it does have an on-diagonal, and automatic scaling default to on-diagonal scaling)\nuse the following petsc solver option. reduce -pc_asm_overlap to lower memory usage or increase it for accuracy\n\nsolve_type = NEWTON\npetsc_options_iname = '-pc_type -ksp_type -ksp_grmres_restart -sub_ksp_type -sub_pc_type -pc_asm_overlap -sub_pc_factor_shift_type -sub_pc_factor_shift_amount ' \npetsc_options_value = 'asm      gmres     200                preonly       lu           1  NONZERO 1e-14  \n\nthose settings can reduce a 5M DOFs problem (replicated mesh, 200 processors in total) that occupied 6GB on each processors to 0.8GB on each processors.\nThanks everyone for helping me!",
                  "url": "https://github.com/idaholab/moose/discussions/24019#discussioncomment-5642487",
                  "updatedAt": "2023-04-17T22:55:51Z",
                  "publishedAt": "2023-04-17T22:48:26Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "NodalGravity kernel",
          "author": {
            "login": "chakra34"
          },
          "bodyText": "Hi Moose group,\nI am currently comparing a linear elastic simulation with force boundary condition for a simple block geometry with dimensions (10mm x 10mm x 10mm) between ANSYS and Moose. So the geometry looks like:\n\nAnd I am keeping the bottom surface fixed displacement and applying a force on the top selected nodes (node set) to basically mimic the following ansys problem:\n\nNow the force in ansys is 10N (-y direction), and when I looked into the NodalGravity kernel which allows for a force boundary condition I saw that it sums up the force from all the nodes in the node set. Since my node set has 19 nodes, I impose the following (g = 10/19 = 0.5263 ;  and nodal mass = 1.0) in the input block:\n[NodalKernels]\n  [./force_y2]\n    type = NodalGravity\n    variable = disp_y\n    boundary = 'top_load_curve'\n    gravity_value = -0.5263 # inverse of nodal mass at cantilever end\n    mass = 1.0 # commented out for testing purposes\n  [../]\n[]\n\nBut when I compare the results, I see a difference in the values between ansys and moose both in stress and displacement.\n\nIs there something I am missing, I can't seem to figure out what might be the issue.\nThanks again,\n-Aritra",
          "url": "https://github.com/idaholab/moose/discussions/24099",
          "updatedAt": "2023-04-29T02:56:32Z",
          "publishedAt": "2023-04-17T19:24:03Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "By comparing stresses, you are essentially chasing a singularity here. The output values of stresses are very sensitive to the method used to average/extrapolate cell values onto the nodes. I recommend you to compare reaction forces on the nodes rather than stress values. Alternatively, compute and compare the surface integral of the vonmises stress.",
                  "url": "https://github.com/idaholab/moose/discussions/24099#discussioncomment-5641377",
                  "updatedAt": "2023-04-17T20:02:27Z",
                  "publishedAt": "2023-04-17T20:02:27Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "chakra34"
                          },
                          "bodyText": "Thanks a lot. Actually I also compared the displacements at the corresponding locations, and they are also different. On another note, I was wondering whether the way I am adding the nodal gravity as a force boundary condition makes sense ? Here is the displacement comparison:",
                          "url": "https://github.com/idaholab/moose/discussions/24099#discussioncomment-5641740",
                          "updatedAt": "2023-04-17T20:53:06Z",
                          "publishedAt": "2023-04-17T20:53:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "I guess ansys is handling this BC differently than your nodal kernel approach in moose. For example, ansys could be integrating the BC on a lower dimensional sideset. Another possibility is that ansys might be using a finer mesh. To examine the later, you can try refining your mesh.",
                          "url": "https://github.com/idaholab/moose/discussions/24099#discussioncomment-5641866",
                          "updatedAt": "2023-04-17T21:09:24Z",
                          "publishedAt": "2023-04-17T21:09:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "There is also no guarantee that ansys was distributing the 10N force uniformly as what you did in moose. It might be simply adding a constraint that the total force on the sideset add up to 10N.",
                          "url": "https://github.com/idaholab/moose/discussions/24099#discussioncomment-5641882",
                          "updatedAt": "2023-04-17T21:12:06Z",
                          "publishedAt": "2023-04-17T21:12:06Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to input 'use_legacy_material_output' ?",
          "author": {
            "login": "KamalnathOSU"
          },
          "bodyText": "I want to print the material properties at the INITIAL and TIMESTEP_END. Hence, in my input file I defined.\n[./GlobalParams]\nuse_legacy_material_output=false\n[../]\n\nBut still material property is not calculated at INITIAL. Can anybody tell me where should I define use_legacy_material_output ?",
          "url": "https://github.com/idaholab/moose/discussions/16754",
          "updatedAt": "2023-04-17T20:38:01Z",
          "publishedAt": "2021-01-20T15:55:00Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "That parameter is not available in the input file, it is something that you set in your application. My application Pika has some legacy flags set: https://github.com/idaholab/pika/blob/b2b5e4db567afbfee8bb5b5aa3726bbb4e5b1407/src/base/PikaApp.C#L24",
                  "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-297150",
                  "updatedAt": "2022-08-05T12:15:08Z",
                  "publishedAt": "2021-01-20T20:05:53Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "KamalnathOSU"
                          },
                          "bodyText": "Thank you. It works.",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-298893",
                          "updatedAt": "2022-08-05T12:15:11Z",
                          "publishedAt": "2021-01-21T12:55:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Great, using it as an example helps me rationalize why my application has legacy flags.",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-299133",
                          "updatedAt": "2022-08-05T12:15:11Z",
                          "publishedAt": "2021-01-21T14:13:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "engrmessi"
                          },
                          "bodyText": "KamalnathOSU please how did you resolve this issue",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5640456",
                          "updatedAt": "2023-04-17T18:09:49Z",
                          "publishedAt": "2023-04-17T18:09:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYou need to go in src/base and modify the <YourAppName>App.C file to enable legacy_material_output\nplease dont do this if you dont really need this. It's legacy for a reason, and one day we might remove it\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5640771",
                          "updatedAt": "2023-04-17T18:49:20Z",
                          "publishedAt": "2023-04-17T18:49:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "engrmessi"
                  },
                  "bodyText": "Hi Idaholab/Moose.\nAttached is my APPname.C file. I didn't see any \"use legacy material\" to\nmodify.\n\u2026\nOn Mon, 17 Apr 2023, 14:49 Guillaume Giudicelli, ***@***.***> wrote:\n Hello\n\n You need to go in src/base and modify the <YourAppName>App.C file to\n enable legacy_material_output\n please dont do this if you dont really need this. It's legacy for a\n reason, and one day we might remove it\n\n Guillaume\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#16754 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/A534X672GEB7C6GZKCGQSTDXBWGDVANCNFSM4WK4KMVA>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641110",
                  "updatedAt": "2023-04-17T19:29:53Z",
                  "publishedAt": "2023-04-17T19:29:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You need to add that line in the validParam() routine\nparams.set<bool>(\"use_legacy_material_output\") = false;\nand set it how you want to enable or disable the legacy material output",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641165",
                          "updatedAt": "2023-04-17T19:44:17Z",
                          "publishedAt": "2023-04-17T19:38:54Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "engrmessi"
                  },
                  "bodyText": "Please at which point do I need to add this parameter. I added it at the\nend previously, it didn't make any difference. Also I tried to recompiled\nmy Application, it failed. Do I need to recompile? And at what line do I\nneed to paste the parameters. Thank you\n\u2026\nOn Mon, 17 Apr 2023, 15:39 Guillaume Giudicelli, ***@***.***> wrote:\n You need to add that line.\n params.set<bool>(\"use_legacy_material_output\") = false;\n and set it how you want to enable or disable the legacy material output\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#16754 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/A534X65WVWPK764AGLUA4R3XBWL5VANCNFSM4WK4KMVA>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641203",
                  "updatedAt": "2023-04-17T19:44:15Z",
                  "publishedAt": "2023-04-17T19:44:13Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Updated my message\nYou do need to recompile, and it needs to succeed, before the change becomes effective",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641215",
                          "updatedAt": "2023-04-17T19:45:17Z",
                          "publishedAt": "2023-04-17T19:45:16Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "engrmessi"
                  },
                  "bodyText": "OK but when I copied and pasted the parameter at the end of the APP. C file\nand recompiled, it failed. At which exact location do I need to paste it.\nYou can mark the location in the photo I sent you earlier. Sorry for the\ninconvenience, I am new to MOOSE.\n\nBest Regards\n\u2026\nOn Mon, 17 Apr 2023, 15:45 Guillaume Giudicelli, ***@***.***> wrote:\n Updated my message\n\n You do need to recompile, and it needs to succeed, before the change\n becomes effective\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#16754 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/A534X645RRO45RQ44CSLRY3XBWMVPANCNFSM4WK4KMVA>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641295",
                  "updatedAt": "2023-04-17T19:51:31Z",
                  "publishedAt": "2023-04-17T19:51:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "in YourAppNameApp::validParams()\nis your repository opensource/ available online?",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641483",
                          "updatedAt": "2023-04-17T20:18:03Z",
                          "publishedAt": "2023-04-17T20:17:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "engrmessi"
                          },
                          "bodyText": "yes, as engrmessi/engrmessi",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641514",
                          "updatedAt": "2023-04-17T20:22:13Z",
                          "publishedAt": "2023-04-17T20:22:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "here https://github.com/engrmessi/engrmessi/blob/47f7c07a775936f48b883bcaef5eddd7a9ab7ca5/src/base/EngrmessiApp.C#L11",
                          "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641549",
                          "updatedAt": "2023-04-17T20:26:25Z",
                          "publishedAt": "2023-04-17T20:26:24Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "engrmessi"
                  },
                  "bodyText": "Everything works very fine now, thanks a lot.\n\nBest Regards\n\u2026\nOn Mon, 17 Apr 2023, 16:26 Guillaume Giudicelli, ***@***.***> wrote:\n here\n https://github.com/engrmessi/engrmessi/blob/47f7c07a775936f48b883bcaef5eddd7a9ab7ca5/src/base/EngrmessiApp.C#L11\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#16754 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/A534X64K3DH7HUAIXIRM6TDXBWRPXANCNFSM4WK4KMVA>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/16754#discussioncomment-5641636",
                  "updatedAt": "2023-04-17T20:38:01Z",
                  "publishedAt": "2023-04-17T20:38:00Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error with installation of mamba",
          "author": {
            "login": "xchengood"
          },
          "bodyText": "Hi,\nError is below. Thank you for your help.",
          "url": "https://github.com/idaholab/moose/discussions/24089",
          "updatedAt": "2023-04-17T15:58:02Z",
          "publishedAt": "2023-04-17T14:41:23Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nSeems you do not have permissions to this directory.\nCan you either:\n\nmake a new directory and install everything there\nchange the permissions to the directory\n\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/24089#discussioncomment-5638353",
                  "updatedAt": "2023-04-17T15:03:45Z",
                  "publishedAt": "2023-04-17T15:03:44Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Seeing that you already have (base) prefixed in your prompt, it would appear you already have Conda installed and initialized.\nInstalling a second Conda instance is not a good idea.\nThe error you are receiving is some other issue entirely however.",
                          "url": "https://github.com/idaholab/moose/discussions/24089#discussioncomment-5638969",
                          "updatedAt": "2023-04-17T15:55:37Z",
                          "publishedAt": "2023-04-17T15:55:36Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}