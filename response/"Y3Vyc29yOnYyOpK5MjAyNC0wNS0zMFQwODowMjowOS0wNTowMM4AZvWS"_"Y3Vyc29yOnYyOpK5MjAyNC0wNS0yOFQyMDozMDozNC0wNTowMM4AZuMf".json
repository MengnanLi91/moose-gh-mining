{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNS0yOFQyMDozMDozNC0wNTowMM4AZuMf"
    },
    "edges": [
      {
        "node": {
          "title": "Parallel Debugging MOOSE in VS Code",
          "author": {
            "login": "Flolaffel"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nHello,\nI'm currently working on getting my MOOSE Application to run with MPI. It didn't run out of the box, so I'd like to debug in parallel. I read the MOOSE documentation on debugging and got parallel debugging from terminal to work. However I'd like to parallel debug in VS Code and am not really sure where to start.\nIs there a common workflow for this?",
          "url": "https://github.com/idaholab/moose/discussions/27742",
          "updatedAt": "2024-05-30T08:49:36Z",
          "publishedAt": "2024-05-30T07:12:25Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nNo we have not set that up yet.\nIf you figure out how to do it please let us know\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27742#discussioncomment-9603262",
                  "updatedAt": "2024-05-30T08:49:37Z",
                  "publishedAt": "2024-05-30T08:49:36Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "error in side reaction postprocessor",
          "author": {
            "login": "zuckarm"
          },
          "bodyText": "hi all,\ni am trying to compute the reaction on side set 'bottom'. i am using the following code:\n[Reaction]\ntype = SidesetReaction\ndirection = '0 -1 0'\nstress_tensor = 'stress'\nboundary = bottom\n[]\nwhere the 'stress' block is defined as:\n[stress]\ntype = ComputeSmallDeformationStress\nelasticity_model = elasticity\noutput_properties = 'stress'\noutputs = exodus\n[]\nhowever, i am getting the following error:\nThe requested non-AD material property 'stress' of type 'RankTwoTensorTempl'\nis already retrieved or declared as a AD property of type 'RankTwoTensorTempl'.\ni used a material AD converter:\n[sigma_yy]\ntype = MaterialADConverter\nad_props_in = stress\nreg_props_out = reg_stress\nboundary = bottom\n[]\nnow i am getting a different error:\nThe requested AD material property 'stress' of type 'double'\nis already retrieved or declared as a AD property of type 'RankTwoTensorTempl'.\ncan anyone suggest on how to resolve this?\nthanks",
          "url": "https://github.com/idaholab/moose/discussions/27715",
          "updatedAt": "2024-05-30T06:45:12Z",
          "publishedAt": "2024-05-28T08:28:13Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIs there an AD version of the material you could be using? Basically you are mixing AD and non AD here and that is not accepted\nYou can use Debug/show_material_props to see which material is declaring and requesting each material property.\nThen make sure everything is consistently AD or not AD.\nFor example you could use ADSidesetReaction\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27715#discussioncomment-9578432",
                  "updatedAt": "2024-05-28T09:08:35Z",
                  "publishedAt": "2024-05-28T09:08:33Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "zuckarm"
                          },
                          "bodyText": "ADSidesetReaction worked, thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/27715#discussioncomment-9601906",
                          "updatedAt": "2024-05-30T06:45:12Z",
                          "publishedAt": "2024-05-30T06:45:10Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Navier Stokes <--> PorousFlow Heat Transfer in MultiApp",
          "author": {
            "login": "aikubo"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A Navier Stokes is the most appropriate category for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (avoid screenshots if possible, triple back quotes before/after pasted text, etc)\n\nQuestion\nYou can see some previous discussion in #27159\nI am trying to couple a multiphase navier stokes simulation subApp to a PorousFlow Main app.\nThe NS app is modeling magma flow in a narrow channel it is largely based on the Gallium solidification example but it has a non-linear melt fraction temperature relationship. It is the smaller domain and makes up the left boundary of the porous flow app. It is intended to model a magmatic dike.\nThe PorousFlow app uses the simple fluid and solves for heat conduction and advection on a large domain on one side of the dike. I added a loglinear permeability temperature relationship.\nThey are coupled in a multiapp with sub cycling and picard iteration so that the NS app can have smaller timesteps. It's a simple grid of two rectangles each with different resolutions which touch on the right and interface boundaries. The meshes are created seperately in each input file and uses the positions flag in the MultiApp set up. Right now it converges and yields a result but I don't think it physically looks right.\nCurrently, here's how the coupling is set up:\n\nthe heat flow from the porous flow app is calculated on 'timestep_begin' using a VariableGradientComponent and transfered to the sub app MultiAppGeneralFieldNearestNodeTransfer where $k*\\delta T *dt$ is calculated and set to the FVFunctorNeumannBC\n\nthe temperature of the NS app is transfered to an auxvariable in the porousflow app using MultiAppGeneralFieldShapeEvaluationTransfer and set to the interface boundary using MatchedValueBC\n\n\nReading in some discussion and my own intuition says that this isn't the best way to do it. @GiudGiud adviced against it in #26189. I know that the heat flow across the BC is not conserved in the current model.\nMy goal is to figure out a transfer that allows spatial variability of heat transfer (mainly in the Y direction) along the length of the dike. We would expect based on our understanding of multiphase flow in the dike and hydrothermal circulation around the dike for there to be \"hotspots\" and \"cold spots\" which will lead to non-homogenous flow.\nFor this reason I didn't use SideLayeredAverage or various similar ones but I think I could use LineValueSampler.\nSome points I was confused about:\n\n\nwhat are the correct dimensions for FVFunctorNeumannBC?\nin 2d-rc-transient.i test I noticed the value for the functor is u_inlet * rho * cp * T_inlet which has dimensions of $L*\\frac{M}{L^3}\\frac{L^2M}{T^2*\\theta}*\\theta = \\frac{M^2}{L^2}$\nBecause of this I set the value to $k\\nabla T * dt$ which a rough approximation of the integral over t. Both k and gradTx come from the parent app. For testing k is constant and isotropic and since we are dealing with a simple grid I just transfer one component from gradT (gradTx >> gradTy). Either way, the way I set it up is definitely suspect.\n\n\nFVFunctorNeumannBC should have + values for heat flux IN and - values for heat flux out correct? It should be similar to the FE NeumannBC where $\\frac{\\partial u}{\\partial n} = \\nabla u \\cdot \\hat{n} = \\text{value}$\n\n\nProbably related to (1) but I tried setting up a postprocessor to conserve heat flux between apps but I couldn't get it to run at all.\n\n\nI did write a custom aux kernal that calculates the heat flow in the porousflow app which I could use to transfer but considering that I added the permeability temperature relationship, permability goes very low around the dike (10e-20) so it is mostly heat conduction anyway. I can add this in in the future or now if it would help but the units and conservation thing were a concern.\nYou can find it here\n\n\nThis is mostly a demo for the thesis which is due next week (it's last minute :( but I'm so close to getting it correct ) so it's okay if it's not perfect I'd really just like to include it for a cool example.\nHere's the repo: https://github.com/aikubo/DOGS/tree/multiapp/sims/multiapp\nAnd here's the Parent input file:\ndepthAtTop = 1500 #m\nL = 1000 #m\nW = 100 #m\n\nnx = 20\nny = 20\n\ngeotherm = '${fparse 10/1000}' #K/m\n\n[Mesh]\n  [gen]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = ${nx}\n    ny = ${ny}\n    xmin= 0\n    xmax = '${fparse L*2}'\n    ymin = 0\n    ymax = ${L}\n  []\n  [cutout]\n    type = SubdomainBoundingBoxGenerator\n    input = gen\n    block_id = 1\n    bottom_left = '0 0 0'\n    top_right = '${W} ${L} 0'\n  []\n  [rename]\n    type = RenameBlockGenerator\n    input = cutout\n    old_block = '0 1'\n    new_block = 'host dike'\n  []\n  [between]\n   type = SideSetsBetweenSubdomainsGenerator\n   input = rename\n   primary_block = 'host'\n   paired_block = 'dike'\n   new_boundary = interface\n  []\n  [delete]\n    type = BlockDeletionGenerator\n    input = between\n    block = 'dike'\n  []\n[]\n\n\n[GlobalParams]\n  PorousFlowDictator = 'dictator'\n  gravity = '0 -9.81 0'\n[]\n\n[UserObjects]\n  [dictator]\n    type = PorousFlowDictator\n    porous_flow_vars = 'T_parent porepressure'\n    number_fluid_phases = 1\n    number_fluid_components = 1\n  []\n[]\n\n[Variables]\n  [T_parent]\n    order = FIRST\n    family = LAGRANGE\n  []\n  [porepressure]\n    order = FIRST\n    family = LAGRANGE\n\n  []\n[]\n\n\n[AuxVariables]\n  [T_cutout]\n  []\n  [GradTx]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [diffx]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [k]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [porosity]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [perm]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [permExp]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [darcy_vel_x]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [darcy_vel_y]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [velMag]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n[]\n\n[AuxKernels]\n   [porosity]\n    type = ParsedAux\n    variable = porosity\n    expression = '0.1'\n  []\n  [permExp]\n    type = ParsedAux\n    variable = permExp\n    coupled_variables = 'T_parent'\n    constant_names= 'm b k0exp'\n    constant_expressions = '-0.01359 -9.1262 -13' #calculated myself via linear\n    expression = 'if(T_parent>400, m*T_parent+b, k0exp)'\n    execute_on = 'initial nonlinear timestep_end'\n  []\n  [perm]\n    type = ParsedAux\n    variable = perm\n    coupled_variables = 'T_parent permExp'\n    constant_names= 'klow'\n    constant_expressions = '10e-20 '\n    expression = 'if(T_parent>900,klow, 10^permExp)'\n    execute_on = 'initial nonlinear timestep_end'\n  []\n  [darcy_vel_x_kernel]\n    type = PorousFlowDarcyVelocityComponent\n    component = x\n    variable = darcy_vel_x\n    fluid_phase = 0\n  []\n  [darcy_vel_y_kernel]\n    type = PorousFlowDarcyVelocityComponent\n    component = y\n    variable = darcy_vel_y\n    fluid_phase = 0\n  []\n  [velMag]\n    type = ParsedAux\n    variable = velMag\n    coupled_variables = 'darcy_vel_x darcy_vel_y'\n    expression = 'sqrt(darcy_vel_x^2 + darcy_vel_y^2)'\n    execute_on = 'initial timestep_end'\n  []\n    [GradTx]\n    type = VariableGradientComponent\n    variable = GradTx\n    gradient_variable = T_parent\n    component = x\n    execute_on = 'initial timestep_end'\n  []\n  [diffx]\n    type = ParsedAux\n    variable = diffx\n    coupled_variables = 'GradTx k'\n    expression = 'k*GradTx'\n    execute_on = 'initial timestep_end'\n  []\n  [k]\n    type = ParsedAux\n    variable = k\n    expression = '5'\n    execute_on = 'initial timestep_end'\n  []\n[]\n\n[Functions]\n  [ppfunc]\n    type = ParsedFunction\n    expression ='1.0135e5+(${depthAtTop})*9.81*1000+(${depthAtTop}-y)*1000*9.81' #1.0135e5-(y)*9.81*1000' #hydrostatic gradientose   + atmospheric pressure in Pa\n  []\n  [tfunc]\n    type = ParsedFunction\n    expression = '285+${depthAtTop}*${geotherm}+(${L}-y)*${geotherm}' #285+(-y)*10/1000 # geothermal 10 C per kilometer in kelvin\n  []\n[]\n\n[ICs]\n  [hydrostatic]\n    type = FunctionIC\n    variable = porepressure\n    function = ppfunc\n  []\n  [geothermal]\n    type = FunctionIC\n    variable = T_parent\n    function = tfunc\n  []\n[]\n\n\n\n[Kernels]\n  [./PorousFlowUnsaturated_HeatConduction]\n    type = PorousFlowHeatConduction\n    #block = 'host'\n    variable = T_parent\n  [../]\n  [./PorousFlowUnsaturated_EnergyTimeDerivative]\n    type = PorousFlowEnergyTimeDerivative\n    #block = 'host'\n    variable = T_parent\n  [../]\n  [./PorousFlowFullySaturated_AdvectiveFlux0]\n    type = PorousFlowFullySaturatedAdvectiveFlux\n    #block = 'host'\n    variable = porepressure\n  [../]\n  [./PorousFlowFullySaturated_MassTimeDerivative0]\n    type = PorousFlowMassTimeDerivative\n    #block = 'host'\n    variable = porepressure\n  [../]\n  [./PorousFlowFullySaturatedUpwind_HeatAdvection]\n    type = PorousFlowFullySaturatedUpwindHeatAdvection\n    variable = T_parent\n    #block = 'host'\n  [../]\n\n[]\n\n[BCs]\n  [Matched_Multi]\n    type = MatchedValueBC\n    variable = T_parent\n    boundary = interface\n    v = T_cutout\n  []\n  [right]\n    type = FunctionDirichletBC\n    variable = T_parent\n    boundary = 'right bottom'\n    function = tfunc\n  []\n  [NeumannBC]\n    type = NeumannBC\n    variable = porepressure\n    boundary = 'right top bottom interface'\n    value = 0\n  []\n\n[]\n\n[FluidProperties]\n  [water]\n    type = SimpleFluidProperties\n  []\n[]\n\n\n[Materials]\n  [PorousFlowActionBase_Temperature_qp]\n    type = PorousFlowTemperature\n\n    temperature = 'T_parent'\n  []\n  [PorousFlowActionBase_Temperature]\n    type = PorousFlowTemperature\n\n    at_nodes = true\n    temperature = 'T_parent'\n  []\n  [PorousFlowActionBase_MassFraction_qp]\n    type = PorousFlowMassFraction\n\n  []\n  [PorousFlowActionBase_MassFraction]\n    type = PorousFlowMassFraction\n\n    at_nodes = true\n  []\n  [PorousFlowActionBase_FluidProperties_qp]\n    type = PorousFlowSingleComponentFluid\n\n    compute_enthalpy = true\n    compute_internal_energy = true\n    fp = water\n    phase = 0\n  []\n  [PorousFlowActionBase_FluidProperties]\n    type = PorousFlowSingleComponentFluid\n\n    at_nodes = true\n    fp = water\n    phase = 0\n  []\n  [PorousFlowUnsaturated_EffectiveFluidPressure_qp]\n    type = PorousFlowEffectiveFluidPressure\n\n  []\n  [PorousFlowUnsaturated_EffectiveFluidPressure]\n    type = PorousFlowEffectiveFluidPressure\n\n    at_nodes = true\n  []\n  [PorousFlowFullySaturated_1PhaseP_qp]\n    type = PorousFlow1PhaseFullySaturated\n\n    porepressure = 'porepressure'\n  []\n  [PorousFlowFullySaturated_1PhaseP]\n    type = PorousFlow1PhaseFullySaturated\n\n    at_nodes = true\n    porepressure = 'porepressure'\n  []\n  [PorousFlowActionBase_RelativePermeability_qp]\n    type = PorousFlowRelativePermeabilityConst\n    phase = 0\n  []\n  [porosity]\n    type = PorousFlowPorosityConst\n    porosity = 'porosity'\n  []\n  [permeability]\n    type = PorousFlowPermeabilityConstFromVar\n    perm_xx = 'perm'\n    perm_yy = 'perm'\n    perm_zz = 'perm'\n  []\n  [Matrix_internal_energy]\n    type = PorousFlowMatrixInternalEnergy\n    density = 2400\n    specific_heat_capacity = 790\n  []\n  [thermal_conductivity]\n    type = PorousFlowThermalConductivityIdeal\n    dry_thermal_conductivity = '3 0 0  0 3 0  0 0 3'\n  []\n[]\n\n[Preconditioning]\n  [mumps]\n    # much better than superlu\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n    petsc_options_value = ' lu       mumps'\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = 'NEWTON'\n  end_time = 3e9\n  line_search = 'none'\n  dtmin = 0.01\n  automatic_scaling = true\n  nl_abs_tol = 1e-9\n  nl_rel_tol = 1e-6\n  verbose = true\n  dt = 5000\n\n  fixed_point_max_its = 10\n  fixed_point_abs_tol = 1e-5\n  fixed_point_rel_tol = 1e-4\n[]\n\n[MultiApps]\n  [./child_app]\n    type = TransientMultiApp\n    app_type = dikesApp\n    input_files = 'nsdikeChild.i'\n    execute_on = 'timestep_begin'\n  [../]\n[]\n\n[Transfers]\n  [./transfer_to_child]\n    type =  MultiAppGeneralFieldShapeEvaluationTransfer\n    from_multi_app = child_app\n    source_variable = T_child\n    variable = T_cutout\n    bbox_factor = 1.2\n  [../]\n  [push_qx]\n    # Transfer from this app to the sub-app\n    # which variable from this app?\n    # which variable in the sub app?\n    type = MultiAppGeneralFieldNearestNodeTransfer\n    to_multi_app = child_app\n    source_variable = GradTx\n    #bbox_factor = 1.2\n    variable = GradTx_from_parent\n  []\n  [push_cond]\n    type = MultiAppGeneralFieldNearestNodeTransfer\n    to_multi_app = child_app\n    source_variable = k\n    variable = k_from_parent\n  []\n[]\n\n[Outputs]\n  checkpoint = true\n[]\n\nHere's the sub app:\ntimeUnit = 1 #s\n\n# Fluid properties\nmu = 100 #Pa S\nrho_liquid = 2700 #kg/m^3\nk_liquid = 4 #W/mK\ncp_liquid = 1100 #J/kgK\n\n# Solid properties\nrho_solid = 3000 #kg/m^3\nk_solid = 4 #W/mK\ncp_solid = 1100 #J/kgK\n\n# Phase change\nL = 300000 #J/kg\nT_solidus = 1170 #K\nT_liquidus = 1438 #K\nalpha_b = 1.2e-4 #K^-1\n#bd=1.7\n\n# Operating conditions\ny_inlet = 1 #m/s\nT_inlet = 1438 #K\np_outlet = 10 #Pa\n\n\n# Numerical scheme\nadvected_interp_method = 'average'\nvelocity_interp_method = 'rc'\n\n\n[Mesh]\n  [gen]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = 15\n    ny = 35\n    xmin= 0\n    xmax = 100\n    ymin = 0\n    ymax = 1000\n  []\n[]\n\n[GlobalParams]\n  rhie_chow_user_object = 'rc'\n[]\n\n[UserObjects]\n  [rc]\n    type = INSFVRhieChowInterpolator\n    u = vel_x\n    v = vel_y\n    pressure = pressure\n  []\n[]\n\n[Variables]\n  [vel_x]\n    type = INSFVVelocityVariable\n    initial_condition = 1e-12\n  []\n  [vel_y]\n    type = INSFVVelocityVariable\n    initial_condition = '${fparse y_inlet*timeUnit}'\n  []\n  [pressure]\n    type = INSFVPressureVariable\n  []\n  [T_child]\n    type = INSFVEnergyVariable\n    initial_condition = ${T_inlet}\n  []\n[]\n\n[AuxVariables]\n  [fl]\n    type = MooseVariableFVReal\n    initial_condition = 0.0\n  []\n  [density]\n    type = MooseVariableFVReal\n  []\n  [th_cond]\n    type = MooseVariableFVReal\n  []\n  [cp_var]\n    type = MooseVariableFVReal\n  []\n  [darcy_coef]\n    type = MooseVariableFVReal\n  []\n  [fch_coef]\n    type = MooseVariableFVReal\n  []\n  [meltfraction]\n    type = MooseVariableFVReal\n  []\n  [GradTx_from_parent]\n    type = MooseVariableFVReal\n  []\n  [qx]\n    type = MooseVariableFVReal\n  []\n  [k_from_parent]\n    type = MooseVariableFVReal\n  []\n  [dtaux]\n    type = MooseVariableFVReal\n  []\n[]\n\n[AuxKernels]\n  [meltfraction]\n    type = ParsedAux\n    variable = meltfraction\n    coupled_variables = 'T_child'\n    constant_names = 'T_solidus T_liquidus bd'\n    constant_expressions = '${T_solidus} ${T_liquidus} 1.7'\n    expression = 'if(T_child > T_solidus, ((T_child - T_solidus)/(T_liquidus - T_solidus))^bd, 1)'\n  []\n  [fl]\n    type = ParsedAux\n    variable = fl\n    coupled_variables = 'T_child meltfraction'\n    constant_names = 'T_solius T_liquidus'\n    constant_expressions = '${T_solidus} ${T_liquidus}'\n    expression = 'if (T_child < T_liquidus, meltfraction, 1)'\n  []\n  [rho_out]\n    type = FunctorAux\n    functor = 'rho_mixture'\n    variable = 'density'\n  []\n  [th_cond_out]\n    type = FunctorAux\n    functor = 'k_mixture'\n    variable = 'th_cond'\n  []\n  [cp_out]\n    type = FunctorAux\n    functor = 'cp_mixture'\n    variable = 'cp_var'\n  []\n  [darcy_out]\n    type = FunctorAux\n    functor = 'Darcy_coefficient'\n    variable = 'darcy_coef'\n  []\n  [fch_out]\n    type = FunctorAux\n    functor = 'Forchheimer_coefficient'\n    variable = 'fch_coef'\n  []\n  [dtaux]\n    type = FunctorAux\n    variable = dtaux\n    functor = 'dtpost'\n  []\n  [qx]\n    type = ParsedAux\n    variable = qx\n    coupled_variables = 'GradTx_from_parent k_from_parent dtaux'\n    expression = 'k_from_parent*GradTx_from_parent*dtaux'\n  []\n\n[]\n\n\n\n[FVKernels]\n  [mass]\n    type = INSFVMassAdvection\n    variable = pressure\n    advected_interp_method = ${advected_interp_method}\n    velocity_interp_method = ${velocity_interp_method}\n    rho = rho_mixture\n  []\n  [u_time]\n    type = INSFVMomentumTimeDerivative\n    variable = vel_x\n    rho = rho_mixture\n    momentum_component = 'x'\n  []\n  [u_advection]\n    type = INSFVMomentumAdvection\n    variable = vel_x\n    advected_interp_method = ${advected_interp_method}\n    velocity_interp_method = ${velocity_interp_method}\n    rho = rho_mixture\n    momentum_component = 'x'\n  []\n  [u_viscosity]\n    type = INSFVMomentumDiffusion\n    variable = vel_x\n    mu = '${fparse mu/timeUnit}'\n    momentum_component = 'x'\n  []\n  [u_pressure]\n    type = INSFVMomentumPressure\n    variable = vel_x\n    momentum_component = 'x'\n    pressure = pressure\n  []\n  [u_friction]\n    type = INSFVMomentumFriction\n    variable = vel_x\n    momentum_component = 'x'\n    linear_coef_name = 'Darcy_coefficient'\n    quadratic_coef_name = 'Forchheimer_coefficient'\n  []\n  [v_time]\n    type = INSFVMomentumTimeDerivative\n    variable = vel_y\n    rho = rho_mixture\n    momentum_component = 'y'\n  []\n  [v_advection]\n    type = INSFVMomentumAdvection\n    variable = vel_y\n    advected_interp_method = ${advected_interp_method}\n    velocity_interp_method = ${velocity_interp_method}\n    rho = rho_mixture\n    momentum_component = 'y'\n  []\n  [v_viscosity]\n    type = INSFVMomentumDiffusion\n    variable = vel_y\n    mu = '${fparse mu/timeUnit}'\n    momentum_component = 'y'\n  []\n  [v_pressure]\n    type = INSFVMomentumPressure\n    variable = vel_y\n    momentum_component = 'y'\n    pressure = pressure\n  []\n  [v_friction]\n    type = INSFVMomentumFriction\n    variable = vel_y\n    momentum_component = 'y'\n    linear_coef_name = 'Darcy_coefficient'\n    quadratic_coef_name = 'Forchheimer_coefficient'\n  []\n  [v_gravity]\n    type = INSFVMomentumGravity\n    variable = vel_y\n    gravity = '0 -9.81 0'\n    rho = '${rho_liquid}'\n    momentum_component = 'y'\n  []\n  [v_buoyancy]\n    type = INSFVMomentumBoussinesq\n    variable = vel_y\n    T_fluid = T_child\n    gravity = '0 -9.81 0'\n    rho = '${rho_liquid}'\n    ref_temperature = ${T_solidus}\n    momentum_component = 'y'\n  []\n  [T_time]\n    type = INSFVEnergyTimeDerivative\n    variable = T_child\n    rho = rho_mixture\n    dh_dt = dh_dt\n  []\n  [energy_advection]\n    type = INSFVEnergyAdvection\n    variable = T_child\n    velocity_interp_method = ${velocity_interp_method}\n    advected_interp_method = ${advected_interp_method}\n  []\n  [energy_diffusion]\n    type = FVDiffusion\n    coeff = 'k_mixture'\n    variable = T_child\n  []\n  [energy_source]\n    type = NSFVPhaseChangeSource\n    variable = T_child\n    L = ${L}\n    liquid_fraction = fl\n    T_liquidus = ${T_liquidus}\n    T_solidus = ${T_solidus}\n    rho = 'rho_mixture'\n  []\n\n[]\n\n[FVBCs]\n  [inlet-u]\n  type = INSFVInletVelocityBC\n  boundary = 'bottom'\n  variable = vel_x\n  functor = '0'\n  []\n  [inlet-v]\n    type = INSFVInletVelocityBC\n    boundary = 'bottom'\n    variable = vel_y\n    functor = 1\n  []\n  [inlet-T]\n    type = FVNeumannBC\n    variable = T_child\n    value = '${fparse y_inlet * timeUnit * rho_liquid * cp_liquid * T_inlet}'\n    boundary = 'bottom'\n  []\n\n  [no-slip-u]\n    type = INSFVNoSlipWallBC\n    boundary = 'right'\n    variable = vel_x\n    function = 0\n  []\n  [no-slip-v]\n    type = INSFVNoSlipWallBC\n    boundary = 'right'\n    variable = vel_y\n    function = 0\n  []\n  [symmetry-u]\n    type = INSFVSymmetryVelocityBC\n    boundary = 'left'\n    variable = vel_x\n    u = vel_x\n    v = vel_y\n    mu = '${fparse mu/timeUnit}'\n    momentum_component = 'x'\n  []\n  [symmetry-v]\n    type = INSFVSymmetryVelocityBC\n    boundary = 'left'\n    variable = vel_y\n    u = vel_x\n    v = vel_y\n    mu = '${fparse mu/timeUnit}'\n    momentum_component = 'y'\n  []\n  [symmetry-p]\n    type = INSFVSymmetryPressureBC\n    boundary = 'left'\n    variable = pressure\n  []\n\n  [outlet_u]\n    type = INSFVMomentumAdvectionOutflowBC\n    variable = vel_x\n    u = vel_x\n    v = vel_y\n    boundary = 'top'\n    momentum_component = 'x'\n    rho = rho_mixture\n  []\n  [outlet_v]\n    type = INSFVMomentumAdvectionOutflowBC\n    variable = vel_y\n    u = vel_x\n    v = vel_y\n    boundary = 'top'\n    momentum_component = 'y'\n    rho = rho_mixture\n  []\n  [outlet_p]\n    type = INSFVOutletPressureBC\n    boundary = 'top'\n    variable = pressure\n    function = '${p_outlet}'\n  []\n  [cooling_side_multi]\n    type = FVFunctorNeumannBC\n    variable = T_child\n    boundary = 'right'\n    functor = 'qx'\n  []\n[]\n\n[FunctorMaterials]\n  [ins_fv]\n    type = INSFVEnthalpyFunctorMaterial\n    rho = rho_mixture\n    cp = cp_mixture\n    temperature = 'T_child'\n  []\n  [eff_cp]\n    type = NSFVMixtureMaterial\n    phase_2_names = '${cp_solid} ${k_solid} ${rho_solid}'\n    phase_1_names = '${cp_liquid} ${k_liquid} ${rho_liquid}'\n    prop_names = 'cp_mixture k_mixture rho_mixture'\n    phase_1_fraction = fl\n  []\n  [mushy_zone_resistance]\n    type = INSFVMushyPorousFrictionMaterial\n    liquid_fraction = 'fl'\n    mu = '${fparse mu/timeUnit}'\n    rho_l = '${rho_liquid}'\n    dendrite_spacing_scaling = 1e-1\n  []\n  [const_functor]\n    type = ADGenericFunctorMaterial\n    prop_names = 'alpha_b'\n    prop_values = '${alpha_b}'\n  []\n[]\n\n\n\n[Executioner]\n  type = Transient\n  end_time = 1e6\n  automatic_scaling = true\n  line_search = 'none'\n  # petsc_options_iname = '-pc_type -pSc_factor_shift_type'\n  # petsc_options_value = 'lu NONZERO'\n\n  solve_type = 'NEWTON'\n  petsc_options_iname = '-pc_type -sub_pc_factor_shift_type -ksp_gmres_restart'\n  petsc_options_value = ' lu       NONZERO                   200'\n  nl_rel_tol = 1e-5\n  nl_abs_tol = 1e-7\n  nl_max_its = 30\n\n  [TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 25\n  []\n[]\n\n[Postprocessors]\n  [t_avg_interface]\n    type = SideAverageValue\n    variable = T_child\n    boundary = 'right'\n  []\n  [t_avg]\n    type = ElementAverageValue\n    variable = T_child\n  []\n  [u_avg]\n    type = ElementAverageValue\n    variable = vel_x\n  []\n  [v_avg]\n    type = ElementAverageValue\n    variable = vel_y\n  []\n  [q_x_side]\n    type = SideAverageValue\n    variable = qx\n    boundary = 'right'\n  []\n  [qxchild]\n    type = SideDiffusiveFluxIntegral\n    variable = T_child\n    boundary = 'right'\n    functor_diffusivity= 'k_mixture'\n    execute_on = 'transfer'\n  []\n  [dtpost]\n    type = TimestepSize\n    execute_on = TIMESTEP_BEGIN\n  []\n[]\n\n\n[Outputs]\n  csv = true\n  exodus = true\n[]\n\n\nAdditional information\nMesh size and type: simple rectangular mesh with ~800 dofs in main and ~2100 in sub (small domain for testing)\nReynolds number: ~100\nDiscretization (finite element CG/DG, finite volume, etc): FV + FE\nModels (turbulence, porous media, etc): mixture models + mushy friction\nSolver method (fully coupled, segregated, multiapps, etc): multi app\nBase input you started from: gallium_melting.i",
          "url": "https://github.com/idaholab/moose/discussions/27710",
          "updatedAt": "2024-05-30T06:00:46Z",
          "publishedAt": "2024-05-26T00:30:22Z",
          "category": {
            "name": "Q&A Modules: Navier-Stokes"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@lindsayad @1runer",
                  "url": "https://github.com/idaholab/moose/discussions/27710#discussioncomment-9564527",
                  "updatedAt": "2024-05-26T23:18:01Z",
                  "publishedAt": "2024-05-26T23:18:00Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "The functor units should match the units of $k * \\nabla T \\cdot \\hat{n}$. A positive value for the functor would indicate that heat is coming into the domain. It can be very tricky to think about. In general the way I remind myself is that a positive residual corresponds to loss. I still have to think about it at least three times through just staring at FVFunctorNeumannBC and even then I am only 95% confident",
                  "url": "https://github.com/idaholab/moose/discussions/27710#discussioncomment-9584121",
                  "updatedAt": "2024-05-28T17:55:00Z",
                  "publishedAt": "2024-05-28T17:54:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aikubo"
                          },
                          "bodyText": "Thanks for the clarification @lindsayad.\nDo you think that transferring $\\nabla T$ using VariableGradientComponent and $k$ then setting it to the FVFunctorNeumannBC would work in the multiApp set up? Is there a better (and/or easier) way?",
                          "url": "https://github.com/idaholab/moose/discussions/27710#discussioncomment-9584863",
                          "updatedAt": "2024-05-28T19:11:36Z",
                          "publishedAt": "2024-05-28T19:11:36Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Are you missing a time derivative in the mass equation? The mixture is weakly compressible\nNearest node for the heat flux is definitely not conservative. You ll have to get the renormalisaton postprocessors set up in both the parent and child app. They compute the integrated flux, then the fields are re-normalized to make the two PPs match",
                  "url": "https://github.com/idaholab/moose/discussions/27710#discussioncomment-9587420",
                  "updatedAt": "2024-05-29T02:05:29Z",
                  "publishedAt": "2024-05-29T02:05:27Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aikubo"
                          },
                          "bodyText": "Thank you, good catch.\nso the process for one iteration would look something like\n\ntransfer temperatures from the boundary of the child app to the parent\ncalculate heat flow at boundary of the parent app and the integral of the heat flux in a post processor\ntransfer heatflow on the boundary using nearest node\ntransfer postprocessor of the heat flux integral\nin the child app renormalize the heat flow based on integral\n\nI could do the same thing back and forth between apps but I think that it should be okay for them to be more loosely coupled, the timescale for porous flow is higher than the timescale of flow in the dike.  Do I need to renormalize on both sides or use heatflux on both sides?\nTo simplify everything I am going to assume that at the boundary around the dike heat flow is purely diffusive (this is reasonable because the dike melts and quenches the host rock around it into glass) so I can use SideDiffusiveFluxIntegral.\nThe original inspiration for transfering temperature on one side and heat flux on the other side are from this example",
                          "url": "https://github.com/idaholab/moose/discussions/27710#discussioncomment-9597192",
                          "updatedAt": "2024-05-29T18:20:53Z",
                          "publishedAt": "2024-05-29T18:20:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You don't need to do the transfers and renormalization of the fields yourself. The from_to_pp_to_conserve parameters of the general field transfers do that for you",
                          "url": "https://github.com/idaholab/moose/discussions/27710#discussioncomment-9599366",
                          "updatedAt": "2024-05-30T00:10:42Z",
                          "publishedAt": "2024-05-30T00:10:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "With regards to \"renormalizing on both sides\", it s actually only the transferred quantity on the receiving side that gets renormalized. We generally would not know what would be the right value to renormalize the source quantity.\nBut if you do from an energy balance done externally for example, then you will need to do this manually.\nAnd if you do it manually, Debug/show_execution_order should be helpful",
                          "url": "https://github.com/idaholab/moose/discussions/27710#discussioncomment-9601577",
                          "updatedAt": "2024-05-30T06:00:46Z",
                          "publishedAt": "2024-05-30T06:00:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error when installing PETSc and libMesh",
          "author": {
            "login": "5trayheart"
          },
          "bodyText": "I'm currently installing cardinal without MOOSE's conda environment to use all the features of cardinal, see the following link.\nhttps://cardinal.cels.anl.gov/without_conda.html\nHowever, I am having trouble installing PETSc and libMesh. I recently found a similar problem with #27115  and tried deinit and rinit of moose, but that didn't solve the problem. It says in cardinal to contact moose's discussion regarding PETSc and libMesh, so I came here.\nPlease save me.\nHere is the output from my terminal.\n./contrib/moose/scripts/update_and_rebuild_petsc.sh\n\n/home/linux/cardinal/contrib/moose/scripts\nINFO: Checking for HDF5...\nINFO: HDF5 installation location was set using HDF5_ROOT=/home/linux/mambaforge3\n##========================================================================================\n                         Configuring PETSc to compile on your system\n##========================================================================================\n##========================================================================================\n                                     ***** WARNING *****\n  Using default C optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with COPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n##========================================================================================\n##========================================================================================\n                                     ***** WARNING *****\n  Using default Cxx optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with CXXOPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n##========================================================================================\n##========================================================================================\n                                     ***** WARNING *****\n  Using default FC optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with FOPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n##========================================================================================\nTESTING: check from config.libraries(config/BuildSystem/config/libraries.py:168)\n##*********************************************************************************************\n           UNABLE to CONFIGURE with GIVEN OPTIONS (see configure.log for details):\n##---------------------------------------------------------------------------------------------\n                     --with-hdf5-dir=/home/linux/mambaforge3 did not work\n##*********************************************************************************************\n\nThere was an error. Exiting...",
          "url": "https://github.com/idaholab/moose/discussions/27184",
          "updatedAt": "2024-05-29T20:18:23Z",
          "publishedAt": "2024-03-25T04:21:01Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nHow did you get HDF5 installed at this location? Where did you find these instructions pointing to this folder for HDF5?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8897381",
                  "updatedAt": "2024-03-25T04:27:53Z",
                  "publishedAt": "2024-03-25T04:27:52Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "5trayheart"
                          },
                          "bodyText": "I used the following commands\nexport HDF5_ROOT=$CONDA_PREFIX\nHere is a link to the instructions\nhttps://cardinal.cels.anl.gov/prereqs.html",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8897770",
                          "updatedAt": "2024-03-25T05:32:12Z",
                          "publishedAt": "2024-03-25T05:32:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Here is a link to the instructions\nhttps://cardinal.cels.anl.gov/prereqs.html\n\nThat is if you are using MOOSE's Conda environment. But I can see where the confusion came from.\nI think all you have to do in this case, is unset that environment variable and try again. However, I don't believe you would have made it to the page you linked unless you ran into some sort of error before this one.\nCan you unset the variable, try again, and post the error that occurs?\nunset HDF5_ROOT\n./contrib/moose/scripts/update_and_rebuild_petsc.sh",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8903078",
                          "updatedAt": "2024-03-25T14:04:07Z",
                          "publishedAt": "2024-03-25T14:04:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "5trayheart"
                          },
                          "bodyText": "Thank you for your comment. Here is the output.\n---------------------------------------------\n./contrib/moose/scripts/update_and_rebuild_petsc.sh\n/home/linux/cardinal/contrib/moose/scripts\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\n\nThe version of PETSc you are using is out-of-date, we recommend updating to the new release\n Available Version: 3.20.5   Installed Version: 3.20.3\nhttps://petsc.org/release/download/\n\n                         Configuring PETSc to compile on your system\n\n                                     ***** WARNING *****\n  Using default C optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with COPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n\n                                     ***** WARNING *****\n  Using default Cxx optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with CXXOPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n\n                                     ***** WARNING *****\n  Using default FC optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with FOPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n\n  Trying to download\n  https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.12/hdf5-1.12.2/src/hdf5-1.12.2.tar.bz2\n  for HDF5\n\n                   Running configure on HDF5; this may take several minutes\n\n                     Running make on HDF5; this may take several minutes\n\n                 Running make install on HDF5; this may take several minutes\n\n        Trying to download https://bitbucket.org/petsc/pkg-fblaslapack for FBLASLAPACK\n\n                     Compiling FBLASLAPACK; this may take several minutes\n\nTESTING: locateCMake from config.packages.cmake(config/BuildSystem/config/packag\n\n           UNABLE to CONFIGURE with GIVEN OPTIONS (see configure.log for details):\n\n  A package requires CMake version 3.21.0 (detected version is 3.16.3): use\n  --download-cmake\n\nThere was an error. Exiting...\n\n---------------------------------------------\n\n**So I tried sudo apt install cmake, and had the following output**\n\n---------------------------------------------\n\n\nsudo apt install cmake\n[sudo] password for linux: \nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ncmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n0 upgraded, 0 newly installed, 0 to remove and 104 not upgraded.\n\n---------------------------------------------\n\nIs this problem caused by my low ubuntu version?",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8914503",
                          "updatedAt": "2024-03-26T14:14:56Z",
                          "publishedAt": "2024-03-26T12:17:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYou should not have to sudo to install or update a package. conda install is not the same as conda update as well\nCan you try\nconda activate moose (or whatever the name of the environment)\nconda update cmake",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8915959",
                          "updatedAt": "2024-03-26T14:16:05Z",
                          "publishedAt": "2024-03-26T14:16:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I think we need to know what moose- related Conda packages you do have installed. My guess is none, going by the content in the OP (original post). Just want to be sure before continuing!\nIf I begin to setup Cardinal on an Ubuntu machine starting with these instructions: https://cardinal.cels.anl.gov/without_conda.html, I am guessing you have installed everything up to this point using apt install?\nCan you post the results from executing the diagnostic shell script? It should shed additional light as to where all your compilers are sitting, etc.\n./contrib/moose/scripts/diagnostics.sh\nI am also wondering if Cmake is required beyond building PETSc... I don't think it is if my memory serves. Therefore, what happens when you do the following:\n./contrib/moose/scripts/update_and_rebuild_petsc.sh --download-cmake=yes\n(let's allow PETSc's configure to simply download a version of Cmake required).",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8917518",
                          "updatedAt": "2024-03-26T16:03:25Z",
                          "publishedAt": "2024-03-26T16:03:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "5trayheart"
                          },
                          "bodyText": "I greatly appreciate your kind comments.\nI am installing cardinal in a base environment to use NekRS. I manually downloaded the latest version of cmake and tried to reinstall it.\nHere is a new error I got.\n\n./contrib/moose/scripts/update_and_rebuild_petsc.sh\n/home/linux/cardinal/contrib/moose/scripts\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\n\n                         Configuring PETSc to compile on your system\n\n                                     ***** WARNING *****\n  Using default C optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with COPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n\n                                     ***** WARNING *****\n  Using default Cxx optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with CXXOPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n\n                                     ***** WARNING *****\n  Using default FC optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with FOPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n\n  Trying to download\n\n(...)\n\n   UNABLE to CONFIGURE with GIVEN OPTIONS (see configure.log for details):\n\n                                PTScotch needs flex installed\n\nThere was an error. Exiting...\n\n\n\nHere are the diagnostics you suggested.\n\n./contrib/moose/scripts/diagnostics.sh\n\nNo LSB modules are available.\n\nSystem Arch: Distributor ID: Ubuntu Description: Ubuntu 20.04.6 LTS Release: 20.04 Codename: focal\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 64\n\nMemory Free: 200860.957 MB\n\n$CC not set\n\nMPICC:\nwhich mpicc:\n        /usr/bin/mpicc\nmpicc -show:\n        gcc -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi\n\nCOMPILER gcc:\ngcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nCopyright (C) 2019 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n        /home/linux/mambaforge3/bin/python\n        Python 3.10.13\n\nMODULES NOT AVAILABLE\n\nPETSC_DIR not set\n\nENVIRONMENT:\nCOLORTERM=truecolor\nCONDA_BACKUP_ADDR2LINE=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-addr2line\nCONDA_BACKUP_AR=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-ar\nCONDA_BACKUP_AS=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-as\nCONDA_BACKUP_BUILD=x86_64-conda-linux-gnu\nCONDA_BACKUP_CC=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-cc\nCONDA_BACKUP_CC_FOR_BUILD=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-cc\nCONDA_BACKUP_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_CMAKE_PREFIX_PATH=/home/linux/mambaforge3/envs/moose:/home/linux/mambaforge3/envs/moose/x86_64-conda-linux-gnu/sysroot/usr\nCONDA_BACKUP_CONDA_BUILD_SYSROOT=/home/linux/mambaforge3/envs/moose/x86_64-conda-linux-gnu/sysroot\nCONDA_BACKUP_CONDA_TOOLCHAIN_BUILD=x86_64-conda-linux-gnu\nCONDA_BACKUP_CONDA_TOOLCHAIN_HOST=x86_64-conda-linux-gnu\nCONDA_BACKUP_CPP=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-cpp\nCONDA_BACKUP_CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/linux/mambaforge3/envs/moose/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_CXX=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-c++\nCONDA_BACKUP_CXXFILT=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-c++filt\nCONDA_BACKUP_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_CXX_FOR_BUILD=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-c++\nCONDA_BACKUP_DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/linux/mambaforge3/envs/moose/include -D_DEBUG -D_FORTIFY_SOURCE=2 -Og -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include -fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_DEBUG_FFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -ffunction-sections -pipe -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_DEBUG_FORTRANFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fcheck=all -fbacktrace -fimplicit-none -fvar-tracking-assignments -ffunction-sections -pipe -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_ELFEDIT=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-elfedit\nCONDA_BACKUP_F95=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-f95\nCONDA_BACKUP_FC_FOR_BUILD=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-gfortran\nCONDA_BACKUP_FFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_FORTRANFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/linux/mambaforge3/envs/moose/include\nCONDA_BACKUP_GCC=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-gcc\nCONDA_BACKUP_GCC_AR=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-gcc-ar\nCONDA_BACKUP_GCC_NM=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-gcc-nm\nCONDA_BACKUP_GCC_RANLIB=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-gcc-ranlib\nCONDA_BACKUP_GFORTRAN=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-gfortran\nCONDA_BACKUP_GPROF=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-gprof\nCONDA_BACKUP_GXX=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-g++\nCONDA_BACKUP_HOST=x86_64-conda-linux-gnu\nCONDA_BACKUP_LD=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-ld\nCONDA_BACKUP_LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/linux/mambaforge3/envs/moose/lib -Wl,-rpath-link,/home/linux/mambaforge3/envs/moose/lib -L/home/linux/mambaforge3/envs/moose/lib -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/linux/mambaforge3/envs/moose/lib -Wl,-rpath-link,/home/linux/mambaforge3/envs/moose/lib -L/home/linux/mambaforge3/envs/moose/lib\nCONDA_BACKUP_LD_GOLD=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-ld.gold\nCONDA_BACKUP_MESON_ARGS=--buildtype release\nCONDA_BACKUP_NM=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-nm\nCONDA_BACKUP_OBJCOPY=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-objcopy\nCONDA_BACKUP_OBJDUMP=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-objdump\nCONDA_BACKUP_RANLIB=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-ranlib\nCONDA_BACKUP_READELF=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-readelf\nCONDA_BACKUP_SIZE=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-size\nCONDA_BACKUP_STRINGS=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-strings\nCONDA_BACKUP_STRIP=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-strip\nCONDA_BACKUP__CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu\nCONDA_BACKUP_build_alias=x86_64-conda-linux-gnu\nCONDA_BACKUP_host_alias=x86_64-conda-linux-gnu\nCONDA_DEFAULT_ENV=base\nCONDA_EXE=/home/linux/mambaforge3/bin/conda\nCONDA_PREFIX=/home/linux/mambaforge3\nCONDA_PROMPT_MODIFIER=(base) \nCONDA_PYTHON_EXE=/home/linux/mambaforge3/bin/python\nCONDA_ROOT=/home/linux/mambaforge3\nCONDA_SHLVL=1\nDISPLAY=141.223.1.2\nGIT_ASKPASS=/home/linux/.vscode-server/bin/863d2581ecda6849923a2118d93a088b0745d9d6/extensions/git/dist/askpass.sh\nHOME=/home/linux\nHOSTTYPE=x86_64\nLANG=C.UTF-8\nLESSCLOSE=/usr/bin/lesspipe %s %s\nLESSOPEN=| /usr/bin/lesspipe %s\nLOGNAME=linux\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\nMOTD_SHOWN=update-motd\nNAME=DESKTOP-\nOLDPWD=/home/linux\nPATH=/home/linux/.vscode-server/bin/863d2581ecda6849923a2118d93a088b0745d9d6/bin/remote-cli:/home/linux/mambaforge3/bin:/home/linux/mambaforge3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/Bandizip:/mnt/c/Program Files/dotnet:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Program Files (x86)/Windows Kits/10/Windows Performance Toolkit:/mnt/c/Users/postech/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/postech/AppData/Local/Programs/Microsoft VS Code/bin:/snap/bin:/home/linux/mambaforge3/envs/moose/wasp/bin\nPWD=/home/linux/cardinal\nSHELL=/bin/bash\nSHLVL=1\nTERM=xterm-256color\nTERM_PROGRAM=vscode\nTERM_PROGRAM_VERSION=1.87.2\nUSER=linux\nVSCODE_GIT_ASKPASS_EXTRA_ARGS=\nVSCODE_GIT_ASKPASS_MAIN=/home/linux/.vscode-server/bin/863d2581ecda6849923a2118d93a088b0745d9d6/extensions/git/dist/askpass-main.js\nVSCODE_GIT_ASKPASS_NODE=/home/linux/.vscode-server/bin/863d2581ecda6849923a2118d93a088b0745d9d6/node\nVSCODE_GIT_IPC_HANDLE=/tmp/vscode-git-c1e656da31.sock\nVSCODE_IPC_HOOK_CLI=/tmp/vscode-ipc-87a27074-e670-4bb4-aa55-7a50e81bf0a3.sock\nWSLENV=VSCODE_WSL_EXT_LOCATION/up\nWSL_DISTRO_NAME=Ubuntu-20.04\nWSL_INTEROP=/run/WSL/13_interop\nXDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\nXML_CATALOG_FILES=file:///home/linux/mambaforge3/etc/xml/catalog file:///etc/xml/catalog\n_=/usr/bin/env\n_CE_CONDA=\n_CE_M=",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8923110",
                          "updatedAt": "2024-03-28T17:46:50Z",
                          "publishedAt": "2024-03-27T04:01:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "hello\n\n                            PTScotch needs flex installed\n\n\nthat is another package that needs to be installed on your HPC.\n@milljm can we get that one from conda?\nCC, FC and CXX should be set to mpicc, mpif90 and mpicxx respectively btw.\nwhich installation pages are you following?",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8928947",
                          "updatedAt": "2024-03-27T14:05:21Z",
                          "publishedAt": "2024-03-27T14:05:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Note: I've had this post ready for hours now. Except when I run through these instructions myself, I run into errors building HDF5. I would rather not leave you hanging though. Perhaps my issue is unique to my environment, and the following instructions will work for you...\nThank you for the diagnostic report, having this available helps, and illuminates additional steps we need to take.\nBecause you have a Conda MOOSE environment available but not activated, I say we should continue to use Conda, but create a special environment just for Cardinal, and see how far we get with that.\nHowever, we should first remove your system-level openmpi package. I feel this is going to cause you greater harm down-the-road for other development purposes. Best to use Conda's compiler stacks where possible, if you plan on using our Conda MOOSE packages for other development (I see that you have an inactive (moose) Conda environment:\nCONDA_BACKUP_GXX=/home/linux/mambaforge3/envs/moose/bin/x86_64-conda-linux-gnu-g++\n# among other variables\n\nFirst things first, remove the Ubuntu system level MPI wrapper:\nsudo apt update\nsudo apt-get remove --auto-remove openmpi-bin\nWith the above complete, the following should no longer report any results:\nwhich mpicc mpicxx mpifort\nNext, install an MPI stack, and other necessary development tools into a new Conda environment called cardinal:\nconda create -n cardinal python=3.10 mpich=4.0.2 \\\n  mpich-mpicc mpich-mpicxx mpich-mpifort \\\n  libxt-devel-cos7-x86_64 libx11-devel-cos7-x86_64 \\\n  bison=3.4 flex packaging jinja2 yaml cmake libtool \\\n  m4 autoconf automake make zlib zfp=0.5.5\nActivate this new environment, set some needed variables.\n(Note: you must do this step every time you open a new WSL Terminal window and when you wish to perform anything Cardinal related):\nconda activate cardinal\nexport CC=mpicc CXX=mpicxx FC=mpifort F90=mpif90 F77=mpif77\nexport C_INCLUDE_PATH=$CONDA_PREFIX/include    # necessary for a successful HDF5 build in PETSc\nNow you can make the attempt at building PETSc, libMesh, WASP, and finally Cardinal:\nexport MOOSE_JOBS=6 METHODS=opt METHOD=opt     # This is for good measure, and to speed things up\ncd $HOME/cardinal\ncontrib/moose/scripts/update_and_rebuild_petsc.sh\n\n#  If the above command succeeds, move on to building libMesh and then WASP:\ncontrib/moose/scripts/update_and_rebuild_libmesh.sh\ncontrib/moose/scripts/update_and_rebuild_wasp.sh\n\n# If the above completed without errors, build Cardinal:\nmake -j 6\n\n# If the above completed without errors, run Cardinal tests:\n./run_tests -j 6",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8931879",
                          "updatedAt": "2024-03-27T17:49:27Z",
                          "publishedAt": "2024-03-27T17:45:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "5trayheart"
                          },
                          "bodyText": "Thanks for your detailed comment.\nI activated the cardinal environment and built PETSc.\nHere is a new error I got.\n\ncontrib/moose/scripts/update_and_rebuild_petsc.sh\n/home/linux/cardinal/contrib/moose/scripts\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nThe version of PETSc you are using is out-of-date, we recommend updating to the new release\n Available Version: 3.20.5   Installed Version: 3.20.3\nhttps://petsc.org/release/download/\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n(...)\n\nTESTING: check from config.libraries(config/BuildSystem/config/libraries.py:168)\n*********************************************************************************************\n           UNABLE to CONFIGURE with GIVEN OPTIONS (see configure.log for details):\n---------------------------------------------------------------------------------------------\n  Downloaded hdf5 could not be used. Please check install in\n  /home/linux/cardinal/contrib/moose/petsc/arch-moose\n*********************************************************************************************\n\nThere was an error. Exiting...\n\nI tried building the libmesh without success of buliding PETSc.\n\n./contrib/moose/scripts/update_and_rebuild_libmesh.sh\nNo LSB modules are available.\nPETSc submodule will be used. PETSc submodule is our default solver.\nIMPORTANT: If you did not run the update_and_rebuild_petsc.sh script yet, please run it before building libMesh\n\n(...)\n\n<<< Found PETSc 3.20.3 installation in /home/linux/cardinal/contrib/moose/scripts/../petsc ... >>>\nchecking whether we can compile a trivial PETSc program... no\nchecking for TAO support via PETSc... no\nconfigure: error: *** PETSc was not found, but --enable-petsc-required was specified.\nRunning make -j 6...\nmake: *** No targets specified and no makefile found.  Stop.\n\n\nIs this problem caused by installed conda and moose?\nThese were installed briefly in October last year and not updated for a while.",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8933148",
                          "updatedAt": "2024-03-27T21:14:01Z",
                          "publishedAt": "2024-03-27T20:16:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "use backquotes instead of ###### to delineate console output",
                          "url": "https://github.com/idaholab/moose/discussions/27184#discussioncomment-8933221",
                          "updatedAt": "2024-03-27T20:25:59Z",
                          "publishedAt": "2024-03-27T20:25:59Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Job is running very slowly",
          "author": {
            "login": "dewan1988"
          },
          "bodyText": "Dear MOOSE experts\nI am running my job in a cluster for coarse grid CFD simulation with transient scenario and incompressible fluid. I am using 3D domain, and moose FV method. The number of elements in my current domain is 123 k and the mesh is unstructured. I have submitted my job with 2 nodes and 90 processors. But the job is converging very slowly (1 time-steps/hr).\n\n\n\nHow can I speed up the simulation? Could you please give me some suggestions?",
          "url": "https://github.com/idaholab/moose/discussions/26910",
          "updatedAt": "2024-05-29T17:26:07Z",
          "publishedAt": "2024-02-27T23:12:45Z",
          "category": {
            "name": "Q&A Modules: Navier-Stokes"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nWith MOOSE NSFV , you should be using a solve type of Newton not PJFNK.\nWith only 123k elements, so 123k * 4 degrees of freedom, 180 processes is too much. A rule of thumb is you start losing scalability below 20k degrees of freedom. Maybe try only 90 processes and only 48 processes and see if it runs faster this way\nOnce this is done, the next step is to work on the preconditioning rather than use the simple LU direct solve\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8612541",
                  "updatedAt": "2024-02-28T02:14:54Z",
                  "publishedAt": "2024-02-28T02:14:53Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "Dear GiudGiud\nThank you for your kind suggestions. In my cluster, each node contains 64 cores/processors. So, for 123k elements I have used 90 cores. Could you please clarify me how many elements can I allot in a single core/processor.\nThe following is the information of output script\n\nI am solving the NSFV for transient scenario. So, in that case is  NEWTON okay?\n\nWhat should I use the preconditioning type?\nShould I use SMP_NEWTON instead of SMP_PJFNK?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8623024",
                          "updatedAt": "2024-02-28T20:44:13Z",
                          "publishedAt": "2024-02-28T20:44:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Newton is ok. SMP_PJFNK in your input is just a name, the name of the \"Preconditioner\" object.\nIf the solve_type is Newton, you should rename it\n123 k * 4 dofs / 20k dofs / element = 24 cores is fine. You can try more to see if it s faster",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8623092",
                          "updatedAt": "2024-02-28T20:51:39Z",
                          "publishedAt": "2024-02-28T20:51:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "I changed the preconditioning in this way but still it is slow as like previous.\n\nThis is the situation after 30 minutes:\n\nThough I solving for Finite volume but it is showing \" Finite Element Types: MONOMIAL.",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8624909",
                          "updatedAt": "2024-02-29T01:47:44Z",
                          "publishedAt": "2024-02-29T01:47:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Though I solving for Finite volume but it is showing \" Finite Element Types: MONOMIAL.\n\nA constant mononmial is the same as a finite volume variable\nThis only shows 250s of run time?\nHow many processes are you using now?\nOptimizing this will get you a few dozen percent improvement. You can also set the pc_mat_solver_package parameter in the petsc_options. mumps tends to be faster, though the others are worth a try too",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8626141",
                          "updatedAt": "2024-02-29T05:26:21Z",
                          "publishedAt": "2024-02-29T05:26:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "are you using RhieChow velocity interpolation and upwinding?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8626145",
                          "updatedAt": "2024-02-29T05:26:49Z",
                          "publishedAt": "2024-02-29T05:26:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "Dear GiudGiud\nThank you for your kind reply. Yes, I am using RhieChow velocity interpolation and upwinding. As per your suggestions, I have changed the preconditioning part:\n\nNow, the linear iteration passing so fast but the non linear iteration part is slow and it's not converging\n\nCurrently, I am using 24 cores also.\nShould I make any change in the executioner part?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8634861",
                          "updatedAt": "2024-02-29T19:18:11Z",
                          "publishedAt": "2024-02-29T19:18:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYou can paste snippets between triple quotes instead of screenshots\n```\npc_mat_solver_package goes in addition to the pc_type and the pc_factor_shift options\nyou can try to turn off the line search. There's advice on these convergence issues there\nhttps://mooseframework.inl.gov/moose/application_usage/failed_solves.html",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8634989",
                          "updatedAt": "2024-02-29T19:32:35Z",
                          "publishedAt": "2024-02-29T19:31:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "I have changed the preconditioning part in this way\n[Preconditioning]                                                     \n  [./SMP_NEWTON]                                                          \n    type = SMP\n    full = true\n    solve_type = 'NEWTON'\n    petsc_options_iname = '-pc_mat_solver_package -pc_type -pc_factor_shift_type'\n    petsc_options_value = 'mumps lu NONZERO'\n[../]\n\nBut still  it is very slow\nI am using this mesh file.",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8636799",
                          "updatedAt": "2024-03-11T18:29:11Z",
                          "publishedAt": "2024-02-29T23:42:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "LU does not scale to larger meshes. LU is a resilient and easy way to solve smaller sized problems. Once the problem gets big enough, you can see very large slowdowns and often run out of memory.\nTo get performance you ll want to examine the state of the art for solving these equations and set up the petsc options to use the desired preconditioners.\nFor incompressible flow, we are working on designing a field split preconditioning that does scale. There's an example in\nmodules/navier_stokes/tests/finite_volume/ins/channel-flow/2d-rc-no-slip.i\nIf it works for you great. If not, then you'll need to find the right combination of options for your problem.\nOur other lead is to use segregated solvers instead. We have implemented a SIMPLE scheme in MOOSE, but it's limited to steady problems for now. We are about to incorporate an optimized linear system solver for it that will solve a lot of these issues. It's all rather on the edge, you can try it if you are comfortable with C++.",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8750006",
                          "updatedAt": "2024-03-11T18:34:04Z",
                          "publishedAt": "2024-03-11T18:33:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dewan1988"
                  },
                  "bodyText": "Dear GiudGiud\nI am using the following PETSC option and values and executioner. The case initially converged 160 time steps in 12 hours and then when I restarted my simulation from the previous time steps it converges 108 time steps in 12 hours. Is it okay? Or are there any suggestions to increase the convergence rate? Am I making any mistake to select the PETSC option and values? Total elements in my domain is 123K and I am using 1 node and 64 processors to make the faster convergence, though 24 processors are enough for this case. I ran this simulation with 24, 48 processors also, I have found double time steps converges for 60 processors than 24, and for 48 it is 1.5 times higher than 24 processors. I have also tried to turn off the line search, but it's not working here.\n[Preconditioning] \n  [./SMP_NEWTON]                          \n    type = SMP\n    full = true\n    solve_type = 'NEWTON'\n    petsc_options_iname = '-pc_mat_solver_package -pc_type -pc_factor_shift_type'\n    petsc_options_value ='mumps ilu NONZERO'\n[]\nand following executioner:\n[Executioner]                              \n  type = Transient\n  [./TimeStepper]\n    type = IterationAdaptiveDT           \n    growth_factor = 1.5\n     optimal_iterations = 8                 \n    linear_iteration_ratio = 50             \n      dt =0.001                               \n    cutback_factor = 0.8                     \n    cutback_factor_at_failure = 0.8       \n  [../]\n dtmin = 1e-6                                   \n  dtmax = 0.03\n nl_rel_tol = 1e-4                            \n nl_abs_tol = 1e-4\n nl_max_its = 150\n  l_tol = 1e-4                                   \n l_max_its = 100\n start_time = 7.79978                         \n end_time  = 10000                            \n #end_time  = 4000\n num_steps = 10000\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8739647",
                  "updatedAt": "2024-03-11T17:34:31Z",
                  "publishedAt": "2024-03-11T02:12:11Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The ILU preconditioning seemingly is not scaling. You can try asm pc with ilu sub-pc with some asm_overlap. that's usually still scalable and it may be able to solve your problem if ILU is already working for you.\nUsing too many processors is a problem if the solver does not scale. We recommend 1 proc per 20k dofs, so here I would say 24 processes is good (120k elements * 4 dofs / 20k = 24)\nbut maybe 24 is too much still. It's worth a try reducing it",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8750043",
                          "updatedAt": "2024-03-11T18:37:34Z",
                          "publishedAt": "2024-03-11T18:37:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "Dear GiudGiud\nThank you for your kind suggestions. I have tried different combinations of the petsc_options and values. Here, I have attached 3 sets:\n1. #petsc_options_iname = '-pc_mat_solver_package -pc_type -pc_factor_shift_type'\n   #petsc_options_value ='mumps asm NONZERO'\n\n2. petsc_options_iname = '-pc_mat_solver_package -pc_type -sub_pc_type -pc_asm_overlap -sub_pc_factor_levels -pc_factor_shift_type' \n    petsc_options_value ='mumps asm ilu 2 1 NONZERO'\n\n3. #petsc_options_iname = '-pc_mat_solver_package -pc_type -pc_factor_shift_type'\n   #petsc_options_value ='mumps ilu NONZERO'\n\nIf I directly use the sets 1/ 2, my case doesn't converge. But when I use the 3rd set first by setting the nl_tol & l_tol: 1e-4 and 1e-3 respectively instead of 1e-6 and 1e-5 and run the simulation for few hours and then using the restart option and the 2nd set of the petsc option iname and value (with nl_tol: 1e-6, l_tol:1e-5) for next simulation then my simulation converge faster then the previous.\nThis time I have used only 24 processors for the 123k elements and got 100 time-steps convergence in 6 hours. But If I increase the nl_tol and l_tol by 10 times number of time steps will increase.\n[Preconditioning]  \n [./SMP_NEWTON]                            \n    type = SMP\n    full = true\n    solve_type = 'NEWTON'\n#petsc_options_iname = '-pc_mat_solver_package -pc_type -pc_factor_shift_type'\n#petsc_options_value ='mumps asm NONZERO'\npetsc_options_iname = '-pc_mat_solver_package -pc_type -sub_pc_type -pc_asm_overlap -sub_pc_factor_levels -pc_factor_shift_type' petsc_options_value ='mumps asm ilu 2 1 NONZERO'\n\t\n#petsc_options_iname = '-pc_mat_solver_package -pc_type -pc_factor_shift_type'\n#petsc_options_value ='mumps ilu NONZERO'\n[]\n\nBut my other case where the elements number is 400k, this process doesn't work. If I use the 3rd petsc option with 81 processors:\n    #petsc_options_iname = '-pc_mat_solver_package -pc_type -pc_factor_shift_type'\n   #petsc_options_value ='mumps ilu NONZERO'\n\nIt shows following error:\n  0 Nonlinear |R| = ^[[32m1.594528e-01^[[39m\nSTRUMPACK: out of memory!\nSTRUMPACK: out of memory!\nSTRUMPACK: out of memory!\n\n===================================================================================\n=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES\n=   PID 124201 RUNNING AT compute-2\n=   EXIT CODE: 9\n=   CLEANING UP REMAINING PROCESSES\n=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES\n===================================================================================\n[proxy:0:1@compute-3] HYD_pmcd_pmip_control_cmd_cb (pm/pmiserv/pmip_cb.c:899): assert (!closed) failed\n[proxy:0:1@compute-3] HYDT_dmxu_poll_wait_for_event (tools/demux/demux_poll.c:76): callback returned error status\n[proxy:0:1@compute-3] main (pm/pmiserv/pmip.c:169): demux engine error waiting for event\nsrun: error: compute-3: task 1: Exited with exit code 7\n\nBut when I use the 2nd set of the petsc option iname and value with nl_tol and l_tol:1e-2 then it converge. Do you have any suggestions for this case?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8819406",
                          "updatedAt": "2024-03-18T01:54:27Z",
                          "publishedAt": "2024-03-17T16:08:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If I directly use the sets 1/ 2, my case doesn't converge. But when I use the 3rd set first by setting the nl_tol & l_tol: 1e-4 and 1e-3 respectively instead of 1e-6 and 1e-5 and run the simulation for few hours and then using the restart option and the 2nd set of the petsc option iname and value (with nl_tol: 1e-6, l_tol:1e-5) for next simulation then my simulation converge faster then the previous.\n\nso it seems you need to use the 3rd one to initialize, then when you are closer to physical solutions the 2nd one is enough. And it's faster than the 3rd one so that's good to know.\n\nBut If I increase the nl_tol and l_tol by 10 times number of time steps will increase.\n\nThis does not seem right? These tolerances are used within each time step. Ideally the solver tolerances do not affect the time dependence of the problem when they are tight enough\n\nSTRUMPACK: out of memory!\n\nThis makes me think there's no mumps option for ILU. You should check the petsc manual.\n\nBut when I use the 2nd set of the petsc option iname and value with nl_tol and l_tol:1e-2 then it converge. Do you have any suggestions for this case?\n\ncan you try to initialize with a smaller processor count and the 3rd option?\nOr use a the 120k element simulation to initialize the 400k one? Are they on the same geometry?\nFor both of these techniques you wont be able to use a checkpoint restart anymore, but you can use an exodus restart with small dof counts like this",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8822199",
                          "updatedAt": "2024-03-18T01:59:06Z",
                          "publishedAt": "2024-03-18T01:59:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "Dear GiudGiud\nYes, they are on the same geometry.\n//can you try to initialize with a smaller processor count and the 3rd option?\nyes, I have tried but it shows out of memory.\n//Or use a the 120k element simulation to initialize the 400k one?\nI have done this by running the simulation with 120k and then comment the initial conditions and used the 2nd petsc option but it works only for l_tol and nl_tol: 1e-2. After few hours running the simulation when I decrease the the tolerance level to 1e-3 it doesn't converge, l/nl iteration part become static.\nI didn't understand your last line.\n//For both of these techniques you wont be able to use a checkpoint restart anymore, but you can use an exodus restart with small dof counts like this.//\nCurrently, I use checkpoint restart by the following command in my input file.\n[Problem]\n  fv_bcs_integrity_check = true\n  coord_type = 'XYZ'             # Problem geometry is 3D.'\n  restart_file_base = Unstr_no_Plugs_hirjet_Arsen_out_cp/LATEST\n[]\n\n[Outputs]\n  exodus = true\n  csv = false\n  checkpoint = true\n[]\n\nIf I want to exodus restart how can I modify the input file, because I have to restart my simulation for every 12 hours.\nIs there any other option to run the big case ( with 400k or more than 400k elements)?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8854623",
                          "updatedAt": "2024-03-20T16:02:40Z",
                          "publishedAt": "2024-03-20T15:23:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\n\nyes, I have tried but it shows out of memory.\n\nCan you try taking other steps to reduce memory consumption?\nMainly trying distributed meshes to reduce that memory cost, and working on a less costly preconditioner\n\nworks only for l_tol and nl_tol: 1e-2.\n\nthis basically means it does not work. You should be able to use scaling and get stronger convergence.\n\nIf I want to exodus restart how can I modify the input file, because I have to restart my simulation for every 12 hours.\n\nSee this https://mooseframework.inl.gov/source/meshgenerators/FileMeshGenerator.html#31f200a5-eaa5-46ff-92ad-284aea51bd63\nCheckpoint restart does not work for using different meshes between the simulation and the checkpoint",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8855413",
                          "updatedAt": "2024-03-20T16:23:11Z",
                          "publishedAt": "2024-03-20T16:23:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "Dear GiudGiud\nI have splited the mesh file (400k elements) in 81 chanks and the by using the following command in the command line:\n$ moose-app-opt -i your_input.i --split-mesh 81 --split-file foo.cpr\nand then ran the simulation using the following command without making any change on the input file.\n$ mpiexec -n 81 moose-app-opt -i your_input.i --use-split --split-file foo.cpr\nBut I am getting the following error:\n'''\n*** ERROR ***\nThe following error occurred in the object \"mixing_len_aux_ker\", of type \"WallDistanceMixingLengthAux\".\nWallDistanceMixingLengthAux only supports replicated meshes.\n'''\nDoes it mean that the splited mesh will not work here, only the replicated mesh (copying the whole mesh file for each processor) will work that I have used earlier?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8867337",
                          "updatedAt": "2024-03-21T15:07:27Z",
                          "publishedAt": "2024-03-21T15:07:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "yeah you are out of luck for doing it in a single step. This technique does not work yet with distributed meshes.\nWe potentially will have this later this year, it s not decided yet.\nwhat you are going to have to do, is first have a fake simulation that saves the wall distance in an auxvariable, then load that instead of using an auxkernel",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8867388",
                          "updatedAt": "2024-03-21T15:11:16Z",
                          "publishedAt": "2024-03-21T15:11:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "What I have understood is that I need to run the simulation as usual for some times then I have to comment the walldistanceaxu kernel block and rerun the simulation by using the splited mesh file. Am I right?\nIf my understanding is right then the worry is my case (400k elements) is not converging, how the walldistance value will be saved in the auxvariable?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8867677",
                          "updatedAt": "2024-03-21T15:33:23Z",
                          "publishedAt": "2024-03-21T15:33:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "the 400k case is not converging, but it probably would if we initialized it from the solution of the 120k problem",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8867770",
                          "updatedAt": "2024-03-21T15:40:45Z",
                          "publishedAt": "2024-03-21T15:40:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "I also tried that but it didn't work.\nI have done this by the following way like:\nI have run the simulation for 120k elements for some time for zero intial conditions for the variables and after converging it for some time-steps I changed the geometry and commented the initial conditions and rerun the simulation from the beginning but the simulation didn't converge.",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-8869485",
                          "updatedAt": "2024-03-21T18:03:01Z",
                          "publishedAt": "2024-03-21T18:03:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "I'm going to start a new thread because the previous one is massive and contains a conda tangent.\nMy first question is: what is your Reynolds number like? This will influence what type of precondtioning/solver method you can apply",
                  "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9551893",
                  "updatedAt": "2024-05-24T22:04:57Z",
                  "publishedAt": "2024-05-24T22:04:56Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Some notes on petsc options:\n\n-pc_mat_solver_package is an old option and is not understood by current PETSc. Please use -pc_factor_mat_solver_type instead\n-pc_type ilu in parallel can only use strumpack as the matrix factorization package, so if you do ilu in parallel there is no need to use the option above. If you use lu, then you have additional packages available like -pc_factor_mat_solver_type mumps (one that @GiudGiud highlighted; this is also the default lu factorization package), -pc_factor_mat_solver_type superlu_dist\nConsider passing -options_left on the command line to make sure that PETSc is taking your PETSc options. If you use a recent enough MOOSE this actually happens for you automatically",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9551932",
                          "updatedAt": "2024-05-24T22:17:51Z",
                          "publishedAt": "2024-05-24T22:17:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "@lindsayad\njust wondering what would be the best one for nonlinear problem (porousflow with partially saturated with transient)\ni'm currently using\n  petsc_options_iname = '-pc_type -pc_hypre_type'\n  petsc_options_value = 'hypre    boomeramg'\n\nnot sure if ilu or anything else is better? what do you think?\nKind regards,\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9565119",
                          "updatedAt": "2024-05-27T01:56:44Z",
                          "publishedAt": "2024-05-27T01:56:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "If your problem is elliptic, then multigrid could very well be the best choice. If your system has a saddle point or is indefinite, then you will probably want to look into a field split, which is what we are pursuing in Navier-Stokes whether through segregation at the nonlinear solve level or at the Krylov solve level. If your problem or a subproblem is advection dominated, then preconditioning that is still an open area of research. I have had good luck with advection dominated problem with the inexact LU with compression that you get from -pc_type ilu -pc_factor_mat_solver_type strumpack. HPDDM is another interesting package that we are exploring for advection dominated problems as well",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9581251",
                          "updatedAt": "2024-05-28T13:37:13Z",
                          "publishedAt": "2024-05-28T13:36:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "@lindsayad\nThe Reynolds number in my case is 5029.\nCurrently, I am using the following petsc options and it is working for some cases but if the inlet section is so refined then it is not converging:\n   petsc_options_iname = '-pc_mat_solver_package -pc_type -sub_pc_type -pc_asm_overlap -sub_pc_factor_levels -pc_factor_shift_type'\n    petsc_options_value ='mumps asm ilu 2 1 NONZERO'\n\nSo, according to your suggestions should I change the -pc_mat_solver_package mumps to -pc_factor_mat_solver_type mumps and -pc_type lu?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9583117",
                          "updatedAt": "2024-05-28T17:08:34Z",
                          "publishedAt": "2024-05-28T16:09:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "What does a typical time step's solve look like? Please use backticks to create code blocks",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9583738",
                          "updatedAt": "2024-05-28T17:12:30Z",
                          "publishedAt": "2024-05-28T17:12:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "@lindsayad, following is the solution of a single time step.\nTime Step 54, time = 0.669691, dt = 0.061222\nPre-SMO residual: 0.00793607\n0 Nonlinear |R| = 7.936072e-03\n0 Linear |R| = 7.936072e-03\n1 Linear |R| = 2.755572e-03\n2 Linear |R| = 2.489114e-04\n3 Linear |R| = 8.918983e-05\n4 Linear |R| = 3.903350e-05\n5 Linear |R| = 2.321761e-05\n6 Linear |R| = 1.570184e-05\n7 Linear |R| = 1.177219e-05\n8 Linear |R| = 8.527621e-06\n9 Linear |R| = 6.361247e-06\n10 Linear |R| = 5.009492e-06\n11 Linear |R| = 4.198449e-06\n12 Linear |R| = 3.617002e-06\n13 Linear |R| = 3.226332e-06\n14 Linear |R| = 2.971982e-06\n15 Linear |R| = 2.877857e-06\n16 Linear |R| = 2.730112e-06\n17 Linear |R| = 2.371489e-06\n18 Linear |R| = 2.099327e-06\n19 Linear |R| = 1.779490e-06\n20 Linear |R| = 1.549898e-06\n21 Linear |R| = 1.282474e-06\n22 Linear |R| = 7.987862e-07\n23 Linear |R| = 5.283119e-07\n24 Linear |R| = 3.874162e-07\n25 Linear |R| = 2.751974e-07\n26 Linear |R| = 2.354450e-07\n27 Linear |R| = 2.224174e-07\n28 Linear |R| = 2.161625e-07\n29 Linear |R| = 2.149922e-07\n30 Linear |R| = 2.149620e-07\n31 Linear |R| = 2.149618e-07\n32 Linear |R| = 2.149583e-07\n33 Linear |R| = 2.149477e-07\n34 Linear |R| = 2.149434e-07\n35 Linear |R| = 2.149425e-07\n36 Linear |R| = 2.149329e-07\n37 Linear |R| = 2.148642e-07\n38 Linear |R| = 2.147871e-07\n39 Linear |R| = 2.146930e-07\n40 Linear |R| = 2.146689e-07\n41 Linear |R| = 2.146659e-07\n42 Linear |R| = 2.146480e-07\n43 Linear |R| = 2.146332e-07\n44 Linear |R| = 2.145068e-07\n45 Linear |R| = 2.143169e-07\n46 Linear |R| = 2.140054e-07\n47 Linear |R| = 2.137964e-07\n48 Linear |R| = 2.134966e-07\n49 Linear |R| = 2.131924e-07\n50 Linear |R| = 2.121424e-07\n51 Linear |R| = 2.078384e-07\n52 Linear |R| = 1.977328e-07\n53 Linear |R| = 1.643942e-07\n54 Linear |R| = 1.167995e-07\n55 Linear |R| = 7.513696e-08\n1 Nonlinear |R| = 5.371659e-03\n0 Linear |R| = 5.371659e-03\n1 Linear |R| = 2.152574e-03\n2 Linear |R| = 2.117357e-04\n3 Linear |R| = 7.037866e-05\n4 Linear |R| = 3.111360e-05\n5 Linear |R| = 1.800756e-05\n6 Linear |R| = 1.209162e-05\n7 Linear |R| = 8.956012e-06\n8 Linear |R| = 6.348354e-06\n9 Linear |R| = 4.671962e-06\n10 Linear |R| = 3.632113e-06\n11 Linear |R| = 2.999884e-06\n12 Linear |R| = 2.542629e-06\n13 Linear |R| = 2.239342e-06\n14 Linear |R| = 2.058640e-06\n15 Linear |R| = 1.995802e-06\n16 Linear |R| = 1.918659e-06\n17 Linear |R| = 1.672280e-06\n18 Linear |R| = 1.474329e-06\n19 Linear |R| = 1.248174e-06\n20 Linear |R| = 1.096758e-06\n21 Linear |R| = 9.154800e-07\n22 Linear |R| = 5.793961e-07\n23 Linear |R| = 3.893507e-07\n24 Linear |R| = 2.839240e-07\n25 Linear |R| = 1.986226e-07\n26 Linear |R| = 1.671127e-07\n27 Linear |R| = 1.561103e-07\n28 Linear |R| = 1.509873e-07\n29 Linear |R| = 1.500069e-07\n30 Linear |R| = 1.499917e-07\n31 Linear |R| = 1.499917e-07\n32 Linear |R| = 1.499890e-07\n33 Linear |R| = 1.499799e-07\n34 Linear |R| = 1.499753e-07\n35 Linear |R| = 1.499748e-07\n36 Linear |R| = 1.499658e-07\n37 Linear |R| = 1.499078e-07\n38 Linear |R| = 1.498348e-07\n39 Linear |R| = 1.497643e-07\n40 Linear |R| = 1.497552e-07\n41 Linear |R| = 1.497548e-07\n42 Linear |R| = 1.497517e-07\n43 Linear |R| = 1.497471e-07\n44 Linear |R| = 1.496584e-07\n45 Linear |R| = 1.495224e-07\n46 Linear |R| = 1.492976e-07\n47 Linear |R| = 1.491667e-07\n48 Linear |R| = 1.489239e-07\n49 Linear |R| = 1.487431e-07\n50 Linear |R| = 1.478794e-07\n51 Linear |R| = 1.455867e-07\n52 Linear |R| = 1.397970e-07\n53 Linear |R| = 1.193011e-07\n54 Linear |R| = 8.658228e-08\n55 Linear |R| = 5.442514e-08\n56 Linear |R| = 3.577573e-08\n2 Nonlinear |R| = 2.310320e-03\n0 Linear |R| = 2.310320e-03\n1 Linear |R| = 1.037535e-04\n2 Linear |R| = 1.412109e-05\n3 Linear |R| = 4.377150e-06\n4 Linear |R| = 1.988819e-06\n5 Linear |R| = 1.094745e-06\n6 Linear |R| = 7.003765e-07\n7 Linear |R| = 4.550137e-07\n8 Linear |R| = 2.925521e-07\n9 Linear |R| = 1.971289e-07\n10 Linear |R| = 1.318675e-07\n11 Linear |R| = 9.674303e-08\n12 Linear |R| = 7.260223e-08\n13 Linear |R| = 5.628509e-08\n14 Linear |R| = 4.661584e-08\n15 Linear |R| = 4.356202e-08\n16 Linear |R| = 4.137240e-08\n17 Linear |R| = 3.791217e-08\n18 Linear |R| = 3.519399e-08\n19 Linear |R| = 3.151400e-08\n20 Linear |R| = 2.898091e-08\n21 Linear |R| = 2.521020e-08\n22 Linear |R| = 2.072345e-08\n3 Nonlinear |R| = 1.648891e-05\n0 Linear |R| = 1.648891e-05\n1 Linear |R| = 1.378066e-06\n2 Linear |R| = 1.445614e-07\n3 Linear |R| = 3.646037e-08\n4 Linear |R| = 1.951068e-08\n5 Linear |R| = 1.672635e-08\n6 Linear |R| = 1.565961e-08\n7 Linear |R| = 1.522822e-08\n8 Linear |R| = 1.476997e-08\n9 Linear |R| = 1.441115e-08\n10 Linear |R| = 1.384423e-08\n11 Linear |R| = 1.297924e-08\n12 Linear |R| = 1.173304e-08\n13 Linear |R| = 1.025172e-08\n14 Linear |R| = 8.908641e-09\n15 Linear |R| = 7.846652e-09\n16 Linear |R| = 7.511634e-09\n17 Linear |R| = 7.329518e-09\n18 Linear |R| = 7.257570e-09\n19 Linear |R| = 7.234636e-09\n20 Linear |R| = 7.227560e-09\n21 Linear |R| = 7.217352e-09\n22 Linear |R| = 7.184781e-09\n23 Linear |R| = 7.167217e-09\n24 Linear |R| = 7.163254e-09\n25 Linear |R| = 7.153134e-09\n26 Linear |R| = 7.108224e-09\n27 Linear |R| = 7.020037e-09\n28 Linear |R| = 6.775274e-09\n29 Linear |R| = 6.133723e-09\n30 Linear |R| = 4.670658e-09\n31 Linear |R| = 3.945391e-09\n32 Linear |R| = 3.471780e-09\n33 Linear |R| = 3.273104e-09\n34 Linear |R| = 3.089447e-09\n35 Linear |R| = 2.977621e-09\n36 Linear |R| = 2.934418e-09\n37 Linear |R| = 2.864268e-09\n38 Linear |R| = 2.834157e-09\n39 Linear |R| = 2.811701e-09\n40 Linear |R| = 2.808687e-09\n41 Linear |R| = 2.808193e-09\n42 Linear |R| = 2.803674e-09\n43 Linear |R| = 2.798858e-09\n44 Linear |R| = 2.786982e-09\n45 Linear |R| = 2.782442e-09\n46 Linear |R| = 2.781110e-09\n47 Linear |R| = 2.780495e-09\n48 Linear |R| = 2.780412e-09\n49 Linear |R| = 2.779556e-09\n50 Linear |R| = 2.777879e-09\n51 Linear |R| = 2.772376e-09\n52 Linear |R| = 2.765887e-09\n53 Linear |R| = 2.750714e-09\n54 Linear |R| = 2.697276e-09\n55 Linear |R| = 2.541699e-09\n56 Linear |R| = 2.161808e-09\n57 Linear |R| = 1.862531e-09\n58 Linear |R| = 1.425345e-09\n59 Linear |R| = 1.134934e-09\n60 Linear |R| = 8.116414e-10\n61 Linear |R| = 6.734633e-10\n62 Linear |R| = 5.704222e-10\n63 Linear |R| = 4.380400e-10\n64 Linear |R| = 3.246971e-10\n65 Linear |R| = 2.696393e-10\n66 Linear |R| = 2.429193e-10\n67 Linear |R| = 2.238164e-10\n68 Linear |R| = 2.118089e-10\n69 Linear |R| = 2.052268e-10\n70 Linear |R| = 2.013866e-10\n71 Linear |R| = 2.006392e-10\n72 Linear |R| = 1.998965e-10\n73 Linear |R| = 1.994812e-10\n74 Linear |R| = 1.991883e-10\n75 Linear |R| = 1.990412e-10\n76 Linear |R| = 1.990121e-10\n77 Linear |R| = 1.985506e-10\n78 Linear |R| = 1.984032e-10\n79 Linear |R| = 1.983576e-10\n80 Linear |R| = 1.982347e-10\n81 Linear |R| = 1.982326e-10\n82 Linear |R| = 1.982201e-10\n83 Linear |R| = 1.981510e-10\n84 Linear |R| = 1.981099e-10\n85 Linear |R| = 1.980806e-10\n86 Linear |R| = 1.975740e-10\n87 Linear |R| = 1.958138e-10\n88 Linear |R| = 1.925315e-10\n89 Linear |R| = 1.811159e-10\n90 Linear |R| = 1.553111e-10\n4 Nonlinear |R| = 1.162941e-08\nSolve Converged!\nFinished Solving",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9584456",
                          "updatedAt": "2024-05-28T18:35:18Z",
                          "publishedAt": "2024-05-28T18:35:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "that solve really doesn't look too bad. It just sounds like it's very slow? How long for a single timestep?",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9588193",
                          "updatedAt": "2024-05-29T03:49:02Z",
                          "publishedAt": "2024-05-29T03:49:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewan1988"
                          },
                          "bodyText": "@lindsayad\nEarlier it was very slow because I did use different preconditioning options. Then GiudGiud recommended some preconditioning options what I am using now. Then it becomes faster then previous. In my previous simulation I used 64 cores only for 50k elements without proper calculations. Now I use the number of processors/cores by calculating as follows: no_elements*4 dfs/20,000. Earlier I got only 175 time_steps solve in 12 hrs and 64 cores but now I am getting more than 175 time steps  solves with only 6 cores/processors.",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9596267",
                          "updatedAt": "2024-05-29T16:53:21Z",
                          "publishedAt": "2024-05-29T16:53:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "What I will say is that solving transient 3D highish Reynolds number problems with a decent number of elements is an active area of work for us. As @GiudGiud mentioned earlier @grmnptr has developed steady segregated solvers that are doing quite well for moderately fine meshes with finite volume. I have shown some nice results using finite elements for high Reynolds and fine meshes. But we do not yet have a great solver for transient, high Reynolds, medium fidelity mesh, finite volume",
                          "url": "https://github.com/idaholab/moose/discussions/26910#discussioncomment-9596574",
                          "updatedAt": "2024-05-29T17:26:07Z",
                          "publishedAt": "2024-05-29T17:26:07Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Unable to find closest node in contact",
          "author": {
            "login": "jin0123456"
          },
          "bodyText": "Hello,\nThis error occurred in my simulation,\nFailure in NearestNodeThread because solution contains inf or not-a-number entries. This is likely due to a failed factorization of the Jacobian matrix. \nNonlinear solve did not converge due to DIVERGED_LINE_SEARCH iterations 0\n\n*** ERROR ***\nUnable to find closest node!\n\nAnd I find it in NearestNodeThread.C,\nif (closest_distance == std::numeric_limits<Real>::max())\n    {\n      for (unsigned int k = 0; k < n_neighbor_nodes; k++)\n      {\n        const Node * cur_node = &_mesh.nodeRef(neighbor_nodes[k]);\n        if (std::isnan((*cur_node)(0)) || std::isinf((*cur_node)(0)) ||\n            std::isnan((*cur_node)(1)) || std::isinf((*cur_node)(1)) ||\n            std::isnan((*cur_node)(2)) || std::isinf((*cur_node)(2)))\n          throw MooseException(\n              \"Failure in NearestNodeThread because solution contains inf or not-a-number \"\n              \"entries.  This is likely due to a failed factorization of the Jacobian \"\n              \"matrix.\");\n      }\n      mooseError(\"Unable to find nearest node!\");\n    }\n\nI want to know what this error represents and how can I fix it?",
          "url": "https://github.com/idaholab/moose/discussions/27728",
          "updatedAt": "2024-05-29T11:35:27Z",
          "publishedAt": "2024-05-29T03:07:26Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nAre you using mesh deformation?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27728#discussioncomment-9588206",
                  "updatedAt": "2024-05-29T03:50:17Z",
                  "publishedAt": "2024-05-29T03:50:15Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jin0123456"
                          },
                          "bodyText": "yes.",
                          "url": "https://github.com/idaholab/moose/discussions/27728#discussioncomment-9588455",
                          "updatedAt": "2024-05-29T04:19:00Z",
                          "publishedAt": "2024-05-29T04:18:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "It's breaking for some reason. Probably the mesh is too deformed\nMake sure to set\nPreset = false to the boundary conditions on displacements\nAnd make the time step smaller\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/27728#discussioncomment-9588898",
                          "updatedAt": "2024-05-29T05:34:42Z",
                          "publishedAt": "2024-05-29T05:34:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jin0123456"
                          },
                          "bodyText": "I did give a wrong boundary condition. I added an extra Pressure bc that caused the deformation to be too large.\nMuch appreciated!",
                          "url": "https://github.com/idaholab/moose/discussions/27728#discussioncomment-9592655",
                          "updatedAt": "2024-05-29T11:35:27Z",
                          "publishedAt": "2024-05-29T11:35:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Unused material property in the postprocessor",
          "author": {
            "login": "alimostafavi24"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nDear MOOSE community,\nI want to calculate the following integral in the domain.\n\\[ \\int_{\\Omega} \\rho(c) (u^2 + v^2) \\, d\\Omega \\]\nrho is a material property function of c (order parameter). c, u, v are non-linear variables.\nHere is my .h and .C files.\n#pragma once\n\n#include \"ElementIntegralPostprocessor.h\"\n#include \"MooseVariableInterface.h\"\n#include \"TimeKernel.h\"\n\n/**\n * This postprocessor computes a volume integral of the specified variable.\n *\n * Note that specializations of this integral are possible by deriving from this\n * class and overriding computeQpIntegral().\n */\nclass TotalKE : public ElementIntegralPostprocessor,\n                                             public MooseVariableInterface<Real>\n{\npublic:\n  static InputParameters validParams();\n\n  TotalKE(const InputParameters & parameters);\n\nprotected:\n  virtual Real computeQpIntegral() override;\n\n  /// Holds the solution at current quadrature points\n  const VariableValue & _u;\n  /// Holds the solution gradient at the current quadrature points\n  const VariableGradient & _grad_u;\n\n\n\n  const VariableValue & _vel_x;\n\n\n    const VariableValue & _vel_y;\n\n\n  const VariableValue & _c;\n\n  \n  const MaterialProperty<Real> & _rho;\n};\n\n#include \"TotalKE.h\"\n// MOOSE includes\n\n\nregisterMooseObject(\"MooseApp\", TotalKE);\n\nInputParameters\nTotalKE::validParams()\n{\n  InputParameters params = ElementIntegralPostprocessor::validParams();\n  params.addRequiredCoupledVar(\"variable\", \"The name of the variable that this object operates on\");\n  params.addRequiredCoupledVar(\"vel_x\", \"vel_x\");\n    params.addRequiredCoupledVar(\"vel_y\", \"vel_y\");\n  params.addRequiredCoupledVar(\"c\", \"c\");\n\n  params.addClassDescription(\"Computes a volume integral of the specified variable\");\n  params.addParam<MaterialPropertyName>(\"rho_name\", \"rho\", \"The name of the density\");\n  return params;\n}\n\nTotalKE::TotalKE(\n    const InputParameters & parameters)\n  : ElementIntegralPostprocessor(parameters),\n    MooseVariableInterface<Real>(this,\n                                 false,\n                                 \"variable\",\n                                 Moose::VarKindType::VAR_ANY,\n                                 Moose::VarFieldType::VAR_FIELD_STANDARD),\n    _u(coupledValue(\"variable\")),\n    _grad_u(coupledGradient(\"variable\")),\n    _vel_x(coupledValue(\"vel_x\")),\n    _vel_y(coupledValue(\"vel_y\")),\n\n    _c(coupledValue(\"c\")),\n    _rho(getMaterialProperty<Real>(\"rho_name\"))\n\n\n{\n  addMooseVariableDependency(&mooseVariableField());\n}\n\nReal\nTotalKE::computeQpIntegral()\n{\n // if (_c[_qp] < -0.0)\n // return 0;\n\n  return 0.5 * _rho[_qp] *( _vel_x[_qp] * _vel_x[_qp] + _vel_y[_qp] * _vel_y[_qp]);\n }\n\nSomething above is wrong, because rho is unused parameter as below. It would be great if someone can assist me to fix it.\n/home/ali/Desktop/DROPLET/energydis2/rb.i:546.5: unused parameter 'Postprocessors/total_kinetic_energy/rho'\nThanks,\nAli",
          "url": "https://github.com/idaholab/moose/discussions/27730",
          "updatedAt": "2024-05-29T03:58:06Z",
          "publishedAt": "2024-05-29T03:35:23Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou named the parameter rho_name not rho\nSo in your input file it should not be\nrho=\nIt should be\nRho_name=",
                  "url": "https://github.com/idaholab/moose/discussions/27730#discussioncomment-9588191",
                  "updatedAt": "2024-05-29T03:48:58Z",
                  "publishedAt": "2024-05-29T03:48:57Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "alimostafavi24"
                          },
                          "bodyText": "Thank you so much.",
                          "url": "https://github.com/idaholab/moose/discussions/27730#discussioncomment-9588289",
                          "updatedAt": "2024-05-29T03:58:07Z",
                          "publishedAt": "2024-05-29T03:58:06Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error in gap heat transfer based mortar",
          "author": {
            "login": "jin0123456"
          },
          "bodyText": "Hello,\nI want to use the existing function provided in MOOSE to solve a gap heat transfer problem. As my problem involves contact heat transfer so it needs coupling Solid Mechanics module to acquire contact pressure. Directly I chose motar way, but it gave me the error\nThe requested mortar interface AutomaticMortarGeneration object does not yet exist!\nI put my input and mesh there. https://github.com/jin0123456/didactic-pancake\nI wonder if it's my mesh problem. In motar way is there a requirement that the lengths of the two sides match? As my sides don't match.\nMuch appreciated for advice.",
          "url": "https://github.com/idaholab/moose/discussions/27683",
          "updatedAt": "2024-05-29T03:12:59Z",
          "publishedAt": "2024-05-21T03:14:26Z",
          "category": {
            "name": "Q&A Modules: Thermal Hydraulics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@gambka",
                  "url": "https://github.com/idaholab/moose/discussions/27683#discussioncomment-9509446",
                  "updatedAt": "2024-05-21T14:31:44Z",
                  "publishedAt": "2024-05-21T14:31:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "gambka"
                  },
                  "bodyText": "Your setup looks to have the thermal contact model test_thm.i as the Master app with the mechanical contact calculation (test_mech.i) as the subapp. Here, you are passing the interface_normal_lm through a transfer from the sub to master app. I've not seen this type of setup before for coupled mechanical and thermal contact. Is there any reason you do not have both mechanical and thermal contact in the same input file as is done in the test moose/modules/combined/test/tests/gap_heat_transfer_mortar/finite-2d/varied_pressure_thermomechanical_mortar.i? Have you verified that your simulation runs when it all in one input? If so, then I suspect it is related the mesh creation or the multiapp transfers. I cannot run your inputs as there are several non-MOOSE objects. If you have a MOOSE only example that generates the same error using your multiapp setup, it would be easier to debug.\nFor the meshing, you create the lower D mesh blocks manually in the master app, but in the sub app they are created automatically by the mechanical contact [Contact] block. Are these blocks consistent? It may be that the master app is requesting the interface_normal_lm which has not yet been created in the sub app yet. To answer your question about differing lengths of the sidesets in contact, it is fine to have sides of different lengths. I recommend making the primary surface the longer surface in your 2D-RZ setup.",
                  "url": "https://github.com/idaholab/moose/discussions/27683#discussioncomment-9510332",
                  "updatedAt": "2024-05-21T15:36:43Z",
                  "publishedAt": "2024-05-21T15:36:43Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jin0123456"
                          },
                          "bodyText": "Thanks for youe advice about the primary surface setting. For your questions,\n1.I don't have both mechanical and thermal contact in the same input file as it's not converged in the case. I have to split it into two apps.\n2.I have tried this in a test before and it went right. I put them there as closed_gap_thm.i and closed_gap_mech.i.\nhttps://github.com/jin0123456/didactic-pancake\n3.I suppose the problem is not interface_normal_lm transfer as I didn't apply side_contact in thermal_contact.\nThe related settings about contact are the same in my case and test so I suppose it's the mesh problem?",
                          "url": "https://github.com/idaholab/moose/discussions/27683#discussioncomment-9515253",
                          "updatedAt": "2024-05-22T01:24:02Z",
                          "publishedAt": "2024-05-22T01:24:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "The coupling between thermal and mechanical contact is often quite tight. You are unlikely to get better overall simulation convergence by splitting them between multiapps. If you are having issues converging thermomechnical contact in a single input, then I think we need to focus our debugging there, not on the multiapp case",
                          "url": "https://github.com/idaholab/moose/discussions/27683#discussioncomment-9551453",
                          "updatedAt": "2024-05-24T20:36:16Z",
                          "publishedAt": "2024-05-24T20:36:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jin0123456"
                          },
                          "bodyText": "OK, I will try it in a single input.",
                          "url": "https://github.com/idaholab/moose/discussions/27683#discussioncomment-9587891",
                          "updatedAt": "2024-05-29T03:13:00Z",
                          "publishedAt": "2024-05-29T03:12:59Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Some Questions on Coupling SPPARKS and MyTrim Modules in Magpie",
          "author": {
            "login": "lvkas521424"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nDear MOOSE and Magpie community maintainers,\nHello, I have a general understanding of the compilation process of MAGPIE by downloading and running it, but I still have many questions. I referred to this question #18986.\n\n\nAre spparks and mytrim co-programmed into libmagpie-opt.so? Instead of them programming their own dynamic link library\n\n\nWhere are the .lo and .d files generated by compiling spparks and mytrim used and compiled into dynamic link libraries?\n\n\nWhere in the magpie code do they interact with the contents of spparks and mytrim?\n\n\nlvkas",
          "url": "https://github.com/idaholab/moose/discussions/27727",
          "updatedAt": "2024-05-29T01:55:02Z",
          "publishedAt": "2024-05-29T01:02:46Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@dschwen @snschune",
                  "url": "https://github.com/idaholab/moose/discussions/27727#discussioncomment-9587333",
                  "updatedAt": "2024-05-29T01:55:03Z",
                  "publishedAt": "2024-05-29T01:55:02Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "A puzzle about the INSADMass kernel",
          "author": {
            "login": "Always-kimi"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A Navier Stokes is the most appropriate category for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (avoid screenshots if possible, triple back quotes before/after pasted text, etc)\n\nQuestion\nHi there,\nI have a puzzle about the INSADMass kernel. Why does the INSADMass kernel need the variable pressure and not the variable velocity.\n  [mass]\n    type = INSADMass\n    variable = p\n    use_displaced_mesh = true\n  []\n  [mass_pspg]\n    type = INSADMassPSPG\n    variable = p\n    use_displaced_mesh = true\n  []\n\nI checked the source file for the INSAMass kernel and it seems call the INSADMaterial. The latter needs two variables which are velocity and pressure.\nregisterMooseObject(\"NavierStokesApp\", INSADMaterial);\n\nInputParameters\nINSADMaterial::validParams()\n{\n  InputParameters params = Material::validParams();\n  params.addClassDescription(\"This is the material class used to compute some of the strong \"\n                             \"residuals for the INS equations.\");\n  params.addRequiredCoupledVar(\"velocity\", \"The velocity\");\n  params.addRequiredCoupledVar(NS::pressure, \"The pressure\");\n  params.addParam<MaterialPropertyName>(\"mu_name\", \"mu\", \"The name of the dynamic viscosity\");\n  params.addParam<MaterialPropertyName>(\"rho_name\", \"rho\", \"The name of the density\");\n  return params;\n}\n\nI feel confused why the variable in the continuity equation is pressure.\nAdditional information\nNo response",
          "url": "https://github.com/idaholab/moose/discussions/27719",
          "updatedAt": "2024-05-29T01:30:34Z",
          "publishedAt": "2024-05-28T13:48:46Z",
          "category": {
            "name": "Q&A Modules: Navier-Stokes"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "grmnptr"
                  },
                  "bodyText": "You can write the incompressible Navier-Stokes equations in a matrix notation (with operators as the blocks):\n| A    B | |u| = |f1|\n| B^T  0 | |p|   |0| \n\n$B$ is the gradient operator and $B^T$ is the divergence operator. In moose, the variable parameter in the kernel definition will determine where the residual entries go in the nonlinear system. You see by (just doing R=matrix*sol-rhs for example) that the residual from $B^T u$ will actually go to the parts of the residual vector which are associated with the pressure degrees of freedom. This is actually the reason why it is hard to solve the Navier-Stokes equations, you have a 0 pressure diagonal and you are forced to solve it by using either a segregated approach or a Schurr-complement-based approach.",
                  "url": "https://github.com/idaholab/moose/discussions/27719#discussioncomment-9581533",
                  "updatedAt": "2024-05-28T14:01:59Z",
                  "publishedAt": "2024-05-28T13:59:08Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Always-kimi"
                          },
                          "bodyText": "Oh, I got it. Thank you very much!",
                          "url": "https://github.com/idaholab/moose/discussions/27719#discussioncomment-9587189",
                          "updatedAt": "2024-05-29T01:30:35Z",
                          "publishedAt": "2024-05-29T01:30:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}