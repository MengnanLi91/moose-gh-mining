{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wNy0xOFQwMjowNTo1My0wNTowMM4AUkJF"
    },
    "edges": [
      {
        "node": {
          "title": "Terminator based on max(feature_volumes)",
          "author": {
            "login": "Thomas-Ulrich"
          },
          "bodyText": "Hi,\nI have the following code:\n[Postprocessors]\n   [./flood_count]\n     type = FeatureFloodCount\n     variable = stress_excess\n     # Must be turned on to build data structures necessary for FeatureVolumeVPP\n     compute_var_to_feature_map = true\n     #threshold = 1e5\n     threshold = 2.9e6\n     execute_on = 'timestep_end'\n     block = 'Fault1D Fault2D'\n   [../]\n[]\n\n[VectorPostprocessors]\n  [./feature_volumes]\n    type = FeatureVolumeVectorPostprocessor\n    flood_counter = flood_count\n    execute_on = 'initial timestep_end'\n    output_centroids = True\n    single_feature_per_element = True\n  [../]\n[]\n\nwhich generates the following output (prefix_feature_volumes_0001.csv):\ncentroid_x,centroid_y,centroid_z,feature_volumes,intersects_bounds,intersects_specified_bounds,percolated,var_num\n4204.4794184423,2993.9039228218,-2984.99526698,264.76911241796,0,0,0,0\n4209.0122646856,2989.0377517617,-2953.8534362391,637.41810668695,0,0,0,0\n4209.0122646856,2989.0377517617,-2953.8534362391,637.41810668694,0,0,0,0\n4204.4794184423,2993.9039228218,-2984.99526698,264.76911241796,0,0,0,0\n\nI would like to trigger the terminator based on the maximum value of detected feature_volumes (if any).\n(let say if max(feature_volumes) > 1000, triggers the Terminator.\nDo you think it is possible?\nThanks in advance,\nThomas Ulrich.",
          "url": "https://github.com/idaholab/moose/discussions/24989",
          "updatedAt": "2023-07-19T11:34:13Z",
          "publishedAt": "2023-07-19T09:54:46Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "Thomas-Ulrich"
                  },
                  "bodyText": "Found the answer:\nhttps://mooseframework.inl.gov/source/postprocessors/VectorPostprocessorReductionValue.html",
                  "url": "https://github.com/idaholab/moose/discussions/24989#discussioncomment-6487855",
                  "updatedAt": "2023-07-19T11:34:14Z",
                  "publishedAt": "2023-07-19T11:34:13Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Question about reproduce GWB result using geochemistry modules",
          "author": {
            "login": "MengnanLi91"
          },
          "bodyText": "Hi,\nI am working on reproducing a GWB test using geochemistry module but only able to match part of the brine concentrations. I suspect there are something wrong with CO2 solubility calculations but couldn't figure it out. Could you give me some hints on this? I've attached the GWB input, MOOSE input and the corresponding results from both GWB and MOOSE.\nThanks so much for your help!\nMOOSE files:\nMOOSE_files.zip\nGWB files:\nGWB_files.zip",
          "url": "https://github.com/idaholab/moose/discussions/24988",
          "updatedAt": "2023-07-19T02:14:02Z",
          "publishedAt": "2023-07-18T22:56:51Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "@WilkAndy or @cpgr; would one of you mind taking a look?",
                  "url": "https://github.com/idaholab/moose/discussions/24988#discussioncomment-6483029",
                  "updatedAt": "2023-07-18T22:57:36Z",
                  "publishedAt": "2023-07-18T22:57:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "cpgr"
                          },
                          "bodyText": "This one might need you @WilkAndy",
                          "url": "https://github.com/idaholab/moose/discussions/24988#discussioncomment-6483881",
                          "updatedAt": "2023-07-19T02:14:03Z",
                          "publishedAt": "2023-07-19T02:14:02Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "[PorousFlow] Varying aperture as missing dimension 2D in 2D?",
          "author": {
            "login": "1runer"
          },
          "bodyText": "Hi,\nProbably a question to @WilkAndy or @cpgr.\nI read the explanation regarding lower-dimensional representation of fractures MultiApp Fracture example, which sounds quite reasonable to me. I wonder if I can translate the 1D in 2D solution also to 2D in 2D assuming the fracture aperture is the missing out-of-plane dimension.\nFor example having a 2D rectangular fracture, the missing dimension in MOOSE would be assumed to be '1'. As long as the aperture is constant, I would expect a porosity of '1' and permeability of 'a\u00b2/12' should work out. The fluid mass e.g. outflowing on a boundary should just be corrected by the aperture as missing dimension.\nHaving an varying aperture in space, the problem becomes more difficult. Following the webpage, I expect, I have to scale the porosity (like 2D in 3D) by 'a * phi' and permeability as '(a\u00b2 * a) / 12'?\nAccordingly the diffusion gets also scaled by porosity and diffusion coefficient.\nBut how would other PorousFlow objects and kernels, like  Dispersion PorousFlowDispersiveFlux or\nmass-flux-dependent objects like PorousFlowSquarePulsePointSource, properly scaled?\nIt should make a difference if I inject 1 kg/s in a 1m fracture or 1mm-wide fracture.\nBest",
          "url": "https://github.com/idaholab/moose/discussions/24796",
          "updatedAt": "2023-07-19T00:26:48Z",
          "publishedAt": "2023-06-23T01:45:02Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@WilkAndy @cpgr if we could please get assistance on this, it's work happening at INL. Thank you!",
                  "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6294627",
                  "updatedAt": "2023-06-27T14:25:33Z",
                  "publishedAt": "2023-06-27T14:25:32Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@JerryLiu2023",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6294696",
                          "updatedAt": "2023-06-27T14:32:24Z",
                          "publishedAt": "2023-06-27T14:32:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "Let's push",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6443795",
                          "updatedAt": "2023-07-13T22:16:14Z",
                          "publishedAt": "2023-07-13T22:16:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cpgr"
                          },
                          "bodyText": "Oops - I saw this and then forgot about it!\nI suppose the kernels should be ok as is, as the mass balance will use the scaled porosity so they should just work?\nI also think that your point source mass should be unchanged as once you have scaled the porosity/permeability in the 'scaled' case Darcy's equation is the same.\nI'm pretty sure I have done something similar with a 2D model representing a thin 3D model, and that you get the same spatial distribution using either a scaled porosity/permeability or a scaled injection rate.\nYou could do two simple models to convince yourself that they are the same - one with a bigger injection, and one with scaled porosity/permeability.",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6445432",
                          "updatedAt": "2023-07-14T03:37:50Z",
                          "publishedAt": "2023-07-14T03:37:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "@cpgr Thanks for your answer.\nTo scale porosity/perm (and each other component of the entire mass balance equation) sounds reasonable to me.\nBut for the mass source, even after two weeks of thinking, I don't come to a really good solution.\nI'm think you are right, to scale poro/perm by aperture or to divide the Dirac by aperture (to get a bigger injection).\nBut I'm not sure, if this is the 'expected' solution or if we have to scale poro/perm AND a Dirac (likePorousFlowSquarePulsePointSource) in the same way?\nIsn't any Dirac just another component of the mass balance at some points, so e.g. it has to be scaled too?\nI'm a bit confused, as e.g. the colleagues from the Golem App scale their Dirac Point Source according to the elemental dimension with aperture or borehole diameter.\nPlease correct me if I am thinking completely wrong.",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6453368",
                          "updatedAt": "2023-07-14T21:26:07Z",
                          "publishedAt": "2023-07-14T21:26:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cpgr"
                          },
                          "bodyText": "Stepping back, are you injecting only in the fracture? If so, say your 'real life' is injection 1 kg/s. Then you want to do that in your dirac kernel too, and only scale the porosity and permeability by aperture as you discussed.",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6454081",
                          "updatedAt": "2023-07-15T01:02:39Z",
                          "publishedAt": "2023-07-15T01:02:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "Thanks @cpgr for the answer.\nOk, I hopefully got it now.\nScaling porosity and permeability is similar to increasing the flow resistance by narrowing down my flow path (fracture) from a 1 m aperture (if the out-of-plane dimension is unscaled) to the 'real' aperture.\nAs my injection should still be e.g. 1 kg/s into the scaled fracture, it will cause a very high pressure increase as apertures are typically in millimeter range and so 3 orders in magnitude lower.\nBut then it is still not fully clear to me, why a Dirac mass source, e.g. 1D in 3D (a injection well in matrix) or 2D in 3D (fracture flow with matrix) is scaled in other Moose-based apps like Golem in Dirac Point Source ? I know that the borehole is implemented differently in PorousFlow.",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6461793",
                          "updatedAt": "2023-07-16T18:37:56Z",
                          "publishedAt": "2023-07-16T18:37:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cpgr"
                          },
                          "bodyText": "From a quick poke around the source code of Golem, it looks like the scaling they have is to non-dimensionalise the variables rather than an aperture correction.",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6463018",
                          "updatedAt": "2023-07-17T00:28:08Z",
                          "publishedAt": "2023-07-17T00:28:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "If you look in GolemMaterialBase is a scaling_factor introduced as material property for a 1D well as cross sectional area and for a 2D fracture as aperture.\nThis is reused in the Dirac  as a multiplier.",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6472366",
                          "updatedAt": "2023-07-17T23:11:13Z",
                          "publishedAt": "2023-07-17T23:11:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cpgr"
                          },
                          "bodyText": "Perhaps they aren't scaling their porosity in their time kernel (if you don't scale porosity, then you would scale the injection rate)?",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6472511",
                          "updatedAt": "2023-07-17T23:55:38Z",
                          "publishedAt": "2023-07-17T23:55:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "The TimeDerivative is scaled by the scaling_factor as well.\nMaybe that's the reason why is was confused.\nBut your explanation sounds logic to me.\nI am so interested in the Dirac because I am trying to use a flow and time dependent aperture law. It looks good for longer time steps, but the pressure increase in the very first time steps is difficult to solve and also needs some minimum aperture.\nRadial_fracture_H.i.txt",
                          "url": "https://github.com/idaholab/moose/discussions/24796#discussioncomment-6483198",
                          "updatedAt": "2023-07-18T23:43:41Z",
                          "publishedAt": "2023-07-18T23:43:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "segfault when running tests",
          "author": {
            "login": "jessecarterMOOSE"
          },
          "bodyText": "Trying to run tests on HPC, I get (paraphrasing here...)\n> ./run_tests --help\nBuilding and linking hit...\nBuilding hit for python with python3-config\nmake[1]: Entering directory '..../framework/contrib/hit'\nmake[1]: 'hit' is up to date\nmake[1]: Leaving directory '..../framework/contrib/hit'\nSegmentation fault\n\nMaybe it's one of the imports that the test harness uses that's causing the segfault? Looks like it doesn't even get around to processing the arguments.",
          "url": "https://github.com/idaholab/moose/discussions/24918",
          "updatedAt": "2023-08-15T21:50:05Z",
          "publishedAt": "2023-07-06T21:37:53Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIs this a new copy of moose?\nand what modules are you running?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6378755",
                  "updatedAt": "2023-07-06T22:49:49Z",
                  "publishedAt": "2023-07-06T22:49:48Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Sorry, should have given more details. Fresh install of Moose, running the framework tests. Didn't build any modules yet.",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6379247",
                          "updatedAt": "2023-07-07T00:53:57Z",
                          "publishedAt": "2023-07-07T00:53:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I meant HPC modules",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6379270",
                          "updatedAt": "2023-07-07T00:59:38Z",
                          "publishedAt": "2023-07-07T00:59:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "did you at least load use.moose moose-dev ? (or some other good enough set)",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6379273",
                          "updatedAt": "2023-07-07T01:00:08Z",
                          "publishedAt": "2023-07-07T01:00:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Oh, I'm running on our local HPC. Built petsc, libmesh, and moose fine.",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6379359",
                          "updatedAt": "2023-07-07T01:15:24Z",
                          "publishedAt": "2023-07-07T01:15:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ah ok.\nDoes MOOSE run fine on a simple test?",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6379424",
                          "updatedAt": "2023-07-07T01:27:00Z",
                          "publishedAt": "2023-07-07T01:27:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Because you built libMesh, can you post your libmesh_diagnostic.log? That contains a ton of helpful information about your working environment.",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6384124",
                          "updatedAt": "2023-07-07T12:28:46Z",
                          "publishedAt": "2023-07-07T12:28:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "@GiudGiud yes I can run MOOSE with a simple input problem (simple_diffusion)\n@milljm no I'm on an airgapped cluster \u2639\ufe0f what kind of info are you looking for?",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6480515",
                          "updatedAt": "2023-07-18T16:46:12Z",
                          "publishedAt": "2023-07-18T16:46:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I ask for the diagnostic logs when I myself do not know what I am looking for\nThey do usually illuminate environment concerns, paths to binaries with versions in them (openmpi-4.3.1/bin \"oh I see you are using OpenMPI, version 4.3.1\"). Paths to things along with modules loaded which may interfere (you have Python-3.10 loaded as a module, yet a path to python reveals you are still using /usr/bin/python, etc). Paths to compilers mixed with paths to other compilers (GCC and Intel compilers possibly colliding for example).\nHonestly, lots of things...",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6480802",
                          "updatedAt": "2023-07-18T17:19:31Z",
                          "publishedAt": "2023-07-18T17:17:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "I'll comb through it to see if anything jumps out. Given that PETSc/libmesh/MOOSE all get built and run fine, my instinct is that it has something to do with my python setup since it crashes on loading the TestHarness.",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6480889",
                          "updatedAt": "2023-07-18T17:26:43Z",
                          "publishedAt": "2023-07-18T17:26:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "ok I think I got this figured out. I was using the system-installed conda environment. I made my own minimal conda environment and it worked.",
                          "url": "https://github.com/idaholab/moose/discussions/24918#discussioncomment-6481967",
                          "updatedAt": "2023-07-18T19:38:23Z",
                          "publishedAt": "2023-07-18T19:38:22Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "periodic boundary condition implementation",
          "author": {
            "login": "karthichockalingam"
          },
          "bodyText": "Hi,\nThis a general question to understand how periodic boundary conditions are implemented in MOOSE.\nMy initial thought was to come up with a relation between the periodic nodes (so I can eliminate one set) on a regular rectilinear mesh.\nLet the relation between the nodes be as follows,\nx = P x\u2019\nSay for 1-D problem with two elements\n(1)-------------(2)------------(3)\nP = [1 0, 0 1, 1 0]\nx = [u1 u2 u3]\nx\u2019 = [u1 u2]\nand solve\n[P^T A P] x\u2019 = P^T b\nI don\u2019t think [P^T A P] is deterministic. So I am missing something?\nThank you very much for your help.\nKind regards,\nKarthik.",
          "url": "https://github.com/idaholab/moose/discussions/24984",
          "updatedAt": "2023-07-18T19:31:55Z",
          "publishedAt": "2023-07-18T16:17:43Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "@roystgnr",
                  "url": "https://github.com/idaholab/moose/discussions/24984#discussioncomment-6480581",
                  "updatedAt": "2023-07-18T16:52:46Z",
                  "publishedAt": "2023-07-18T16:52:45Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "roystgnr"
                  },
                  "bodyText": "[P^T A P] is indeed how we do things under the hood, in libMesh.  The final solution is then well-defined and completely determined (or at least as close to that as you can get in a world where threading and MPI have to assume associativity for speed but yet floating-point operations are not in general associative).  But you're correct that the specific P isn't yet well-defined, because e.g. for any pair of Degrees of Freedom with a 1-to-1 mapping you can always pick either one of them to be the \"independent\" DoF and the other to be the \"constrained\" DoF.  It's an especially giant pain when you have multiple periodic constraints intersecting, like at the corners of a periodic square or the corners or edges of a periodic cube.  On a distributed mesh a node's owner might not even see the node that (via a constraint of a constraint) it is ultimately tied to.\nOn adaptively refined grids, we always constrain finer elements' DoFs to coarser ones, so we can do adaptive constraints and periodic constraints in one swoop ... but when that's not enough to disambiguate things we just use element and node id() comparisons to make the remaining choices.  This is deterministic in the critical \"no disagreements between different processors in parallel\" sense, and technically it's deterministic in the sense that we never assign ids randomly, but since ids can end up being influenced by input file, mesh partitioning, mesh renumbering settings, etc, it's not very \"deterministic\" in the broad spirit of that term.",
                  "url": "https://github.com/idaholab/moose/discussions/24984#discussioncomment-6481019",
                  "updatedAt": "2023-07-18T17:41:23Z",
                  "publishedAt": "2023-07-18T17:41:22Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "karthichockalingam"
                          },
                          "bodyText": "@roystgnr Thank you for your detailed response. Yes, P is not well defined as you pointed out in the case of corners of a periodic square.\nHowever, I have a fundamental question. Please refer to the below-attached image. After applying periodic boundary conditions on a 1-D domain the final resulting matrix (P^T A P) in equation (3.14) has a zero determinant. Why is that?",
                          "url": "https://github.com/idaholab/moose/discussions/24984#discussioncomment-6481258",
                          "updatedAt": "2023-07-18T18:04:50Z",
                          "publishedAt": "2023-07-18T18:04:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "For that differential equation, the infinite-dimensional linear operator itself has a null space!  -\u0394u = 0 is true for any \u00fb\u2261C with constant C, and this constant satisfies the periodic boundary conditions too.  So for any solution u there's a whole family of solutions u+c\u00fb and the problem isn't able to distinguish between them.  You need something to further constrain the problem (make it a time-dependent PDE with initial conditions, for example) if you want a well-defined unique solution, or if you actually don't care which of the family of solutions you come up with then you can add an artificial constraint or use a null-space-aware algebraic solver.\nUnder the hood in libMesh, there's one less interesting catch: we use a square (non-invertible) rather than a rectangular P, so we end up with a P^T A P that has zero-rows and -columns for the constrained DoFs, rather than a smaller matrix of only unconstrained DoFs' terms.  We have various options (e.g. throwing the constraint equations themselves into those rows) for how to solve that, and we hope to make \"just use the smaller matrix\" an option too in the future.",
                          "url": "https://github.com/idaholab/moose/discussions/24984#discussioncomment-6481613",
                          "updatedAt": "2023-07-18T18:49:07Z",
                          "publishedAt": "2023-07-18T18:49:06Z",
                          "isAnswer": true
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "karthichockalingam"
                          },
                          "bodyText": "Thank you so much for your response. Now it all makes sense!",
                          "url": "https://github.com/idaholab/moose/discussions/24984#discussioncomment-6481923",
                          "updatedAt": "2023-07-18T19:31:42Z",
                          "publishedAt": "2023-07-18T19:31:42Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "FeatureFloodCount on lower_dimensional_block (2D surface in 3D mesh)",
          "author": {
            "login": "Thomas-Ulrich"
          },
          "bodyText": "Hi,\nI'm modeling fluid injection in a 3D geothermal system, including lower-dimensional features such as faults (2D surface and 1D well). I'm trying to check at each time step the areas of connected patches of faults that are overstressed (that is that comply with a given failure criterion).\nTo compute the failure criterion, I compute normal and shear reactions on the 2D fault based on the 3D stress and the fault normal orientation.\ne.g. normal_traction = stress_ij * normal_i* normal_j  where normal is the fault local normal.\nIn my mesh, I have a sideset Fault1 and an associated LowerDBlock Fault1LD, e.g.:\n[Mesh]\n  [./file]\n    type = FileMeshGenerator\n    file = ExtendedBottom_1703_small.msh\n  [../]\n  [./Fault1LDgenerator]\n    type = LowerDBlockFromSidesetGenerator\n    new_block_id = 111\n    new_block_name = 'Fault1LD'\n    sidesets = 'Fault1'\n    input = file\n  [../]\n\nIf I use FeatureFloodCount restricted on the sideset 'Fault1`, based on a variable defined on the whole domain (e.g. temperature T), then it works as expected (e.g. FeatureVolumeVectorPostprocessor returns a non zero area):\n   [./flood_count]\n     type = FeatureFloodCount\n     variable = T\n     # Must be turned on to build data structures necessary for FeatureVolumeVPP\n     compute_var_to_feature_map = true\n     threshold = 1e5\n     execute_on = TIMESTEP_END\n     boundary = 'Fault1'\n   [../]\n\nBut what I need is to use FeatureFloodCount on the block Fault1LD, based on a variable only defined there.\nFor example:\n\n[AuxVariables]\n  [./stress_excess]\n    order = CONSTANT\n    family = MONOMIAL\n  [../]\n[]\n\n  #my user defined aux kernel computing stress_excess only on the 2D elements\n  [AuxKernels]\n    [./stress_excess_ini_ker]\n    type = FunctionAux\n    variable = 'stress_excess'\n    function = '-1e10'\n    execute_on = 'initial'\n  [../]\n\n  [./stress_excess_ker]\n    type = TigerStressExcessAux\n    variable = 'stress_excess'\n    total_stress = stress\n    mu_s = 0.3\n    cohesion = 0\n    execute_on = 'TIMESTEP_END'\n    block = 'Fault1LD Fault2LD'\n  [../]\n  [../]\n  \n   [./flood_count]\n     type = FeatureFloodCount\n     variable = stress_excess\n     # Must be turned on to build data structures necessary for FeatureVolumeVPP\n     compute_var_to_feature_map = true\n     threshold = 1e5\n     execute_on = TIMESTEP_END\n   [../]\n\nThen I get 0 in FeatureVolumeVectorPostprocessor, probably because LowerDBlocks are ignored by FeatureFloodCount?\nHow could I make FeatureFloodCount work on the block Fault1LD only?\nDo I need to modify the code FeatureFloodCount, or could I use existing moose elements to make it work?\nThanks in advance,\nThomas Ulrich.",
          "url": "https://github.com/idaholab/moose/discussions/24879",
          "updatedAt": "2023-07-18T18:53:14Z",
          "publishedAt": "2023-06-30T09:47:28Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\ndoes it work if you boundary restrict the FeatureFloodCount? (Just a guess)\nand did you confirm the auxkernel higher in the pipeline works as expected?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/24879#discussioncomment-6325741",
                  "updatedAt": "2023-06-30T12:59:05Z",
                  "publishedAt": "2023-06-30T12:59:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Thomas-Ulrich"
                          },
                          "bodyText": "Hi,\nThe FeatureFloodCount works if the boundary restricts the FeatureFloodCount when the variable is defined on the volume.\n(in other words, this example modules/phase_field/test/tests/feature_volume_vpp_test/boundary_area_3D_single.i)\nworks).\nI think if I use the boundary restrictions (on FeatureFloodCount), FeatureFloodCount may use the solution on the volume elements containing the boundary, compute the solution on the boundary, and apply the FeatureFloodCount.\nBut my variable (stress excess) is already computed on the fault (on the lower dimensional fault, not the boundary), and not in the volume (well it is only initialized in the volume at the first step to a -1e10).\nThe auxkernel works fine (confirmed looking at the exodus file).",
                          "url": "https://github.com/idaholab/moose/discussions/24879#discussioncomment-6325802",
                          "updatedAt": "2023-06-30T13:07:55Z",
                          "publishedAt": "2023-06-30T13:06:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Thomas-Ulrich"
                          },
                          "bodyText": "Hi,\nSo I m now able to restrict the FeatureFloodCount to the 2D LD elements only, with the following changes:\ndiff --git a/modules/phase_field/src/postprocessors/FeatureFloodCount.C b/modules/phase_field/src/postprocessors/FeatureFloodCount.C\nindex 10cd679cda..7c14feadc6 100644\n--- a/modules/phase_field/src/postprocessors/FeatureFloodCount.C\n+++ b/modules/phase_field/src/postprocessors/FeatureFloodCount.C\n@@ -100,6 +100,7 @@ FeatureFloodCount::validParams()\n {\n   InputParameters params = GeneralPostprocessor::validParams();\n   params += BoundaryRestrictable::validParams();\n+  params += BlockRestrictable::validParams();\n \n   params.addRequiredCoupledVar(\n       \"variable\",\n@@ -191,6 +192,7 @@ FeatureFloodCount::FeatureFloodCount(const InputParameters & parameters)\n     Coupleable(this, false),\n     MooseVariableDependencyInterface(this),\n     BoundaryRestrictable(this, false),\n+    BlockRestrictable(this, false),\n     _fe_vars(getCoupledMooseVars()),\n     _vars(getCoupledStandardMooseVars()),\n     _dof_map(_vars[0]->dofMap()),\n@@ -372,6 +374,9 @@ FeatureFloodCount::execute()\n   {\n     for (const auto & current_elem : _mesh.getMesh().active_local_element_ptr_range())\n     {\n+\n+      if (!hasBlocks(current_elem->subdomain_id()))\n+        continue;\n       // Loop over elements or nodes\n       if (_is_elemental)\n       {\n(moose) ulrich@heisenbug:/export/dump/ulrich/myLibs/moose$ git diff modules/phase_field/include/postprocessors/FeatureFloodCount.h\ndiff --git a/modules/phase_field/include/postprocessors/FeatureFloodCount.h b/modules/phase_field/include/postprocessors/FeatureFloodCount.h\nindex e601a282ca..9f48fd5ece 100644\n--- a/modules/phase_field/include/postprocessors/FeatureFloodCount.h\n+++ b/modules/phase_field/include/postprocessors/FeatureFloodCount.h\n@@ -14,6 +14,7 @@\n #include \"InfixIterator.h\"\n #include \"MooseVariableDependencyInterface.h\"\n #include \"BoundaryRestrictable.h\"\n+#include \"BlockRestrictable.h\"\n \n #include <iterator>\n #include <list>\n@@ -40,7 +41,8 @@ class MooseMesh;\n class FeatureFloodCount : public GeneralPostprocessor,\n                           public Coupleable,\n                           public MooseVariableDependencyInterface,\n-                          public BoundaryRestrictable\n+                          public BoundaryRestrictable,\n+                          public BlockRestrictable\n {\n public:\n   static InputParameters validParams();\nI'm using the following postprocessors:\n[Postprocessors]\n  [./stress_excess_max]\n    type = ElementExtremeValue\n    variable = stress_excess\n    execute_on = ' timestep_end'\n  [../]\n   [./flood_count]\n     type = FeatureFloodCount\n     variable = stress_excess\n     # Must be turned on to build data structures necessary for FeatureVolumeVPP\n     compute_var_to_feature_map = true\n     threshold = 2.9e6\n     execute_on = 'timestep_end'\n     block = 'Fault1D Fault2D'\n   [../]\n  [./Volume]\n    type = VolumePostprocessor\n    execute_on = 'initial'\n  [../]\n  [./volume_fraction]\n    type = FeatureVolumeFraction\n    mesh_volume = Volume\n    feature_volumes = feature_volumes\n    execute_on = 'initial timestep_end'\n  [../]\n[]\n\n[VectorPostprocessors]\n  [./feature_volumes]\n    type = FeatureVolumeVectorPostprocessor\n    flood_counter = flood_count\n    execute_on = 'initial timestep_end'\n    output_centroids = True\n    single_feature_per_element = False\n    #outputs = none\n  [../]\n[]\n\nBut the areas computed by FeatureVolumeVectorPostprocessor are much larger than expected (e.g. when looking with paraview at the file with filter threshold).\nI speculate that the wrong quadrature rule is used, or the wrong Jacobian when computing the integral here when using low-dimensional elements:\nhttps://github.com/idaholab/moose/blob/next/modules/phase_field/src/vectorpostprocessors/FeatureVolumeVectorPostprocessor.C#L229\nWhat do you think?",
                          "url": "https://github.com/idaholab/moose/discussions/24879#discussioncomment-6479340",
                          "updatedAt": "2023-07-18T14:48:18Z",
                          "publishedAt": "2023-07-18T14:48:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@permcody you have worked on that feature right?",
                          "url": "https://github.com/idaholab/moose/discussions/24879#discussioncomment-6479451",
                          "updatedAt": "2023-07-18T14:57:20Z",
                          "publishedAt": "2023-07-18T14:57:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Thomas-Ulrich"
                          },
                          "bodyText": "In the end, if I use single_feature_per_element = True then the area gets realistic values, So I now have a working workflow.",
                          "url": "https://github.com/idaholab/moose/discussions/24879#discussioncomment-6481491",
                          "updatedAt": "2023-07-18T18:34:51Z",
                          "publishedAt": "2023-07-18T18:34:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Thomas-Ulrich"
                          },
                          "bodyText": "But my understanding is that the computeIntegral is not doing what it was intended for because it should not integrate the quantity (e.g. stress excess) but a derived quantity with value 1 if the quantity is above the threshold and 0 if below.",
                          "url": "https://github.com/idaholab/moose/discussions/24879#discussioncomment-6481650",
                          "updatedAt": "2023-07-18T18:53:15Z",
                          "publishedAt": "2023-07-18T18:53:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "FSI dynamics",
          "author": {
            "login": "Eilloo"
          },
          "bodyText": "Hi all,\nI have been trying to use the dynamics module for some simple FSI cases.\nI have had some success with the first input file below, which simulates a 2D flexible flag in perpendicular flow. The behaviour of this case agrees well with the expected results.\nHere is a snapshot of this simulation:\n\nHowever, using a similar setup for geometry which is parallel, rather than perpendicular to the flow, I have encountered a number of problems:\nIt's worth mentioning that how far through the simulation gets before something blows up is quite sensitive to the material properties. This makes sense, as does the fact that stiffer, denser solids are generally handled better than softer, lighter ones. For my actual simulation, I have prescribed, physical properties which approximate rubber.\nThe pattern I keep seeing is very high accelerations in the solid, only at the edges of the solid (shown in the picture). These tend to ramp up to huge values (1e7 and above) within the space of a few (small) timesteps. Initialising the fluid field before starting the simulation, and using inactive_tsteps in the Newmark integrator do not solve this.\nExample of very high accelerations developing at the solid edges. Fluid field is hidden, but moves from left to right:\n\nFor some simulations, the pressure field also drastically changes in magnitude. There is no checker-boarding in space or anything like that, but I have seen the entire region above and below alternate between very high and very low values with each timestep (ie - everything in the top half might be a very high value with everything below the centreline very low. Then, the next time step this will be reversed).\nExample of the alternating, large magnitude pressure field. This is a slightly different geometry to the above image, with a narrower flag:\n\nI have tested a range of mesh resolutions, which seems to have little to no effect.\nI am hoping someone might have insight as to why the first input file works well, but the second input file blows up in this manner. Usually I would be suspicious of boundary conditions, initialisation and so on, but haven's seen anything obvious yet.\nI don't have much experience with the tensor dynamics kernels, so it is very possible I have set something up wrong here.\nInput files are below - I have tried with some variations in the geometry too.\nI appreciate there's a lot of info in here, so please ask if clarification is needed!\nMostly I'm interested in whether I've missed anything key in the tensor dynamics setup, or thoughts on how the dynamics kernels (and/or Newmark time scheme?) would interact with the usual fsi approach.\nAny thoughts are very welcome!\nThanks\nP.S: Some things which may look odd in the input file, but I do not believe to be the cause:\nI am using a multiapp to handle mesh morphing in the fluid region. This has worked well for other situations, and the behaviour described above still occurs if I use the 'normal' method of a diffusion kernel acting on disp_ in the main solve instead.\nThe interface kernels are custom, but only exist to handle the fact velocity is a vector variable. They are also the reason for the 'dummy' variables. The workaround is not especially elegant, but it has been put to the test previously and gives identical results to the existing PenaltyInterfaceDiffusion interface kernels for a range of cases.\nInput file 1: This case works well, and uses an initialised fluid field as the solution IC:\nfilename = exampleFile\nvelocity_FS = 10\n\n[GlobalParams]\n    displacements = 'disp_x disp_y'\n    integrate_p_by_parts = true\n[]\n\ninput_file = 'path_to_initialisation_file'\n\n[Mesh]\n    file = ${input_file}\n[]\n\n[UserObjects]\n    [init_soln_vel_fluid_x]\n        type = SolutionUserObject\n        system_variables = vel_fluid_x\n        mesh = ${input_file}\n        timestep = LATEST\n    []\n    [init_soln_vel_fluid_y]\n        type = SolutionUserObject\n        system_variables = vel_fluid_y\n        mesh = ${input_file}\n        timestep = LATEST\n    []\n    [init_soln_p]\n        type = SolutionUserObject\n        system_variables = p\n        mesh = ${input_file}\n        timestep = LATEST\n    []\n[]\n\n[Functions]\n    [init_cond_func_vel_fluid_x]\n        type = SolutionFunction\n        solution = init_soln_vel_fluid_x\n    []\n    [init_cond_func_vel_fluid_y]\n        type = SolutionFunction\n        solution = init_soln_vel_fluid_y\n    []\n    [init_cond_func_p]\n        type = SolutionFunction\n        solution = init_soln_p\n    []\n[]\n\n[Variables]\n    [vel_fluid]\n        family = LAGRANGE_VEC\n        block = 0\n        [InitialCondition]\n            type = VectorFunctionIC\n            function_x = init_cond_func_vel_fluid_x\n            function_y = init_cond_func_vel_fluid_y\n        []\n    []\n    [p]\n        block = 0\n        [InitialCondition]\n            type = FunctionIC\n            function = init_cond_func_p\n        []\n    []\n    [disp_x]\n    []\n    [disp_y]\n    []\n\n    [dummy_var_fluid]\n        block = 0\n    []\n    [dummy_vec_solid]\n        family = LAGRANGE_VEC\n        block = 1\n    []\n[]\n\n[Kernels]\n    [vel_time]\n      type = INSADMomentumTimeDerivative\n      variable = vel_fluid\n      use_displaced_mesh = true\n    []\n    [fluid_mass]\n        type = INSADMass\n        variable = p\n        use_displaced_mesh = true\n    []\n    [momentum_advection]\n        type = INSADMomentumAdvection\n        variable = vel_fluid\n        use_displaced_mesh = true\n    []\n    [momentum_pressure]\n        type = INSADMomentumPressure\n        variable = vel_fluid\n        pressure = p\n        use_displaced_mesh = true\n    []\n    [momentum_viscous]\n        type = INSADMomentumViscous\n        variable = vel_fluid\n        use_displaced_mesh = true\n    []\n    [mass_pspg]\n        type = INSADMassPSPG\n        variable = p\n        use_displaced_mesh = true\n    []\n    [momentum_supg]\n        type = INSADMomentumSUPG\n        variable = vel_fluid\n        velocity = vel_fluid\n        use_displaced_mesh = true\n    []\n\n    [vel_mesh]                # Correction due to mesh displacement\n        type = ADConvectedMeshFluidVector\n        disp_x = disp_x\n        disp_y = disp_y\n        variable = vel_fluid\n        use_displaced_mesh = true\n    []\n\n      [nullKernel_disp_x]\n        type = NullKernel\n        variable = disp_x\n        block = '0'\n      []\n      [nullKernel_disp_y]\n        type = NullKernel\n        variable = disp_y\n        block = '0'\n      []\n\n    [dummy_vec_nullkernel]\n        type = ADVectorDiffusion        #Can also use time derivative for transient\n        variable = dummy_vec_solid\n    []\n    [dummy_var_fluid_nullkernel]\n        type = ADDiffusion              #Can also use time derivative for transient\n        variable = dummy_var_fluid\n    []\n[]\n\n[InterfaceKernels]\n    [penalty_interface_fluid]\n        type = ADCoupledPenaltyInterfaceDiffusionVectorSideOnly\n        boundary = interface_fluid_side\n        variable = vel_fluid\n        neighbor_var = dummy_vec_solid\n        penalty = 1e6\n        primary_coupled_vector_var = vel_fluid\n        secondary_coupled_var_x = vel_x\n        secondary_coupled_var_y = vel_y\n    []\n    [penalty_interface_solid_x]\n        type = ADCoupledPenaltyInterfaceDiffusionFieldSideOnly\n        boundary = interface_solid_side\n        variable = disp_x\n        neighbor_var = dummy_var_fluid\n        penalty = 1e6\n        primary_coupled_var = vel_x\n        secondary_coupled_vector_var = vel_fluid\n        component = 0\n    []\n    [penalty_interface_solid_y]\n        type = ADCoupledPenaltyInterfaceDiffusionFieldSideOnly\n        boundary = interface_solid_side\n        variable = disp_y\n        neighbor_var = dummy_var_fluid\n        penalty = 1e6\n        primary_coupled_var = vel_y\n        secondary_coupled_vector_var = vel_fluid\n        component = 1\n    []\n[]\n\n\n[BCs]\n    [fluid_inlet]\n        type = ADVectorFunctionDirichletBC\n        function_x = ${velocity_FS}\n        function_y = 0\n        variable = vel_fluid\n        boundary = left\n    []\n    [fluid_outlet]\n        type = ADDirichletBC\n        variable = p\n        value = 0\n        boundary = right\n    []\n    [fluid_noSlip]\n        type = ADVectorFunctionDirichletBC\n        function_x = 0\n        function_y = 0\n        variable = vel_fluid\n        boundary = 'top bottom'\n    []\n\n    [no_disp_x]\n        type = ADDirichletBC\n        variable = disp_x\n        boundary = 'bottom top left right'\n        value = 0\n    []\n    [no_disp_y]\n        type = ADDirichletBC\n        variable = disp_y\n        boundary = 'bottom top left right'\n        value = 0\n    []\n[]\n\n[Modules/TensorMechanics/DynamicMaster]\n    [solid_domain]\n        add_variables = true            #This adds variables disp_, and aux variables vel_ and accel_ if they do not exist\n        strain = SMALL\n        incremental = false\n        use_automatic_differentiation = true\n        volumetric_locking_correction = True\n        block = '1'\n        density = 3e3\n    []\n[]\n\n[Materials]\n    [elasticity_tensor_flap]\n      type = ADComputeIsotropicElasticityTensor\n      youngs_modulus = 4e6\n      poissons_ratio = 0.3\n      block = '1'\n    []\n    [stress]\n      type = ADComputeLinearElasticStress\n      block = '1'\n    []\n    [fluid_properties]\n      type = ADGenericConstantMaterial\n      block = '0'\n      prop_names = 'rho mu T cp k'\n      prop_values = '1  1  288  1  1'\n    []\n    [ins_mat]\n      type = INSADStabilized3Eqn\n      velocity = vel_fluid\n      pressure = p\n      temperature = '288'\n      block = '0'\n      use_displaced_mesh = true\n    []\n[]\n\n[Preconditioning]\n    [SMP]\n      type = SMP\n      full = true\n    []\n[]\n  \n[Executioner]\n    type = Transient\n    end_time = 5\n    nl_rel_tol = 1e-8\n    nl_abs_tol = 1e-10\n    nl_max_its = 25\n  \n    solve_type = 'NEWTON'\n    petsc_options_iname = '-pc_type -pc_factor_shift_type'\n    petsc_options_value = 'lu           NONZERO'\n  \n    line_search = default\n    automatic_scaling = true\n    off_diagonals_in_auto_scaling = true\n  \n    dtmax = 0.01\n    [TimeStepper]\n      type = IterationAdaptiveDT\n      dt = 0.01\n      growth_factor = 1.5\n      cutback_factor = 0.8\n    []\n\n    [TimeIntegrator]\n        type = NewmarkBeta\n        inactive_tsteps = 1\n    []\n[]\n  \n  \n[Outputs]\n    [testOutput]\n        type = Exodus\n        file_base = ${filename}\n    []\n[]\n\n\n[MultiApps]\n    [subApp]\n      type = TransientMultiApp\n      input_files = perpendicularFlap_elasticMesh_sub.i\n      execute_on = timestep_end\n    []\n  []\n  \n  [Transfers]\n    [fluidMeshTransfer_x]\n      type = MultiAppCopyTransfer\n      source_variable = disp_x_subapp\n      variable = disp_x\n      from_multi_app = subApp\n    []\n    [fluidMeshTransfer_y]\n      type = MultiAppCopyTransfer\n      source_variable = disp_y_subapp\n      variable = disp_y\n      from_multi_app = subApp\n    []\n  \n    [toSubAppTransfer_x]\n      type = MultiAppCopyTransfer\n      source_variable = disp_x\n      variable = disp_x_aux\n      to_multi_app = subApp\n    []\n    [toSubAppTransfer_y]\n      type = MultiAppCopyTransfer\n      source_variable = disp_y\n      variable = disp_y_aux\n      to_multi_app = subApp\n    []\n[]\n\nInput file 2: This setup produces outputs such as those shown below. Some of the images are from runs with slightly different material properties.\nfilename = 'exampleFile'\nvelocity_FS = 2 #FSI3\n\n[GlobalParams]\n  displacements = 'disp_x disp_y'   # Displacements for the solid region to be used for calculating the stress later\n  integrate_p_by_parts = true\n[]\n\ninitialisation_file = 'path_to_initialisation_file'\n\n[Mesh]\n  file = ${initialisation_file}\n[]\n\n[UserObjects]\n  [init_fluidVel_x]\n    type = SolutionUserObject\n    system_variables = fluid_vel_x\n    mesh = ${initialisation_file}\n    timestep = LATEST\n  []\n  [init_fluidVel_y]\n    type = SolutionUserObject\n    system_variables = fluid_vel_y\n    mesh = ${initialisation_file}\n    timestep = LATEST\n  []\n  [init_p]\n    type = SolutionUserObject\n    system_variables = p\n    mesh = ${initialisation_file}\n    timestep = LATEST\n  []\n[]\n\n[Functions]\n  [init_cond_fluidVel_x]\n    type = SolutionFunction\n    solution = init_fluidVel_x\n  []\n  [init_cond_fluidVel_y]\n    type = SolutionFunction\n    solution = init_fluidVel_y\n  []\n  [init_cond_p]\n    type = SolutionFunction\n    solution = init_p\n  []\n\n  [inlet_xVel]\n    type = ParsedFunction\n    expression = '1.5*${velocity_FS}*(4/0.1681)*y*(0.41-y)'\n  []\n  [inlet_yVel]\n    type = ParsedFunction\n    expression = 0\n  []\n[]\n\n[Variables]\n  [fluid_vel]\n    family = LAGRANGE_VEC\n    block = 'fluid'\n    [InitialCondition]\n      type = VectorFunctionIC\n      function_x = init_cond_fluidVel_x\n      function_y = init_cond_fluidVel_y\n    []\n  []\n  [p] \n    block = 'fluid'\n    [InitialCondition]\n      type = FunctionIC\n      function = init_cond_p\n    []\n  []\n  [disp_x]\n  []\n  [disp_y\n  []\n\n  [dummy_vec_solid]\n    block = 'solid_cylinder solid_tail'\n    family = LAGRANGE_VEC\n  []\n  [dummy_var_fluid]\n    block = 'fluid'\n  []\n[]\n\n[Kernels]\n  [vel_time]\n    type = INSADMomentumTimeDerivative\n    variable = fluid_vel\n    use_displaced_mesh = true\n  []\n  [fluid_mass]\n    type = INSADMass\n    variable = p\n    use_displaced_mesh = true\n  []\n  [momentum_advection]\n    type = INSADMomentumAdvection\n    variable = fluid_vel\n    use_displaced_mesh = true\n  []\n  [momentum_pressure]\n    type = INSADMomentumPressure\n    variable = fluid_vel\n    pressure = p\n    use_displaced_mesh = true\n  []\n  [momentum_viscous]\n    type = INSADMomentumViscous\n    variable = fluid_vel\n    use_displaced_mesh = true\n  []\n  [mass_pspg]\n    type = INSADMassPSPG\n    variable = p\n    use_displaced_mesh = true\n  []\n  [momentum_supg]\n    type = INSADMomentumSUPG\n    variable = fluid_vel\n    velocity = fluid_vel\n    use_displaced_mesh = true\n  []\n\n  [vel_mesh]                # Correction due to mesh displacement\n    type = ADConvectedMeshFluidVector\n    disp_x = disp_x\n    disp_y = disp_y\n    variable = fluid_vel\n    use_displaced_mesh = true\n  []\n\n  [disp_x_fluid_null]\n    type = NullKernel\n    variable = disp_x\n    block = 'fluid'\n  []\n  [disp_y_fluid_null]\n    type = NullKernel\n    variable = disp_y\n    block = 'fluid'\n  []\n\n  [dummy_vec_nullkernel]\n    type = ADVectorTimeDerivative\n    variable = dummy_vec_solid\n  []\n  [dummy_var_fluid_nullkernel]\n    type = ADTimeDerivative\n    variable = dummy_var_fluid\n  []\n[]\n\n[InterfaceKernels]\n  [penalty_interface_fluid]\n    type = ADCoupledPenaltyInterfaceDiffusionVectorSideOnly\n    boundary = interface_fluid_side\n    variable = fluid_vel\n    neighbor_var = dummy_vec_solid\n    penalty = 1e6\n    primary_coupled_vector_var = fluid_vel\n    secondary_coupled_var_x = vel_x\n    secondary_coupled_var_y = vel_y\n    # use_displaced_mesh = true\n  []\n  [penalty_interface_solid_x]\n    type = ADCoupledPenaltyInterfaceDiffusionFieldSideOnly\n    boundary = interface_solid_side\n    variable = disp_x\n    neighbor_var = dummy_var_fluid\n    penalty = 1e6\n    primary_coupled_var = vel_x\n    secondary_coupled_vector_var = fluid_vel\n    component = 0\n    # use_displaced_mesh = true\n  []\n  [penalty_interface_solid_y]\n    type = ADCoupledPenaltyInterfaceDiffusionFieldSideOnly\n    boundary = interface_solid_side\n    variable = disp_y\n    neighbor_var = dummy_var_fluid\n    penalty = 1e6\n    primary_coupled_var = vel_y\n    secondary_coupled_vector_var = fluid_vel\n    component = 1\n    # use_displaced_mesh = true\n  []\n[]\n\n[BCs]\n  [fluid_inlet]\n    type = ADVectorFunctionDirichletBC\n    variable = fluid_vel\n    boundary = 'inlet'\n    function_x = 'inlet_xVel'\n    function_y = 'inlet_yVel'\n  []\n  [pressure_outlet]\n    type = ADDirichletBC\n    value = 0\n    variable = p\n    boundary = 'outlet'\n  []\n  [fluid_noSlip]\n    type = ADVectorFunctionDirichletBC\n    variable = fluid_vel\n    function_x = 0\n    function_y = 0\n    boundary = 'top bottom'\n  []\n  [no_disp_x]\n    type = ADDirichletBC\n    variable = disp_x\n    boundary = 'inlet outlet top bottom pinnedCylinder'\n    value = 0\n  []\n  [no_disp_y]\n    type = ADDirichletBC\n    variable = disp_y\n    boundary = 'inlet outlet top bottom pinnedCylinder'\n    value = 0\n  []\n[]\n\n[Materials]\n  [elasticity_tensor_cylinder]\n    type = ADComputeIsotropicElasticityTensor\n    youngs_modulus = 210e9\n    poissons_ratio = 0.29\n    block = 'solid_cylinder'\n  []\n  [elasticity_tensor_tail]\n    type = ADComputeIsotropicElasticityTensor\n    youngs_modulus = 5.6e6\n    poissons_ratio = 0.4\n    block = 'solid_tail'\n  []\n  [density_cylinder]\n    type = ADGenericConstantMaterial\n    prop_names = 'density'\n    prop_values = 7e3\n    block = 'solid_cylinder'\n  []\n  [density_tail]\n    type = ADGenericConstantMaterial\n    prop_names = 'density'\n    prop_values = 1e3\n    block = 'solid_tail'\n  []\n  [stress]\n    type = ADComputeLinearElasticStress\n    block = 'solid_cylinder solid_tail'\n  []\n  [fluid_properties]\n    type = ADGenericConstantMaterial\n    block = 'fluid'\n    prop_names = 'rho mu T cp k'\n    prop_values = '1000  1  293  1  1'\n  []\n  [ins_mat]\n    type = INSADStabilized3Eqn\n    velocity = fluid_vel\n    pressure = p\n    temperature = '293'\n    block = 'fluid'\n    alpha = 1.0\n    use_displaced_mesh = true\n  []\n[]\n\n[Modules/TensorMechanics/DynamicMaster]\n  [solid]\n      add_variables = true\n      strain = FINITE\n      use_automatic_differentiation = true\n      volumetric_locking_correction = true\n      block = 'solid_cylinder solid_tail'\n  []\n[]\n\n[Preconditioning]\n  [SMP]\n    type = SMP\n    full = true\n  []\n[]\n\n[Executioner]\n  type = Transient\n  end_time = 30\n  # dt = 0.05\n  nl_rel_tol = 1e-10\n  nl_abs_tol = 1e-12\n  nl_max_its = 30\n\n  solve_type = 'NEWTON'\n  petsc_options_iname = '-pc_type -pc_factor_shift_type'\n  petsc_options_value = 'lu           NONZERO'\n\n  dtmax = 0.001\n  [TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 1E-5\n    growth_factor = 1.5\n    cutback_factor = 0.8\n  []\n\n  [TimeIntegrator]\n    type = NewmarkBeta\n    inactive_tsteps = 1\n  []\n\n[]\n\n\n[Outputs]\n  [testOutput]\n      type = Exodus\n      file_base = ${filename}\n  []\n[]\n\n\n[MultiApps]\n  [subApp]\n    type = TransientMultiApp\n    input_files = 'path_to_subapp.i'\n    execute_on = timestep_end\n  []\n[]\n\n[Transfers]\n  [fluidMeshTransfer_x]\n    type = MultiAppCopyTransfer\n    source_variable = disp_x_subapp\n    variable = disp_x\n    from_multi_app = subApp\n  []\n  [fluidMeshTransfer_y]\n    type = MultiAppCopyTransfer\n    source_variable = disp_y_subapp\n    variable = disp_y\n    from_multi_app = subApp\n  []\n\n  [toSubAppTransfer_x]\n    type = MultiAppCopyTransfer\n    source_variable = disp_x\n    variable = disp_x_aux\n    to_multi_app = subApp\n  []\n  [toSubAppTransfer_y]\n    type = MultiAppCopyTransfer\n    source_variable = disp_y\n    variable = disp_y_aux\n    to_multi_app = subApp\n  []\n[]",
          "url": "https://github.com/idaholab/moose/discussions/24896",
          "updatedAt": "2023-07-18T17:28:31Z",
          "publishedAt": "2023-07-04T14:58:53Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "maxnezdyur"
                  },
                  "bodyText": "Deleted previous, answer because I missed your initialization. I am assuming you are initializing from a steady state solution, if not, I would ramp your velocity BC over time. So the flagpole problem, is harder because the density ratio between the solid and fluid are much closer than the leaflet problem. I'm not sure about those large pressure oscillations in time.",
                  "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6369550",
                  "updatedAt": "2023-07-06T04:03:28Z",
                  "publishedAt": "2023-07-06T04:03:28Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Hi, thanks for the insight.\nRegarding initialisation, the perpendicular problem which works is indeed from a steady state solution. However, the flag parallel to the flow is initialised from the last time step of a fluid-only transient solve, which is stopped a little before we see any vortex shedding.\nI have tried ramping the velocity at the inlet with no luck so far, although I'll experiment a little more with the specific ramp function and duration over which we change the velocity.\nI have wondered if those large pressure oscillations appear because the accelerations become so high? The thinking is that even though the change in displacements between time steps are small, they (incorrectly) happen so fast that we get massive pressures on one side and very small on the other. It's hard to verify this though.\nOut of interest, why is the problem harder to solve when the solid and fluid densities are similar?\nWith this in mind, I tried some runs with the solid density 100x larger. Sure enough, the simulations got much further through; however, they still failed with some odd behaviour.\nOne thing I noticed is that the accelerations in a specific direction actually oscillate on the mesh in a striped pattern, with the velocities looking similar. However, the displacements look fine so maybe this is okay?\nBelow are some images of this, and how the simulation looks when it fails. The fluid domain is hidden in all of these images.\nSince I've not had much luck with the velocity ramp, I will also try ramping the density of the solid down, if I can at least get a higher density version working.\nStriped acceleration in y:\n\nHow things look as they start going wrong:\n\nFinal time step before failure:",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6382753",
                          "updatedAt": "2023-07-07T09:39:54Z",
                          "publishedAt": "2023-07-07T09:39:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "1: Have you tried initializing the fluid field to zero and then ramping up the velocity BC (confused whether you're ramping up with the initialization or without). I want to make sure that there isn't a large pressure spike on the solid part right from the start.\n2: It's a bit of the chicken and egg problem for the large accelerations and the pressure oscillations. They are both probably causing each other with such a tightly coupled system.\n3: One reason the close densities become a problem is the added mass effect the fluid now has on the solid (more explanation in the literature).\n4: The odd behavior you are seeing is basically divergence of the solid system(probably a better word here just can think of it). Fix the root causes and you probably won't see that problem.\n5: Adding a bit of damping into the solid dynamics will probably smooth out the \"stripes\" of acceleration. This can be done with the  hht_alpha parameter in your DynamicMaster block. A value of around -0.1 could be a good start but I don't know if it's optimal.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6384963",
                          "updatedAt": "2023-07-07T13:59:14Z",
                          "publishedAt": "2023-07-07T13:57:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Ah yes, sorry I wasn't clear - I have tried ramping from zero, as well as the original no ramp from initialised. To be sure there is no pressure spike, I have chosen a point around the middle of the upper flag boundary and plotted pressure with time. There is no initial spike, but the pressure immediately oscillates and diverges.\nMakes sense\nThanks, I'll read up on added mass effect - I'd not come across this before.\nAlso makes sense... I guess here we suspect that the root cause is inappropriate initialisation?\nThis isn't something I've tried yet - I will see if it affects either the initialised or ramped velocity cases and report back",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6385315",
                          "updatedAt": "2023-07-07T14:30:06Z",
                          "publishedAt": "2023-07-07T14:30:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "An update here:\nFirstly, setting the hht_alpha parameter as suggested above does indeed stabilise the spatial oscillations in velocity and acceleration.\nIt definitely sounds like added mass effect will be a problem for some of these test cases. However, we should still be able to get something working using a similar geometry, but choosing arbitrary material properties in the fluid to avoid similar densities.\nI went ahead and tried some runs using air as the fluid, and setting the velocity to something where we'd expect some vortex shedding if it were a cylinder-only case to try and get some motion. It's worth noting that the geometry is fractionally offset in y, so this should help too.\nUnfortunately, the behaviour is still diverging. This time, the effects are most noticeable in the fluid field, so I will provide images of these below.\nRegardless of whether I initialise from a steady state run, or use velocity ramping, a similar pattern emerges. The pressure in the whole domain oscillates between two 'solutions' each time step which are a few orders of magnitude apart. The velocity field winds up looking like the image below, even if we initialise it from a steady run.\nThe accelerations and velocities in the solid are similar to before, but smoother with the damping parameter, in that they are high on the solid edge and increase to very high values as the simulation progresses.\nI looked in more detail at a case where we solve just the fluid field by fixing the interface in place as a BC. The rest of the input file was left the same, apart from a no-slip condition also imposed at the interface. Using the steady state run to initialise things and observing the subsequent behaviour of the pressure field, a similar oscillation occurred, but the magnitudes were much less that in the case where the interface is allowed to move. Perhaps the problem is related to the time scheme after all? Although again, the first input file ran without any problems.\nHere are images showing the pressure field at two consecutive time steps, then also the velocity field reached before the simulation stops converging:",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6404227",
                          "updatedAt": "2023-07-10T13:13:38Z",
                          "publishedAt": "2023-07-10T12:59:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "Interesting results. Not sure how to confirm a \"time-scheme\" problem, @GiudGiud might know better about the right person to answer that.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6417055",
                          "updatedAt": "2023-07-11T14:16:21Z",
                          "publishedAt": "2023-07-11T14:16:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "An issue is that the same time integrator is applied to all variables. One already documented issue related to this is #19228 which is about the problems when Crank-Nicolson is applied to pressure. I wouldn't be surprised if there are similar problems here in which Newmark-Beta may not make sense for all variables/physics in the input",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6417558",
                          "updatedAt": "2023-07-11T15:00:16Z",
                          "publishedAt": "2023-07-11T15:00:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "Since the IntertialForce hand-codes the Newmark-Beta scheme within the Kernel when supplying the Velocity and Acceleration AuxKernels (happens when add_variables=true for the DynamicMaster), the time integrator then could be set to BDF2 or ImplicitEuler and test the hypothesis? Very good chances that I missing something with my suggestion.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6418205",
                          "updatedAt": "2023-07-11T15:59:33Z",
                          "publishedAt": "2023-07-11T15:59:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "A similar suspicion was the reason for running the case mentioned in my previous post where I freeze everything solid in place, but leave the kernels there to make sure the time integrator is the same. Here there were definitely some pressure oscillations, but they definitely weren't as bad. To me, this does sound like an inappropriate time integrator.\nI wasn't sure whether MOOSE would only apply one integrator. An interesting scenario is if I set a time integrator to something else in the executioner block, but leave the dynamics master action in place, MOOSE doesn't complain. In fact, when it outputs various information such as the mesh etc. at the start of the simulation, the time integrator is reported as whatever I specified - and not Newmark Beta.\nI have no idea how this works with the dynamics kernels, whether this is overwritten and it is actually using Newmark after all. The simulation results don't look any different though, so I would guess this is the case?\nI'll caveat all of the above by saying this is from memory of what I ran a while ago, and I am not able to re-run things to verify this right now. I'll do so tomorrow and confirm that this behaviour is definitely the case.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6418270",
                          "updatedAt": "2023-07-11T16:05:16Z",
                          "publishedAt": "2023-07-11T16:05:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@cbolisetti @sveerara for behavior of dynamics with non-Newmark-Beta time integration",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6420198",
                          "updatedAt": "2023-07-11T20:11:28Z",
                          "publishedAt": "2023-07-11T20:11:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@recuero might also know too",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6421167",
                          "updatedAt": "2023-07-11T23:25:02Z",
                          "publishedAt": "2023-07-11T23:25:01Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Eilloo"
                  },
                  "bodyText": "To check I understand what's going on here:\nHaving looked at the code for the inertia kernel and the Newmark velocity and acceleration auxkernels, I believe that the velocity and acceleration are calculated using the Newmark scheme regardless of what we choose as the time integrator, since the equation is hard-coded into the aux kernels.\nThe inertia kernel then uses current and old values for velocity and acceleration, but also displacement. Since displacement is not calculated in an auxkernel like this, it will be subject to whatever time integrator was selected in the executioner block.\nThe inertia residual therefore ends up being a combination of different time integrators if we specify something other than Newmark in the executioner.\nI am also conscious that we lose the option of 'inactive timesteps', in the Newmark scheme which I gather could be a problem in some cases.\nI am trying to think about how one might want to manage multiple time integrators if this was an option. At first I thought assigning variables to a particular integrator made sense, but now I see this leads to residual calculations where multiple integrators are used. @lindsayad, when you say different integrators for different equations, would this mean specifying the integrator to be used for each kernel somehow?\nIn terms of how this affects the example case above, I have tried two variants: one with the same fluid properties as the 'broken' input file above, but with the solid density 1000x larger, and the other also with 1000x solid density, but treating the fluid as air. The original case still does not get past the first few time steps.\nBoth are initialised from a steady state run where the solid is fixed, and both used the implicit Euler time scheme in the executioner.\nIn the first case, the simulation starts off well, with no pressure oscillations, and reasonable motion of the flag. A little way in, it stops converging rather abruptly as the elements inside the solid suddenly seem to collapse. Below are two images of consecutive time steps right as this happens. The behaviour seems strange, but it was looking good until that point so I'd say an improvement. Maybe we're back to added mass effect issues or something like that.\n\n\nThe second case struggled more, despite the larger density ratio. There were no pressure oscillations at first, but as the simulation ran the upper/lower oscillation behaviour started to develop. Eventually the fluid velocity field diverged, with large spikes appearing near the solid surface. The image below shows the state of affairs as this is developing. It is also possible to see the vortex shedding frequency in the fluid is quite fast, and the solid is influenced my multiple vortices simultaneously. Perhaps this is simply difficult to solve?",
                  "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6435867",
                  "updatedAt": "2023-07-13T10:17:46Z",
                  "publishedAt": "2023-07-13T08:45:39Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "\"Since displacement is not calculated in an auxkernel like this, it will be subject to whatever time integrator was selected in the executioner block.\"\nI think you are talking about this part _u_old = &this->valueOld();, which just gets the value of the displacement at the previous time step, so no \"time integration\" here. so the inertialForce will not be \"mixing\" time integrators.\nIf you get a chance can you run the 'broken' input file with the correct fluid and solid properties, but initialize field to 0 and ramp the velocity BC?",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6439620",
                          "updatedAt": "2023-07-13T13:53:30Z",
                          "publishedAt": "2023-07-13T13:53:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "No I mean that each physics should have its own time integration scheme. Eg one may use explicit Euler; another may use implicit Euler, etc. It would be uniform across an equations non-time kernels. So an explicit scheme for one equation would use old values of the equation-variable as well as old values of all coupled variables, even if a coupled nonlinear variable was using an implicit scheme for its equation",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6441447",
                          "updatedAt": "2023-07-13T16:47:43Z",
                          "publishedAt": "2023-07-13T16:47:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "That's what is happening now? The solid system is solved with NewmarkBeta via the IntertialForce kernel (hand-coded) and the fluid system is solved with ImplicitEuler via the  INSADTauMaterial using adCoupledVectorDot interface.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6442117",
                          "updatedAt": "2023-07-13T18:05:00Z",
                          "publishedAt": "2023-07-13T18:04:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Yea if you\u2019re confident there\u2019s no side effects then the issues must be elsewhere",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6444170",
                          "updatedAt": "2023-07-13T23:48:50Z",
                          "publishedAt": "2023-07-13T23:48:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "Confidence Level 6/10.\nThere are a few monolithic FSI solvers that use both BDF2 time integrators and NewmarkBeta for time integration in the fluid and solid domains respectively and either mortar coupling or some other type interface method for coupling between the domains. So unless MOOSE is doing some overriding of the time integrator (couldn't find anything that did that) not sure if there are side effects from using using both.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6452714",
                          "updatedAt": "2023-07-14T19:22:22Z",
                          "publishedAt": "2023-07-14T19:22:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "It wouldn't do anything funny with uses of the TimeIntegrator class. My only concern would be accidental use of u_dot, u_dotdot, etc. computed by the BDF2 time integrator in residual objects or materials that should be performing Newmark-Beta.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6468036",
                          "updatedAt": "2023-07-17T13:49:42Z",
                          "publishedAt": "2023-07-17T13:49:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Eilloo"
                  },
                  "bodyText": "Yes, that is the line I was talking about.\nWhat I mean is that the 'main' time integrator (eg, implicit Euler) will have played a part in calculating those values of _u at the previous time step, even though we are just reading them out when it comes to the inertia kernel.\nPerhaps I have misunderstood the role of the time integrator object?\nI have run the flag case with correct material properties, ramping the velocity inlet. I used a linear ramp whilst also maintaining the parabolic profile, and varied the time over which the ramp would occur to test more and less aggressive ramps.\nI tried this for both the Newmark case, and the case where I set the time integrator to be implicit Euler in the executioner.\nThe Newmark and implicit Euler versions yielded near identical behaviour, with the exception of the oscillating pressure field across the whole fluid domain in the Newmark run.\nIn all cases, the simulation makes it to around 7-10 time steps through (even if the time step is set to be very small), during which time the acceleration at the edges in the solid increases to very large magnitudes and the simulation stops converging.\nBelow is an image of the solid acceleration immediately before the simulation fails to converge:",
                  "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6440369",
                  "updatedAt": "2023-07-13T15:45:49Z",
                  "publishedAt": "2023-07-13T15:01:54Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "The pressure field only oscillates in time for the Newmark case, the high and low pressure changes 1 timestep apart? Just want to confirm what you mean by this line: with the exception of the oscillating pressure field across the whole fluid domain in the Newmark run.\np.s. I would try to reply to the posts using the Write a reply blocks instead of creating new text blocks, makes it easier for other people to follow in the future.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6440459",
                          "updatedAt": "2023-07-13T15:10:21Z",
                          "publishedAt": "2023-07-13T15:09:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Apologies, I made an error in the file inspection regarding the pressure.\nIn fact, both cases exhibit pressure oscillations which are 1 time step apart, as shown in the graph below.\nHere, the red line is implicit Euler, and the blue line is Newmark:",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6440895",
                          "updatedAt": "2023-07-13T15:48:56Z",
                          "publishedAt": "2023-07-13T15:48:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "Ok, good to know. I have experienced this similar problem with my FSI code, but I chalked it up to a stabilization (SUPG,PSPG) problem because of the embedded approach I am taking. Most of the FSI divergence problems that I know about are related to partioned solvers and related to the coupling of them within a time-step. I haven't heard of anything related to this divergence of pressure between time steps.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6441108",
                          "updatedAt": "2023-07-13T16:09:35Z",
                          "publishedAt": "2023-07-13T16:09:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "One of your previous parts you said: I looked in more detail at a case where we solve just the fluid field by fixing the interface in place as a BC. The rest of the input file was left the same, apart from a no-slip condition also imposed at the interface. Using the steady state run to initialise things and observing the subsequent behaviour of the pressure field, a similar oscillation occurred, but the magnitudes were much less that in the case where the interface is allowed to move. Perhaps the problem is related to the time scheme after all? Although again, the first input file ran without any problems.. It might be worth while to do this with both the implicitEuler and the NewmarkBeta again and see if the problem persists in both time integrators.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6452523",
                          "updatedAt": "2023-07-14T19:19:16Z",
                          "publishedAt": "2023-07-14T18:51:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Good idea - I have run both cases one after the other, each starting from the same initialisation file to make sure we're looking at identical properties besides the time integrator.\nPressure vs time at a downstream point in the fluid domain for the first few time steps looks like this:\n\nAgain, red is ImplicitEuler, blue is Newmark. It's hard to make out the implicit euler graph, so here's a zoomed in version of the start:\n\nClearly there's a pressure spike even though we have initialised the solution. As I understand it, this is known to cause oscillations in Newmark, but we can use the 'inactive timesteps' option to mitigate the problem. Below is the first few steps of the Newmark graph with inactive_tsteps = 2:\n\nThis definitely improves things, and the smaller oscillations seem to remain constant in amplitude.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6468006",
                          "updatedAt": "2023-07-17T13:47:00Z",
                          "publishedAt": "2023-07-17T13:46:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "What I mean is that the 'main' time integrator (eg, implicit Euler) will have played a part in calculating those values of _u at the previous time step, even though we are just reading them out when it comes to the inertia kernel.\n\nWhy do you think that implicit Euler played a part in calculating the old values of _u? u_old is just the solution of u at the previous time step. We just shift the solution vectors back in time. As long as the computation of u is correct, then u_old should be as well.",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6468152",
                          "updatedAt": "2023-07-17T13:57:40Z",
                          "publishedAt": "2023-07-17T13:57:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "I think I am happy with this now: The only case where we would be mixing time integrators would be if we had some other kernel acting on the displacement which uses udot or uddot in its calculation, as this would be calculated from the 'main' time integrator? Since this is not the case in the input file above, it seems that all of the solid mechanics is handled by the Newmark scheme and the fluid physics is handled by the scheme specified in the executioner block. If this is correct, it sounds to me like what we want...",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6479390",
                          "updatedAt": "2023-07-18T14:52:27Z",
                          "publishedAt": "2023-07-18T14:52:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "The only case where we would be mixing time integrators would be if we had some other kernel acting on the displacement which uses udot or uddot in its calculation, as this would be calculated from the 'main' time integrator?\n\nthat's right",
                          "url": "https://github.com/idaholab/moose/discussions/24896#discussioncomment-6480900",
                          "updatedAt": "2023-07-18T17:28:31Z",
                          "publishedAt": "2023-07-18T17:28:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Restart simulation does not converge while single simulation does",
          "author": {
            "login": "ZoeyChen1993"
          },
          "bodyText": "Could anyone help me with this?\nI am solving a coupled thermal-chemical problem over element activating domain, the simulation converges when in a single simulation, but it doesn't converge just after couple seconds of the restart simulation. I'm not sure if this is because the mesh or the input file. For example, the simulation runs for 20s in a single simulation, if I start a simulation in 10s, then I tried to restart the simulation using a restart input file for another 10s, the restart simulation starts to not converge at 0.02s.\nThe first input file starts with regular mesh creating process. There are two variables defined, then with a constant BC, an userobject block.\n[Mesh]\n  [gen]\n    type = GeneratedMeshGenerator\n    dim = 2\n    xmin = 0\n    ymin = 0\n    xmax = 2\n    ymax = 0.1\n    nx = 50\n    ny = 4\n  []\n    [initial_ink]\n    input = gen\n    type = SubdomainBoundingBoxGenerator\n    bottom_left = '0 0 0'\n    top_right = ' 0.1 0.1 0'\n    block_id = 1\n  []\n    [sidesets]\n    input = initial_ink\n    type = SideSetsAroundSubdomainGenerator\n    normal = '1 0 0'\n    block = 1\n    new_boundary = 'moving_interface'\n  []\n[]\n\n[Variables]\n  [temp]\n    block = '1'\n    order = FIRST\n    family = LAGRANGE\n    initial_condition = 20\n  []\n  [./Cure]\n    order = FIRST\n    family = LAGRANGE\n    initial_condition = '0.1'\n    block = '1'\n  [../]\n.........\n[BCs]\n\n  [./temp_bottom]\n    type = DirichletBC\n    #type = NeumannBC\n    variable = temp\n    boundary = bottom\n    value = '200'    \n  [../]\n[]\n..........\n[UserObjects]\n  [activated_elem_uo]\n    type = ActivateElementsByPath\n    execute_on = timestep_begin\n    activate_distance = 0.06\n    function_x = fx\n    function_y = fy\n    active_subdomain_id = 1\n    expand_boundary_name = 'moving_interface'\n  []\n[]`\nThen the restart simulation input file is only different in these blocks. All of the parameters and set up are the same.\n`[Mesh]\n  [gen]\n    type = FileMeshGenerator\n    file =2d_thermalchemo/2d_thermalchemo.e-s110\n    use_for_exodus_restart = true\n  []\n\n    [sidesets]\n    input = gen\n    type = SideSetsAroundSubdomainGenerator\n    normal = '1 0 0'\n    block = 1\n    new_boundary = 'moving_interface'\n  []\n[]\n\n[Variables]\n  [temp]\n    initial_from_file_var = temp\n    initial_from_file_timestep = LATEST\n    block = '1'\n  []\n  [./Cure]\n    order = FIRST\n    family = LAGRANGE\n    initial_from_file_var = Cure\n    initial_from_file_timestep = LATEST\n    block = '1'\n  [../]\n\n[]\n\nThe not converge result starts from here\nTime Step 2, time = 0.02, dt = 0.01\n0 Nonlinear |R| = 2.795042e+01\n0 Linear |R| = 2.795042e+01\n1 Linear |R| = 8.141958e-15\nNonlinear solve did not converge due to DIVERGED_FNORM_NAN iterations 0\nSolve Did NOT Converge!\nAborting as solve did not converge",
          "url": "https://github.com/idaholab/moose/discussions/24979",
          "updatedAt": "2023-07-18T16:53:22Z",
          "publishedAt": "2023-07-17T20:37:04Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nExodus restart is hard to set up. You need to make sure every single variable is being initialized properly.\nyou could output every single variable and material property to exodus on INITIAL and check that everything was initialized.\nIf anything is uninitialized, that's how the restart fails to converge.\nOne thing that is very tricky to get right are the stateful material properties.\nYou could try with Checkpoint restart, should be easier to set up\nhttps://mooseframework.inl.gov/application_usage/restart_recover.html\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/24979#discussioncomment-6471913",
                  "updatedAt": "2023-07-17T22:01:14Z",
                  "publishedAt": "2023-07-17T21:23:38Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ZoeyChen1993"
                          },
                          "bodyText": "Thank you very much for the solutions, I have checked the variables to exodus on INITIAL but didn't find the problem. I tried to use Checkpoint restart, it works well, the restart simulation runs normally, but it seems the out.e files can only be default in the folder with input file, where can I set additional folder to put the files.",
                          "url": "https://github.com/idaholab/moose/discussions/24979#discussioncomment-6480544",
                          "updatedAt": "2023-07-18T16:48:45Z",
                          "publishedAt": "2023-07-18T16:48:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You can include the folder in the name of the restart file (restart_file_base iirc)",
                          "url": "https://github.com/idaholab/moose/discussions/24979#discussioncomment-6480585",
                          "updatedAt": "2023-07-18T16:53:23Z",
                          "publishedAt": "2023-07-18T16:53:22Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "generate_output = 'l2norm_strain'",
          "author": {
            "login": "ddm42"
          },
          "bodyText": "Hello,\nI am a student and have read the documentation for the \"ComputeFiniteStrain\" and \"ComputeLagrangianStrain\" objects thoroughly; however, I may be lacking in fundamental knowledge of finite strain theory, so forgive me if this should be obvious.\nWhen I am using the tensor mechanics module with finite strain kinematics, and my material is ComputeNeoHookeanStress, whose formulation uses Green-Lagrange strain, when one of my \"generate_output\" parameters is 'l2norm_strain', will this output the frobenius norm of the true logarithmic strain tensor?\nBelow is my input file:\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n  large_kinematics = true # used in NeoHookean among others\n[]\n\n[AuxVariables]\n  [pressure]\n    order = CONSTANT\n    family = MONOMIAL\n  []\n[]\n\n[Mesh]\n  file = .../sphericalOctant-mm.msh\n[]\n\n[Modules/TensorMechanics/Master]\n  [all]\n    strain = FINITE # ComputeLagrangianStrain\n    #displacements = 'disp_x disp_y disp_z' exists because of GlobalParams\n    add_variables = true # adds displacement variables\n    new_system = true # UpdatedLagrangianStressDivergence kernels are \"new\"\n    formulation = UPDATED # UpdatedLagrangianStressDivergence kernel\n    volumetric_locking_correction = true\n    generate_output = 'l2norm_pk2_stress l2norm_cauchy_stress l2norm_strain'\n    # 80% sure l2norm_strain is true strain (read docs best I could)\n  []\n[]\n\n[BCs]\n  [bottom_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = 75\n    value = 0 # meters\n    preset = false # true = value of BC is applied before solve begins\n  []\n  [bottom_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = 74\n    value = 0 # meters\n    preset = false\n  []\n  [bottom_z]\n    type = DirichletBC\n    variable = disp_z\n    boundary = 67\n    value = 0\n    preset = false\n  []\n  [Pressure]\n    [right]\n      boundary = 65\n      function = 4.3*t # 14.5 kPa = 148.1 cm. H2O\n      use_automatic_differentiation = true\n    []\n  []\n[]\n\n[AuxKernels]\n  [pressure]\n    type = FunctionAux\n    variable = pressure\n    function = '4.3*t'\n    boundary = 65\n    execute_on = TIMESTEP_END\n  []\n[]\n\n[Materials]\n  [stress]\n    type = ComputeNeoHookeanStress\n    mu = 3.36 # kPa\n    lambda = 164.4 # kPa\n    use_displaced_mesh = true # helps with stability?\n  []\n[]\n\n[Preconditioning]\n  [SMP] # for user-defined preconditioners\n    type = SMP # Single Matrix Preconditioner\n    full = true\n  []\n[]\n\n[Functions]\n  [timestepper]\n      type = PiecewiseConstant\n      x = '0 0.09 0.12 1' # time_t\n      y = '0.01 0.001 0.00001' # time_dt\n      direction = LEFT\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = NEWTON\n  end_time = 1 # <--- failing at .1655\n  #dt = .005\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu superlu_dist'\n  nl_abs_tol = 1e-10\n\n  [TimeStepper]\n    type = FunctionDT\n    function = timestepper\n  []\n[]\n\n[Outputs]\n  exodus = true\n  print_linear_residuals = false\n[]",
          "url": "https://github.com/idaholab/moose/discussions/24978",
          "updatedAt": "2023-07-18T15:54:28Z",
          "publishedAt": "2023-07-17T20:03:46Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@jiangwen84 @yfche",
                  "url": "https://github.com/idaholab/moose/discussions/24978#discussioncomment-6472116",
                  "updatedAt": "2023-07-17T22:00:28Z",
                  "publishedAt": "2023-07-17T22:00:28Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "I do not have an answer on top off my head. @hugary1995  Should know it.",
                          "url": "https://github.com/idaholab/moose/discussions/24978#discussioncomment-6472169",
                          "updatedAt": "2023-07-17T22:13:59Z",
                          "publishedAt": "2023-07-17T22:12:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Yes with large_kinematics = true we are incrementally approximating the logarithmic strain.",
                          "url": "https://github.com/idaholab/moose/discussions/24978#discussioncomment-6479878",
                          "updatedAt": "2023-07-18T15:37:07Z",
                          "publishedAt": "2023-07-18T15:37:07Z",
                          "isAnswer": true
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ddm42"
                          },
                          "bodyText": "Awesome thanks! 'l2norm_strain' gives the Frobenius norm of a rank 2 tensor? Just double checking that it is not the matrix 2-norm induced by a vector definition. Thank you again",
                          "url": "https://github.com/idaholab/moose/discussions/24978#discussioncomment-6479965",
                          "updatedAt": "2023-07-18T15:45:05Z",
                          "publishedAt": "2023-07-18T15:45:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Ha, I don't think we have a good name for that one -- it predates me. You are right: it is the Frobenius norm.",
                          "url": "https://github.com/idaholab/moose/discussions/24978#discussioncomment-6480039",
                          "updatedAt": "2023-07-18T15:52:08Z",
                          "publishedAt": "2023-07-18T15:52:07Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Inserting nuclei in phase field model",
          "author": {
            "login": "vermaprk"
          },
          "bodyText": "Hello, I am trying to insert nuclei in my phase-field model using the discrete nucleation system. I am using the free energy penalty based nucleation method. As per my understanding, the DiscreteNucleation material property forces the variable to a certain desired value using simple harmonic form penalty, which affects the overall free energy of the system, thus allowing nuclei formation.\ntherefore, total free energy = actual free energy + nucleation free energy penalty\nI have following confusions related to my problem which comprises of two variables eta (phase order parameter) and mu (chemical potential). I am trying to force mu to a desired value using DiscreteNucleation which will cause the change in order parameter via change in total free energy.\n\nMy free energy term (without the nucleation penalty) already contains mu. So when I force mu to a desired value, it already gets modified. Then, why I have to add the DiscreteNucleation term again by using DerivativeSumMaterial material property.\nI have tried different values of mu but this doesn't causes any change in eta.\nI tried forcing the eta term to 1 instead of mu. This works but I am not sure whether this is correct approach.\nAnother issue is related to hold time. Once the hold time is over the simulation undergoes convergence issues. Is increasing the hold time to simulation time a good approach for this issue ?\nThe nucleation rate which is introduced using userobject DiscreteNucleationInserter has probability. So, is this probability the no. of nuclei inserted per unit area per unit time or anything else between 0 to 1.\nLastly please refer some materials to read about the free energy penalty based nucleation.\n\nThanks",
          "url": "https://github.com/idaholab/moose/discussions/24947",
          "updatedAt": "2023-07-18T07:05:53Z",
          "publishedAt": "2023-07-11T09:39:16Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@amjokisaari @dschwen on phase nucleation please",
                  "url": "https://github.com/idaholab/moose/discussions/24947#discussioncomment-6421171",
                  "updatedAt": "2023-07-11T23:26:41Z",
                  "publishedAt": "2023-07-11T23:26:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "vermaprk"
                          },
                          "bodyText": "@laagesen on nuclei insertion in PFM",
                          "url": "https://github.com/idaholab/moose/discussions/24947#discussioncomment-6446663",
                          "updatedAt": "2023-07-14T07:14:42Z",
                          "publishedAt": "2023-07-14T07:14:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "My free energy term (without the nucleation penalty) already contains mu. So when I force mu to a desired value, it already gets modified. Then, why I have to add the DiscreteNucleation term again by using DerivativeSumMaterial material property.\n\n\nYou need to add the penalty term (mu-mu0)^2\n\n\nI have tried different values of mu but this doesn't causes any change in eta.\n\n\nIs this a grand potential model? You might want to force a change to eta instead. The key is to apply the change forcing a change in a certain field variable and then let the system evolve towards a lower energy state by diffusion of solute.\n\n\nI tried forcing the eta term to 1 instead of mu. This works but I am not sure whether this is correct approach.\n\n\nYes. See 2.\n\n\nAnother issue is related to hold time. Once the hold time is over the simulation undergoes convergence issues. Is increasing the hold time to simulation time a good approach for this issue ?\n\n\nThe hold time is related to the diffusional timescale of the system. You can monitor the penalty term by adding outputs = exodus to the discrete nucleation material. Pay attention to how long it takes for the penalty field to go to approximately zero. This should be your hold time.\n\n\nThe nucleation rate which is introduced using userobject DiscreteNucleationInserter has probability. So, is this probability the no. of nuclei inserted per unit area per unit time or anything else between 0 to 1.\nLastly please refer some materials to read about the free energy penalty based nucleation.\n\n\nIt is the probability to insert a nucleus per unit area and unit time. Check out the DiscreteNucleationTimeStep post processor for setting a reasonable time step limit, and the DiscreteNucleationMarker for pre-refining the mesh before nucleus insertion.\nI never got to writing a stand alone paper about the free energy based nucleation system, but it was featured in https://www.sciencedirect.com/science/article/pii/S0927025622004839",
                  "url": "https://github.com/idaholab/moose/discussions/24947#discussioncomment-6468104",
                  "updatedAt": "2023-07-17T13:55:04Z",
                  "publishedAt": "2023-07-17T13:55:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "vermaprk"
                          },
                          "bodyText": "Thanks",
                          "url": "https://github.com/idaholab/moose/discussions/24947#discussioncomment-6474968",
                          "updatedAt": "2023-07-18T07:05:54Z",
                          "publishedAt": "2023-07-18T07:05:53Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}