{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wNi0wOVQxNjowMDozNy0wNTowMM4AM-k3"
    },
    "edges": [
      {
        "node": {
          "title": "Stochastic - error in SamplerReceiver",
          "author": {
            "login": "MatiasAllay"
          },
          "bodyText": "Hi MOOSE,\nI'm using the stochastic module to perform a SOBOL sensitivity analysis. However, I get the following error\n*** ERROR *** runner4: The following error occurred in the object \"stochastic\", of type \"SamplerReceiver\". runner4:  runner4: The desired parameter 'Materials/velocity_k/value' was not located for the 'stochastic' object, it either does not exist or has not been declared as controllable\nNot sure what is going on... I get the same error for every parameter. Any idea?\nCheers,\nMatias",
          "url": "https://github.com/idaholab/moose/discussions/18046",
          "updatedAt": "2022-06-12T21:39:27Z",
          "publishedAt": "2021-06-09T19:25:50Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "MatiasAllay"
                  },
                  "bodyText": "Hi MOOSE,\nMy questions was not really precise, I leave here a simplified version of my problem. I would appreciate if someone can take a look.\nI am sure is some stupid mistake!\nhttps://drive.google.com/drive/folders/1Fu-Yn80egK4Bep-PHbB5dE3G85_rKFWP?usp=sharing\nCheers,\nMatias",
                  "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-853691",
                  "updatedAt": "2022-06-12T21:39:29Z",
                  "publishedAt": "2021-06-10T16:13:27Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "The first error message says that porosity is not declared as controllable. To make the control system work, the parameters need to be declared as controllable. See https://mooseframework.inl.gov/syntax/Controls/index.html\nAfter looking at PorousFlowPorosityConst, the porosity is a variable name and it probably cannot be declared as controllable variable.\n\nFor your case, I recommend you use `MultiAppCommandLineControl. See https://mooseframework.inl.gov/source/controls/MultiAppCommandLineControl.html\nIt is a more flexible way to set the parameters because parameter values are directly set through command line which does not require them to be controllable.",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-853799",
                          "updatedAt": "2022-06-12T21:39:29Z",
                          "publishedAt": "2021-06-10T16:36:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "MatiasAllay"
                          },
                          "bodyText": "Hi,\nThanks for you reply. I took some time to perform some tests. The MultiAppCommandLineControl sounds like a excellent solution. However, when I apply it to my problem I get the error\nexecute_on: The sampler object, 'sobol', is being used by the 'cmdline' object, thus the 'execute_on' of the sampler must include 'PRE_MULTIAPP_SETUP'.\nBut apparently, the Sampler/Sobol does not have the flag execute_on (https://mooseframework.inl.gov/moose/source/samplers/SobolSampler.html). Any idea how can I solve this?\nThanks a lot!\nMatias",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-873195",
                          "updatedAt": "2022-06-12T21:39:34Z",
                          "publishedAt": "2021-06-15T13:06:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Comment out the following line, recompile, and run with execute_on set as recommend. I am not remembering why that was disabled, but we will see if that gets it working for you.\n\n  \n    \n      moose/modules/stochastic_tools/src/samplers/SobolSampler.C\n    \n    \n         Line 24\n      in\n      48fe2f2\n    \n  \n  \n    \n\n        \n          \n           params.suppressParameter<ExecFlagEnum>(\"execute_on\");",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-873528",
                          "updatedAt": "2022-06-12T21:39:56Z",
                          "publishedAt": "2021-06-15T14:21:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "MatiasAllay"
                          },
                          "bodyText": "Thanks! That worked perfectly.\nMatias",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-874026",
                          "updatedAt": "2022-12-27T23:19:52Z",
                          "publishedAt": "2021-06-15T15:59:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Great, I will have to change that in MOOSE.",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-876008",
                          "updatedAt": "2022-12-27T23:19:52Z",
                          "publishedAt": "2021-06-16T02:19:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Coupled Variable Name",
          "author": {
            "login": "rtaylo45"
          },
          "bodyText": "Im writing a kernel and that works with two coupled variables. I was wondering if there is a way to retrieve the name of a specific couple variable while inside the code for the kernel. For example, if my input file has this example kernel\n[kernels]\n  [exampleKernel]\n    type = ExampleKernel\n    variable = nonLinearVar\n    coupled_var_1 = example_coupled_var_1\n    coupled_var_2 = example_coupled_var_2\n  []\n[]\n\nIn my kernel i can use _var.name() to retrieve the name of the primal variable. But how would get the name of the associated inputs for coupled_var_1 and coupled_var_2? Something like _coupled_var_1.name() and _coupled_var_1.name(). From examples i have seen with multi-physics coupling, the type of the coupled variable is VariableValue which is not a type which I can extract a variable name from.",
          "url": "https://github.com/idaholab/moose/discussions/18084",
          "updatedAt": "2022-09-06T14:25:43Z",
          "publishedAt": "2021-06-14T16:52:28Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nI would do getFieldVar(\"parameter_name\", 0).name()  (might be ->name())\n0 is the component, I'm assuming you are not using array variables.\n\"parameter_name\" is \"coupled_var_i\" here\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18084#discussioncomment-869548",
                  "updatedAt": "2022-09-06T14:25:50Z",
                  "publishedAt": "2021-06-14T16:56:31Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "See here for example:\n\n  \n    \n      moose/modules/tensor_mechanics/src/materials/ComputeVariableEigenstrain.C\n    \n    \n         Line 36\n      in\n      06bc1f7\n    \n  \n  \n    \n\n        \n          \n           const VariableName & iname = getVar(\"args\", i)->name();",
                          "url": "https://github.com/idaholab/moose/discussions/18084#discussioncomment-869903",
                          "updatedAt": "2022-09-06T14:26:08Z",
                          "publishedAt": "2021-06-14T18:39:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "Awesome, this works. Thanks for the help",
                          "url": "https://github.com/idaholab/moose/discussions/18084#discussioncomment-870314",
                          "updatedAt": "2022-09-06T14:26:19Z",
                          "publishedAt": "2021-06-14T20:24:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MultiApp--Moose Test Issue",
          "author": {
            "login": "AhmedAlmetwally"
          },
          "bodyText": "Bug Description\nI am building a multiapp using Moose. I had the following issues while running the tests. Mostly related to MPI and Libmoose library.\nmesh/unique_ids.replicated_mesh .................................................. [min_cpus=2] FAILED (CRASH)\nsamplers/distribute.scale/execute ............................................................. FAILED (CRASH)\nperformance.multiprocess/mpi ................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nreporters/base.base .............................................................. [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/parallel_consistency.test ................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.parallel ......................................... [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.mpi .......................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\npostprocessors/num_residual_eval.test ............................................ [min_cpus=2] FAILED (CRASH)\noutputs/json/distributed.info/default ............................................ [min_cpus=2] FAILED (CRASH)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject ........ [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.partitioner/parmetis ........................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/work_balance.work_balance/replicated ........................ [min_cpus=2] FAILED (CRASH)\nmesh/checkpoint.test_2 ........................................................... [min_cpus=2] FAILED (CRASH)\npreconditioners/hmg.hmg ........................................ [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nsamplers/base.parallel/mpi ....................................................... [min_cpus=2] FAILED (CRASH)\noutputs/xml.parallel/replicated .................................................. [min_cpus=3] FAILED (CRASH)\nsystem_interfaces.partitioner/ptscotch ........................................... [min_cpus=2] FAILED (CRASH)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank ............... [min_cpus=2] FAILED (CRASH)\nmeshgenerators/meta_data_store.test_meta_data_with_use_split ..................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/parallel_consistency.broadcast .............................. [min_cpus=2] FAILED (CRASH)\nmesh/checkpoint.test_2a .......................................................... [min_cpus=2] FAILED (CRASH)\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel ................... [min_cpus=2] FAILED (CRASH)\ninterfacekernels/2d_interface.parallel_fdp_test .................................. [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.solver/superlu ................................................. [min_cpus=2] FAILED (CRASH)\nbcs/periodic.testperiodic_vector ............................................................. FAILED (ERRMSG)\nmesh/custom_partitioner.group/custom_linear_partitioner .......................... [min_cpus=2] FAILED (CRASH)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer ......................... [min_cpus=2] FAILED (CRASH)\nreporters/mesh_info.info/default ................................................. [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.solver/mumps ................................................... [min_cpus=2] FAILED (CRASH)\nauxkernels/vector_postprocessor_visualization.test ............................... [min_cpus=3] FAILED (CRASH)\nbcs/dmg_periodic.check_one_step .................................................. [min_cpus=2] FAILED (CRASH)\noutputs/vtk.files/parallel ....................................................... [min_cpus=2] FAILED (CRASH)\nics/depend_on_uo.ic_depend_on_uo ................................................. [min_cpus=2] FAILED (CRASH)\nbcs/periodic.testperiodic_dp ..................................................... [min_cpus=2] FAILED (CRASH)\nfvkernels/mms/non-orthogonal.compact ............................................. [min_cpus=2] FAILED (CRASH)\nrestart/restartable_types.parallel/first ......................................... [min_cpus=2] FAILED (CRASH)\npreconditioners/hmg.hmg_3D ..................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nics/depend_on_uo.scalar_ic_from_uo ............................................... [min_cpus=2] FAILED (CRASH)\nmesh/mesh_only.mesh_only_checkpoint .............................................. [min_cpus=3] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement ............. [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\ntransfers/multiapp_nearest_node_transfer.parallel ................................ [min_cpus=2] FAILED (CRASH)\nfvkernels/mms/non-orthogonal.extended ............................................ [min_cpus=2] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart .................. [min_cpus=2] FAILED (CRASH)\noutputs/vtk.solution/diff_serial_mesh_parallel ................................... [min_cpus=2] FAILED (CRASH)\ninterfaces/random.parallel_verification .......................................... [min_cpus=2] FAILED (CRASH)\nmeshgenerators/distributed_rectilinear/partition.2D_3 ............................ [min_cpus=3] FAILED (CRASH)\nrestart/kernel_restartable.parallel_error/error1 ................................. [min_cpus=2] FAILED (CRASH)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor ................ [min_cpus=3] FAILED (CRASH)\noutputs/xml.parallel/distributed ................................................. [min_cpus=3] FAILED (CRASH)\nIssue #1\nmesh/unique_ids.distributed_mesh ................................................... [skipped dependency] SKIP\nmesh/unique_ids.replicated_mesh: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/unique_ids\nmesh/unique_ids.replicated_mesh: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i unique_ids.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/unique_ids.replicated_mesh: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/unique_ids.replicated_mesh: MPIR_Init_thread(586)..............:\nmesh/unique_ids.replicated_mesh: MPID_Init(224).....................: channel initialization failed\nmesh/unique_ids.replicated_mesh: MPIDI_CH3_Init(105)................:\nmesh/unique_ids.replicated_mesh: MPID_nem_init(324).................:\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_init(175).............:\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_get_business_card(401):\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/unique_ids.replicated_mesh: (unknown)(): Invalid group\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh: Exit Code: 8\nmesh/unique_ids.replicated_mesh: ################################################################################\nmesh/unique_ids.replicated_mesh: Tester failed, reason: CRASH\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh .................................................. [min_cpus=2] FAILED (CRASH)\nIssue #2\nvariables/fe_hermite_convergence.hermite_convergance/periodic ............................................. OK\nsamplers/distribute.scale/plot ..................................................... [skipped dependency] SKIP\nsamplers/distribute.scale/execute: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/samplers/distribute\nsamplers/distribute.scale/execute: Running command: /Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\nsamplers/distribute.scale/execute: mpiexec -n 1 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i distribute.i Outputs/file_base=distribute_1 Postprocessors/test/test_type=getGlobalSamples Samplers/sampler/num_rows=1\nsamplers/distribute.scale/execute: Traceback (most recent call last):\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\", line 47, in \nsamplers/distribute.scale/execute:     execute('distribute.i', 'distribute_none', 1, args.processors, 'getGlobalSamples')\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\", line 29, in execute\nsamplers/distribute.scale/execute:     local = pandas.read_csv('{}.csv'.format(file_base))\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 605, in read_csv\nsamplers/distribute.scale/execute:     return _read(filepath_or_buffer, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 457, in _read\nsamplers/distribute.scale/execute:     parser = TextFileReader(filepath_or_buffer, **kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 814, in init\nsamplers/distribute.scale/execute:     self._engine = self._make_engine(self.engine)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1045, in _make_engine\nsamplers/distribute.scale/execute:     return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1862, in init\nsamplers/distribute.scale/execute:     self._open_handles(src, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1363, in _open_handles\nsamplers/distribute.scale/execute:     storage_options=kwds.get(\"storage_options\", None),\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/common.py\", line 647, in get_handle\nsamplers/distribute.scale/execute:     newline=\"\",\nsamplers/distribute.scale/execute: FileNotFoundError: [Errno 2] No such file or directory: 'distribute_1.csv'\nsamplers/distribute.scale/execute: mpiexec -n 1 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i distribute.i Outputs/file_base=distribute_1 Postprocessors/test/test_type=getGlobalSamples Samplers/sampler/num_rows=1\nsamplers/distribute.scale/execute: Traceback (most recent call last):\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\", line 47, in \nsamplers/distribute.scale/execute:     execute('distribute.i', 'distribute_none', 1, args.processors, 'getGlobalSamples')\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\", line 29, in execute\nsamplers/distribute.scale/execute:     local = pandas.read_csv('{}.csv'.format(file_base))\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 605, in read_csv\nsamplers/distribute.scale/execute:     return _read(filepath_or_buffer, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 457, in _read\nsamplers/distribute.scale/execute:     parser = TextFileReader(filepath_or_buffer, **kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 814, in init\nsamplers/distribute.scale/execute:     self._engine = self._make_engine(self.engine)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1045, in _make_engine\nsamplers/distribute.scale/execute:     return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1862, in init\nsamplers/distribute.scale/execute:     self._open_handles(src, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1363, in _open_handles\nsamplers/distribute.scale/execute:     storage_options=kwds.get(\"storage_options\", None),\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/common.py\", line 647, in get_handle\nsamplers/distribute.scale/execute:     newline=\"\",\nsamplers/distribute.scale/execute: FileNotFoundError: [Errno 2] No such file or directory: 'distribute_1.csv'\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute: Exit Code: 1\nsamplers/distribute.scale/execute: ################################################################################\nsamplers/distribute.scale/execute: Tester failed, reason: CRASH\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute ............................................................. FAILED (CRASH)\nIssue #3\nfvbcs/fv_neumannbc.fvbcs_internal ......................................................................... OK\nperformance.multiprocess/mpi: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/performance\nperformance.multiprocess/mpi: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nperformance.multiprocess/mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nperformance.multiprocess/mpi: MPIR_Init_thread(586)..............:\nperformance.multiprocess/mpi: MPID_Init(224).....................: channel initialization failed\nperformance.multiprocess/mpi: MPIDI_CH3_Init(105)................:\nperformance.multiprocess/mpi: MPID_nem_init(324).................:\nperformance.multiprocess/mpi: MPID_nem_tcp_init(175).............:\nperformance.multiprocess/mpi: MPID_nem_tcp_get_business_card(401):\nperformance.multiprocess/mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nperformance.multiprocess/mpi: (unknown)(): Invalid group\nperformance.multiprocess/mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nperformance.multiprocess/mpi: MPIR_Init_thread(586)..............:\nperformance.multiprocess/mpi: MPID_Init(224).....................: channel initialization failed\nperformance.multiprocess/mpi: MPIDI_CH3_Init(105)................:\nperformance.multiprocess/mpi: MPID_nem_init(324).................:\nperformance.multiprocess/mpi: MPID_nem_tcp_init(175).............:\nperformance.multiprocess/mpi: MPID_nem_tcp_get_business_card(401):\nperformance.multiprocess/mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nperformance.multiprocess/mpi: (unknown)(): Invalid group\nperformance.multiprocess/mpi: ################################################################################\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: Unable to match the following pattern against the program's output:\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: Num Processors:\\s+2\\s+Num Threads:\\s+1\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: ################################################################################\nperformance.multiprocess/mpi: Tester failed, reason: EXPECTED OUTPUT MISSING\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi ................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nrestrictable/block_api_test.block_undefined_var_block ..................................................... OK\nreporters/base.base: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/reporters/base\nreporters/base.base: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i base.i --allow-test-objects --error --error-unused --error-override --no-gdb-backtrace\nreporters/base.base: Fatal error in MPI_Init_thread: Invalid group, error stack:\nreporters/base.base: MPIR_Init_thread(586)..............:\nreporters/base.base: MPID_Init(224).....................: channel initialization failed\nreporters/base.base: MPIDI_CH3_Init(105)................:\nreporters/base.base: MPID_nem_init(324).................:\nreporters/base.base: MPID_nem_tcp_init(175).............:\nreporters/base.base: MPID_nem_tcp_get_business_card(401):\nreporters/base.base: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nreporters/base.base: (unknown)(): Invalid group\nreporters/base.base:\nreporters/base.base:\nreporters/base.base: Exit Code: 8\nreporters/base.base: ################################################################################\nreporters/base.base: Tester failed, reason: CRASH\nreporters/base.base:\nreporters/base.base .............................................................. [min_cpus=2] FAILED (CRASH)\nreporters/constant_reporter.errors/no_values .............................................................. OK\nIssue #4\nvectorpostprocessors/element_value_sampler.element_value_sampler/lagrange ................................. OK\nvectorpostprocessors/parallel_consistency.test: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/parallel_consistency\nvectorpostprocessors/parallel_consistency.test: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i parallel_consistency.i --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test: Exit Code: 8\nvectorpostprocessors/parallel_consistency.test: ################################################################################\nvectorpostprocessors/parallel_consistency.test: Tester failed, reason: CRASH\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test ................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/intersection_points_along_line.intersecting_elems/3d ................................. OK\nvectorpostprocessors/csv_reader.parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/csv_reader\nvectorpostprocessors/csv_reader.parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i read.i UserObjects/tester/rank=1 Outputs/csv=false --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/csv_reader.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.parallel: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.parallel: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.parallel: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/csv_reader.parallel: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.parallel: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.parallel: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.parallel: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/csv_reader.parallel: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel: Exit Code: 8\nvectorpostprocessors/csv_reader.parallel: ################################################################################\nvectorpostprocessors/csv_reader.parallel: Tester failed, reason: CRASH\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel ......................................... [min_cpus=2] FAILED (CRASH)\ntime_integrators/implicit-euler.monomials ................................................................. OK\n\nIssue #5\nrestrictable/block_api_test.ids/blocks .................................................................... OK\nsystem_interfaces.mpi: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.mpi: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.mpi: MPIR_Init_thread(586)..............:\nsystem_interfaces.mpi: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.mpi: MPIDI_CH3_Init(105)................:\nsystem_interfaces.mpi: MPID_nem_init(324).................:\nsystem_interfaces.mpi: MPID_nem_tcp_init(175).............:\nsystem_interfaces.mpi: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.mpi: (unknown)(): Invalid group\nsystem_interfaces.mpi: ################################################################################\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: Unable to match the following pattern against the program's output:\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: Num Processors:\\s+2\\s+Num Threads:\\s+1\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: ################################################################################\nsystem_interfaces.mpi: Tester failed, reason: EXPECTED OUTPUT MISSING\nsystem_interfaces.mpi:\nsystem_interfaces.mpi .......................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\npreconditioners/fdp.jacobian_fdp_coloring_diagonal_test_fail .............................................. OK\nIssue #6\ntime_integrators/rk-2.jacobian/2d-quadratic_num-of-jacobian-calls ......................................... OK\npostprocessors/num_residual_eval.test: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/postprocessors/num_residual_eval\npostprocessors/num_residual_eval.test: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i num_residual_eval.i --error --error-unused --error-override --no-gdb-backtrace\npostprocessors/num_residual_eval.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_residual_eval.test: MPIR_Init_thread(586)..............:\npostprocessors/num_residual_eval.test: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_residual_eval.test: MPIDI_CH3_Init(105)................:\npostprocessors/num_residual_eval.test: MPID_nem_init(324).................:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(175).............:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npostprocessors/num_residual_eval.test: (unknown)(): Invalid group\npostprocessors/num_residual_eval.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_residual_eval.test: MPIR_Init_thread(586)..............:\npostprocessors/num_residual_eval.test: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_residual_eval.test: MPIDI_CH3_Init(105)................:\npostprocessors/num_residual_eval.test: MPID_nem_init(324).................:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(175).............:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npostprocessors/num_residual_eval.test: (unknown)(): Invalid group\npostprocessors/num_residual_eval.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_residual_eval.test: MPIR_Init_thread(586)..............:\npostprocessors/num_residual_eval.test: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_residual_eval.test: MPIDI_CH3_Init(105)................:\npostprocessors/num_residual_eval.test: MPID_nem_init(324).................:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(175).............:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npostprocessors/num_residual_eval.test: (unknown)(): Invalid group\npostprocessors/num_residual_eval.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_residual_eval.test: MPIR_Init_thread(586)..............:\npostprocessors/num_residual_eval.test: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_residual_eval.test: MPIDI_CH3_Init(105)................:\npostprocessors/num_residual_eval.test: MPID_nem_init(324).................:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(175).............:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npostprocessors/num_residual_eval.test: (unknown)(): Invalid group\npostprocessors/num_residual_eval.test:\npostprocessors/num_residual_eval.test:\npostprocessors/num_residual_eval.test: Exit Code: 8\npostprocessors/num_residual_eval.test: ################################################################################\npostprocessors/num_residual_eval.test: Tester failed, reason: CRASH\npostprocessors/num_residual_eval.test:\npostprocessors/num_residual_eval.test ............................................ [min_cpus=2] FAILED (CRASH)\npostprocessors/change_over_fixed_point.change_over_fixed_point_error/change_with_respect_to_initial_error_this OK\nIssue #7\noutputs/postprocessor.show_hide ........................................................................... OK\noutputs/json/distributed.info/default: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/json/distributed\noutputs/json/distributed.info/default: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i distributed.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/json/distributed.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/json/distributed.info/default: MPIR_Init_thread(586)..............:\noutputs/json/distributed.info/default: MPID_Init(224).....................: channel initialization failed\noutputs/json/distributed.info/default: MPIDI_CH3_Init(105)................:\noutputs/json/distributed.info/default: MPID_nem_init(324).................:\noutputs/json/distributed.info/default: MPID_nem_tcp_init(175).............:\noutputs/json/distributed.info/default: MPID_nem_tcp_get_business_card(401):\noutputs/json/distributed.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/json/distributed.info/default: (unknown)(): Invalid group\noutputs/json/distributed.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/json/distributed.info/default: MPIR_Init_thread(586)..............:\noutputs/json/distributed.info/default: MPID_Init(224).....................: channel initialization failed\noutputs/json/distributed.info/default: MPIDI_CH3_Init(105)................:\noutputs/json/distributed.info/default: MPID_nem_init(324).................:\noutputs/json/distributed.info/default: MPID_nem_tcp_init(175).............:\noutputs/json/distributed.info/default: MPID_nem_tcp_get_business_card(401):\noutputs/json/distributed.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/json/distributed.info/default: (unknown)(): Invalid group\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default: Exit Code: 8\noutputs/json/distributed.info/default: ################################################################################\noutputs/json/distributed.info/default: Tester failed, reason: CRASH\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default ............................................ [min_cpus=2] FAILED (CRASH)\noutputs/intervals.sync_times .............................................................................. OK\nIssue #8\nbcs/periodic.orthogonal_pbc_on_square_test ................................................................ OK\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/userobjects/setup_interface_count\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i general.i --error --error-unused --error-override --no-gdb-backtrace\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Fatal error in MPI_Init_thread: Invalid group, error stack:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIR_Init_thread(586)..............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_Init(224).....................: channel initialization failed\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIDI_CH3_Init(105)................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_init(324).................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(175).............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_get_business_card(401):\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: (unknown)(): Invalid group\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Fatal error in MPI_Init_thread: Invalid group, error stack:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIR_Init_thread(586)..............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_Init(224).....................: channel initialization failed\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIDI_CH3_Init(105)................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_init(324).................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(175).............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_get_business_card(401):\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: (unknown)(): Invalid group\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Exit Code: 8\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: ################################################################################\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Tester failed, reason: CRASH\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject ........ [min_cpus=2] FAILED (CRASH)\nbcs/nodal_normals.small_sqaure ............................................................................ OK\nIssue #9\noutputs/misc.default_names ................................................................................ OK\nsystem_interfaces.partitioner/parmetis: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.partitioner/parmetis: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.partitioner/parmetis: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.partitioner/parmetis: MPIR_Init_thread(586)..............:\nsystem_interfaces.partitioner/parmetis: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.partitioner/parmetis: MPIDI_CH3_Init(105)................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_init(324).................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(175).............:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.partitioner/parmetis: (unknown)(): Invalid group\nsystem_interfaces.partitioner/parmetis: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.partitioner/parmetis: MPIR_Init_thread(586)..............:\nsystem_interfaces.partitioner/parmetis: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.partitioner/parmetis: MPIDI_CH3_Init(105)................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_init(324).................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(175).............:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.partitioner/parmetis: (unknown)(): Invalid group\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis: Exit Code: 8\nsystem_interfaces.partitioner/parmetis: ################################################################################\nsystem_interfaces.partitioner/parmetis: Tester failed, reason: CRASH\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis ........................................... [min_cpus=2] FAILED (CRASH)\noutputs/exodus.hide_output ................................................................................ OK\nIssue #10\nconstraints/equal_value_embedded_constraint.penalty/1D_3D ................................................. OK\nvectorpostprocessors/work_balance.work_balance/replicated: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/work_balance\nvectorpostprocessors/work_balance.work_balance/replicated: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i work_balance.i --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/work_balance.work_balance/replicated: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/work_balance.work_balance/replicated: MPIR_Init_thread(586)..............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/work_balance.work_balance/replicated: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_init(324).................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/work_balance.work_balance/replicated: (unknown)(): Invalid group\nvectorpostprocessors/work_balance.work_balance/replicated: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/work_balance.work_balance/replicated: MPIR_Init_thread(586)..............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/work_balance.work_balance/replicated: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_init(324).................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/work_balance.work_balance/replicated: (unknown)(): Invalid group\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Exit Code: 8\nvectorpostprocessors/work_balance.work_balance/replicated: ################################################################################\nvectorpostprocessors/work_balance.work_balance/replicated: Tester failed, reason: CRASH\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated ........................ [min_cpus=2] FAILED (CRASH)\nmesh/checkpoint.test_2: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/checkpoint\nmesh/checkpoint.test_2: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i checkpoint_split.i Outputs/file_base=test_2 --use-split --split-file checkpoint_split_in --error --error-unused --error-override --no-gdb-backtrace\nmesh/checkpoint.test_2: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2: MPID_nem_init(324).................:\nmesh/checkpoint.test_2: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2: (unknown)(): Invalid group\nmesh/checkpoint.test_2: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2: MPID_nem_init(324).................:\nmesh/checkpoint.test_2: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2: (unknown)(): Invalid group\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2: Exit Code: 8\nmesh/checkpoint.test_2: ################################################################################\nmesh/checkpoint.test_2: Tester failed, reason: CRASH\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2 ........................................................... [min_cpus=2] FAILED (CRASH)\npostprocessors/num_iterations.methods/explicit_euler ...................................................... OK\nIssue #11\ntransfers/multiapp_mesh_function_transfer.errors/mismatch_exec_on ......................................... OK\npreconditioners/hmg.hmg: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/preconditioners/hmg\npreconditioners/hmg.hmg: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i diffusion_hmg.i --error --error-unused --error-override --no-gdb-backtrace\npreconditioners/hmg.hmg: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg: MPID_nem_init(324).................:\npreconditioners/hmg.hmg: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npreconditioners/hmg.hmg: (unknown)(): Invalid group\npreconditioners/hmg.hmg: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg: MPID_nem_init(324).................:\npreconditioners/hmg.hmg: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npreconditioners/hmg.hmg: (unknown)(): Invalid group\npreconditioners/hmg.hmg: ################################################################################\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: Unable to match the following pattern against the program's output:\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: using\\s+allatonce\\s+MatPtAP()\\s+implementation\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: ################################################################################\npreconditioners/hmg.hmg: Tester failed, reason: EXPECTED OUTPUT MISSING\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg ........................................ [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nsamplers/base.parallel/mpi: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/samplers/base\nsamplers/base.parallel/mpi: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i mpi.i --allow-test-objects --error --error-unused --error-override --no-gdb-backtrace\nsamplers/base.parallel/mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsamplers/base.parallel/mpi: MPIR_Init_thread(586)..............:\nsamplers/base.parallel/mpi: MPID_Init(224).....................: channel initialization failed\nsamplers/base.parallel/mpi: MPIDI_CH3_Init(105)................:\nsamplers/base.parallel/mpi: MPID_nem_init(324).................:\nsamplers/base.parallel/mpi: MPID_nem_tcp_init(175).............:\nsamplers/base.parallel/mpi: MPID_nem_tcp_get_business_card(401):\nsamplers/base.parallel/mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsamplers/base.parallel/mpi: (unknown)(): Invalid group\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi: Exit Code: 8\nsamplers/base.parallel/mpi: ################################################################################\nsamplers/base.parallel/mpi: Tester failed, reason: CRASH\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi ....................................................... [min_cpus=2] FAILED (CRASH)\ntransfers/multiapp_userobject_transfer.two_pipes .......................................................... OK\nIssue #11\nrestrictable/block_api_test.has/isBlockSubset ............................................................. OK\noutputs/xml.parallel/replicated: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/xml\noutputs/xml.parallel/replicated: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i xml.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/xml.parallel/replicated: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/xml.parallel/replicated: MPIR_Init_thread(586)..............:\noutputs/xml.parallel/replicated: MPID_Init(224).....................: channel initialization failed\noutputs/xml.parallel/replicated: MPIDI_CH3_Init(105)................:\noutputs/xml.parallel/replicated: MPID_nem_init(324).................:\noutputs/xml.parallel/replicated: MPID_nem_tcp_init(175).............:\noutputs/xml.parallel/replicated: MPID_nem_tcp_get_business_card(401):\noutputs/xml.parallel/replicated: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/xml.parallel/replicated: (unknown)(): Invalid group\noutputs/xml.parallel/replicated:\noutputs/xml.parallel/replicated: ################################################################################\noutputs/xml.parallel/replicated: Tester failed, reason: CRASH\noutputs/xml.parallel/replicated:\noutputs/xml.parallel/replicated .................................................. [min_cpus=3] FAILED (CRASH)\ninterfacekernels/1d_interface.ik_save_in .................................................................. OK\nIssue #12\nreporters/base.errors/requested_different_type ............................................................ OK\nsystem_interfaces.partitioner/ptscotch: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.partitioner/ptscotch: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.partitioner/ptscotch: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.partitioner/ptscotch: MPIR_Init_thread(586)..............:\nsystem_interfaces.partitioner/ptscotch: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.partitioner/ptscotch: MPIDI_CH3_Init(105)................:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_init(324).................:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_init(175).............:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.partitioner/ptscotch: (unknown)(): Invalid group\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch: Exit Code: 8\nsystem_interfaces.partitioner/ptscotch: ################################################################################\nsystem_interfaces.partitioner/ptscotch: Tester failed, reason: CRASH\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch ........................................... [min_cpus=2] FAILED (CRASH)\ninterfacekernels/1d_interface.ik_save_in_other_side ....................................................... OK\nbcs/periodic.testperiodic ................................................................................. OK\noutputs/debug.show_material_properties_consumed ........................................................... OK\nics/random_ic_test.test_threaded .......................................................... [min_threads=2] OK\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/misc/exception\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i parallel_exception_residual_transient.i Kernels/exception/rank=1 --error --error-unused --error-override --no-gdb-backtrace\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIR_Init_thread(586)..............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_Init(224).....................: channel initialization failed\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIDI_CH3_Init(105)................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_init(324).................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(175).............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_get_business_card(401):\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: (unknown)(): Invalid group\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIR_Init_thread(586)..............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_Init(224).....................: channel initialization failed\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIDI_CH3_Init(105)................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_init(324).................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(175).............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_get_business_card(401):\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: (unknown)(): Invalid group\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Exit Code: 8\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Tester failed, reason: CRASH\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank ............... [min_cpus=2] FAILED (CRASH)\nnodalkernels/constraint_enforcement.vi/rsls ............................................................... OK\nIssue #13\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/meshgenerators/meta_data_store\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i mesh_meta_data_store.i --use-split --split-file split2 --error --error-unused --error-override --no-gdb-backtrace\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPIR_Init_thread(586)..............:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPIDI_CH3_Init(105)................:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_init(324).................:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_init(175).............:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: (unknown)(): Invalid group\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPIR_Init_thread(586)..............:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPIDI_CH3_Init(105)................:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_init(324).................:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_init(175).............:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: (unknown)(): Invalid group\nmeshgenerators/meta_data_store.test_meta_data_with_use_split:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Exit Code: 8\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: ################################################################################\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Tester failed, reason: CRASH\nmeshgenerators/meta_data_store.test_meta_data_with_use_split:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split ..................... [min_cpus=2] FAILED (CRASH)\ntime_steppers/timesequence_stepper.restart_failure/timesequence_restart_failure_1 ......................... OK\nmultiapps/steffensen.variables_transient/app_end_transfers_end ............................................ OK\nmaterials/output.invalid_outputs .......................................................................... OK\nuserobjects/threaded_general_user_object.thread_copies_guo/th2 ............................ [min_threads=2] OK\nmesh/high_order_elems.test_prism6_refine .................................................................. OK\nvectorpostprocessors/parallel_consistency.broadcast: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/parallel_consistency\nvectorpostprocessors/parallel_consistency.broadcast: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i parallel_consistency.i AuxKernels/viewit/use_broadcast=true Outputs/file_base=broadcast_out --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/parallel_consistency.broadcast: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.broadcast: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.broadcast: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.broadcast: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.broadcast: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.broadcast: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.broadcast: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.broadcast: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.broadcast: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.broadcast: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.broadcast: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.broadcast: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.broadcast: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.broadcast: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.broadcast: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.broadcast: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.broadcast:\nvectorpostprocessors/parallel_consistency.broadcast:\nvectorpostprocessors/parallel_consistency.broadcast: Exit Code: 8\nvectorpostprocessors/parallel_consistency.broadcast: ################################################################################\nvectorpostprocessors/parallel_consistency.broadcast: Tester failed, reason: CRASH\nvectorpostprocessors/parallel_consistency.broadcast:\nvectorpostprocessors/parallel_consistency.broadcast .............................. [min_cpus=2] FAILED (CRASH)\nfvkernels/fv_simple_diffusion.unstructured-rz ............................................................. OK\nmesh/checkpoint.test_2a: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/checkpoint\nmesh/checkpoint.test_2a: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i checkpoint_split.i Outputs/file_base=test_2a --use-split --split-file checkpoint_split_in.cpa --error --error-unused --error-override --no-gdb-backtrace\nmesh/checkpoint.test_2a: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2a: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2a: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2a: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2a: MPID_nem_init(324).................:\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2a: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2a: (unknown)(): Invalid group\nmesh/checkpoint.test_2a: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2a: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2a: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2a: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2a: MPID_nem_init(324).................:\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2a: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2a: (unknown)(): Invalid group\nmesh/checkpoint.test_2a: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2a: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2a: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2a: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2a: MPID_nem_init(324).................:\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2a: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2a: (unknown)(): Invalid group\nmesh/checkpoint.test_2a: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2a: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2a: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2a: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2a: MPID_nem_init(324).................:\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2a: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2a: (unknown)(): Invalid group\nmesh/checkpoint.test_2a:\nmesh/checkpoint.test_2a:\nmesh/checkpoint.test_2a: Exit Code: 8\nmesh/checkpoint.test_2a: ################################################################################\nmesh/checkpoint.test_2a: Tester failed, reason: CRASH\nmesh/checkpoint.test_2a:\nmesh/checkpoint.test_2a .......................................................... [min_cpus=2] FAILED (CRASH)\nuserobjects/layered_average.layered_average/block_restricted .............................................. OK\npostprocessors/find_value_on_line.depth_exceeded .......................................................... OK\nconstraints/equal_value_embedded_constraint.penalty/3D_3D ................................................. OK\npostprocessors/num_iterations.methods/heun ................................................................ OK\ntransfers/reporter_transfer.clone_type/type_specified ................... [insufficient slots,min_cpus=6] SKIP\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/dgkernels/2d_diffusion_dg\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i no_mallocs_with_action.i Outputs/file_base=no_mallocs_with_action_parallel --error --error-unused --error-override --no-gdb-backtrace\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPIR_Init_thread(586)..............:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_Init(224).....................: channel initialization failed\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPIDI_CH3_Init(105)................:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_init(324).................:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_init(175).............:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_get_business_card(401):\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: (unknown)(): Invalid group\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPIR_Init_thread(586)..............:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_Init(224).....................: channel initialization failed\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPIDI_CH3_Init(105)................:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_init(324).................:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_init(175).............:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_get_business_card(401):\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: (unknown)(): Invalid group\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Exit Code: 8\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: ################################################################################\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Tester failed, reason: CRASH\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel ................... [min_cpus=2] FAILED (CRASH)\ntime_integrators/convergence.explicit_midpoint/level2 ..................................................... OK\nproblems/eigen_problem/eigensolvers.coupled_system ........................................................ OK\ninterfacekernels/2d_interface.parallel_fdp_test: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/interfacekernels/2d_interface\ninterfacekernels/2d_interface.parallel_fdp_test: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i coupled_value_coupled_flux.i Preconditioning/smp/type=FDP --error-unused --error-override --no-gdb-backtrace\ninterfacekernels/2d_interface.parallel_fdp_test: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfacekernels/2d_interface.parallel_fdp_test: MPIR_Init_thread(586)..............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_Init(224).....................: channel initialization failed\ninterfacekernels/2d_interface.parallel_fdp_test: MPIDI_CH3_Init(105)................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_init(324).................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(175).............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_get_business_card(401):\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ninterfacekernels/2d_interface.parallel_fdp_test: (unknown)(): Invalid group\ninterfacekernels/2d_interface.parallel_fdp_test: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfacekernels/2d_interface.parallel_fdp_test: MPIR_Init_thread(586)..............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_Init(224).....................: channel initialization failed\ninterfacekernels/2d_interface.parallel_fdp_test: MPIDI_CH3_Init(105)................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_init(324).................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(175).............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_get_business_card(401):\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ninterfacekernels/2d_interface.parallel_fdp_test: (unknown)(): Invalid group\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test: Exit Code: 8\ninterfacekernels/2d_interface.parallel_fdp_test: ################################################################################\ninterfacekernels/2d_interface.parallel_fdp_test: Tester failed, reason: CRASH\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test .................................. [min_cpus=2] FAILED (CRASH)\nmaterials/derivative_material_interface.warn .............................................................. OK\nvariables/optionally_coupled.catch_out_of_bound_default_access/coupled .................................... OK\nsamplers/base.global_vs_local/base_1rank .................................................................. OK\nfunctions/solution_function.nonexistent_var_err ........................................................... OK\nfunctions/piecewise_multilinear.twoDa ..................................................................... OK\nrestrictable/block_api_test.mat/hasBlockMaterialProperty_false ............................................ OK\noutputs/checkpoint.block/recover_with_checkpoint_block .................................................... OK\nreporters/base.special_types .............................................................................. OK\noutputs/exodus.nodal_output ............................................................................... OK\nsystem_interfaces.solver/superlu: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.solver/superlu: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.solver/superlu: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.solver/superlu: MPIR_Init_thread(586)..............:\nsystem_interfaces.solver/superlu: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.solver/superlu: MPIDI_CH3_Init(105)................:\nsystem_interfaces.solver/superlu: MPID_nem_init(324).................:\nsystem_interfaces.solver/superlu: MPID_nem_tcp_init(175).............:\nsystem_interfaces.solver/superlu: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.solver/superlu: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.solver/superlu: (unknown)(): Invalid group\nsystem_interfaces.solver/superlu: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.solver/superlu: MPIR_Init_thread(586)..............:\nsystem_interfaces.solver/superlu: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.solver/superlu: MPIDI_CH3_Init(105)................:\nsystem_interfaces.solver/superlu: MPID_nem_init(324).................:\nsystem_interfaces.solver/superlu: MPID_nem_tcp_init(175).............:\nsystem_interfaces.solver/superlu: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.solver/superlu: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.solver/superlu: (unknown)(): Invalid group\nsystem_interfaces.solver/superlu:\nsystem_interfaces.solver/superlu:\nsystem_interfaces.solver/superlu: Exit Code: 8\nsystem_interfaces.solver/superlu: ################################################################################\nsystem_interfaces.solver/superlu: Tester failed, reason: CRASH\nsystem_interfaces.solver/superlu:\nsystem_interfaces.solver/superlu ................................................. [min_cpus=2] FAILED (CRASH)\nmultiapps/picard.steady_with_custom_convergence_check ..................................................... OK\noutputs/console.norms ..................................................................................... OK\ninterfacekernels/1d_interface.reaction_1D_steady_CSVDiff .................................................. OK\nbcs/periodic.testperiodic_vector: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/bcs/periodic\nbcs/periodic.testperiodic_vector: Running command: /Users/almeag-mac/projects1/moose/test/moose_test-opt -i periodic_vector_bc_test.i --error --error-unused --error-override --no-gdb-backtrace\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: ERROR: Bad FEType.family == LAGRANGE_VEC\nbcs/periodic.testperiodic_vector: Stack frames: 17\nbcs/periodic.testperiodic_vector: 0: 0   libmesh_opt.0.dylib                 0x000000010ffd53bb libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits >&) + 1067\nbcs/periodic.testperiodic_vector: 1: 1   libmesh_opt.0.dylib                 0x000000010ffd1e2d libMesh::MacroFunctions::report_error(char const*, int, char const*, char const*) + 269\nbcs/periodic.testperiodic_vector: 2: 2   libmesh_opt.0.dylib                 0x00000001100a6147 libMesh::FEGenericBase::build(unsigned int, libMesh::FEType const&) + 4471\nbcs/periodic.testperiodic_vector: 3: 3   libmesh_opt.0.dylib                 0x00000001100ae548 libMesh::FEGenericBase::compute_periodic_constraints(libMesh::DofConstraints&, libMesh::DofMap&, libMesh::PeriodicBoundaries const&, libMesh::MeshBase const&, libMesh::PointLocatorBase const*, unsigned int, libMesh::Elem const*) + 168\nbcs/periodic.testperiodic_vector: 4: 4   libmesh_opt.0.dylib                 0x000000010ff9d218 (anonymous namespace)::ComputeConstraints::operator()(libMesh::StoredRange<libMesh::MeshBase::const_element_iterator, libMesh::Elem const*> const&) const + 280\nbcs/periodic.testperiodic_vector: 5: 5   libmesh_opt.0.dylib                 0x000000010ff9c68c libMesh::DofMap::create_dof_constraints(libMesh::MeshBase const&, double) + 844\nbcs/periodic.testperiodic_vector: 6: 6   libmesh_opt.0.dylib                 0x0000000110686721 libMesh::System::reinit_constraints() + 33\nbcs/periodic.testperiodic_vector: 7: 7   libmesh_opt.0.dylib                 0x0000000110685c6a libMesh::System::init_data() + 202\nbcs/periodic.testperiodic_vector: 8: 8   libmesh_opt.0.dylib                 0x000000011068e7f8 libMesh::System::init() + 40\nbcs/periodic.testperiodic_vector: 9: 9   libmesh_opt.0.dylib                 0x0000000110657312 libMesh::EquationSystems::init() + 1234\nbcs/periodic.testperiodic_vector: 10: 10  libmoose-opt.0.dylib                0x000000010eeff433 FEProblemBase::init() + 1043\nbcs/periodic.testperiodic_vector: 11: 11  libmoose-opt.0.dylib                0x000000010f1f183c ActionWarehouse::executeActionsWithAction(std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator > const&) + 940\nbcs/periodic.testperiodic_vector: 12: 12  libmoose-opt.0.dylib                0x000000010f228cf8 ActionWarehouse::executeAllActions() + 232\nbcs/periodic.testperiodic_vector: 13: 13  libmoose-opt.0.dylib                0x000000010f69c2e0 MooseApp::runInputFile() + 80\nbcs/periodic.testperiodic_vector: 14: 14  libmoose-opt.0.dylib                0x000000010f697a4c MooseApp::run() + 2684\nbcs/periodic.testperiodic_vector: 15: 15  moose_test-opt                      0x000000010e52ec34 main + 132\nbcs/periodic.testperiodic_vector: 16: 16  libdyld.dylib                       0x00007fff67d0bcc9 start + 1\nbcs/periodic.testperiodic_vector: [0] /Users/almeag-mac/projects/raccoon/moose/scripts/../libmesh/src/fe/fe_base.C, line 335, compiled Jun 13 2021 at 10:45:15\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: *** ERROR ***\nbcs/periodic.testperiodic_vector: ERROR: Bad FEType.family == LAGRANGE_VEC\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: Stack frames: 6\nbcs/periodic.testperiodic_vector: 0: 0   libmesh_opt.0.dylib                 0x000000010ffd53bb libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits >&) + 1067\nbcs/periodic.testperiodic_vector: 1: 1   libmoose-opt.0.dylib                0x000000010f6c9854 moose::internal::mooseErrorRaw(std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator >, std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator >) + 852\nbcs/periodic.testperiodic_vector: 2: 2   libmoose-opt.0.dylib                0x000000010f044a6e void mooseError<char const*>(char const*&&) + 270\nbcs/periodic.testperiodic_vector: 3: 3   libmoose-opt.0.dylib                0x000000010f698404 MooseApp::run() + 5172\nbcs/periodic.testperiodic_vector: 4: 4   moose_test-opt                      0x000000010e52ec34 main + 132\nbcs/periodic.testperiodic_vector: 5: 5   libdyld.dylib                       0x00007fff67d0bcc9 start + 1\nbcs/periodic.testperiodic_vector: application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\nbcs/periodic.testperiodic_vector: [unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nbcs/periodic.testperiodic_vector: :\nbcs/periodic.testperiodic_vector: system msg for write_line failure : Bad file descriptor\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: ERROR: Bad FEType.family == LAGRANGE_VEC\nbcs/periodic.testperiodic_vector: Stack frames: 17\nbcs/periodic.testperiodic_vector: 0: 0   libmesh_opt.0.dylib                 0x000000010ffd53bb libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits >&) + 1067\nbcs/periodic.testperiodic_vector: 1: 1   libmesh_opt.0.dylib                 0x000000010ffd1e2d libMesh::MacroFunctions::report_error(char const*, int, char const*, char const*) + 269\nbcs/periodic.testperiodic_vector: 2: 2   libmesh_opt.0.dylib                 0x00000001100a6147 libMesh::FEGenericBase::build(unsigned int, libMesh::FEType const&) + 4471\nbcs/periodic.testperiodic_vector: 3: 3   libmesh_opt.0.dylib                 0x00000001100ae548 libMesh::FEGenericBase::compute_periodic_constraints(libMesh::DofConstraints&, libMesh::DofMap&, libMesh::PeriodicBoundaries const&, libMesh::MeshBase const&, libMesh::PointLocatorBase const*, unsigned int, libMesh::Elem const*) + 168\nbcs/periodic.testperiodic_vector: 4: 4   libmesh_opt.0.dylib                 0x000000010ff9d218 (anonymous namespace)::ComputeConstraints::operator()(libMesh::StoredRange<libMesh::MeshBase::const_element_iterator, libMesh::Elem const*> const&) const + 280\nbcs/periodic.testperiodic_vector: 5: 5   libmesh_opt.0.dylib                 0x000000010ff9c68c libMesh::DofMap::create_dof_constraints(libMesh::MeshBase const&, double) + 844\nbcs/periodic.testperiodic_vector: 6: 6   libmesh_opt.0.dylib                 0x0000000110686721 libMesh::System::reinit_constraints() + 33\nbcs/periodic.testperiodic_vector: 7: 7   libmesh_opt.0.dylib                 0x0000000110685c6a libMesh::System::init_data() + 202\nbcs/periodic.testperiodic_vector: 8: 8   libmesh_opt.0.dylib                 0x000000011068e7f8 libMesh::System::init() + 40\nbcs/periodic.testperiodic_vector: 9: 9   libmesh_opt.0.dylib                 0x0000000110657312 libMesh::EquationSystems::init() + 1234\nbcs/periodic.testperiodic_vector: 10: 10  libmoose-opt.0.dylib                0x000000010eeff433 FEProblemBase::init() + 1043\nbcs/periodic.testperiodic_vector: 11: 11  libmoose-opt.0.dylib                0x000000010f1f183c ActionWarehouse::executeActionsWithAction(std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator > const&) + 940\nbcs/periodic.testperiodic_vector: 12: 12  libmoose-opt.0.dylib                0x000000010f228cf8 ActionWarehouse::executeAllActions() + 232\nbcs/periodic.testperiodic_vector: 13: 13  libmoose-opt.0.dylib                0x000000010f69c2e0 MooseApp::runInputFile() + 80\nbcs/periodic.testperiodic_vector: 14: 14  libmoose-opt.0.dylib                0x000000010f697a4c MooseApp::run() + 2684\nbcs/periodic.testperiodic_vector: 15: 15  moose_test-opt                      0x000000010e52ec34 main + 132\nbcs/periodic.testperiodic_vector: 16: 16  libdyld.dylib                       0x00007fff67d0bcc9 start + 1\nbcs/periodic.testperiodic_vector: [0] /Users/almeag-mac/projects/raccoon/moose/scripts/../libmesh/src/fe/fe_base.C, line 335, compiled Jun 13 2021 at 10:45:15\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: *** ERROR ***\nbcs/periodic.testperiodic_vector: ERROR: Bad FEType.family == LAGRANGE_VEC\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: Stack frames: 6\nbcs/periodic.testperiodic_vector: 0: 0   libmesh_opt.0.dylib                 0x000000010ffd53bb libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits >&) + 1067\nbcs/periodic.testperiodic_vector: 1: 1   libmoose-opt.0.dylib                0x000000010f6c9854 moose::internal::mooseErrorRaw(std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator >, std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator >) + 852\nbcs/periodic.testperiodic_vector: 2: 2   libmoose-opt.0.dylib                0x000000010f044a6e void mooseError<char const*>(char const*&&) + 270\nbcs/periodic.testperiodic_vector: 3: 3   libmoose-opt.0.dylib                0x000000010f698404 MooseApp::run() + 5172\nbcs/periodic.testperiodic_vector: 4: 4   moose_test-opt                      0x000000010e52ec34 main + 132\nbcs/periodic.testperiodic_vector: 5: 5   libdyld.dylib                       0x00007fff67d0bcc9 start + 1\nbcs/periodic.testperiodic_vector: application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\nbcs/periodic.testperiodic_vector: [unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nbcs/periodic.testperiodic_vector: :\nbcs/periodic.testperiodic_vector: system msg for write_line failure : Bad file descriptor\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: Exit Code: 1\nbcs/periodic.testperiodic_vector: ################################################################################\nbcs/periodic.testperiodic_vector: Tester failed, reason: ERRMSG\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector ............................................................. FAILED (ERRMSG)\noutputs/csv.sort .......................................................................................... OK\nIssue #14\ninterfaces/postprocessorinterface.missing_errors/by_name .................................................. OK\ninterfaces/vectorpostprocessorinterface.missing_errors/by_name ............................................ OK\ninterfaces/userobjectinterface.has_uo/name_T .............................................................. OK\nmisc/exception.parallel_exception_jacobian_transient ...................................................... OK\nnodalkernels/constraint_enforcement.vi/rsls_amg ........................................................... OK\nauxkernels/solution_aux.exodus_direct ..................................................................... OK\ntag.systems/test_tag_nodal_kernels ........................................................................ OK\ngeomsearch/2d_moving_penetration.pl_test3q ................................................................ OK\nmesh_modifiers/block_deleter.delete/BlockDeleterTest12 .................................................... OK\nmaterials/material.exception/serial ....................................................................... OK\nkernels/hfem.variable_dirichlet ........................................................................... OK\nmesh/mesh_generation.annular/disc ......................................................................... OK\ngeomsearch/3d_moving_penetration.pl_test3q ................................................................ OK\nmultiapps/steffensen_postprocessor.pp_transient/app_begin_transfers_begin_steffensen_sub .................. OK\nkernels/vector_fe.jacobian ................................................................................ OK\nmultiapps/secant_postprocessor.pp_transient/app_begin_transfers_begin_secant_sub .......................... OK\ntransfers/multiapp_conservative_transfer.userobject_transfer_csv .......................................... OK\nmesh/custom_partitioner.group/custom_linear_partitioner: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/custom_partitioner\nmesh/custom_partitioner.group/custom_linear_partitioner: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i custom_linear_partitioner_test.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner: Exit Code: 8\nmesh/custom_partitioner.group/custom_linear_partitioner: ################################################################################\nmesh/custom_partitioner.group/custom_linear_partitioner: Tester failed, reason: CRASH\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner .......................... [min_cpus=2] FAILED (CRASH)\nmeshgenerators/break_mesh_by_block_generator.surrounding_block_restricted/split_all ....................... OK\ntime_steppers/iteration_adaptive.pps_lim .................................................................. OK\nuserobjects/setup_interface_count.setup_interface_count/NodalSideUserObject ............................... OK\nmesh/high_order_elems.test_prism15_refine ................................................................. OK\nmultiapps/secant.variables_transient/app_end_transfers_begin .............................................. OK\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/transfers/multiapp_vector_pp_transfer\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i master.i --error --error-unused --error-override --no-gdb-backtrace\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Exit Code: 8\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: ################################################################################\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Tester failed, reason: CRASH\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer ......................... [min_cpus=2] FAILED (CRASH)\npostprocessors/num_iterations.methods/implicit_euler ...................................................... OK\ninterfacekernels/2d_interface.vector_2d ................................................................... OK\nproblems/eigen_problem/eigensolvers.eigen_scalar_kernel ................................................... OK\nmaterials/derivative_material_interface.bad_evaluation/nan ................................................ OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4ns ......................................... OK\nreporters/mesh_info.info/default: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/reporters/mesh_info\nreporters/mesh_info.info/default: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i mesh_info.i --error --error-unused --error-override --no-gdb-backtrace\nreporters/mesh_info.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\nreporters/mesh_info.info/default: MPIR_Init_thread(586)..............:\nreporters/mesh_info.info/default: MPID_Init(224).....................: channel initialization failed\nreporters/mesh_info.info/default: MPIDI_CH3_Init(105)................:\nreporters/mesh_info.info/default: MPID_nem_init(324).................:\nreporters/mesh_info.info/default: MPID_nem_tcp_init(175).............:\nreporters/mesh_info.info/default: MPID_nem_tcp_get_business_card(401):\nreporters/mesh_info.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nreporters/mesh_info.info/default: (unknown)(): Invalid group\nreporters/mesh_info.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\nreporters/mesh_info.info/default: MPIR_Init_thread(586)..............:\nreporters/mesh_info.info/default: MPID_Init(224).....................: channel initialization failed\nreporters/mesh_info.info/default: MPIDI_CH3_Init(105)................:\nreporters/mesh_info.info/default: MPID_nem_init(324).................:\nreporters/mesh_info.info/default: MPID_nem_tcp_init(175).............:\nreporters/mesh_info.info/default: MPID_nem_tcp_get_business_card(401):\nreporters/mesh_info.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nreporters/mesh_info.info/default: (unknown)(): Invalid group\nreporters/mesh_info.info/default:\nreporters/mesh_info.info/default: ################################################################################\nreporters/mesh_info.info/default: Tester failed, reason: CRASH\nreporters/mesh_info.info/default:\nreporters/mesh_info.info/default ................................................. [min_cpus=2] FAILED (CRASH)\nfunctions/solution_function.solution_function/grad_p1 ..................................................... OK\nproblems/eigen_problem/eigensolvers.dg_krylovschur ........................................................ OK\nreporters/mesh_info.info/limit ............................................................................ OK\nmultiapps/secant.variables_transient/app_end_transfers_end ................................................ OK\nsystem_interfaces.solver/mumps: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.solver/mumps: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.solver/mumps: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.solver/mumps: MPIR_Init_thread(586)..............:\nsystem_interfaces.solver/mumps: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.solver/mumps: MPIDI_CH3_Init(105)................:\nsystem_interfaces.solver/mumps: MPID_nem_init(324).................:\nsystem_interfaces.solver/mumps: MPID_nem_tcp_init(175).............:\nsystem_interfaces.solver/mumps: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.solver/mumps: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.solver/mumps: (unknown)(): Invalid group\nsystem_interfaces.solver/mumps: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.solver/mumps: MPIR_Init_thread(586)..............:\nsystem_interfaces.solver/mumps: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.solver/mumps: MPIDI_CH3_Init(105)................:\nsystem_interfaces.solver/mumps: MPID_nem_init(324).................:\nsystem_interfaces.solver/mumps: MPID_nem_tcp_init(175).............:\nsystem_interfaces.solver/mumps: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.solver/mumps: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.solver/mumps: (unknown)(): Invalid group\nsystem_interfaces.solver/mumps:\nsystem_interfaces.solver/mumps:\nsystem_interfaces.solver/mumps: Exit Code: 8\nsystem_interfaces.solver/mumps: ################################################################################\nsystem_interfaces.solver/mumps: Tester failed, reason: CRASH\nsystem_interfaces.solver/mumps:\nsystem_interfaces.solver/mumps ................................................... [min_cpus=2] FAILED (CRASH)\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4nstt ....................................... OK\nfunctions/solution_function.solution_function/grad_p2 ..................................................... OK\noutputs/console.transient ................................................................................. OK\ninterfacekernels/1d_interface.reaction_1D_transient ....................................................... OK\nauxkernels/vector_postprocessor_visualization.test: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/auxkernels/vector_postprocessor_visualization\nauxkernels/vector_postprocessor_visualization.test: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i vector_postprocessor_visualization.i --error --error-unused --error-override --no-gdb-backtrace\nauxkernels/vector_postprocessor_visualization.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nauxkernels/vector_postprocessor_visualization.test: MPIR_Init_thread(586)..............:\nauxkernels/vector_postprocessor_visualization.test: MPID_Init(224).....................: channel initialization failed\nauxkernels/vector_postprocessor_visualization.test: MPIDI_CH3_Init(105)................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_init(324).................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(175).............:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_get_business_card(401):\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nauxkernels/vector_postprocessor_visualization.test: (unknown)(): Invalid group\nauxkernels/vector_postprocessor_visualization.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nauxkernels/vector_postprocessor_visualization.test: MPIR_Init_thread(586)..............:\nauxkernels/vector_postprocessor_visualization.test: MPID_Init(224).....................: channel initialization failed\nauxkernels/vector_postprocessor_visualization.test: MPIDI_CH3_Init(105)................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_init(324).................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(175).............:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_get_business_card(401):\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nauxkernels/vector_postprocessor_visualization.test: (unknown)(): Invalid group\nauxkernels/vector_postprocessor_visualization.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nauxkernels/vector_postprocessor_visualization.test: MPIR_Init_thread(586)..............:\nauxkernels/vector_postprocessor_visualization.test: MPID_Init(224).....................: channel initialization failed\nauxkernels/vector_postprocessor_visualization.test: MPIDI_CH3_Init(105)................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_init(324).................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(175).............:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_get_business_card(401):\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nauxkernels/vector_postprocessor_visualization.test: (unknown)(): Invalid group\nauxkernels/vector_postprocessor_visualization.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nauxkernels/vector_postprocessor_visualization.test: MPIR_Init_thread(586)..............:\nauxkernels/vector_postprocessor_visualization.test: MPID_Init(224).....................: channel initialization failed\nauxkernels/vector_postprocessor_visualization.test: MPIDI_CH3_Init(105)................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_init(324).................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(175).............:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_get_business_card(401):\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nauxkernels/vector_postprocessor_visualization.test: (unknown)(): Invalid group\nauxkernels/vector_postprocessor_visualization.test:\nauxkernels/vector_postprocessor_visualization.test:\nauxkernels/vector_postprocessor_visualization.test: Exit Code: 8\nauxkernels/vector_postprocessor_visualization.test: ################################################################################\nauxkernels/vector_postprocessor_visualization.test: Tester failed, reason: CRASH\nuxkernels/vector_postprocessor_visualization.test:\nauxkernels/vector_postprocessor_visualization.test ............................... [min_cpus=3] FAILED (CRASH)\nbcs/dmg_periodic.check_one_step: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/bcs/dmg_periodic\nbcs/dmg_periodic.check_one_step: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i dmg_periodic_bc.i /UserObjects/uo/type=CheckGhostedBoundaries /UserObjects/uo/total_num_bdry_sides=160 Outputs/hide=\"pid\" Outputs/exodus=false Executioner/num_steps=1 --error --error-unused --error-override --no-gdb-backtrace\nbcs/dmg_periodic.check_one_step: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/dmg_periodic.check_one_step: MPIR_Init_thread(586)..............:\nbcs/dmg_periodic.check_one_step: MPID_Init(224).....................: channel initialization failed\nbcs/dmg_periodic.check_one_step: MPIDI_CH3_Init(105)................:\nbcs/dmg_periodic.check_one_step: MPID_nem_init(324).................:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(175).............:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_get_business_card(401):\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nbcs/dmg_periodic.check_one_step: (unknown)(): Invalid group\nbcs/dmg_periodic.check_one_step: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/dmg_periodic.check_one_step: MPIR_Init_thread(586)..............:\nbcs/dmg_periodic.check_one_step: MPID_Init(224).....................: channel initialization failed\nbcs/dmg_periodic.check_one_step: MPIDI_CH3_Init(105)................:\nbcs/dmg_periodic.check_one_step: MPID_nem_init(324).................:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(175).............:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_get_business_card(401):\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nbcs/dmg_periodic.check_one_step: (unknown)(): Invalid group\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step: Exit Code: 8\nbcs/dmg_periodic.check_one_step: ################################################################################\nbcs/dmg_periodic.check_one_step: Tester failed, reason: CRASH\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step .................................................. [min_cpus=2] FAILED (CRASH)\nmultiapps/secant_postprocessor.pp_transient/app_end_transfers_begin_secant_sub ............................ OK\noutputs/vtk.files/parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/vtk\noutputs/vtk.files/parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i vtk_parallel.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/vtk.files/parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.files/parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.files/parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.files/parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.files/parallel: MPID_nem_init(324).................:\noutputs/vtk.files/parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.files/parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.files/parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/vtk.files/parallel: (unknown)(): Invalid group\noutputs/vtk.files/parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.files/parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.files/parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.files/parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.files/parallel: MPID_nem_init(324).................:\noutputs/vtk.files/parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.files/parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.files/parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/vtk.files/parallel: (unknown)(): Invalid group\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel: Exit Code: 8\noutputs/vtk.files/parallel: ################################################################################\noutputs/vtk.files/parallel: Tester failed, reason: CRASH\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel ....................................................... [min_cpus=2] FAILED (CRASH)\nproblems/eigen_problem/eigensolvers.eigen_as_master ....................................................... OK\nics/depend_on_uo.ic_depend_on_uo: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/ics/depend_on_uo\nics/depend_on_uo.ic_depend_on_uo: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i geometric_neighbors_ic.i --error --error-unused --error-override --no-gdb-backtrace\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo: Exit Code: 8\nics/depend_on_uo.ic_depend_on_uo: ################################################################################\nics/depend_on_uo.ic_depend_on_uo: Tester failed, reason: CRASH\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo ................................................. [min_cpus=2] FAILED (CRASH)\nbcs/periodic.testperiodic_dp: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/bcs/periodic\nbcs/periodic.testperiodic_dp: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i periodic_bc_displaced_problem.i --error --error-unused --error-override --no-gdb-backtrace\nbcs/periodic.testperiodic_dp: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/periodic.testperiodic_dp: MPIR_Init_thread(586)..............:\nbcs/periodic.testperiodic_dp: MPID_Init(224).....................: channel initialization failed\nbcs/periodic.testperiodic_dp: MPIDI_CH3_Init(105)................:\nbcs/periodic.testperiodic_dp: MPID_nem_init(324).................:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(175).............:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_get_business_card(401):\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nbcs/periodic.testperiodic_dp: (unknown)(): Invalid group\nbcs/periodic.testperiodic_dp: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/periodic.testperiodic_dp: MPIR_Init_thread(586)..............:\nbcs/periodic.testperiodic_dp: MPID_Init(224).....................: channel initialization failed\nbcs/periodic.testperiodic_dp: MPIDI_CH3_Init(105)................:\nbcs/periodic.testperiodic_dp: MPID_nem_init(324).................:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(175).............:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_get_business_card(401):\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nbcs/periodic.testperiodic_dp: (unknown)(): Invalid group\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp: Exit Code: 8\nbcs/periodic.testperiodic_dp: ################################################################################\nbcs/periodic.testperiodic_dp: Tester failed, reason: CRASH\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp ..................................................... [min_cpus=2] FAILED (CRASH)\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4qns ........................................ OK\nmultiapps/picard.steady_with_postprocessor_convergence .................................................... OK\nmultiapps/steffensen.variables_transient/app_begin_transfers_end_steffensen_sub ........................... OK\ninterfacekernels/1d_interface.reaction_1D_transient_Jac ................................................... OK\ntag.controls-tagging ...................................................................................... OK\nauxkernels/solution_aux.exodus_interp_restart/part2 ....................................................... OK\nmultiapps/picard_postprocessor.pp_transient/app_begin_transfers_begin_picard_sub .......................... OK\nfvkernels/mms/non-orthogonal.compact: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/fvkernels/mms/non-orthogonal\nfvkernels/mms/non-orthogonal.compact: Running command: python -m unittest -v test.TestCompactADR\nfvkernels/mms/non-orthogonal.compact: The 'mms' package requires sympy for symbolic function evaluation, it can be installed by running pip install sympy --user.\nfvkernels/mms/non-orthogonal.compact: Running: /Users/almeag-mac/projects1/moose/test/moose_test-opt -i advection-diffusion-reaction.i Mesh/uniform_refine=0\nfvkernels/mms/non-orthogonal.compact: test (test.TestCompactADR) ... Fatal error in MPI_Init_thread: Invalid group, error stack:\nfvkernels/mms/non-orthogonal.compact: MPIR_Init_thread(586)..............:\nfvkernels/mms/non-orthogonal.compact: MPID_Init(224).....................: channel initialization failed\nfvkernels/mms/non-orthogonal.compact: MPIDI_CH3_Init(105)................:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_init(324).................:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_init(175).............:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_get_business_card(401):\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nfvkernels/mms/non-orthogonal.compact: (unknown)(): Invalid group\nfvkernels/mms/non-orthogonal.compact: ERROR\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: ======================================================================\nfvkernels/mms/non-orthogonal.compact: ERROR: test (test.TestCompactADR)\nfvkernels/mms/non-orthogonal.compact: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.compact: Traceback (most recent call last):\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/almeag-mac/projects1/moose/test/tests/fvkernels/mms/non-orthogonal/test.py\", line 7, in test\nfvkernels/mms/non-orthogonal.compact:     df1 = mms.run_spatial('advection-diffusion-reaction.i', 7, mpi=2)\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/almeag-mac/projects1/moose/python/mms/runner.py\", line 129, in run_spatial\nfvkernels/mms/non-orthogonal.compact:     return _runner(*args, rtype=SPATIAL, **kwargs)\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/almeag-mac/projects1/moose/python/mms/runner.py\", line 102, in _runner\nfvkernels/mms/non-orthogonal.compact:     raise IOError(\"The CSV output does not exist: {}\".format(csv))\nfvkernels/mms/non-orthogonal.compact: OSError: The CSV output does not exist: None\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.compact: Ran 1 test in 0.311s\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: FAILED (errors=1)\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: Exit Code: 1\nfvkernels/mms/non-orthogonal.compact: ################################################################################\nfvkernels/mms/non-orthogonal.compact: Tester failed, reason: CRASH\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact ............................................. [min_cpus=2] FAILED (CRASH)\ntime_steppers/iteration_adaptive.multi_piecewise_linear_function_point .................................... OK\nrestart/restartable_types.parallel/first: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/restart/restartable_types\nrestart/restartable_types.parallel/first: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i restartable_types.i --error --error-unused --error-override --no-gdb-backtrace\nrestart/restartable_types.parallel/first: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/restartable_types.parallel/first: MPIR_Init_thread(586)..............:\nrestart/restartable_types.parallel/first: MPID_Init(224).....................: channel initialization failed\nrestart/restartable_types.parallel/first: MPIDI_CH3_Init(105)................:\nrestart/restartable_types.parallel/first: MPID_nem_init(324).................:\nrestart/restartable_types.parallel/first: MPID_nem_tcp_init(175).............:\nrestart/restartable_types.parallel/first: MPID_nem_tcp_get_business_card(401):\nrestart/restartable_types.parallel/first: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/restartable_types.parallel/first: (unknown)(): Invalid group\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first: Exit Code: 8\nrestart/restartable_types.parallel/first: ################################################################################\nrestart/restartable_types.parallel/first: Tester failed, reason: CRASH\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first ......................................... [min_cpus=2] FAILED (CRASH)\nrestart/restartable_types.parallel/second .......................................... [skipped dependency] SKIP\ngeomsearch/3d_moving_penetration.pl_test4q ................................................................ OK\npreconditioners/hmg.hmg_3D: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/preconditioners/hmg\npreconditioners/hmg.hmg_3D: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i diffusion_hmg.i Mesh/dmg/dim=3 Mesh/dmg/nz=10 Outputs/file_base=diffusion_hmg_3d_out -log_view --error --error-unused --error-override --no-gdb-backtrace\npreconditioners/hmg.hmg_3D: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg_3D: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg_3D: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg_3D: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg_3D: MPID_nem_init(324).................:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npreconditioners/hmg.hmg_3D: (unknown)(): Invalid group\npreconditioners/hmg.hmg_3D: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg_3D: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg_3D: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg_3D: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg_3D: MPID_nem_init(324).................:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npreconditioners/hmg.hmg_3D: (unknown)(): Invalid group\npreconditioners/hmg.hmg_3D: ################################################################################\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: Unable to match the following pattern against the program's output:\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: PETSc\\s+Preconditioner:\\s+hmg\\s+strong_threshold:\\s+0.7\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: ################################################################################\npreconditioners/hmg.hmg_3D: Tester failed, reason: EXPECTED OUTPUT MISSING\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D ..................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\noutputs/iterative.start_stop/output_end_step .............................................................. OK\nmultiapps/steffensen_postprocessor.pp_transient/app_end_transfers_end_steffensen_sub ...................... OK\nics/depend_on_uo.scalar_ic_from_uo: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/ics/depend_on_uo\nics/depend_on_uo.scalar_ic_from_uo: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i scalar_ic_from_uo.i --error --error-unused --error-override --no-gdb-backtrace\nics/depend_on_uo.scalar_ic_from_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.scalar_ic_from_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.scalar_ic_from_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_init(324).................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.scalar_ic_from_uo: (unknown)(): Invalid group\nics/depend_on_uo.scalar_ic_from_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.scalar_ic_from_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.scalar_ic_from_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_init(324).................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.scalar_ic_from_uo: (unknown)(): Invalid group\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo: Exit Code: 8\nics/depend_on_uo.scalar_ic_from_uo: ################################################################################\nics/depend_on_uo.scalar_ic_from_uo: Tester failed, reason: CRASH\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo ............................................... [min_cpus=2] FAILED (CRASH)\noutputs/console.transient_perf_int ........................................................................ OK\nnodalkernels/constraint_enforcement.vi/ssls_amg ........................................................... OK\nmesh/mesh_only.mesh_only_checkpoint: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/mesh_only\nmesh/mesh_only.mesh_only_checkpoint: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i mesh_only.i Mesh/parallel_type=distributed --mesh-only 3d_chimney.cpr --error --error-unused --error-override --no-gdb-backtrace\nmesh/mesh_only.mesh_only_checkpoint: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/mesh_only.mesh_only_checkpoint: MPIR_Init_thread(586)..............:\nmesh/mesh_only.mesh_only_checkpoint: MPID_Init(224).....................: channel initialization failed\nmesh/mesh_only.mesh_only_checkpoint: MPIDI_CH3_Init(105)................:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_init(324).................:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_init(175).............:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_get_business_card(401):\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/mesh_only.mesh_only_checkpoint: (unknown)(): Invalid group\nmesh/mesh_only.mesh_only_checkpoint: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/mesh_only.mesh_only_checkpoint: MPIR_Init_thread(586)..............:\nmesh/mesh_only.mesh_only_checkpoint: MPID_Init(224).....................: channel initialization failed\nmesh/mesh_only.mesh_only_checkpoint: MPIDI_CH3_Init(105)................:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_init(324).................:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_init(175).............:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_get_business_card(401):\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/mesh_only.mesh_only_checkpoint: (unknown)(): Invalid group\nmesh/mesh_only.mesh_only_checkpoint:\nmesh/mesh_only.mesh_only_checkpoint:\nmesh/mesh_only.mesh_only_checkpoint: Exit Code: 8\nmesh/mesh_only.mesh_only_checkpoint: ################################################################################\nmesh/mesh_only.mesh_only_checkpoint: Tester failed, reason: CRASH\nmesh/mesh_only.mesh_only_checkpoint:\nmesh/mesh_only.mesh_only_checkpoint .............................................. [min_cpus=3] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/custom_partitioner\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i custom_linear_partitioner_test_displacement.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Exit Code: 8\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: ################################################################################\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Tester failed, reason: CRASH\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement ............. [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.tester_fail: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/csv_reader\nvectorpostprocessors/csv_reader.tester_fail: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i read.i UserObjects/tester/rank=1 UserObjects/tester/gold='1 2 3' Outputs/csv=false --error --error-unused --error-override --no-gdb-backtrace --keep-cout --redirect-output tester_fail\nvectorpostprocessors/csv_reader.tester_fail: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.tester_fail: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.tester_fail: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/csv_reader.tester_fail: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.tester_fail: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.tester_fail: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.tester_fail: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/csv_reader.tester_fail: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Unable to match the following pattern against the program's output:\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: The supplied gold data does not match the VPP data on the given rank.\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail: Tester failed, reason: EXPECTED ERROR MISSING\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\ngeomsearch/2d_moving_penetration.pl_test3ns ............................................................... OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4qnstt ...................................... OK\ntransfers/multiapp_nearest_node_transfer.parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/transfers/multiapp_nearest_node_transfer\ntransfers/multiapp_nearest_node_transfer.parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i parallel_master.i --error --error-unused --error-override --no-gdb-backtrace\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel: Exit Code: 8\ntransfers/multiapp_nearest_node_transfer.parallel: ################################################################################\ntransfers/multiapp_nearest_node_transfer.parallel: Tester failed, reason: CRASH\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel ................................ [min_cpus=2] FAILED (CRASH)\nmaterials/stateful_prop.ad/reg ............................................................................ OK\nmaterials/derivative_material_interface.postprocessor_coupling/parsed_material ............................ OK\nfvkernels/mms/non-orthogonal.extended: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/fvkernels/mms/non-orthogonal\nfvkernels/mms/non-orthogonal.extended: Running command: python -m unittest -v test.TestExtendedADR\nfvkernels/mms/non-orthogonal.extended: The 'mms' package requires sympy for symbolic function evaluation, it can be installed by running pip install sympy --user.\nfvkernels/mms/non-orthogonal.extended: Running: /Users/almeag-mac/projects1/moose/test/moose_test-opt -i extended-adr.i Mesh/uniform_refine=0\nfvkernels/mms/non-orthogonal.extended: test (test.TestExtendedADR) ... Fatal error in MPI_Init_thread: Invalid group, error stack:\nfvkernels/mms/non-orthogonal.extended: MPIR_Init_thread(586)..............:\nfvkernels/mms/non-orthogonal.extended: MPID_Init(224).....................: channel initialization failed\nfvkernels/mms/non-orthogonal.extended: MPIDI_CH3_Init(105)................:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_init(324).................:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_init(175).............:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_get_business_card(401):\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nfvkernels/mms/non-orthogonal.extended: (unknown)(): Invalid group\nfvkernels/mms/non-orthogonal.extended: ERROR\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: ======================================================================\nfvkernels/mms/non-orthogonal.extended: ERROR: test (test.TestExtendedADR)\nfvkernels/mms/non-orthogonal.extended: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.extended: Traceback (most recent call last):\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/almeag-mac/projects1/moose/test/tests/fvkernels/mms/non-orthogonal/test.py\", line 23, in test\nfvkernels/mms/non-orthogonal.extended:     df1 = mms.run_spatial('extended-adr.i', 7, mpi=2)\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/almeag-mac/projects1/moose/python/mms/runner.py\", line 129, in run_spatial\nfvkernels/mms/non-orthogonal.extended:     return _runner(*args, rtype=SPATIAL, **kwargs)\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/almeag-mac/projects1/moose/python/mms/runner.py\", line 102, in _runner\nfvkernels/mms/non-orthogonal.extended:     raise IOError(\"The CSV output does not exist: {}\".format(csv))\nfvkernels/mms/non-orthogonal.extended: OSError: The CSV output does not exist: None\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.extended: Ran 1 test in 0.261s\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: FAILED (errors=1)\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: Exit Code: 1\nfvkernels/mms/non-orthogonal.extended: ################################################################################\nfvkernels/mms/non-orthogonal.extended: Tester failed, reason: CRASH\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended ............................................ [min_cpus=2] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/custom_partitioner\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i custom_linear_partitioner_restart_test.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner_restart:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Exit Code: 8\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: ################################################################################\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Tester failed, reason: CRASH\nmesh/custom_partitioner.group/custom_linear_partitioner_restart:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart .................. [min_cpus=2] FAILED (CRASH)\nkernels/vector_fe.coupled_vector_gradient ................................................................. OK\nvectorpostprocessors/csv_reader.read_preic ................................................................ OK\nmesh/high_order_elems.test_pyramid13 ...................................................................... OK\ntime_steppers/iteration_adaptive.multi_piecewise_linear_function_change ................................... OK\ngeomsearch/2d_moving_penetration.pl_test3qns .............................................................. OK\npostprocessors/num_iterations.methods/l_stable_dirk3 ...................................................... OK\nmultiapps/secant.variables_transient/app_begin_transfers_end_secant_sub ................................... OK\noutputs/vtk.solution/diff_serial_mesh_parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/vtk\noutputs/vtk.solution/diff_serial_mesh_parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i vtk_diff_serial_mesh_parallel.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/vtk.solution/diff_serial_mesh_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.solution/diff_serial_mesh_parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_init(324).................:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/vtk.solution/diff_serial_mesh_parallel: (unknown)(): Invalid group\noutputs/vtk.solution/diff_serial_mesh_parallel:\noutputs/vtk.solution/diff_serial_mesh_parallel: ################################################################################\noutputs/vtk.solution/diff_serial_mesh_parallel: Tester failed, reason: CRASH\noutputs/vtk.solution/diff_serial_mesh_parallel:\noutputs/vtk.solution/diff_serial_mesh_parallel ................................... [min_cpus=2] FAILED (CRASH)\nmaterials/stateful_prop.ad/ad ............................................................................. OK\ngeomsearch/3d_moving_penetration.pl_test4tt ............................................................... OK\ninterfaces/random.parallel_verification: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/interfaces/random\ninterfaces/random.parallel_verification: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i random.i --error --error-unused --error-override --no-gdb-backtrace\ninterfaces/random.parallel_verification: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfaces/random.parallel_verification: MPIR_Init_thread(586)..............:\ninterfaces/random.parallel_verification: MPID_Init(224).....................: channel initialization failed\ninterfaces/random.parallel_verification: MPIDI_CH3_Init(105)................:\ninterfaces/random.parallel_verification: MPID_nem_init(324).................:\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(175).............:\ninterfaces/random.parallel_verification: MPID_nem_tcp_get_business_card(401):\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ninterfaces/random.parallel_verification: (unknown)(): Invalid group\ninterfaces/random.parallel_verification: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfaces/random.parallel_verification: MPIR_Init_thread(586)..............:\ninterfaces/random.parallel_verification: MPID_Init(224).....................: channel initialization failed\ninterfaces/random.parallel_verification: MPIDI_CH3_Init(105)................:\ninterfaces/random.parallel_verification: MPID_nem_init(324).................:\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(175).............:\ninterfaces/random.parallel_verification: MPID_nem_tcp_get_business_card(401):\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ninterfaces/random.parallel_verification: (unknown)(): Invalid group\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification: Exit Code: 8\ninterfaces/random.parallel_verification: ################################################################################\ninterfaces/random.parallel_verification: Tester failed, reason: CRASH\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification .......................................... [min_cpus=2] FAILED (CRASH)\ninterfaces/random.test_par_mesh .................................................... [skipped dependency] SKIP\ninterfaces/random.threads_verification ............................................. [skipped dependency] SKIP\nmaterials/derivative_material_interface.postprocessor_coupling/derivative_parsed_material ................. OK\noutputs/console._console_const ............................................................................ OK\nmesh/mesh_generation.annular_except1_deprecated ........................................................... OK\nbcs/periodic.testwedge .................................................................................... OK\nproblems/eigen_problem/eigensolvers.eigen_as_sub .......................................................... OK\nnodalkernels/constraint_enforcement.unbounded ............................................................. OK\nmeshgenerators/distributed_rectilinear/partition.2D_3: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/meshgenerators/distributed_rectilinear/partition\nmeshgenerators/distributed_rectilinear/partition.2D_3: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i squarish_partition.i --error --error-unused --error-override --no-gdb-backtrace\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3: Exit Code: 8\nmeshgenerators/distributed_rectilinear/partition.2D_3: ################################################################################\nmeshgenerators/distributed_rectilinear/partition.2D_3: Tester failed, reason: CRASH\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3 ............................ [min_cpus=3] FAILED (CRASH)\nkernels/vector_fe.comp_error .............................................................................. OK\nrestart/kernel_restartable.parallel_error/error2 ................................... [skipped dependency] SKIP\nrestart/kernel_restartable.parallel_error/error1: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/restart/kernel_restartable\nrestart/kernel_restartable.parallel_error/error1: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i kernel_restartable.i --error --error-unused --error-override --no-gdb-backtrace\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1: Exit Code: 8\nrestart/kernel_restartable.parallel_error/error1: ################################################################################\nrestart/kernel_restartable.parallel_error/error1: Tester failed, reason: CRASH\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1 ................................. [min_cpus=2] FAILED (CRASH)\nrestart/kernel_restartable.thread_error/threads_error .............................. [skipped dependency] SKIP\nrestart/kernel_restartable.thread_error/with_threads ............................... [skipped dependency] SKIP\nmesh/high_order_elems.test_pyramid14 ...................................................................... OK\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/relationship_managers/geometric_neighbors\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i geometric_edge_neighbors.i Mesh/Partitioner/type=GridPartitioner Mesh/Partitioner/nx=1 Mesh/Partitioner/ny=3 --error --error-unused --error-override --no-gdb-backtrace\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Exit Code: 8\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: ################################################################################\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Tester failed, reason: CRASH\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor ................ [min_cpus=3] FAILED (CRASH)\noutputs/xml.parallel/distributed: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/xml\noutputs/xml.parallel/distributed: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i xml.i VectorPostprocessors/distributed/parallel_type=DISTRIBUTED Outputs/file_base=xml_distributed_out --error --error-unused --error-override --no-gdb-backtrace\noutputs/xml.parallel/distributed: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/xml.parallel/distributed: MPIR_Init_thread(586)..............:\noutputs/xml.parallel/distributed: MPID_Init(224).....................: channel initialization failed\noutputs/xml.parallel/distributed: MPIDI_CH3_Init(105)................:\noutputs/xml.parallel/distributed: MPID_nem_init(324).................:\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(175).............:\noutputs/xml.parallel/distributed: MPID_nem_tcp_get_business_card(401):\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/xml.parallel/distributed: (unknown)(): Invalid group\noutputs/xml.parallel/distributed: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/xml.parallel/distributed: MPIR_Init_thread(586)..............:\noutputs/xml.parallel/distributed: MPID_Init(224).....................: channel initialization failed\noutputs/xml.parallel/distributed: MPIDI_CH3_Init(105)................:\noutputs/xml.parallel/distributed: MPID_nem_init(324).................:\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(175).............:\noutputs/xml.parallel/distributed: MPID_nem_tcp_get_business_card(401):\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/xml.parallel/distributed: (unknown)(): Invalid group\noutputs/xml.parallel/distributed: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/xml.parallel/distributed: MPIR_Init_thread(586)..............:\noutputs/xml.parallel/distributed: MPID_Init(224).....................: channel initialization failed\noutputs/xml.parallel/distributed: MPIDI_CH3_Init(105)................:\noutputs/xml.parallel/distributed: MPID_nem_init(324).................:\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(175).............:\noutputs/xml.parallel/distributed: MPID_nem_tcp_get_business_card(401):\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/xml.parallel/distributed: (unknown)(): Invalid group\noutputs/xml.parallel/distributed:\noutputs/xml.parallel/distributed: ################################################################################\noutputs/xml.parallel/distributed: Tester failed, reason: CRASH\noutputs/xml.parallel/distributed:\noutputs/xml.parallel/distributed ................................................. [min_cpus=3] FAILED (CRASH)\nrunWorker Exception: Traceback (most recent call last):\nFile \"/Users/almeag-mac/projects1/moose/python/TestHarness/schedulers/Scheduler.py\", line 445, in runJob\nself.queueJobs(Jobs, j_lock)\nFile \"/Users/almeag-mac/projects1/moose/python/TestHarness/schedulers/Scheduler.py\", line 266, in queueJobs\nself.__runner_pool_jobs.add(self.run_pool.apply_async(self.runJob, (job, Jobs, j_lock)))\nFile \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/multiprocessing/pool.py\", line 362, in apply_async\nraise ValueError(\"Pool not running\")\nValueError: Pool not running\nrunWorker Exception: Traceback (most recent call last):\nFile \"/Users/almeag-mac/projects1/moose/python/TestHarness/schedulers/Scheduler.py\", line 445, in runJob\nself.queueJobs(Jobs, j_lock)\nFile \"/Users/almeag-mac/projects1/moose/python/TestHarness/schedulers/Scheduler.py\", line 266, in queueJobs\nself.__runner_pool_jobs.add(self.run_pool.apply_async(self.runJob, (job, Jobs, j_lock)))\nFile \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/multiprocessing/pool.py\", line 362, in apply_async\nraise ValueError(\"Pool not running\")\nValueError: Pool not running\n\n\nSteps to Reproduce\nRunning Moose tests.\nImpact\nCould not build a Moose multiapp.",
          "url": "https://github.com/idaholab/moose/discussions/18081",
          "updatedAt": "2021-06-21T15:41:47Z",
          "publishedAt": "2021-06-14T13:56:08Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Closed issue and moved to a discussion - this is a local environment issue and not a moose issue.\nPlease see: https://mooseframework.inl.gov/help/troubleshooting.html#failingtests, specifically the section titled \"gethostbyname failed\"",
                  "url": "https://github.com/idaholab/moose/discussions/18081#discussioncomment-868411",
                  "updatedAt": "2022-06-15T16:02:20Z",
                  "publishedAt": "2021-06-14T14:04:37Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Help on ContactSplit",
          "author": {
            "login": "matthiasneuner"
          },
          "bodyText": "Hi,\nI am currently trying to set up a simulation using the ContactSplit module for preconditioning a contact module.\nHowever, I end up with following PETSc message:\n[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Petsc has generated inconsistent data\n[0]PETSC ERROR: Unhandled case, must have at least two fields, not 0\n\nMy Contact and ContactSplit definition look like:\n\n[Preconditioning]\n  active='contact_fsp'\n\n  [contact_fsp]\n      type = FSP\n      topsplit = 'contact_interior' \n      [contact_interior]\n        splitting = 'contact interior'\n        splitting_type = multiplicative\n      []\n      [./interior]\n          type = ContactSplit\n          vars = 'disp_x disp_y disp_z'\n          uncontact_primary   = 'set_hammer_1'\n          uncontact_secondary    = 'set_channel_1'\n          uncontact_displaced = '1'\n          include_all_contact_nodes = 1\n          petsc_options_iname = '-ksp_type -ksp_max_it -ksp_rtol -ksp_gmres_restart -pc_type -pc_hypre_type -pc_hypre_boomeramg_max_iter -pc_hypre_strong_threshold'\n          petsc_options_value = ' preonly 10 1e-4 201                hypre    boomeramg      1                            0.25'\n      [../]\n      [./contact]\n          type = ContactSplit\n          vars = 'disp_x disp_y disp_z'\n          contact_primary   = 'set_hammer_1'\n          contact_secondary    = 'set_channel_1'\n          contact_displaced = '1'\n          include_all_contact_nodes = 1\n          petsc_options_iname = '-ksp_type -ksp_max_it -pc_type -pc_asm_overlap -sub_pc_type   -pc_factor_levels'\n          petsc_options_value = '  preonly 10 asm  1 lu 0'\n      [../]\n  []\n\n  [smp_strumpack]\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type -pc_solver_type'\n    petsc_options_value = 'lu strumpack'\n  []\n\n[]\n\n[Contact]\n  [1]\n    formulation = penalty\n    primary = set_hammer_1\n    secondary = set_channel_1\n    model = frictionless\n    penalty = 1e+6\n  []\n[]\n\nwhich I actually implemented following the testcases from the contact module.\nI am not sure what the options contact_displaced = '1' and uncontact_displaced = '1' should do, but deactivating them doesn't resolve any issue.\nPlease, can somebody give a hint on the correct implementation of ContactSplit?\n\u20ac: Please find the files to reproduce the error here: https://fileshare.uibk.ac.at/d/ce29dee42ca04aa3a7e3/",
          "url": "https://github.com/idaholab/moose/discussions/17501",
          "updatedAt": "2022-11-03T15:00:53Z",
          "publishedAt": "2021-04-03T11:44:16Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "@dewenyushu I remembered you used ContactSplit before, and could you help answer the question?",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-575273",
                  "updatedAt": "2022-11-03T15:01:40Z",
                  "publishedAt": "2021-04-06T14:37:47Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dewenyushu"
                  },
                  "bodyText": "Hi @matthiasneuner\nI need to admit that this problem seems to be a more complicated case than what I've played with before. Let's see if I may be of some help to you:\nBy looking at your mesh, I realized that you actually have two sets of contact surfaces, i.e., set_hammer_1 in contact with set_channel_1, and set_hammer_2 in contact with set_channel_2. However, the sideset 'set_hammer_2' is missing in your mesh thus your FSP block.\nSince FSP uses the sideset info to extract the degree of freedoms and apply different preconditioners on the corresponding sub-problems, I would suggest making this correction in your mesh and add the second pair of contact surfaces in FSP.\nI am not sure about the  contact_displaced  and uncontact_displace  usage here though. @fdkong do you have some suggestions?",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576272",
                  "updatedAt": "2022-11-03T15:01:53Z",
                  "publishedAt": "2021-04-06T18:06:58Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Hello, thank you for the response.\nIndeed, I want to include later on a second contact pair (set_hammer_2/set_channel_2). However, currently it has no contact definition assigned, so only (set_hammer_1/set_channel_1) is active at the moment.\nBut this should not be the cause of the problem?",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576590",
                          "updatedAt": "2022-11-03T15:01:53Z",
                          "publishedAt": "2021-04-06T19:26:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewenyushu"
                          },
                          "bodyText": "Oh, yeah this should not be the cause of the problem then. Would you mind creating a new issue for this? I suspect this example contains some corner cases that are not properly taken care of in the FSP implementation.",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576791",
                          "updatedAt": "2022-11-03T15:01:53Z",
                          "publishedAt": "2021-04-06T20:09:58Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "I am not sure about the contact_displaced and uncontact_displace usage here though. @fdkong do you have some suggestions?\n\ncontact_displaced and uncontact_displace set a flag to check if we want to use a displaced mesh or not. If use the displaced mesh, geometric search information from the displaced mesh will be taken.\n            if (displaced)\n            {\n              std::shared_ptr<DisplacedProblem> displaced_problem =\n                  dmm->_nl->_fe_problem.getDisplacedProblem();\n              if (!displaced_problem)\n              {\n                std::ostringstream err;\n                err << \"Cannot use a displaced contact (\" << it.second.first << \",\"\n                    << it.second.second << \") with an undisplaced problem\";\n                mooseError(err.str());\n              }\n              locator = displaced_problem->geomSearchData()._penetration_locators[it.first];\n            }\n            else\n              locator = dmm->_nl->_fe_problem.geomSearchData()._penetration_locators[it.first];",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576909",
                  "updatedAt": "2022-11-03T15:01:52Z",
                  "publishedAt": "2021-04-06T20:46:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Petsc has generated inconsistent data\n[0]PETSC ERROR: Unhandled case, must have at least two fields, not 0\n\nIt  seems all petsc options were  removed before calling PETSc, and PETSc got nothing from MOOSE, and could not set FSP",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576915",
                  "updatedAt": "2022-11-03T15:01:51Z",
                  "publishedAt": "2021-04-06T20:48:20Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "You could take out of these blocks, and try again. Especially I did not use FSP with a Predictor block.\n  [TimeStepper]\n    type = IterationAdaptiveDT\n    optimal_iterations = 8\n    iteration_window = 3\n    linear_iteration_ratio = 1000\n    growth_factor = 1.5\n    cutback_factor = 0.5\n    dt = 1.0e-4\n  []\n  [Predictor]\n    type = SimplePredictor\n    scale = 1.0\n    skip_after_failed_timestep = true\n  []",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576935",
                  "updatedAt": "2022-11-03T15:01:51Z",
                  "publishedAt": "2021-04-06T20:52:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Unfortunately, this does not resolve the issue.",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-579580",
                          "updatedAt": "2022-11-03T15:01:43Z",
                          "publishedAt": "2021-04-07T12:40:04Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Has the issue been created? If so please link it to this discussion",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-705445",
                  "updatedAt": "2022-11-03T15:01:43Z",
                  "publishedAt": "2021-05-06T17:02:28Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Not yet; I had no time to investigate this further, and I am still not sure if it is not my fault. I will create an issue with a MWE in the next few days.",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-707418",
                          "updatedAt": "2022-11-03T15:04:54Z",
                          "publishedAt": "2021-05-07T05:02:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Thanks. Right, it might not be your fault at all. However, a MWE will help us to get things fixed.\nThanks so much.",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-719151",
                          "updatedAt": "2022-11-03T15:01:42Z",
                          "publishedAt": "2021-05-10T15:36:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Any news on this?",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-805255",
                          "updatedAt": "2022-11-03T15:01:41Z",
                          "publishedAt": "2021-05-31T05:09:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "@matthiasneuner Do you have  a MWE yet?",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-811900",
                          "updatedAt": "2022-11-03T15:01:42Z",
                          "publishedAt": "2021-06-01T16:32:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Hello, sorry for the long delay. I am trying to create a MWE, but somehow i fail to create a simple MWE which works even for a monolithic preconditioning. Please, can you tell me what is wrong with this simple MWE:\nhttps://fileshare.uibk.ac.at/d/8f9f69e228674162b027/\n? The contact never becomes active, and penetration occurs.\nIn any case, switching to  contact split basically invokes the same error initially discussed in this thread ..\nThank you in advance!",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-857745",
                          "updatedAt": "2022-11-03T15:01:42Z",
                          "publishedAt": "2021-06-11T11:37:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Create an issue here #18072\nWill look into it when there is a chance",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-859651",
                          "updatedAt": "2022-11-03T15:01:42Z",
                          "publishedAt": "2021-06-11T17:59:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "import chigger failed",
          "author": {
            "login": "hugary1995"
          },
          "bodyText": "Hi all,\nI wanted to use chigger to generate some animations, but I can't seem to load chigger even though I have moose environment activated. What am I missing?\nThanks,\nGary",
          "url": "https://github.com/idaholab/moose/discussions/18032",
          "updatedAt": "2022-11-03T09:28:57Z",
          "publishedAt": "2021-06-08T13:16:00Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "I forgot to add moose/python to PYTHONPATH...",
                  "url": "https://github.com/idaholab/moose/discussions/18032#discussioncomment-839936",
                  "updatedAt": "2022-11-03T09:29:05Z",
                  "publishedAt": "2021-06-08T13:22:28Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "I am glad I am not the only one, I do this all the time. \ud83e\udd23",
                          "url": "https://github.com/idaholab/moose/discussions/18032#discussioncomment-858555",
                          "updatedAt": "2022-11-03T09:29:06Z",
                          "publishedAt": "2021-06-11T14:47:15Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Phase field models of grain growth",
          "author": {
            "login": "alikhan-zada"
          },
          "bodyText": "Hello everyone,\nNovice question alert! \ud83d\ude43\nI started studying the phase field method and there are some things that I am confused about.\nSo, if I have a multicomponent-multiphase material, say something like steel, would the standard grain growth model implemented in MOOSE be enough to visualize grain growth under a heat source, like in welding?\nI am not interested in dendrite formation or the various phase transformations that might take place at different temperatures, but rather with the end microstructure of the steel.\nAlso, what other free energy components would I need to add in order to find out the material properties across the weld structure, for instance I would like to see how the stress and hardness varies across the weld as well as along it, in the various regions such as the HAZ, CGHAZ and FGHAZ?\nAnother thing that I would like to do is to know the different component segregation across the weld profile and if both, the component and the above part can be done together or not?\nThank you for reading my post, I would really appreciate your answers\nRegards,\nAli",
          "url": "https://github.com/idaholab/moose/discussions/17912",
          "updatedAt": "2022-06-09T08:20:09Z",
          "publishedAt": "2021-05-22T16:10:34Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "laagesen"
                  },
                  "bodyText": "Hi Ali- we have done something similar to what it sounds like you want to do. Please see the following paper:\nhttps://www.tandfonline.com/doi/full/10.1080/00295450.2020.1838877\nHere we used the standard grain growth model in MOOSE, although it is not really physically parameterized for a particular material system. We didn't really look at properties in this case- to do that, you would need to take the resulting microstructures and import them into a crystal plasticity model to see how they behave under deformation.",
                  "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-783471",
                  "updatedAt": "2022-06-09T08:24:56Z",
                  "publishedAt": "2021-05-25T21:46:49Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "Thank you @laagesen, for your answer, but what I want to do is something similar to\n\nIs it possible to have two decoupled domains where I can transfer the whole heat profile changing in time to another domain which solves only the phase field equation using MultiApps?\nOr the other option is to just output the heat data over the domain w.r.t. space and time and just use that as an input to the second domain.\nRegards,\nAli",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-793628",
                          "updatedAt": "2022-06-09T08:25:00Z",
                          "publishedAt": "2021-05-27T15:18:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "The first option would be doable via MultiApps. There are many different ways to transfer data among MultiApps. The documentation page below has a comprehensive list:\nhttps://mooseframework.inl.gov/syntax/Transfers/\nHowever before delving into this too deeply, I would try to figure out how you are going to model the heat transfer problem. Do you have a model for this in mind already?",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-793936",
                          "updatedAt": "2022-06-09T08:25:01Z",
                          "publishedAt": "2021-05-27T16:19:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "The heat source model for a laser is already implemented in MOOSE:\nhttps://mooseframework.inl.gov/source/materials/FunctionPathEllipsoidHeatSource.html\nI am thinking of changing the source term to that of a double ellipsoid model which should be fairly simple. The only thing that would be left would be to match the domain sizes and non-dimensionalize the parameters accordingly.",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-793983",
                          "updatedAt": "2022-06-09T08:25:00Z",
                          "publishedAt": "2021-05-27T16:27:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "SudiptaBiswas"
                          },
                          "bodyText": "I did similar simulations by changing the GB Mobility as a function of time as the heat source moves. That is also another way to simplify the grain growth simulations.",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-799890",
                          "updatedAt": "2022-08-24T11:27:04Z",
                          "publishedAt": "2021-05-28T22:22:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "@SudiptaBiswas, is it possible for you to share the input files?\nHonestly, it would save me from doing a lot of things myself :)",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-800666",
                          "updatedAt": "2022-08-24T11:27:04Z",
                          "publishedAt": "2021-05-29T07:38:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "SudiptaBiswas"
                          },
                          "bodyText": "Sorry, it involves a small development that was not checked in to MOOSE. I don't have an input file that will come in handy for you. You can design the mobility material following the heat source material you mentioned and provide the material as mobility for the grain growth kernels.",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-811928",
                          "updatedAt": "2022-08-24T11:27:04Z",
                          "publishedAt": "2021-06-01T16:39:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "Hello again Larry and Sudipta,\nThank you for your help so far.\nAfter going through some literature, I have come across something that is very close to what I want to do (which was already in my initial question).\nSo, I have two equations:\n\nThe total free energy F is given as:\n\nThe local free energy density consists of two parts and is given as:\n\nFgrain is similar to the grain growth model in MOOSE, with an interaction term for phase and grain interaction, given as:\n\nFphase is differentiates between phases and is given by:\n\nWhere phi(tau) is a function dependent on the temporal changes in the system i.e. tau = T/Tl; Tl is the liquidus temperature and T is the temperature at a given position and time.\nMy question is how do I implement the first two equations in MOOSE? They should be coupled together? No?\nI understand that I can take in the temperature using a SolutionFunction or a SolutionUO or use a MultiApp (but the mesh will need to be the same, which I don't want).\nDo I need to code the equations myself or is there a MOOSE system that I can utilize?\nRegards,\nAli",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-843229",
                          "updatedAt": "2022-08-24T11:27:04Z",
                          "publishedAt": "2021-06-09T03:35:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "The first two equations are Allen-Cahn equations and you should be able to implement them with existing phase-field kernels. Please have a look at the phase-field documentation page:\nhttps://mooseframework.inl.gov/modules/phase_field/index.html\nAnd in particular the page on basic phase-field equations will explain the basic idea of how to use existing kernels to implement the Allen-Cahn equation:\nhttps://mooseframework.inl.gov/modules/phase_field/Phase_Field_Equations.html\nIf you are wanting to simulate a large number of grains, it may be more convenient in the long run to write an action that will add all the necessary kernels. In this case, as in the current MOOSE grain growth model, you would not be using the free energy based approach but instead deriving the evolution equations explicitly and adding the corresponding kernels. More information about the grain growth model can be found here:\nhttps://mooseframework.inl.gov/modules/phase_field/Grain_Growth_Model.html",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-843300",
                          "updatedAt": "2022-08-24T11:27:06Z",
                          "publishedAt": "2021-06-09T04:21:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "Thank you Larry,\nSo I will have to write my own Kernel and implement an action system like in the Grain Growth Model?\nAlso the equations are coupled together as can be seen in the local free energy. They also have derivatives w.r.t. different variables.\nAli",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-843314",
                          "updatedAt": "2022-08-24T11:27:06Z",
                          "publishedAt": "2021-06-09T04:31:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "To try to answer your question as directly as possible, you can probably avoid writing any new kernels or actions if you don't mind having a very complex/long input file. This is totally fine and will not cause any problems, it just may be harder to read. To do this, you would create a DerivativeParsedMaterial that includes f_phase and f_grain. Then you would need to add kernels to create an Allen-Cahn equation for each variable as described in\nhttps://mooseframework.inl.gov/modules/phase_field/Phase_Field_Equations.html\nThe AllenCahn kernel for each variable will take the DerivativeParsedMaterial as an input parameter and will automatically take the partial derivative of the free energy with respect to that variable. You do not need to do anything special because of the fact that the variables are coupled together in the free energy; taking the partial derivative wrt each variable is all you need to do and this is handled automatically. ACInterface kernels will also be needed for the terms resulting from f_grad.\nIf you want to have simpler input files, you would need to create a new action that would handle the coupling terms in f_grain. You could probably use an AllenCahn kernel for d(f_phase)/d(zeta).",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-847537",
                          "updatedAt": "2022-08-24T11:27:06Z",
                          "publishedAt": "2021-06-09T19:52:24Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Groundwater models - boundary conditions",
          "author": {
            "login": "josebastiase"
          },
          "bodyText": "Hi,\nI'm working on a groundwater model (HM - coupled) to simulate land subsidence due to heavy groundwater extraction. My questions:\n\n\nFor the top layer of the model which is unsaturated and drained, I'm using PorousFlowPiecewiseLinearSink to fix the porepressure at the boundaries. I would like to match the boundary conditions with the water head level observed in the field. But I guess that is not really possible with PorousFlowPiecewiseLinearSink, but trying different values of the flux_function flag one can get really close. Is this approach correct/ideal?\n\n\nThe model is bounded in one side by a river (I'm using the water level of the river as BC). What kind of boundary condition would be more suitable in this situation? I would use PorousFlowPiecewiseLinearSink again but a line sink/source may also do the job?\n\n\nThanks in advance!\nJose",
          "url": "https://github.com/idaholab/moose/discussions/18031",
          "updatedAt": "2022-10-26T16:59:50Z",
          "publishedAt": "2021-06-08T12:21:21Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "What you're doing is fine, but below I've written some additional thoughts.\n(1) You might like to fix the porepressure (or head) using a DirichletBC instead, or a combo of DirichletBC (on some bdys) and PorousFlowPiecewiseLinearSink (on the other bdys), depending on your situation.  DirichletBC has the advantage of blindly agreeing with head observations, while the Sink has the advantage of agreeing with known fluxes.  If you know fluxes and/or heads then you can calibrate your hydraulic conductivity (permeability) to achieve the flows or heads observed.  When using the sink make sure your flux_function is chosen sensibly by checking the porepressure or head hasn't gone too crazy, and checking the fluxes out of the model are reasonable.\n(2) Depending on your mesh and geometry, a PorousFlowPolyLineSink might be more appropriate, because the river could be thought of as a polyline.   Have a read about 1/3rd down this page for comments about perennial, ephemeral and rate-limited streams, and riverbed conductance: https://mooseframework.inl.gov/modules/porous_flow/sinks.html .  But it will accomplish essentially the same thing as your PiecewiseLinearSink (assuming your boundary for the PiecewiseLinearSink is set appropriately).  Also be aware that both these approaches can essentially fix porepressure or head at the nodal positions surrounding the sink if you choose a \"strong\" flux_function, but that could be exactly what you want, as the river can be thought of as an infinite supply or demand of water.\na",
                  "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-855940",
                  "updatedAt": "2022-10-26T17:00:02Z",
                  "publishedAt": "2021-06-10T23:07:21Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "josebastiase"
                          },
                          "bodyText": "Hi Andy,\nThanks for your reply. I'm currently using a combination of  PorousFlowPiecewiseLinearSink and DirichletBC in the model:\nPorousFlowPiecewiseLinearSink  for the top layer\nDirichletBC for all confined layers\nThe problem with DirichletBC is that the first layer of the model is unsaturated and drained, so a DirichletBC will fill with water the unsaturated zone and eventually water will leave the model from the top. Whereas with PorousFlowPiecewiseLinearSink I got more \"control\" so the latter doesn't happen, but is really hard to achieve a decent calibration.",
                          "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-857208",
                          "updatedAt": "2022-10-26T17:00:02Z",
                          "publishedAt": "2021-06-11T08:48:08Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Traiwit"
                  },
                  "bodyText": "Hi @josebastiase and @WilkAndy\nnot related to the q's, but on the same model\nJust wondering, how did you set the initial stress for this kind of problem?\nI always have my geometry shrink due to gravity during the equilibrium step (1st step) using a simple rhogh.\n[Kernels]\n  [flux]\n  type = PorousFlowAdvectiveFlux\n  use_displaced_mesh = true\n  variable = porepressure\n  gravity = '0 0 -9.81'\n  fluid_component = 0\n[]\n[]\n\n  [./ini_xx]\n    type = PiecewiseLinear\n    axis = z\n      x = '0 1000'\n      y = '-29430000 0'\n  [../]\n  [./ini_yy]\n    type = PiecewiseLinear\n    axis = z\n      x = '0 1000'\n      y = '-29430000 0'\n  [../]\n  [./ini_zz]\n    type = PiecewiseLinear\n    axis = z\n      x = '0 1000'\n      y = '-19620000 0'  #2000*9.81*1000\n  [../]\n\n[./ini_stress]\n    type = ComputeEigenstrainFromInitialStress\n    eigenstrain_name = ini_stress\n    initial_stress = 'ini_xx 0 0 0 ini_yy 0 0 0 ini_zz'\n  [../]\n\nKind regards,\nTraiwit",
                  "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-856011",
                  "updatedAt": "2022-10-26T17:00:03Z",
                  "publishedAt": "2021-06-10T23:52:52Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "Have a look at porous_flow/examples/coal_mining/coarse_with_fluid.i\na",
                          "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-856079",
                          "updatedAt": "2022-10-26T17:00:03Z",
                          "publishedAt": "2021-06-11T00:36:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi @WilkAndy,\nI did, not sure where are these equations from, but I will give it a go, thanks!\n  [./ini_pp]\n    type = ParsedFunction\n    vars = 'bulk p0 g    rho0'\n    vals = '2E3 0.0 1E-5 1E3'\n    value = '-bulk*log(exp(-p0/bulk)+g*rho0*z/bxculk)'\n  [../]\n  [./ini_xx]\n    type = ParsedFunction\n    vars = 'bulk p0 g    rho0 biot'\n    vals = '2E3 0.0 1E-5 1E3  0.7'\n    value = '0.8*(2500*10E-6*z+biot*(-bulk*log(exp(-p0/bulk)+g*rho0*z/bulk)))'\n  [../]\n  [./ini_zz]\n    type = ParsedFunction\n    vars = 'bulk p0 g    rho0 biot'\n    vals = '2E3 0.0 1E-5 1E3  0.7'\n    value = '2500*10E-6*z+biot*(-bulk*log(exp(-p0/bulk)+g*rho0*z/bulk))'\n  [../]",
                          "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-856086",
                          "updatedAt": "2022-10-26T17:00:03Z",
                          "publishedAt": "2021-06-11T00:39:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "250010E-6z is the gravitational load from a dry system.  biot is because you must prescribe effective stresses, not total stresses.  (-bulk * log ....) is a fancy way of doing hydrostatic: see https://mooseframework.inl.gov/modules/porous_flow/tests/gravity/gravity_tests.html",
                          "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-856203",
                          "updatedAt": "2022-10-26T17:00:04Z",
                          "publishedAt": "2021-06-11T01:41:54Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Installing libMesh into specified location",
          "author": {
            "login": "lloydgoodman"
          },
          "bodyText": "In the instructions for the hpc installation of moose (https://mooseframework.inl.gov/getting_started/installation/hpc_install_moose.html) there is mention of how to install PETSc to a specified location and then setting the PETSC_DIR variable.\nIs it possible to do the same for libMesh?  Or is there some reason why you wouldn't want to or its not necessary?",
          "url": "https://github.com/idaholab/moose/discussions/18054",
          "updatedAt": "2021-06-21T20:19:59Z",
          "publishedAt": "2021-06-10T10:14:20Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "cticenhour"
                  },
                  "bodyText": "This is possible. We track pretty closely with libMesh, so there is a danger of straying too far in version and having failures, but for development purposes you can build your own libmesh and set the LIBMESH_DIR environment variable to tell MOOSE where it's located. If you aren't doing active libmesh development, you should consider remaining with the submodule version supplied with MOOSE.",
                  "url": "https://github.com/idaholab/moose/discussions/18054#discussioncomment-851664",
                  "updatedAt": "2021-06-10T12:51:54Z",
                  "publishedAt": "2021-06-10T12:51:38Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Question on Point Sources",
          "author": {
            "login": "rwalkerlewis"
          },
          "bodyText": "Hello,\nI am having a bit of trouble understanding exactly how the mass flux (mass over time) either inputted or generated (such as in the Peaceman Borehole case) relates to the source term, q (defined as density over time). How is the unit discrepancy resolved?\nThank you,\nRobert",
          "url": "https://github.com/idaholab/moose/discussions/18044",
          "updatedAt": "2022-06-16T10:34:14Z",
          "publishedAt": "2021-06-09T18:56:13Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "Mathematically it is because the delta function has units m^{-3}.  This is mentioned after Eqn(1) of https://mooseframework.inl.gov/modules/porous_flow/sinks.html .   I'm not sure whether this answers your question, or whether you've read the documentation and that's what caused your confusion!  Please tell me if you're still confused and i'll attempt to explain further.",
                  "url": "https://github.com/idaholab/moose/discussions/18044#discussioncomment-848284",
                  "updatedAt": "2022-06-16T10:34:23Z",
                  "publishedAt": "2021-06-09T23:29:39Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "I think the dirac delta function is dimensionless actually.\ns in your Eqn(1) actually has units of kg.s^{-1}. It gets \"sifted\" out from the volume integral due to the dirac delta function.\nFor example, (\\int f(x) \\delta(x) dV) has the same units as f.\nI guess it's okay to say dirac delta has units of m^{-3} if you consider some sort of regularized measure but that'll depend on your spatial dimensionality then...\nNope, I lied -- see below. I wanted to say \\delta * dV is dimensionless.",
                          "url": "https://github.com/idaholab/moose/discussions/18044#discussioncomment-848426",
                          "updatedAt": "2022-06-16T10:34:33Z",
                          "publishedAt": "2021-06-10T00:58:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "Mathematically the delta function has units 1/V, and yes, it depends on the dimensionality (so m^{-2} in 2D problems, etc).  See, eg, https://physics.stackexchange.com/questions/33760/what-are-the-units-or-dimensions-of-the-dirac-delta-function .  So the units in Eqn(1) and the following discussion are correct.\nBut, oh dear, I think this explanation involving delta functions might not be enlightening to @rwalkerlewis .   Would it be helpful to explain a bit further, @rwalkerlewis  ?",
                          "url": "https://github.com/idaholab/moose/discussions/18044#discussioncomment-848455",
                          "updatedAt": "2022-06-16T10:34:33Z",
                          "publishedAt": "2021-06-10T01:08:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Okay, you are right. Thanks,",
                          "url": "https://github.com/idaholab/moose/discussions/18044#discussioncomment-848465",
                          "updatedAt": "2022-06-16T10:34:34Z",
                          "publishedAt": "2021-06-10T01:13:15Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error executing an input file: Failed to get real path",
          "author": {
            "login": "rl3fz"
          },
          "bodyText": "Hello everyone, I'm on Step 2 of the tutorial, and after following the instructions to execute the 'pressure_diffusion.i' file, I keep getting:\n*** ERROR ***\nFailed to get real path for pressure_diffusion.i\nI was previously able to execute the file using the command ../babbler-opt -i pressure_diffusion.i but not anymore. I've tried retracing my steps and updating MOOSE and Conda, but I'm not sure what changed.\nAny advice would be greatly appreciated.",
          "url": "https://github.com/idaholab/moose/discussions/18034",
          "updatedAt": "2022-08-29T15:18:44Z",
          "publishedAt": "2021-06-08T16:56:19Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Are you on Windows WSL?",
                  "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-841317",
                  "updatedAt": "2022-08-29T15:18:44Z",
                  "publishedAt": "2021-06-08T17:35:38Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If so, see this thread #18011",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-841319",
                          "updatedAt": "2021-06-08T17:36:02Z",
                          "publishedAt": "2021-06-08T17:36:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If not, could you please let us know what ls -l returns in your directory. Thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-841321",
                          "updatedAt": "2021-06-08T17:36:30Z",
                          "publishedAt": "2021-06-08T17:36:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Oh that other thread was with you too. Ok see right above",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-841355",
                          "updatedAt": "2021-06-08T17:44:07Z",
                          "publishedAt": "2021-06-08T17:44:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rl3fz"
                          },
                          "bodyText": "Yes, hello again! Contents and permissions:\n-rw-r--r-- 1 rl3fz rl3fz  1550 Jun  8 11:55 pressure_diffusion.i.txt\n-rw-r--r-- 1 rl3fz rl3fz 90744 Jun  8 11:55 pressure_diffusion_out.e",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-841389",
                          "updatedAt": "2021-06-08T17:50:01Z",
                          "publishedAt": "2021-06-08T17:50:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok so you renamed your file pressure_diffusion.i.txt\nSo there is no pressure_diffusion.i file in the folder.\nRe-rename it, or use the correct name after the -i",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-841593",
                          "updatedAt": "2021-06-08T18:32:49Z",
                          "publishedAt": "2021-06-08T18:32:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rl3fz"
                          },
                          "bodyText": "Ok, I understand now. The .i file came from a copy of the simple_diffusion input file, renamed and edited to match the code for pressure_diffusion. (I previously thought I was getting this error whether I included this file in the folder or not, but I must have just misspelled it at some point.) Is there anyway to create an input file from a txt file as the tutorial suggests? (The .txt is added automatically)",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-846761",
                          "updatedAt": "2021-06-09T16:49:49Z",
                          "publishedAt": "2021-06-09T16:49:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "permcody"
                          },
                          "bodyText": "An input file is a text file (ASCII), the only difference is the extension. You should be able to rename the file (i.e. drop the .txt extension) without doing anything else to \"create\" your input file.",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-847317",
                          "updatedAt": "2022-08-29T15:18:55Z",
                          "publishedAt": "2021-06-09T18:55:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rl3fz"
                          },
                          "bodyText": "\".txt\" does not appear when I try to rename it; I can't edit it out manually. Is there a way to do so from the command line?",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-847521",
                          "updatedAt": "2022-08-29T15:18:55Z",
                          "publishedAt": "2021-06-09T19:46:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rl3fz"
                          },
                          "bodyText": "(even when I end the name  with \".i\")",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-847524",
                          "updatedAt": "2022-08-29T15:18:55Z",
                          "publishedAt": "2021-06-09T19:47:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "try mv pressure_diffusion.i.txt pressure_diffusion.i",
                          "url": "https://github.com/idaholab/moose/discussions/18034#discussioncomment-847562",
                          "updatedAt": "2022-08-29T15:18:55Z",
                          "publishedAt": "2021-06-09T19:55:43Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}