{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0wMS0yMFQxNTowNjoxNy0wNjowMM4AOh3e"
    },
    "edges": [
      {
        "node": {
          "title": "runing tests failed",
          "author": {
            "login": "jxj-ward"
          },
          "bodyText": "Dear all.\nI just started learning moose. When I installed moose to the step \"./run_tests -j4 \"according to tutorial, it showed that some tests about \"fvkernels\" failed. I would like to know how to solve this situation.\nThanks",
          "url": "https://github.com/idaholab/moose/discussions/20158",
          "updatedAt": "2022-02-23T06:52:01Z",
          "publishedAt": "2022-01-28T06:16:37Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDo these tests pass when ran individually? (go in test/tests/fvkernels/mms and run the test there, or in test/tests/fvkernels/one-var-diffusion actually, that's a simpler test)\nAnd you should have more information higher in the run_tests log, could you please paste it for some of the tests here?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20158#discussioncomment-2066889",
                  "updatedAt": "2022-01-28T14:36:06Z",
                  "publishedAt": "2022-01-28T14:35:46Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Phase field model for two phase system",
          "author": {
            "login": "vermaprk"
          },
          "bodyText": "Hi\nI want to model two phases eta1 and eta2  (eta1+eta2=1). I am using SwitchingFunctionConstraintEta for sum of order parameter at a point to be 1. My eta1 kernels are AllenCahn and ACInterface and SwitchingFunctionConstraintEta and TimeDerivative.(Total 4)\nQues.1 Do I have to put these kernels again for eta2 or just SwitchingConstraintEta is enough ?\nQues.2 When I model using eta only and other phase as (1-eta) I get convergence using DerivativeTwoPhaseMaterial but when I am trying same problem using eta1 and eta2 separately with DerivativeMultiPhaseMaterial and MultiBarrierFunctionMaterial properties there is a convergence issue. Can we solve the same problem like the later approach?",
          "url": "https://github.com/idaholab/moose/discussions/20141",
          "updatedAt": "2022-06-07T03:51:12Z",
          "publishedAt": "2022-01-26T18:26:16Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "vermaprk"
                  },
                  "bodyText": "RESOLVED",
                  "url": "https://github.com/idaholab/moose/discussions/20141#discussioncomment-2064970",
                  "updatedAt": "2022-06-07T03:51:12Z",
                  "publishedAt": "2022-01-28T08:47:24Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Is the order of stress and strain components correct in AbaqusUMATStress?",
          "author": {
            "login": "ngrilli"
          },
          "bodyText": "Dear MOOSE Team,\nIn AbaqusUMATStress.C the ordering of the stress and strain seems to be different from the Abaqus convention:\nhttps://abaqus-docs.mit.edu/2017/English/SIMACAEMODRefMap/simamod-c-conventions.htm\nIn Abaqus the ordering of the stress vector is:\nsigma_{xx}, sigma_{yy}, sigma_{zz}, sigma_{xy}, sigma_{xz}, sigma_{yz}\nhowever, the way stress[qp] is assigned at line 283 of AbaqusUMATStress is different\nbecause the RankTwoTensor constructor follow another convention where sigma{yz} and  sigma{xy}\nare inverted.\nI was investigating this problem because of convergence issues I was facing when using the UMAT plugin.\nBasically the model converged well with one element but gave wrong results as soon as more elements were used.\nThis problem was reported by @edwardXZ06 some time ago:\n#19574\nbut I jumped too quickly to the conclusion that the timestepper was the problem.\nNow I tried changing line 174 with the strain components order:\n{{0, 0}, {1, 1}, {2, 2}, {0, 1}, {0, 2},{1, 2} }};\nand the stress at line 284 to:\n_aqSTRESS[0], _aqSTRESS[1], _aqSTRESS[2], _aqSTRESS[5], _aqSTRESS[4], _aqSTRESS[3]);\nand all the convergence problems disappeared and I can get the same results as in Abaqus.\nOf course, if my UMAT is adapted one can use the current AbaqusUMATStress,\nhowever, I think the idea here is to provide compatibility between Abaqus UMAT and MOOSE,\ntherefore it would be better to follow Abaqus convention here.\nShould AbaqusUMATStress be changed? Can you please check?\nThank you\nBest Regards,\nNicol\u00f2",
          "url": "https://github.com/idaholab/moose/discussions/20109",
          "updatedAt": "2022-06-24T13:36:56Z",
          "publishedAt": "2022-01-23T13:20:12Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "matthiasneuner"
                  },
                  "bodyText": "Hi, I am using Abaqus Umats through a wrapper (https://github.com/matthiasneuner/chamois/blob/master/src/materials/ConvertRankTwoTensorToVoigt.C), and I am using following ordering:\n  _the_rank_two_tensor_in_voigt[_qp][0] = _the_rank_two_tensor[_qp]( 0, 0 );\n  _the_rank_two_tensor_in_voigt[_qp][1] = _the_rank_two_tensor[_qp]( 1, 1 );\n  _the_rank_two_tensor_in_voigt[_qp][2] = _the_rank_two_tensor[_qp]( 2, 2 );\n  _the_rank_two_tensor_in_voigt[_qp][3] = _the_rank_two_tensor[_qp]( 0, 1 );\n  _the_rank_two_tensor_in_voigt[_qp][4] = _the_rank_two_tensor[_qp]( 0, 2 );\n  _the_rank_two_tensor_in_voigt[_qp][5] = _the_rank_two_tensor[_qp]( 1, 2 );\n\n  if ( _multiply_shear_terms_x2 )\n  {\n    _the_rank_two_tensor_in_voigt[_qp][3] *= 2;\n    _the_rank_two_tensor_in_voigt[_qp][4] *= 2;\n    _the_rank_two_tensor_in_voigt[_qp][5] *= 2;\n  }\n\nThis seems consistent with your workaround -- if that helps.",
                  "url": "https://github.com/idaholab/moose/discussions/20109#discussioncomment-2028414",
                  "updatedAt": "2022-07-01T13:45:22Z",
                  "publishedAt": "2022-01-23T17:51:17Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "recuero"
                  },
                  "bodyText": "Thanks for bringing this up. Opened an issue: #20111.\nDefinitely, our intention is to provide compatibility and not add additional steps on the user side.",
                  "url": "https://github.com/idaholab/moose/discussions/20109#discussioncomment-2028443",
                  "updatedAt": "2022-07-01T13:45:22Z",
                  "publishedAt": "2022-01-23T18:00:50Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "Dear @recuero\nAlso, the order of the components of _aqDDSDDE\nthat are provided to fillSymmetric21FromInputVector\nto give the _jacobian_mult\nmay be wrong for the same reason.\nI will analyse this and let you know.\nThe reason why this bug cannot be noticed in the tests\nis because on one element with pure tension BC there is no shear strain or stress\nso you will not notice the difference.\nBest Regards,\nNicol\u00f2",
                  "url": "https://github.com/idaholab/moose/discussions/20109#discussioncomment-2033987",
                  "updatedAt": "2022-07-01T13:45:24Z",
                  "publishedAt": "2022-01-24T13:30:30Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ngrilli"
                          },
                          "bodyText": "Thank you for opening the issue",
                          "url": "https://github.com/idaholab/moose/discussions/20109#discussioncomment-2033990",
                          "updatedAt": "2022-07-01T13:45:25Z",
                          "publishedAt": "2022-01-24T13:30:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "You are right. The order of Jacobian entries coming from UMAT does not match MOOSE's convention for the same reasons. The changes in #20112 improve the Jacobian coming from UMAT for a problem where shear stiffness depends on the direction. I tested that on a one-element problem, but should work also for other problems (not sure what constitutive equations you are using).",
                          "url": "https://github.com/idaholab/moose/discussions/20109#discussioncomment-2054830",
                          "updatedAt": "2022-07-01T13:45:26Z",
                          "publishedAt": "2022-01-26T23:13:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ngrilli"
                          },
                          "bodyText": "@recuero  Thank you very much for this, at a first glance it looks correct.\nI am using an anisotropic elasto-plastic model, therefore I will check with the new jacobian and I should be able to see some changes in the convergence rate.\nI will let you know if there are further problems.\nBest Regards,\nNicol\u00f2",
                          "url": "https://github.com/idaholab/moose/discussions/20109#discussioncomment-2059844",
                          "updatedAt": "2022-07-05T12:47:43Z",
                          "publishedAt": "2022-01-27T15:45:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Missing library libtimpi_opt.so.6 after update",
          "author": {
            "login": "Flolaffel"
          },
          "bodyText": "I updated moose yesterday and ran into following issue when I tried running my project:\n./intro-opt: error while loading shared libraries: libtimpi_opt.so.6: cannot open shared object file: No such file or directory\nRunning ldd myproject-opt gives the result libtimpi_opt.so.6 => not found\nPlease help me fix this. Thanks!",
          "url": "https://github.com/idaholab/moose/discussions/20137",
          "updatedAt": "2022-09-11T13:12:32Z",
          "publishedAt": "2022-01-26T10:22:20Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDid you update both MOOSE and libmesh? Like you followed the instructions here?\nhttps://mooseframework.inl.gov/getting_started/installation/index.html#update\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20137#discussioncomment-2050956",
                  "updatedAt": "2022-09-11T13:12:32Z",
                  "publishedAt": "2022-01-26T13:38:14Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Ah, just forgot make clobberall. Thank you.",
                          "url": "https://github.com/idaholab/moose/discussions/20137#discussioncomment-2051172",
                          "updatedAt": "2022-09-11T13:12:35Z",
                          "publishedAt": "2022-01-26T14:12:37Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "elem_side_builder.h missing in moose-libmesh",
          "author": {
            "login": "WulfHans"
          },
          "bodyText": "I updated my moose environment today and the previously working project failed building with:\nmoose_projects/moose/framework/build/header_symlinks/Assembly.h:24:10: fatal error: libmesh/elem_side_builder.h: No such file or directory\n   24 | #include \"libmesh/elem_side_builder.h\"\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\n\nThe versions installed in the moose environment are reported as up-to-date as well:\n$ conda list\n# packages in environment at /home/hans/miniconda3/envs/moose:\n#\n# Name                    Version                   Build  Channel\nmoose-libmesh             2021.10.27              build_0    idaholab\nmoose-libmesh-vtk         6.3.0                   build_7    idaholab\n\nI inspected the header in the environment and indeed, this header file is missing, whereas all the other elem* header are present:\n~/miniconda3/envs/moose/libmesh/include/libmesh$ ls elem*.h\nelem_assembly.h  elem_cutter.h  elem.h  elem_hash.h  elem_internal.h  elem_quality.h  elem_range.h\n\nAccording to the docs, for instance\nhttps://mooseframework.inl.gov/docs/doxygen/libmesh/files.html\nit should be there. I also tried reupdating everything, reinstalling libmesh, nothing helped.\nPlease help me to fix this. Thanks in advance!",
          "url": "https://github.com/idaholab/moose/discussions/20122",
          "updatedAt": "2022-06-28T09:11:54Z",
          "publishedAt": "2022-01-25T15:58:53Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\n2021.10.27 is too old, the conda update failed.\nThe easiest thing to do is to wipe out the environment and re-run the conda commands to re-create it\nguillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20122#discussioncomment-2044238",
                  "updatedAt": "2022-06-28T09:13:24Z",
                  "publishedAt": "2022-01-25T16:04:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "WulfHans"
                          },
                          "bodyText": "Well it does not say so:\nconda update --all\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n# All requested packages already installed.\n\nBut it might be not completely correct here...",
                          "url": "https://github.com/idaholab/moose/discussions/20122#discussioncomment-2044259",
                          "updatedAt": "2022-06-28T09:13:25Z",
                          "publishedAt": "2022-01-25T16:07:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yeah the dependency resolution does not allow a newer update at this point\nIt thinks it\u2019s up to date\nthis is why you ll have to wipe it",
                          "url": "https://github.com/idaholab/moose/discussions/20122#discussioncomment-2044275",
                          "updatedAt": "2022-06-28T09:13:28Z",
                          "publishedAt": "2022-01-25T16:10:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "I just saw @GiudGiud post. But I am going to continue posting mine...\nYou are also using our older idaholab channel. The new channel is:\nconda config --add channels https://conda.software.inl.gov/public\nBe sure to remove the old one:\nconda config --remove channels idaholab\nThis older channel will someday be removed.\nAlso, you may wish to upgrade to mamba. It is a much quicker Conda package installer.\nAlso.... the inability to upgrade is probably due to the fact a python version in moose-tools is preventing it. You should probably wipe the entire 'moose' environment, and reinstall, for the least amount of headaches.",
                  "url": "https://github.com/idaholab/moose/discussions/20122#discussioncomment-2044306",
                  "updatedAt": "2022-06-28T09:13:29Z",
                  "publishedAt": "2022-01-25T16:12:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "WulfHans"
                          },
                          "bodyText": "Wiping the whole environment and upgrading to mamba solved it. Thanks for the fast and efficient help!",
                          "url": "https://github.com/idaholab/moose/discussions/20122#discussioncomment-2045221",
                          "updatedAt": "2022-06-28T09:13:34Z",
                          "publishedAt": "2022-01-25T18:22:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "FYI - not sure who added it, but adding [Solved] to the title prefix isn't necessary. Marking as \"answered\" is sufficient!",
                  "url": "https://github.com/idaholab/moose/discussions/20122#discussioncomment-2045250",
                  "updatedAt": "2022-06-28T09:19:12Z",
                  "publishedAt": "2022-01-25T18:27:11Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Peacock segmentation fault on Mac Monterey",
          "author": {
            "login": "jing-chen1122"
          },
          "bodyText": "Hi,\nI am having problem when trying to run peacock on my Mac Monterey:\n( ~ % mamba activate moose\n~ % cd ~/projects/moose\nmoose % peacock\nNo executable found for method type(s): opt, dbg, oprof, devel\nzsh: segmentation fault  peacock )\nrocess:               python3.7 [46382]\nPath:                  /Users/USER/*/python\nIdentifier:            python3.7\nVersion:               ???\nCode Type:             X86-64 (Translated)\nParent Process:        zsh [45795]\nResponsible:           Terminal [45793]\nUser ID:               501\nDate/Time:             2022-01-24 19:04:20.6359 +0800\nOS Version:            macOS 12.0.1 (21A559)\nReport Version:        12\nAnonymous UUID:        9E42E1B1-160E-DCEC-5CB4-4D30DB93E2F3\nSleep/Wake UUID:       B72ACBDD-7B44-42E5-8462-ADC38D2438DD\nTime Awake Since Boot: 11000 seconds\nTime Since Wake:       5309 seconds\nSystem Integrity Protection: enabled\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000000000131\nException Codes:       0x0000000000000001, 0x0000000000000131\nException Note:        EXC_CORPSE_NOTIFY\nTermination Reason:    Namespace SIGNAL, Code 11 Segmentation fault: 11\nTerminating Process:   exc handler [46382]\nIs there any solution to this issue? Thanks in advance!",
          "url": "https://github.com/idaholab/moose/discussions/20114",
          "updatedAt": "2022-06-15T11:47:58Z",
          "publishedAt": "2022-01-24T11:14:34Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "The error is indicating you must first build a binary for which Peacock can interface with. For MOOSE, this usually means either building the test suite, or modules:\ncd ~/projects/moose/test\nmake\n# or\ncd ~/projects/moose/modules\nmake\nOnce either (or both) of those locations are built, you can then invoke peacock while somewhere within that path:\ncd ~/projects/moose/test/tests/kernels/simple_diffusion\npeacock -i simple_diffusion.i\n#       ^instructing peacock to run the simple_diffusion.i input file in this directory\npeacock is designed to search for a binary in this current directory. If not found, it goes 'up' one directory and searches there. It does this until it can't go up any more directories.",
                  "url": "https://github.com/idaholab/moose/discussions/20114#discussioncomment-2034336",
                  "updatedAt": "2022-06-30T15:37:43Z",
                  "publishedAt": "2022-01-24T14:16:39Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "convergence issue with AbauqsUMATstress",
          "author": {
            "login": "edwardXZ06"
          },
          "bodyText": "Hi, everyone:\nI met a question when running the AbaqusUMATstress feature in the MOOSE.\nFirst the umat file works well with Abaqus regardless of element number. Second, there is no issue when running in the MOOSE on a single element test. However, when I just increase the element number, it becomes more and more difficult to converge. I have manipulated the executioner module a little bit but it doesn't work.\nHas anyone met similar issues or any advice on this problem? I have attached the input file in txt format.\nMany thanks!\nXiang\nelastic_test.txt",
          "url": "https://github.com/idaholab/moose/discussions/19574",
          "updatedAt": "2023-06-05T12:35:58Z",
          "publishedAt": "2021-12-07T20:11:19Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "Dear @edwardXZ06\nTry using the adaptive time stepper, as explained here:\nhttps://mooseframework.inl.gov/source/materials/abaqus/AbaqusUMATStress.html\nIt is because your UMAT may be asking a time step cut using PNEWDT,\nbut if you don't add the adaptive time stepper, the time step cut is not handled properly in moose.\nDepending on your specific UMAT, that may cause problems.\nYou need to add the user objects, postprocessor and adaptive time stepper as explained here:\nmodules/tensor_mechanics/test/tests/umat/time_step/elastic_timestep.i\nMay not be the solution to your problem, but try anyway.\nBest Regards,\nNicol\u00f2",
                  "url": "https://github.com/idaholab/moose/discussions/19574#discussioncomment-1784710",
                  "updatedAt": "2023-06-05T12:36:04Z",
                  "publishedAt": "2021-12-10T10:14:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "@edwardXZ06\nPlease check this other discussion, I think I ultimately found the origin of the convergence problem\nwhen running the umat with more elements:\n#20109\nHope this helps\nNicol\u00f2",
                  "url": "https://github.com/idaholab/moose/discussions/19574#discussioncomment-2027365",
                  "updatedAt": "2023-06-05T12:36:11Z",
                  "publishedAt": "2022-01-23T13:23:33Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "'ElementIntegralVariablePostprocessor'",
          "author": {
            "login": "Leni-Yeo"
          },
          "bodyText": "Good afternoon, I hope you are all doing well.\nI am trying to do a bi-crystal simulation with an assigned energy and compare the value to that of the 'ElementIntegralVariablePostprocessor' output; using the 'AuxKernels' setup generally used in moose. I am using the 'PolycrystalKernel'  action for kernels setup. I am trying to have 'mu' as a constant input with a calculated grain boundary width. However, no matter the variation of 'mu', I am unable to have a match with both energies. I have tried using  'ElementIntegralVariablePostprocessor' for a number of setups where the grain boundary width is constant and 'mu' is calculated;  however, it always fails to yield the right value regardless of the inputs I have.\nI may have a wrong setup or missing a point. I was wondering if anyone has ever been able to have a match between output from 'ElementIntegralVariablePostprocessor' and the actual energy that is assigned in the input file ?. Please let me know if there is maybe a verified version/file in MOOSE that I may have missed too.\nIt would greatly help if you could also take a look at the files I have attached. I have an input file and its output file as well as the material class I quickly made to verify 'ElementIntegralVariablePostprocessor'.\nThank you in advance for your help.\nFiles.zip",
          "url": "https://github.com/idaholab/moose/discussions/19977",
          "updatedAt": "2022-06-14T11:30:33Z",
          "publishedAt": "2022-01-07T19:44:19Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "laagesen"
                  },
                  "bodyText": "I'm not sure exactly what you mean by \"assigning\" an energy to the system. I see that you added a DerivativeParsedMaterial for the local free energy density that look likes it matches the Moelans model, and that you are adding it to the gradient term using the TotalFreeEnergy AuxKernel, passing it to an AuxVariable. Then you are integrating the AuxVariable using the ElementIntegralVariablePostprocessor. This all seems fine and should give you the correct integrated free energy for the entire system. However, I'm not sure what you are comparing that number to when you say you are \"assigning\" an energy, can you please elaborate? Are you trying to calculate the energy for this configuration analytically or something?",
                  "url": "https://github.com/idaholab/moose/discussions/19977#discussioncomment-1993950",
                  "updatedAt": "2022-06-14T11:31:07Z",
                  "publishedAt": "2022-01-18T23:06:10Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Leni-Yeo"
                          },
                          "bodyText": "Thank you for answering.\nSorry for the misunderstanding.\nI am trying to compare the integrated free energy for the entire system using the ElementIntegralVariablePostprocessor to the sigma value that is inputted in the material block -- 0.708 J/m2.  It is my understanding that by multiplying the grain boundary area to the sigma value, we should be able to get the same total energy as what we get with the ElementIntegralVariablePostprocessor. However, both values are different for some reason. So, I was wandering if I may have a mistake in the setup or if it is an understanding issue. The main question is whether anyone was able to equate both results under any specific setup.",
                          "url": "https://github.com/idaholab/moose/discussions/19977#discussioncomment-1994158",
                          "updatedAt": "2022-06-14T11:31:07Z",
                          "publishedAt": "2022-01-19T00:13:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "The grain boundary energy value you gave is the energy per unit area of interface, so you are correct that you need to multiply by the area of the interface. It looks like you're using your own custom material property to calculate kappa. Can you do the same calculation using the GBEvolution material that comes with the framework instead to see if the results are consistent? The next thing to be aware of is that there is a length scale parameter in GBEvolution that sets the units for the problem. By default it is in nm, so if your simulation domain is set to be 300 by 300 in your input file, that means 300 nm by 300 nm. So I'm not sure if you did the same thing in your custom material property but that is another thing to check - you must use this length scale when you're determining the GB area for your analytical calculation of GB energy * GB area, which you are trying to match to the postprocessor calculation.",
                          "url": "https://github.com/idaholab/moose/discussions/19977#discussioncomment-1994206",
                          "updatedAt": "2022-06-14T11:33:11Z",
                          "publishedAt": "2022-01-19T00:31:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Leni-Yeo"
                          },
                          "bodyText": "Thank you.\nI will try that and get back to you.",
                          "url": "https://github.com/idaholab/moose/discussions/19977#discussioncomment-1994279",
                          "updatedAt": "2022-06-14T11:33:08Z",
                          "publishedAt": "2022-01-19T00:58:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Leni-Yeo"
                          },
                          "bodyText": "Good afternoon,\nSorry for the delay.\nI ran a bi-crystal by modifying the built-in file \"grain_growth_2D_voronoi.i\". I attached the input / output files and a snip of the total energy from integrating the free energy for the entire system using the ElementIntegralVariablePostprocessor.\n\n\nThe total energy is at around \"1337 eV\". The mu value \"1.89385\" is the one calculated by \"GBEvolution\". I ran the simulation once at first and copied the value to the \"DerivativeParsedMaterial\" of the \"free_energy\" material.\n\n\nThe total energy when using sigma = 0.708 J/m^2 = 4.4 eV/nm^2 ; wGB = 14 nm; the height of the block  = 300 nm is: 4.4 * 14 * 300 =  18,480 eV. It is ~13.8 times greater.\n\n\nI may have a mistake so I am not really 100% sure, but no matter the changes I tried both values are different somewhat.\ngrain_growth_2D_voronoi_BICRYSTAL.zip",
                          "url": "https://github.com/idaholab/moose/discussions/19977#discussioncomment-2009001",
                          "updatedAt": "2022-06-14T11:36:09Z",
                          "publishedAt": "2022-01-20T18:47:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "It looks like you are getting the correct result from the postprocessor but there is an error in your hand calculation. I think you may be getting confused by the fact that this is a 2D simulation, which is a \"slice\" of the real system in 3D. In the real 3D system, if the GB runs along the y-direction, then it sits in the yz plane and the x-direction is normal to the GB. So to calculate the energy of the GB, you would multiply sigma times the size in the y-direction times the size in the z-direction. You don't multiply by wGB, as this width (in the x-direction) is associated with the diffuse interface representation of the grain boundary.\nSo assume the domain is 1 nm in the z-direction (the actual number doesn't matter). Your total energy for the analytical calculation is 4.4 eV/nm^2 * 300 nm * 1 nm = 1320 eV. To compare to the postprocessor, you need to recognize that the integrated value is also for a 2D domain which also has an arbitrary length in the z-direction; since we set it to 1 nm the total energy is 1337 eV/nm * 1 nm = 1337 eV. So you see the two vales are very close. You can repeat the exercise with any value you want for the length of the domain in the z-direction and you'll find the same thing. The factor of ~13.8 difference that you were getting is because you were multiplying the hand calculation by 14, which is incorrect.\nYou can probably make the postprocessor calculation match the hand calculation even more closely by increasing the resolution of the mesh in the interfacial region.",
                          "url": "https://github.com/idaholab/moose/discussions/19977#discussioncomment-2010042",
                          "updatedAt": "2022-06-14T11:36:12Z",
                          "publishedAt": "2022-01-20T21:24:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Leni-Yeo"
                          },
                          "bodyText": "Oh I see. It makes sense now.\nI just did a trial run with my own material and the values are much closer.\nThank you so much. You were of great help.",
                          "url": "https://github.com/idaholab/moose/discussions/19977#discussioncomment-2010253",
                          "updatedAt": "2022-06-14T11:36:14Z",
                          "publishedAt": "2022-01-20T21:55:05Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Installation on ALCF Theta",
          "author": {
            "login": "smpark7"
          },
          "bodyText": "I'm currently trying to install a MOOSE App on Theta. I don't have any specific requirements concerning PETSc and libMesh versions. Can anyone provide me with the appropriate environment for installing MOOSE on Theta?\nI tried the following:\nmodule load cray-hdf5-parallel\nmodule load cray-fftw\nmodule load cray-petsc\nmodule load cray-tpsl\nmodule load cray-python\nexport CC=cc\nexport CXX=CC\nexport FC=ftn\nexport EPYTHON=python3.8\nexport PETSCDIR=$PETSC_DIR\nexport CRAYPE_LINK_TYPE=dynamic\nexport CRAY_ADD_RPATH=yes\n\nwhich I (blindly) adapted from my experience with another supercomputer. This resulted in a seemingly successful installation, but when I run an input file (that works on other machines), the simulation run gets killed by the OOM killer despite using only ~32GB of the available 192GB memory per node.",
          "url": "https://github.com/idaholab/moose/discussions/20082",
          "updatedAt": "2022-06-17T21:51:37Z",
          "publishedAt": "2022-01-19T17:21:42Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDoes the test suite pass?\nHow did you measure the 32 GB ?\nHow did you request the 192 GB ?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-1999817",
                  "updatedAt": "2022-06-17T21:51:41Z",
                  "publishedAt": "2022-01-19T17:51:36Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "Hello,\nThanks for the quick reply. I have trouble running run_tests despite providing export MOOSE_MPI_COMMAND=aprun and alias mpiexec=aprun. I get the following error message for all tests:\nnts.nts: Running command: aprun -n 1 /gpfs/mira-home/smpark3/projects/moltres/moltres-opt -i nts.i --no-gdb-backtrace\nnts.nts: apsched: request exceeds max nodes, alloc\nnts.nts: apsched: request exceeds max nodes, alloc\nnts.nts: \nnts.nts: \nnts.nts: Exit Code: 1\nnts.nts: ################################################################################\nnts.nts: Tester failed, reason: CRASH\nnts.nts: \nnts.nts ....................................................................................... FAILED (CRASH)\n\nThe test input files seem to run successfully when I manually run aprun -n 64 ... individually, but this approach obviously does not compare the output against the gold copy. I measured 32 GB using MemoryUsage postprocessor with the default settings which report total memory usage across all ranks. I assumed 192 GB of memory is provided based on Theta's machine info.\nSun Myung",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2000120",
                          "updatedAt": "2022-06-17T21:52:03Z",
                          "publishedAt": "2022-01-19T18:48:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Well it depends on the cluster. I think if you requested a full node, you should get the whole memory for it. Is that what you are doing with -n 1 ?\n\"apsched: request exceeds max nodes, alloc\" is this the oom output?\naprun -n 1 on the inputs individually works too?\nDoesnt theta have some minimum ressource request? Like at least 4 nodes?",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2000312",
                          "updatedAt": "2022-06-17T21:52:11Z",
                          "publishedAt": "2022-01-19T19:22:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "No, the OOM killer error occurred when I ran a non-test input file with aprun -n 64 on 1 compute node, whereas the apsched: request exceeds max nodes, alloc error occurred when I tried to run run_tests.\nYes aprun -n 1 works when individually running tests. I ran it on the debug queue which allows me to request 1 to 8 compute nodes.",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2000397",
                          "updatedAt": "2022-06-17T21:52:15Z",
                          "publishedAt": "2022-01-19T19:36:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Are you using run_tests on the debug queue as well?\nThis troubleshooting page: http://www.archer.ac.uk/documentation/troubleshooting.php (for another cluster, is there a theta one?)\nis saying that aprun is being run on the head node instead of within a job script.\nI m not familiar with aprun. Is this more of an mpirun command or more of an interactive qsub command",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2000440",
                          "updatedAt": "2022-06-17T21:52:32Z",
                          "publishedAt": "2022-01-19T19:47:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "Yes I ran run_tests on the debug queue as well. Unfortunately, Theta's user guide doesn't seem to have troubleshooting information relevant to my issues.\naprun seems to function similarly to mpirun; I just replace mpirun with aprun in my commands. aprun is responsible for starting jobs on the compute nodes from the service nodes from which we send commands. Theta provides this illustration as an overview of what qsub and aprun do:",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2000557",
                          "updatedAt": "2022-06-17T21:52:32Z",
                          "publishedAt": "2022-01-19T20:09:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ah ok so you're running all this from a qsub interactive session?\nwhy would you get this apsched: request exceeds max nodes, alloc then though...\nSeems like 1 should be ok.",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2000741",
                          "updatedAt": "2022-06-17T21:52:35Z",
                          "publishedAt": "2022-01-19T20:38:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "Yes it's very perplexing to get this error even though I'm in the interactive session and run_tests ran aprun -n 1.\nRegarding the OOM issue, I tried running the same input file with fewer MPI ranks per node (32 vs 64 previously). I still got the same OOM killer triggered at the same point during the simulation (right after 5 time steps). This seems to imply that the OOM issue is not hardware-related.\nI found an old thread from 2018 in the moose-users group about installing MOOSE on Theta. Could the comment from Fande about static vs dynamic libs be relevant to my OOM killer issue?",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2000790",
                          "updatedAt": "2022-06-17T21:52:36Z",
                          "publishedAt": "2022-01-19T20:47:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Oh great let's tag @fdkong since he was running stuff on theta.\nI dont think it's related though. I'd double check that your interactive session has requested enough memory. There should be a way to get that from qstat",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2000990",
                          "updatedAt": "2022-06-17T21:52:35Z",
                          "publishedAt": "2022-01-19T21:26:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Does \"aprun -n 1 /gpfs/mira-home/smpark3/projects/moltres/moltres-opt -i nts.i\" run if you type it manually?\nI did not see any indication that this is related to how did you build moose. I ran MOOSE a couple of years ago but I did not do \"run_tests\".   I tried to use a job script to launch the simulation I want.",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2001351",
                          "updatedAt": "2022-06-17T21:52:39Z",
                          "publishedAt": "2022-01-19T22:34:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "Yes, all test input files run if I run them individually like that. From manually scanning through some of the postprocessor values, the values seem to indicate that the results match the gold copies. So I'm not too concerned about run_tests not running correctly.\nDo you have any advice/ideas about the OOM killer issue? I tried asking the ALCF support desk, but they recommended directing my question to the MOOSE developers.",
                          "url": "https://github.com/idaholab/moose/discussions/20082#discussioncomment-2001414",
                          "updatedAt": "2022-06-17T21:52:43Z",
                          "publishedAt": "2022-01-19T22:50:39Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "SegFault at mpirun for DynamicStressDivergenceTensors",
          "author": {
            "login": "BoZeng1997"
          },
          "bodyText": "Hi,\nI was running a multiapp (tensor_mechanics and phase-field) and with multiple cpus on ubuntu. It worked fine until I changed the kernel from ADStressDivergenceTensors to ADDynamicStressDivergenceTensors. After that, I encountered Segmentation fault (signal 11) only at running it with multiple cpus.\nI have updated moose and its dependencies. I looked at the moose debugging document but the parallel method does not seem to apply to ubuntu(https://mooseframework.inl.gov/application_development/debugging.html). For backtrace information, I need help on how to get that with mpirun on ubuntu for moose.\nBo",
          "url": "https://github.com/idaholab/moose/discussions/20073",
          "updatedAt": "2022-06-16T18:29:33Z",
          "publishedAt": "2022-01-18T22:42:32Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nTo run gdb in parallel you can do it like this:\nmpirun -n 2 gdb --ex run --ex bt --args app-dbg -i input_file.i\nIf you need more than this, the instructions listed in the link you pasted can be translated directly from lldb to gdb, no tricks\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-1994123",
                  "updatedAt": "2022-06-16T18:29:48Z",
                  "publishedAt": "2022-01-19T00:00:58Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "BoZeng1997"
                  },
                  "bodyText": "Hi,\nHere is the backtrace from gdb for the SIGSEGV signal\nThread 1 \"raccoon-opt\" received signal SIGSEGV, Segmentation fault.\n0x00007ffff5ecf35a in libMesh::PetscVector<double>::operator()(unsigned long) const () from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#0  0x00007ffff5ecf35a in libMesh::PetscVector<double>::operator()(unsigned long) const () from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#1  0x00007ffff5a4e8cb in MultiAppFieldTransfer::transferDofObject(libMesh::DofObject*, libMesh::DofObject*, MooseVariableFieldBase&, MooseVariableFieldBase&, libMesh::NumericVector<double>&, libMesh::NumericVector<double>&) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#2  0x00007ffff5a82de1 in MultiAppFieldTransfer::transfer(FEProblemBase&, FEProblemBase&) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#3  0x00007ffff5a8319b in MultiAppCopyTransfer::execute() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#4  0x00007ffff5472130 in FEProblemBase::execMultiAppTransfers(MooseEnumItem, Transfer::DIRECTION) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#5  0x00007ffff547291f in FEProblemBase::execMultiApps(MooseEnumItem, bool) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#6  0x00007ffff50842c1 in FixedPointSolve::solveStep(double&, double&, std::set<unsigned long, std::less<unsigned long>, std::allocator<unsigned long> > const&) () from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#7  0x00007ffff509f31a in FixedPointSolve::solve() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#8  0x00007ffff5b4d0ce in TimeStepper::step() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#9  0x00007ffff508246e in Transient::takeStep(double) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#10 0x00007ffff507ec8f in Transient::execute() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#11 0x00007ffff60a76e3 in MooseApp::executeExecutioner() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#12 0x00007ffff60ad483 in MooseApp::run() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#13 0x0000555555557153 in main ()\n(gdb) \nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff5ecf35a in libMesh::PetscVector<double>::operator()(unsigned long) const () from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#0  0x00007ffff5ecf35a in libMesh::PetscVector<double>::operator()(unsigned long) const () from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#1  0x00007ffff5a4e8cb in MultiAppFieldTransfer::transferDofObject(libMesh::DofObject*, libMesh::DofObject*, MooseVariableFieldBase&, MooseVariableFieldBase&, libMesh::NumericVector<double>&, libMesh::NumericVector<double>&) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#2  0x00007ffff5a82de1 in MultiAppFieldTransfer::transfer(FEProblemBase&, FEProblemBase&) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#3  0x00007ffff5a8319b in MultiAppCopyTransfer::execute() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#4  0x00007ffff5472130 in FEProblemBase::execMultiAppTransfers(MooseEnumItem, Transfer::DIRECTION) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#5  0x00007ffff547291f in FEProblemBase::execMultiApps(MooseEnumItem, bool) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#6  0x00007ffff50842c1 in FixedPointSolve::solveStep(double&, double&, std::set<unsigned long, std::less<unsigned long>, std::allocator<unsigned long> > const&) () from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#7  0x00007ffff509f31a in FixedPointSolve::solve() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#8  0x00007ffff5b4d0ce in TimeStepper::step() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#9  0x00007ffff508246e in Transient::takeStep(double) ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#10 0x00007ffff507ec8f in Transient::execute() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#11 0x00007ffff60a76e3 in MooseApp::executeExecutioner() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#12 0x00007ffff60ad483 in MooseApp::run() ()\n   from /home/bozeng/projects/raccoon/moose/framework/libmoose-opt.so.0\n#13 0x0000555555557153 in main ()\n\nPlease tell me what should I do next to solve this.",
                  "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-2008344",
                  "updatedAt": "2022-06-16T18:29:47Z",
                  "publishedAt": "2022-01-20T17:05:28Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "Here is the input file for main app elasticity\nE = 9.8e3 #9.8Gpa\nnu = 0.13\nK = '${fparse E/3/(1-2*nu)}'\nG = '${fparse E/2/(1+nu)}'\nLambda = '${fparse E*nu/(1+nu)/(1-2*nu)}'\n\nGc = 9.1e-2 # 91N/m\nl = 0.12 #0.35\nsigma_ts = 27\nsigma_cs = 77\ndelta = 9.66 #1.16\nc1 = '${fparse (1+nu)*sqrt(Gc)/sqrt(2*pi*E)}'\nc2 = '${fparse (3-nu)/(1+nu)}'\nahead = -1\nV = 20\nnx = 25 #100 #90 #300 #90\nny = 20 #50 #30 #100 #30\nlength = 5\nheight = 4\nrefine = 5 #3\n\n\n[Functions]\n  [bc_func]\n    type = ParsedFunction\n    value = c1*((x-V*t-ahead)^2+y^2)^(0.25)*(c2-cos(atan2(y,(x-V*t-ahead))))*sin(0.5*atan2(y,(x-V*t-ahead)))\n    vars = 'c1 c2 V ahead'\n    vals = '${c1} ${c2} ${V} ${ahead}'\n  []\n[]\n\n[MultiApps]\n  [fracture]\n    type = TransientMultiApp\n    input_files = fracture.i\n    cli_args = 'E=${E};K=${K};G=${G};Lambda=${Lambda};Gc=${Gc};l=${l};nx=${nx};ny=${ny};refine=${refine};length=${length};height=${height}'\n    execute_on = 'TIMESTEP_END'\n  []\n[]\n\n[Transfers]\n  [from_d]\n    type = MultiAppCopyTransfer\n    multi_app = fracture\n    direction = from_multiapp\n    variable = 'd'\n    source_variable = 'd'\n  []\n  [to_psie_active]\n    type = MultiAppCopyTransfer\n    multi_app = fracture\n    direction = to_multiapp\n    variable = 'psie_active ce'\n    source_variable = 'psie_active ce'\n  []\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Mesh]\n  [top_half]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = ${nx}\n    ny = '${fparse ny/2}'\n    xmin = 0\n    xmax = ${length}\n    ymin = 0 #-25 #-50\n    ymax = '${fparse height/2}'\n    boundary_id_offset = 0\n    boundary_name_prefix = top_half\n  []\n  [top_stitch]\n    type = BoundingBoxNodeSetGenerator\n    input = top_half\n    new_boundary = top_stitch\n    bottom_left = '1 0 0'\n    top_right = '5 0 0'\n  []\n  [top_crack]\n    type = BoundingBoxNodeSetGenerator\n    input = top_stitch\n    new_boundary = top_crack\n    bottom_left = '0 0 0'\n    top_right = '1 0 0'\n  []\n  [bottom_half]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = ${nx}\n    ny = '${fparse ny/2}'\n    xmin = 0\n    xmax = ${length}\n    ymin = '${fparse -height/2}'\n    ymax = 0 # 25 # 50\n    boundary_id_offset = 7\n    boundary_name_prefix = bottom_half\n  []\n  [bottom_stitch]\n    type = BoundingBoxNodeSetGenerator\n    input = bottom_half\n    new_boundary = bottom_stitch\n    bottom_left = '1 0 0'\n    top_right = '5 0 0'\n  []\n  [bottom_crack]\n    type = BoundingBoxNodeSetGenerator\n    input = bottom_stitch\n    new_boundary = bottom_crack\n    bottom_left = '0 0 0'\n    top_right = '1 0 0'\n  []\n  [stitch]\n    type = StitchedMeshGenerator\n    inputs = 'top_crack bottom_crack'\n    stitch_boundaries_pairs = 'top_stitch bottom_stitch'\n  []\n  construct_side_list_from_node_list = true\n  [fix_point]\n    type = ExtraNodesetGenerator\n    input = stitch\n    new_boundary = fix_point\n    coord = '0 -2'\n  []\n  [crack_tip]\n    type = ExtraNodesetGenerator\n    input = fix_point\n    new_boundary = crack_tip\n    coord = '0 0 0'\n  []\n[]\n\n[Adaptivity]\n  marker = marker\n  initial_marker = marker\n  initial_steps = ${refine}\n  stop_time = 0\n  max_h_level = ${refine}\n  [Markers]\n    [marker]\n      type = BoxMarker\n      bottom_left = '0 -0.2 0' # 0.7\n      top_right = '5 0.2 0'\n      outside = DO_NOTHING\n      inside = REFINE\n    []\n  []\n[]\n\n[Variables]\n  [disp_x]\n  []\n  [disp_y]\n  []\n  [strain_zz]\n  []\n[]\n\n[AuxVariables]\n  [d]\n    [InitialCondition]\n      type = FunctionIC\n      function = 'if(y=0&x<=1,1,0)'\n    []\n  []\n[]\n\n[Kernels]\n  [solid_x]\n    type = ADDynamicStressDivergenceTensors\n    # displacements = disp_x\n    variable = disp_x\n    component = 0\n    out_of_plane_strain = strain_zz\n    zeta = 1e-6\n  []\n  [solid_y]\n    type = ADDynamicStressDivergenceTensors\n    # displacements = disp_y\n    variable = disp_y\n    component = 1\n    out_of_plane_strain = strain_zz\n    zeta = 1e-6\n  []\n  [plane_stress]\n    type = ADWeakPlaneStress\n    variable = 'strain_zz'\n    displacements = 'disp_x disp_y'\n  []\n[]\n\n[BCs]\n  [fix_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = fix_point\n    value = 0\n  []\n  [bottom_y]\n    type = FunctionDirichletBC\n    variable = disp_y\n    boundary = bottom_half_bottom\n    function = bc_func\n  []\n  [top_y]\n    type = FunctionDirichletBC\n    variable = disp_y\n    boundary = top_half_top\n    function = bc_func\n  []\n[]\n\n[Materials]\n  [bulk]\n    type = ADGenericConstantMaterial\n    prop_names = 'E K G lambda Gc l'\n    prop_values = '${E} ${K} ${G} ${Lambda} ${Gc} ${l}'\n  []\n  [degradation]\n    type = PowerDegradationFunction\n    f_name = g\n    function = (1-d)^p*(1-eta)+eta\n    phase_field = d\n    parameter_names = 'p eta '\n    parameter_values = '2 0'\n  []\n  [strain]\n    type = ADComputePlaneSmallStrain\n    out_of_plane_strain = 'strain_zz'\n    displacements = 'disp_x disp_y'\n  []\n  [elasticity]\n    type = SmallDeformationIsotropicElasticity\n    bulk_modulus = K\n    shear_modulus = G\n    phase_field = d\n    degradation_function = g\n    decomposition = NONE\n    output_properties = 'psie_active psie'\n    outputs = exodus\n  []\n  [stress]\n    type = ComputeSmallDeformationStress\n    elasticity_model = elasticity\n    output_properties = 'stress'\n  []\n  [crack_geometric]\n    type = CrackGeometricFunction\n    f_name = alpha\n    function = 'd'\n    phase_field = d\n  []\n  [kumar_material]\n    type = NucleationMicroForce\n    normalization_constant = c0\n    tensile_strength = '${sigma_ts}'\n    compressive_strength = '${sigma_cs}'\n    delta = '${delta}'\n    external_driving_force_name = ce\n    output_properties = 'ce'\n    outputs = exodus\n  []\n[]\n\n[Postprocessors]\n  [Jint]\n    type = PhaseFieldJIntegral\n    J_direction = '1 0 0'\n    strain_energy_density = psie\n    displacements = 'disp_x disp_y'\n    boundary = 'top_half_left top_half_top top_half_right bottom_half_right bottom_half_bottom bottom_half_left'\n  []\n  [Jint_over_Gc]\n    type = ScalePostprocessor\n    value = 'Jint'\n    scaling_factor = '${fparse 1.0/Gc}'\n  []\n[]\n\n[Executioner]\n  type = Transient\n\n  solve_type = NEWTON\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu       superlu_dist                 '\n  automatic_scaling = true\n\n  nl_rel_tol = 1e-8\n  nl_abs_tol = 1e-10\n\n  dt = 1e-2\n  end_time = 1e-1\n\n  fixed_point_max_its = 500\n  accept_on_max_fixed_point_iteration = false\n  fixed_point_rel_tol = 1e-3\n  fixed_point_abs_tol = 1e-5\n[]\n\n[Outputs]\n  exodus = true\n  file_base = L0.12_ahead-1meshd_ftol35eta0itr500zeta6\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-2008441",
                          "updatedAt": "2022-06-16T18:29:49Z",
                          "publishedAt": "2022-01-20T17:21:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "Here is the input file for subapp fracture\n[Mesh]\n  [top_half]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = ${nx}\n    ny = '${fparse ny/2}'\n    xmin = 0\n    xmax = ${length}\n    ymin = 0 #-25 #-50\n    ymax = '${fparse height/2}'\n    boundary_id_offset = 0\n    boundary_name_prefix = top_half\n  []\n  [top_stitch]\n    type = BoundingBoxNodeSetGenerator\n    input = top_half\n    new_boundary = top_stitch\n    bottom_left = '1 0 0'\n    top_right = '5 0 0'\n  []\n  [top_crack]\n    type = BoundingBoxNodeSetGenerator\n    input = top_stitch\n    new_boundary = top_crack\n    bottom_left = '0 0 0'\n    top_right = '1 0 0'\n  []\n  [bottom_half]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = ${nx}\n    ny = '${fparse ny/2}'\n    xmin = 0\n    xmax = ${length}\n    ymin = '${fparse -height/2}'\n    ymax = 0 # 25 # 50\n    boundary_id_offset = 7\n    boundary_name_prefix = bottom_half\n  []\n  [bottom_stitch]\n    type = BoundingBoxNodeSetGenerator\n    input = bottom_half\n    new_boundary = bottom_stitch\n    bottom_left = '1 0 0'\n    top_right = '5 0 0'\n  []\n  [bottom_crack]\n    type = BoundingBoxNodeSetGenerator\n    input = bottom_stitch\n    new_boundary = bottom_crack\n    bottom_left = '0 0 0'\n    top_right = '1 0 0'\n  []\n  [stitch]\n    type = StitchedMeshGenerator\n    inputs = 'top_crack bottom_crack'\n    stitch_boundaries_pairs = 'top_stitch bottom_stitch'\n  []\n  construct_side_list_from_node_list = true\n  [fix_point]\n    type = ExtraNodesetGenerator\n    input = stitch\n    new_boundary = fix_point\n    coord = '0 -2'\n  []\n  [crack_tip]\n    type = ExtraNodesetGenerator\n    input = fix_point\n    new_boundary = crack_tip\n    coord = '0 0 0'\n  []\n[]\n\n[Adaptivity]\n  marker = marker\n  initial_marker = marker\n  initial_steps = ${refine}\n  stop_time = 0\n  max_h_level = ${refine}\n  [Markers]\n    [marker]\n      type = BoxMarker\n      bottom_left = '0 -0.2 0' # 0.7\n      top_right = '5 0.2 0'\n      outside = DO_NOTHING\n      inside = REFINE\n    []\n  []\n[]\n\n[Variables]\n  [d]\n    [InitialCondition]\n      type = FunctionIC\n      function = 'if(y=0&x<=1,1,0)'\n    []\n  []\n[]\n\n[AuxVariables]\n  [bounds_dummy]\n  []\n  [psie_active]\n    order = CONSTANT\n    family = MONOMIAL\n  []\n  [ce]\n    order = CONSTANT\n    family = MONOMIAL\n  []\n[]\n\n[Bounds]\n  [conditional]\n    type = ConditionalBoundsAux\n    variable = bounds_dummy\n    bounded_variable = d\n    fixed_bound_value = 0\n    threshold_value = 0.95\n  []\n  [upper]\n    type = ConstantBoundsAux\n    variable = bounds_dummy\n    bounded_variable = d\n    bound_type = upper\n    bound_value = 1\n  []\n[]\n\n[Kernels]\n  [diff]\n    type = ADPFFDiffusion\n    variable = d\n    fracture_toughness = Gc\n    regularization_length = l\n    normalization_constant = c0\n  []\n  [source]\n    type = ADPFFSource\n    variable = d\n    free_energy = psi\n  []\n[]\n\n[Materials]\n  [fracture_properties]\n    type = ADGenericConstantMaterial\n    prop_names = 'E K G lambda Gc l'\n    prop_values = '${E} ${K} ${G} ${Lambda} ${Gc} ${l}'\n  []\n  [degradation]\n    type = PowerDegradationFunction\n    f_name = g\n    function = (1-d)^p*(1-eta)+eta\n    phase_field = d\n    parameter_names = 'p eta '\n    parameter_values = '2 0'\n  []\n  [crack_geometric]\n    type = CrackGeometricFunction\n    f_name = alpha\n    function = 'd'\n    phase_field = d\n  []\n  [psi]\n    type = ADDerivativeParsedMaterial\n    f_name = psi\n    function = 'g*psie_active+(ce+Gc/c0/l)*alpha'\n    args = 'd psie_active ce'\n    material_property_names = 'alpha(d) g(d) Gc c0 l'\n    derivative_order = 1\n  []\n[]\n\n[Executioner]\n  type = Transient\n\n  solve_type = NEWTON\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package -snes_type'\n  petsc_options_value = 'lu       superlu_dist                  vinewtonrsls'\n  automatic_scaling = true\n\n  nl_rel_tol = 1e-8\n  nl_abs_tol = 1e-10\n[]\n\n[Outputs]\n  print_linear_residuals = false\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-2008448",
                          "updatedAt": "2022-06-16T18:29:52Z",
                          "publishedAt": "2022-01-20T17:22:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so the meshes start the same right, but then the displacements modify them.\nDo they modify them the same way?\nCan you please try using a Projection transfer instead of a copy transfer?\nhttps://mooseframework.inl.gov/source/transfers/MultiAppProjectionTransfer.html",
                          "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-2008898",
                          "updatedAt": "2022-06-16T18:29:52Z",
                          "publishedAt": "2022-01-20T18:29:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "Projection transfer does not have this error.\nWhat I do not understand is why ADStressDivergenceTensors with Copy transfer does not produce this error. The displacement also modifies the mesh there.",
                          "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-2009783",
                          "updatedAt": "2022-07-26T05:58:04Z",
                          "publishedAt": "2022-01-20T20:40:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Are the displacement exactly the same? Or is there more with one than the other?",
                          "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-2009822",
                          "updatedAt": "2022-07-26T05:58:04Z",
                          "publishedAt": "2022-01-20T20:45:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "If ADDynamicStressDivergenceTensors with zeta=0 is equivalent to ADStressDivergenceTensors so that they produced the same displacement under the same prescribed boundary condition, then yes. ADDynamicStressDivergenceTensors with zeta=0 also produce this error when parallel computed",
                          "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-2009897",
                          "updatedAt": "2022-07-26T05:58:04Z",
                          "publishedAt": "2022-01-20T20:58:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I suspect copy trasnfers dont like displaced meshes. I ll run some tests",
                          "url": "https://github.com/idaholab/moose/discussions/20073#discussioncomment-2009931",
                          "updatedAt": "2022-07-26T05:58:04Z",
                          "publishedAt": "2022-01-20T21:06:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}