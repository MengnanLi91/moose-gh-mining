{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wMS0xM1QwODo0MTo1My0wNjowMM4AASvL"
    },
    "edges": [
      {
        "node": {
          "title": "does not have an associated \"Action\" error",
          "author": {
            "login": "Ali-toghraee"
          },
          "bodyText": "Dear MOOSE users,\nI have already  had an input file to model oxidation which incorporates AC+CH+Tensor Mech.\nNow, I want to modify it to have a bi-crystal model. To add the the Grain Growth equation   only on my metal phase (grains)\nI tried to add \"ACGrGrPoly\" and \"ACInterface\" kernels only to my metal phase (grains) eta_Fe and eta_Fe2. However, when I run the input I receive an error message of  \"section 'ACBulk00' does not have an associated \"Action\".\nCan anybody help on this one?\nI have attached the input file.\n4eta_bi_grgr_forgithub.txt",
          "url": "https://github.com/idaholab/moose/discussions/16707",
          "updatedAt": "2022-07-19T07:29:25Z",
          "publishedAt": "2021-01-15T05:38:24Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nFrom your input file it looks like the kernel blocks you added (ACGrGrPoly ACinterface) are after the [] closing the Kernels. So MOOSE is searching for an action, rather than a kernel. They need to be moved up inside the\n[Kernels]\n...\n[]\nstructure.\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/16707#discussioncomment-284332",
                  "updatedAt": "2022-07-19T07:29:42Z",
                  "publishedAt": "2021-01-15T12:13:47Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Ali-toghraee"
                          },
                          "bodyText": "Oh, Thanks for catching that Guillaume!",
                          "url": "https://github.com/idaholab/moose/discussions/16707#discussioncomment-285052",
                          "updatedAt": "2022-07-19T07:29:44Z",
                          "publishedAt": "2021-01-15T17:31:28Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PerformanceData Postprocessor Error",
          "author": {
            "login": "singhgp4321"
          },
          "bodyText": "I am trying to obtain the jacobian compute metrics for a simulation in Bison app using the PerformanceData postprocessor and keep getting the following error:\nUnknown section_name: FEProblem::computeJacobianInternal in PerfGraph::getTime()\nIf you are attempting to retrieve the root use \"Root\".\nIf I change the data_type to calls in the postprocessor block, it shows the same error in PerfGraph::getNumCalls().\nI tried to debug it. Looking at the top frame when it is about to error out: it tries to find section_name (holding the string FEProblem::computeJacobianInternal) inside the container _section_time. The _section_time container seems to have many objects except FEProblem::computeJacobianInternal.\nWhen I remove this postprocessor and turn on the perf_graph I see the computeJacobianInternal related information in the generated performance log table - indicating the jacobian metrics are being determined successfully but postprocessor is not able to retrieve that information.\nI am wondering why is the PerformanceData postprocessor failing to retrieve the jacobian compute information.\nThanks,\nGyan",
          "url": "https://github.com/idaholab/moose/discussions/16625",
          "updatedAt": "2023-01-16T09:41:36Z",
          "publishedAt": "2021-01-07T00:27:49Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@friedmud Can you help with this?",
                  "url": "https://github.com/idaholab/moose/discussions/16625#discussioncomment-267278",
                  "updatedAt": "2023-01-16T10:20:19Z",
                  "publishedAt": "2021-01-07T15:54:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "singhgp4321"
                  },
                  "bodyText": "Just wondering if someone could help with this. Let me know if you need any information.",
                  "url": "https://github.com/idaholab/moose/discussions/16625#discussioncomment-279582",
                  "updatedAt": "2023-01-16T10:20:20Z",
                  "publishedAt": "2021-01-13T16:26:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "PerformanceData is deprecrated. You should use PerfGraphData instead.\nI believe this is what you're looking for:\n[Postprocessors]\n  [jacobian_time]\n    type = PerfGraphData\n    section_name = 'FEProblem::computeJacobianInternal'\n    data_type = TOTAL\n  []\n[]\n\nSee here for more information: https://mooseframework.inl.gov/source/postprocessors/PerfGraphData.html",
                  "url": "https://github.com/idaholab/moose/discussions/16625#discussioncomment-279629",
                  "updatedAt": "2023-01-16T10:20:21Z",
                  "publishedAt": "2021-01-13T16:45:56Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "singhgp4321"
                          },
                          "bodyText": "Actually I am using the PerfGraphData and not the PerformanceData object. I got confused in naming due to this page (https://mooseframework.inl.gov/source/postprocessors/PerformanceData.html).",
                          "url": "https://github.com/idaholab/moose/discussions/16625#discussioncomment-279728",
                          "updatedAt": "2023-01-16T10:20:21Z",
                          "publishedAt": "2021-01-13T17:26:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "loganharbour"
                          },
                          "bodyText": "This is interesting. I added the above to test/tests/kernels/simple_diffusion/simple_diffusion.i and got the intended results.\nCan you please open an issue within the BISON repository (do not do it here) with the input that you are running? Respond here when you do and I'll take a look there.",
                          "url": "https://github.com/idaholab/moose/discussions/16625#discussioncomment-279764",
                          "updatedAt": "2023-01-16T10:20:21Z",
                          "publishedAt": "2021-01-13T17:42:56Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Alright, found the issue. Postprocessors by default run on TIMESTEP_END. If the section that you are executing has not been called yet before TIMESTEP_END, it doesn't exist it'll error out that it can't find the section.\nFor the time being, your solution is to set execute_on to something on your own that occurs after the section has been called, which in your case is almost anything but TIMESTEP_END. Something like NONLINEAR or FINAL.\nThis isn't a very friendly error. We actually register the section very early on (well before INITIAL), therefore we do know that it exists. I'll work on a change that makes this more meaningful. I'd prefer that it just return 0 in all cases if it has been registered but has not been called yet.",
                  "url": "https://github.com/idaholab/moose/discussions/16625#discussioncomment-283069",
                  "updatedAt": "2023-01-16T10:20:21Z",
                  "publishedAt": "2021-01-14T22:12:31Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "#16704 will resolve this issue in the future - valid sections will return zero if they have not ran yet.",
                  "url": "https://github.com/idaholab/moose/discussions/16625#discussioncomment-283094",
                  "updatedAt": "2023-01-16T10:20:22Z",
                  "publishedAt": "2021-01-14T22:27:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "singhgp4321"
                          },
                          "bodyText": "Thanks Logan! Appreciate it very much!",
                          "url": "https://github.com/idaholab/moose/discussions/16625#discussioncomment-283217",
                          "updatedAt": "2023-01-16T10:20:22Z",
                          "publishedAt": "2021-01-14T23:46:58Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Help understanding syncSolutions() when sub-cycling",
          "author": {
            "login": "aprilnovak"
          },
          "bodyText": "Hi all,\nI'm wrapping nekRS as a MOOSE app, and wanted to get some clarification on the syncSolutions() function when using subcycling. Suppose I run nekRs with a smaller dt than a master app, and I have a transfer of heat flux to nekRS on timestep_end. If I'm subcycling nekRS, is that heat flux field fixed for each of the smaller steps taken by nekRS?\n\n\nIf it is, isn't it extra work to keep calling syncSolutions() for each nekRS dt? In this function, I'm writing into nekRS arrays and doing a problem-wide normalization to conserve power, which requires some global communication in nekRS.\n\n\nIf it isn't, what is going on under the hood here?\n\n\nThanks!\n-April",
          "url": "https://github.com/idaholab/moose/discussions/16680",
          "updatedAt": "2022-09-11T09:33:09Z",
          "publishedAt": "2021-01-13T17:14:58Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "If I'm subcycling nekRS, is that heat flux field fixed for each of the smaller steps taken by nekRS?\n\nI believe so, the sub-application is just solved with smaller timesteps until it reaches the desired time from the main application. There is no communication during this and the main is just waiting for sub-cycling to finish.\n\n* If it is, isn't it extra work to keep calling `syncSolutions()` for each nekRS dt?\n\n\nI am not familiar with this function, I have not used ExternalProblem. But I imagine that you are correct. Perhaps @permcody will weigh in.",
                  "url": "https://github.com/idaholab/moose/discussions/16680#discussioncomment-282286",
                  "updatedAt": "2022-09-11T09:44:46Z",
                  "publishedAt": "2021-01-14T16:20:01Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "Thanks! I'm basically wondering if I can add some type of check like the following:\nsyncSolutions(Direction)\n{\n  case ExternalProblem::Direction::TO_EXTERNAL_APP:\n  {\n    if (!first_step_in_subcycle) // already sent all info to nekRS\n      return;\n\n    send_heat_flux_to_nekrs();\n    // other stuff in syncSolutions()\n  }\n}\n\nWe'll be running nekRS with a time step about 10x smaller than it's master App, so this could help with speedup.",
                          "url": "https://github.com/idaholab/moose/discussions/16680#discussioncomment-282311",
                          "updatedAt": "2022-09-11T09:44:49Z",
                          "publishedAt": "2021-01-14T16:30:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "I would give it a shot, don't see why not.",
                          "url": "https://github.com/idaholab/moose/discussions/16680#discussioncomment-282564",
                          "updatedAt": "2022-09-11T09:44:49Z",
                          "publishedAt": "2021-01-14T18:04:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "Thanks!\nFor data transfer in the reverse direction (nekRS -> master), I had also suggested to @friedmud that we could save some overhead by only copying the nekRS solution from the GPU back to the host CPU at those master-sub synchronization steps. But if I recall correctly, I think @friedmud said that MOOSE does some type of interpolation operation between the actual Transfer points? (thereby requiring copying from device -> host each nekRS step). That discussion was basically the origin of my question here. If @friedmud wants to comment here that'd be helpful as well!",
                          "url": "https://github.com/idaholab/moose/discussions/16680#discussioncomment-282597",
                          "updatedAt": "2022-09-11T09:44:49Z",
                          "publishedAt": "2021-01-14T18:15:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Problem with compiling your application",
          "author": {
            "login": "xchengood"
          },
          "bodyText": "Hi everyone. I am a new learner for Moose. When I try to compile and test your application on this page (https://mooseframework.inl.gov/getting_started/new_users.html#new-users) I got the following issue.\n\nI do not know how to solve it.",
          "url": "https://github.com/idaholab/moose/discussions/16630",
          "updatedAt": "2022-08-27T08:52:06Z",
          "publishedAt": "2021-01-07T17:11:43Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "cticenhour"
                  },
                  "bodyText": "You need to first create an application in your projects directory. Then you can build and test it. The intent of the directions was for you to create an app with a name you decide (replace YourAppName with another name, which could be anything you want).\nAssuming that you have installed and tested MOOSE itself successfully, you need to follow the directions on that page closely. I'll recap here. Each line should be entered into your prompt separately.\ncd projects\n./moose/scripts/stork.sh YourAppName  # REPLACE YourAppName WITH A CUSTOM NAME\n\nThen you can perform each of the three commands you have in your original question, replacing YourAppName with the name of your app.",
                  "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-267528",
                  "updatedAt": "2022-08-27T08:54:21Z",
                  "publishedAt": "2021-01-07T17:41:27Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "You can also take a look at our tutorial: https://mooseframework.inl.gov/getting_started/examples_and_tutorials/tutorial01_app_development/index.html",
                          "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-267919",
                          "updatedAt": "2023-04-05T09:31:51Z",
                          "publishedAt": "2021-01-07T21:39:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xchengood"
                          },
                          "bodyText": "You need to first create an application in your projects directory. Then you can build and test it. The intent of the directions was for you to create an app with a name you decide (replace YourAppName with another name, which could be anything you want).\nAssuming that you have installed and tested MOOSE itself successfully, you need to follow the directions on that page closely. I'll recap here. Each line should be entered into your prompt separately.\ncd projects\n./moose/scripts/stork.sh YourAppName  # REPLACE YourAppName WITH A CUSTOM NAME\n\nThen you can perform each of the three commands you have in your original question, replacing YourAppName with the name of your app.\n\nThank you. This issue does not appear according to your instructions. But I got another error.\n\nDo I need to install 'libmesh-config'? Could you tell me how to install it?",
                          "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-269782",
                          "updatedAt": "2023-04-05T09:31:51Z",
                          "publishedAt": "2021-01-08T17:43:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xchengood"
                          },
                          "bodyText": "You can also take a look at our tutorial: https://mooseframework.inl.gov/getting_started/examples_and_tutorials/tutorial01_app_development/index.html\n\nThank you. I am studying this step by step.",
                          "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-269784",
                          "updatedAt": "2023-04-05T09:32:11Z",
                          "publishedAt": "2021-01-08T17:44:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "you are operating in the (base) environment as denoted in your screen shot. You'll want to first activate the moose environment and then run make:\n (base) xinchen@xins-mbp apple % conda activate moose\n (moose) xinchen@xins-mbp apple %\n# ^^^^^ moose environment (and thus libmesh) is available. Now you can run make:\n\ncd ~/projects/moose/test\nmake -j4\nIf make succeeds, then you can ./run_tests -j 4",
                          "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-269910",
                          "updatedAt": "2022-08-27T08:54:22Z",
                          "publishedAt": "2021-01-08T18:41:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "Thank you. This issue does not appear according to your instructions. But I got another error.\n\nThis means that it cannot find libMesh. You are running in the \"base\" conda environment, so you must activate your moose environment whenever you want to do moose-based development. Run conda activate moose in your Terminal and then try your build and test commands again.",
                          "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-269911",
                          "updatedAt": "2022-08-27T08:54:59Z",
                          "publishedAt": "2021-01-08T18:41:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xchengood"
                          },
                          "bodyText": "you are operating in the (base) environment as denoted in your screen shot. You'll want to first activate the moose environment and then run make:\n (base) xinchen@xins-mbp apple % conda activate moose\n (moose) xinchen@xins-mbp apple %\n# ^^^^^ moose environment (and thus libmesh) is available. Now you can run make:\n\ncd ~/projects/moose/test\nmake -j4\nIf make succeeds, then you can ./run_tests -j 4\n\nThank you. I fix this issue. When I try Mesh in 'Example 1 : As Simple as it Gets',\n\nI got the following issue. Could you help me solve it? Thank you.",
                          "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-279802",
                          "updatedAt": "2022-08-27T08:55:24Z",
                          "publishedAt": "2021-01-13T17:57:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "This section just explaining what is in the input file, go down to the \"Running the Problem\" section to execute the example.",
                          "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-282200",
                          "updatedAt": "2022-08-27T08:55:36Z",
                          "publishedAt": "2021-01-14T15:53:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "You should also consider going through our tutorial carefully, it will help you understand how MOOSE is intended to work.\nhttps://mooseframework.inl.gov/getting_started/examples_and_tutorials/tutorial01_app_development/index.html",
                          "url": "https://github.com/idaholab/moose/discussions/16630#discussioncomment-282206",
                          "updatedAt": "2022-08-27T08:55:36Z",
                          "publishedAt": "2021-01-14T15:54:48Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Scaling in the s5_energycurve.i file",
          "author": {
            "login": "LiuPengPeter"
          },
          "bodyText": "Hello,\nI still couldn't understand the scaling factor and how it works on the residual.\nIn the s5_energycurve.i file, the scaling=1e+04 for variable c. And in the expression of material, kappa_c1e-27, M/1e-27 and f_loc1e-27.\nCould anyone explain?",
          "url": "https://github.com/idaholab/moose/discussions/16379",
          "updatedAt": "2024-04-21T01:06:22Z",
          "publishedAt": "2020-12-03T01:24:56Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jessecarterMOOSE"
                  },
                  "bodyText": "1e-27 is for the value of the property, sort of like a unit conversion. It's the same everywhere so it doesn't affect the solution. Probably @dschwen can tell you more about why that value is chosen. The variable scaling affects the residual.\nYou might want to try the newer automatic scaling feature in moose. You can set automatic_scaling=true in your Executioner block and remove the scaling parameter for the variable.",
                  "url": "https://github.com/idaholab/moose/discussions/16379#discussioncomment-145559",
                  "updatedAt": "2024-04-21T01:06:22Z",
                  "publishedAt": "2020-12-03T02:02:48Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "LiuPengPeter"
                          },
                          "bodyText": "Than you!",
                          "url": "https://github.com/idaholab/moose/discussions/16379#discussioncomment-145591",
                          "updatedAt": "2024-04-21T01:20:31Z",
                          "publishedAt": "2020-12-03T03:21:55Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "LiuPengPeter"
                  },
                  "bodyText": "@dschwen , could you tell me more details about scaling.\n\nWhy and when does the scaling factor need to be added to variables and material parameters?\n\n2)How the scaling factor influence the residual?\nFor example (maybe not right), there is a residual R1 whose variable is eta1 and coupled variable is eta2. M is the material parameter.\nR1 = M*eta1^2 + eta2 + partial(f)/(eta1) +  partial(eta1)/(x).\nAfter add scaling=1e9 to eta1, what will R1 become?\nThe R1 may be not right, I just want to use it as an example to know hoe the factor influence residual.\n3)In the case of s5_energycurve.i file from tutorial of phase filed, the scaling=1e+04 for variable c. And in the expression of material, kappa_c1e-27, M/1e-27 and f_loc1e-27. Could you explain it?\nThank you a lot!",
                  "url": "https://github.com/idaholab/moose/discussions/16379#discussioncomment-145635",
                  "updatedAt": "2024-05-09T07:08:19Z",
                  "publishedAt": "2020-12-03T06:23:22Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "LiuPengPeter"
                  },
                  "bodyText": "I still couldn't understand the scaling factor...Could anyone help?",
                  "url": "https://github.com/idaholab/moose/discussions/16379#discussioncomment-148841",
                  "updatedAt": "2024-05-09T07:08:24Z",
                  "publishedAt": "2020-12-07T11:40:19Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "@lindsayad Just noticed this sitting around. The only information that I can find on our auto scaling is here: https://mooseframework.inl.gov/newsletter/2019_07.html. Do we have more information somewhere else?",
                          "url": "https://github.com/idaholab/moose/discussions/16379#discussioncomment-279478",
                          "updatedAt": "2024-05-09T07:08:24Z",
                          "publishedAt": "2021-01-13T15:47:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "We have this documentation page: https://mooseframework.inl.gov/source/systems/NonlinearSystemBase.html. @LiuPengPeter can you see whether this helps you in your understanding?",
                          "url": "https://github.com/idaholab/moose/discussions/16379#discussioncomment-279790",
                          "updatedAt": "2024-05-09T07:09:42Z",
                          "publishedAt": "2021-01-13T17:51:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "LiuPengPeter"
                          },
                          "bodyText": "Than you all of guys!",
                          "url": "https://github.com/idaholab/moose/discussions/16379#discussioncomment-280748",
                          "updatedAt": "2024-05-09T07:09:44Z",
                          "publishedAt": "2021-01-13T23:31:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Elements flipped even though Newton solver converged",
          "author": {
            "login": "tairoon1"
          },
          "bodyText": "Hello everyone,\nI have been using MOOSE to compute some mechanics problems. I implemented a Neo-Hookean material model by defining a new strain function which sets the deformation gradient as the total strain, and a stress function which computes the 1st Piola Kirchhoff stress for a Neo-Hookean solid. The stiffness is derived using Automatic Differentiation. The files can be found here.\nNow, for small deformations, this yields good results and I can also observe a quadratic convergence of the Newton solver. But I noticed for a specific problem, that sometimes the Newton solver would converge, but the end result would contain flipped elements\n. This example contains a uniaxial compression in the x direction of a square. The input and mesh file leading to this final deformation are also given in the link above. Using a smaller timestep size seems to prevent this problem from happening. However, I am confused, whether something with my code is wrong or what is going on?\nI also used another FE solver and compared it with the MOOSE results. For \"easy\" simulations, both results are exactly the same, the residuals in each Newton iteration are exactly the same, but for more complicated case, MOOSE converges a lot slower... When I take a very small timestep in MOOSE dt=0.01 (element does not flip over), the same result is obtained as my other solver, which only needs a single time step dt=1. I also tried to not use Automatic Differentiation and directly code the consistent stiffness, however, it also lead to the same suboptimal convergence.\nOne difference in both solvers that I noticed is that MOOSE seems to use some line search algorithm that prevents the residual from increasing. In my other solver, that is not the case and always a full Jacobian step is used. Is there any way to deactivate the line search in MOOSE?\nCan anyone help me with this? I am really confused and honestly frustrated that MOOSE sometimes flips over my elements without any indicator or warnings...\nThanks.",
          "url": "https://github.com/idaholab/moose/discussions/16682",
          "updatedAt": "2023-01-08T06:02:00Z",
          "publishedAt": "2021-01-13T17:22:30Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "A few things you could try:\n\nTo turn off line search, you can add line_search = none in your executioner block.\nFor mechanics problems, if memory isn't a concern, few solvers can beat superlu in terms of speed. To use that, set\n\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu       superlu_dist                 '\n\n\nAutomatic scaling might also help: automatic_scaling = true\nWe've seen before that if you take a very large step, setting preset = false in your FunctionDirichletBCs can improve convergence and avoid spurious results.",
                  "url": "https://github.com/idaholab/moose/discussions/16682#discussioncomment-280226",
                  "updatedAt": "2023-01-08T06:02:03Z",
                  "publishedAt": "2021-01-13T19:04:33Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "tairoon1"
                          },
                          "bodyText": "Wow, number 4 did the magic! Thank you so much!\nDo you mind telling me, what preset = false does?",
                          "url": "https://github.com/idaholab/moose/discussions/16682#discussioncomment-280322",
                          "updatedAt": "2023-01-08T06:02:03Z",
                          "publishedAt": "2021-01-13T19:47:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "You can take a look at the documentation for DirichletBC:\nhttps://mooseframework.inl.gov/source/bcs/DirichletBC.html\npreset = true and preset = false correspond to two different ways of enforcing Dirichlet BC. Depending on your use case, one maybe better than the other.",
                          "url": "https://github.com/idaholab/moose/discussions/16682#discussioncomment-280382",
                          "updatedAt": "2023-01-08T06:02:03Z",
                          "publishedAt": "2021-01-13T20:22:26Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Also, in response to your complaint\n\nI am really confused and honestly frustrated that MOOSE sometimes flips over my elements without any indicator or warnings\n\nThere is actually a very useful Userobject to do exactly that: https://mooseframework.inl.gov/source/userobject/ElementQualityChecker.html\nwhich can be used together with an Auxkernel for visualization purposes:\nhttps://mooseframework.inl.gov/source/auxkernels/ElementQualityAux.html",
                  "url": "https://github.com/idaholab/moose/discussions/16682#discussioncomment-280519",
                  "updatedAt": "2023-01-08T06:02:04Z",
                  "publishedAt": "2021-01-13T21:33:55Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "tairoon1"
                          },
                          "bodyText": "I see. Thank you for the hint.",
                          "url": "https://github.com/idaholab/moose/discussions/16682#discussioncomment-280675",
                          "updatedAt": "2023-02-21T20:13:33Z",
                          "publishedAt": "2021-01-13T22:51:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Chrono Error",
          "author": {
            "login": "dorton21"
          },
          "bodyText": "Perhaps I'm missing something obvious but while compiling the Makefile within the test folder, I get this error.\nFor further context I am using GCC version 8.4.0 and openmpi as the hoc wrapper. I played with editing the PerfNode.h file by adding in a \"#include \" statement, however, unless this is a bug, I do not think this should be the solution. I would appreciate any suggestions anyone may have.\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h\n:39:32: error: 'chrono' in namespace 'std' does not name a type\nvoid setStartTime(const std::chrono::time_pointstd::chrono::steady_clock time)                                                                                                          ^~~~~~\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h\n:39:27: note: suggested alternative: 'conj'\nvoid setStartTime(const std::chrono::time_pointstd::chrono::steady_clock time)\n^~~                                                                                                                                                          conj\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h\n:39:50: error: expected unqualified-id before '<' token\nvoid setStartTime(const std::chrono::time_pointstd::chrono::steady_clock time)                                                                                                                            ^\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h\n:39:50: error: expected ')' before '<' token\nvoid setStartTime(const std::chrono::time_pointstd::chrono::steady_clock time)\n~                             ^                                                                                                                                                            )\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:39:40: error: expected ';' at end of member declaration\nvoid setStartTime(const std::chrono::time_pointstd::chrono::steady_clock time)\n^~~~~~~~~~\n;                                                                                                          /home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:39:50: error: expected unqualified-id before '<' token\nvoid setStartTime(const std::chrono::time_pointstd::chrono::steady_clock time)                                                                                                                            ^                                                                                                          /home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h\n:47:27: error: 'chrono' in namespace 'std' does not name a type\nstd::chrono::steady_clock::duration totalTime() const;\n^~~\nconj\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:101:8: error: 'chrono' in namespace 'std' does not name a type\nstd::chrono::steady_clock::duration childrenTime() const;\n^~~~~~\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:101:3: note: suggested alternative: 'conj'\nstd::chrono::steady_clock::duration childrenTime() const;\n^~~\nconj\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:113:8: error: 'chrono' in namespace 'std' does not name a type\nstd::chrono::time_pointstd::chrono::steady_clock _start_time;\n^~~~~~\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:113:3: note: suggested alternative: 'conj'\nstd::chrono::time_pointstd::chrono::steady_clock _start_time;\n^~~\nconj\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:116:8: error: 'chrono' in namespace 'std' does not name a type\nstd::chrono::steady_clock::duration _total_time;\n^~~~~~\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:116:3: note: suggested alternative: 'conj'\nstd::chrono::steady_clock::duration _total_time;\n^~~\nconj\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h: In constructor 'PerfNode::PerfNode(PerfID)':\n/home/las_djorton/spack/opt/spack/linux-fedora33-skylake/gcc-8.4.0/moose-20-12-24-ht5jaewmwjovksfstgkyjaz2ibwtvcah/framework/build/header_symlinks/PerfNode.h:29:40: error: class 'PerfNode' does not have any field named '_total_time'\nPerfNode(const PerfID id) : _id(id), _total_time(0), _num_calls(0) {}\nAny ideas on what I am doing wrong?",
          "url": "https://github.com/idaholab/moose/discussions/16599",
          "updatedAt": "2022-06-27T20:08:48Z",
          "publishedAt": "2020-12-29T20:34:09Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@milljm Did you see this?",
                  "url": "https://github.com/idaholab/moose/discussions/16599#discussioncomment-279451",
                  "updatedAt": "2022-06-27T20:08:47Z",
                  "publishedAt": "2021-01-13T15:40:31Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "What do you get when you\n#include <chrono>\n\nat the top of PerfNode.h?\nIt's possible that some includes are different within your stack, and we need to add an include there.",
                  "url": "https://github.com/idaholab/moose/discussions/16599#discussioncomment-279639",
                  "updatedAt": "2022-06-27T20:08:47Z",
                  "publishedAt": "2021-01-13T16:51:05Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "loganharbour"
                          },
                          "bodyText": "I see now that you said this:\n\nI played with editing the PerfNode.h file by adding in a \"#include \" statement, however, unless this is a bug, I do not think this should be the solution.\n\nI assume that means that including chrono worked? It could be very well possible that you're missing that include from elsewhere in the framework which causes this, in which case the solution is indeed to include it.\nI've created a PR to add this: #16681. Please confirm that the include resolves your issue and we'll get this merged.",
                          "url": "https://github.com/idaholab/moose/discussions/16599#discussioncomment-279708",
                          "updatedAt": "2022-06-27T20:08:47Z",
                          "publishedAt": "2021-01-13T17:16:28Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Moose plastic hardening simulation - Computationaly slower than ANSYS",
          "author": {
            "login": "abarun22"
          },
          "bodyText": "Dear all,\nI recently used MOOSE  to predict the hardening behaviour of certain class of steel specimens subjected to tensile loads. A PieceWise linear function was used to model the hardening response. The results were matching precisely with ANSYS until the start of necking, with some differences noticed thereafter, nevertheless a seemingly realistic behaviour overall. However the problem is that MOOSE is extremely slow in comparison to ANSYS's computational effort.  For a model with say ~1500 nodes, Ansys took only 5-6 mins, whereas MOOSE clocks it in 2.5hrs, which is exhorbitant, considering that we may need to go for a constrained optimization problem thereafter. We typically expect this problem to be solved in 10 iterations and that might take more than a day to complete the whole analysis. From the compiler perspective, we've chosen the right optimization (O3) that enables a faster computation. Attached please find the input file i was using to simulate this model. Any advice on how to achieve a computational time comparable to ANSYS would be greatly appreciated?\nKind regards,\nPieceWise_test.txt\nArun",
          "url": "https://github.com/idaholab/moose/discussions/16372",
          "updatedAt": "2022-06-10T14:39:03Z",
          "publishedAt": "2020-12-02T17:45:53Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "Could you quickly check whether ANSYS and MOOSE are solving the same number of DOFs?\nHow many time steps does MOOSE and ANSYS take?\nFor each time step, how many nonlinear iteration does MOOSE and ANSYS?\nWhat solver does ANSYS use? How long does ANSYS and MOOSE to solve per iteration and time step?",
                  "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146194",
                  "updatedAt": "2022-06-10T14:39:17Z",
                  "publishedAt": "2020-12-03T16:09:15Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "my 2c. In my experience, MOOSE is generally slower than either ANSYS or ABAQUS.  For non-AD solutions in plasticity i have found moose to be about 2 - 2.5 times slower with a LU preconditioner and newton solution.\nThe relative convergence criteria are about 1e-3 in ANSYS and ABAQUS. Also double check the elements you are using. The defaults in commercial software, use hybrid elements with an extra pressure degree of freedom, that is not yet implemented in MOOSE. Also turn on volumetric locking to get better convergence and speed.\nIn your input file, change the solution to NEWTON instead of PJFNK and see if it helps.\nCheers\nSrinath",
                  "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146203",
                  "updatedAt": "2022-06-10T14:39:17Z",
                  "publishedAt": "2020-12-03T16:17:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "For nonlinear mechanics problem, we do not implement exact Jacobians, so Newton should take take more iterations than PJFNK to converge. In our experience, for non-AD, NEWTON is usually slower than PJFNK. I am curious to know your experience on the speed of NEWTON and PJFNK for non-AD.",
                          "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146222",
                          "updatedAt": "2022-07-02T13:23:35Z",
                          "publishedAt": "2020-12-03T16:30:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "Your IsotropicPlasticityStressUpdate is set to run on the displaced mesh. I don't think that's right. You might try using AD versions of all mechanics models ADIsotropicPlasticityStressUpdate, ADComputeMultipleInelasticStress, ADComputeIsotropicElasticityTensor, and use_asutomatic_differentiation = true in the Master action. (you may need to recompile your moose with a larger AD vector size for 3D...)\nBut the execution time difference is too substantial to be explained away by that most likely. Without the mesh and function files we cannot run your input though.\n1500 nodes is a really tiny problem and should absolutely not take that long to run at all.",
                  "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146395",
                  "updatedAt": "2022-07-02T13:23:38Z",
                  "publishedAt": "2020-12-03T17:06:39Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Dear all,\nThanks for the suggestions and indeed they worked very well. On top of that i resized my ANSYS mesh (~5600 degrees of freedom) to match closely with my MOOSE mesh. Also i was not exactly specifying the number of cores in my parallel execution launcher earlier and that took me to the serial mode.  With all these modifications i am now getting comparables times.  Ansys took 26 mins to finish the computation, while moose did that in 33mins, which seems quite satisfying going in to subsequent phase of activities. For your reference i am attaching here the required files for running the computation. Is there any way that i could tune the execution options to achieve an enhanced performance.\nKind regards,\nArun\nPlasticityModelling_files.zip",
                          "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146676",
                          "updatedAt": "2022-07-02T13:23:46Z",
                          "publishedAt": "2020-12-03T19:58:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Thanks for all your suggestions. With much hope i am now testing my plasticity model for large meshes say of the order of 50k Dof's. Moose crossed that in 32 hrs. I've not exactly quantified the speed in Ansys as i am trying to sort out certain issues associated with the batch job execution internaly. My speculation is that ANSYS scales much slower than MOOSE for bigger models, perhaps would take double the time attained by MOOSE. I shall give you a clear update in this as we progress with ANSYS testing.\nAnother issue i noted recently was that the newer version of MOOSE seems to be much slower than the earlier version (dated May'2020) with the former one requiring 55mins to complete the execution for the smaller test case i mentioned intially. From my tests I see  that the petsc versions are not influencing the performance. I wonder whether the new developments in MOOSE are reponsible for this degraded performance. Any advise on this would help me decide on the performance aspects of the code before going in to the subsequent phase of developments.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-277247",
                          "updatedAt": "2022-07-02T13:24:03Z",
                          "publishedAt": "2021-01-12T17:56:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "@abarun22 It's probably better to start a new discussion on the slow down using recent moose, so that people know this is unanswered.",
                          "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-279652",
                          "updatedAt": "2022-08-17T22:06:11Z",
                          "publishedAt": "2021-01-13T16:55:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to preserve sideset names with FancyExtruderGenerator",
          "author": {
            "login": "aprilnovak"
          },
          "bodyText": "Hi all,\nI'd like to make a pincell mesh, starting from a circle mesh generated with the AnnularMeshGenerator, that is then extruded in the z-direction. I want to apply a boundary condition on the surface of the pincell, but am unsure how to access that sideset following the extrusion process.\nAnnularMeshGenerator labels the circle surface as a sideset with name rmax. But once I extrude it in the z-direction and try to apply a BC on rmax, I get a: the following side set ids do not exist on the mesh error. I feel like I'm just missing something basic here, but none of the sideset generators jump out at me as being capable of creating a sideset for the curved cylinder surface (b/c the normal isn't fixed). All the tests for the extrude-type mesh generators also only apply BCs on the bottom/top sidesets that you can name in the generator itself.\nAny suggestions for how to get a valid sideset on the cylinder surface? Here's my mesh:\n[Mesh]\n  [circle]\n    type = AnnularMeshGenerator\n    nr = 8\n    nt = 8\n    rmin = 0\n    rmax = 0.4E-2\n    growth_r = 1.2\n  []\n\n  [extrude]\n    type = FancyExtruderGenerator\n    input = circle\n    heights = '0.8'\n    num_layers = '10'\n    direction = '0 0 1'\n    bottom_sideset = '100'\n    top_sideset = '101'\n  []\n[]\n\n[Outputs]\n  exodus = true\n[]\n\nThanks!\n-April",
          "url": "https://github.com/idaholab/moose/discussions/16623",
          "updatedAt": "2023-04-05T23:17:01Z",
          "publishedAt": "2021-01-06T23:49:37Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hi April,\n@friedmud made such meshes for BEAVRS, you may see them in the mockingbird repository. I can't add you rn, maybe @smharper ?\n@smharper may also be able to help directly as he used the FancyExtruder recently.\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/16623#discussioncomment-266719",
                  "updatedAt": "2023-04-05T23:17:06Z",
                  "publishedAt": "2021-01-07T11:10:47Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "I figured out my problem! It seems that the FancyExtruder doesn't retain sideset names when you extrude, but does preserve sideset numbers. I was trying to apply BCs based on names, which I had assumed would transfer correctly.\nIs there a reason why the sideset names aren't also transferred? I think it could be very helpful to transfer those as well as the sideset IDs.",
                          "url": "https://github.com/idaholab/moose/discussions/16623#discussioncomment-277483",
                          "updatedAt": "2023-04-05T23:17:06Z",
                          "publishedAt": "2021-01-12T19:45:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Probably just an oversight.",
                          "url": "https://github.com/idaholab/moose/discussions/16623#discussioncomment-279440",
                          "updatedAt": "2023-04-05T23:17:06Z",
                          "publishedAt": "2021-01-13T15:37:51Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Installing MOOSE with OpenMPI3.1.3 and GCC 7",
          "author": {
            "login": "jinca"
          },
          "bodyText": "Hello,\nI used the following modules:\nmodule purge\nmodule load slurm\nmodule load dot\nmodule load turbovnc/2.0.1\nmodule load vgl/2.5.1/64\nmodule load singularity/current\nmodule load rhel7/global\nmodule load cmake/latest\nmodule load gcc/7\nmodule load openmpi-3.1.3-gcc-7.2.0-b5ihosk\nI followed the procedures of the Website:\nsource ~/.moose_profile\necho $CC\nwhich mpicc\ncd\nmkdir projects\ngit clone https://github.com/idaholab/moose\ncd moose\ngit checkout master\ncd ~/projects/moose\nunset PETSC_DIR PETSC_ARCH\n./scripts/update_and_rebuild_petsc.sh --download-mumps=0 --with-64-bit-indices=1\ncd petsc/\nmake PETSC_DIR=/home/ir-inca1/moose/scripts/../petsc PETSC_ARCH=arch-moose check\ncd ..\n./scripts/update_and_rebuild_libmesh.sh\ncd ~/projects/moose/test\nmake -j 4\n./run_tests -j 4\nI was able to build PETSC and LibMesh, but when it comes to run the tests, I got the following errors:\npostprocessors/num_elems.test_split ............................................ [min_cpus=4] FAILED (CSVDIFF)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor .............. [min_cpus=3] FAILED (EXODIFF)\noutputs/xml.parallel/distributed ............................................... [min_cpus=3] FAILED (XMLDIFF)\nmesh/splitting.use_split ....................................................... [min_cpus=3] FAILED (EXODIFF)\nrelationship_managers/evaluable.evaluable_neighbors_replicated ................. [min_cpus=3] FAILED (EXODIFF)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor_3D ........... [min_cpus=3] FAILED (EXODIFF)\nmesh/centroid_partitioner.centroid_partitioner_test ............................ [min_cpus=4] FAILED (EXODIFF)\nrelationship_managers/evaluable.edge_neighbor .................................. [min_cpus=3] FAILED (EXODIFF)\nRan 2416 tests in 772.2 seconds.\n2366 passed, 96 skipped, 0 pending, 50 FAILED\nMAX FAILURES REACHED",
          "url": "https://github.com/idaholab/moose/discussions/16544",
          "updatedAt": "2022-08-02T05:54:50Z",
          "publishedAt": "2020-12-18T18:26:46Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "There should be some more detailed error information farther up in your terminal window. I suspect there is a problem with the MPI.",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-235821",
                  "updatedAt": "2022-08-02T05:55:22Z",
                  "publishedAt": "2020-12-23T00:10:06Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "@jinca\nWhich operating system are you using?\nI suggest you try the manual installation with gcc 9.2.0 and mpich 3.3 on the website\nIt usually works fine for us\nNicol\u00f2",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-256245",
                  "updatedAt": "2022-08-02T05:55:22Z",
                  "publishedAt": "2021-01-02T05:36:08Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jinca"
                          },
                          "bodyText": "The OS is Scientific Linux release 7.9 (Nitrogen)",
                          "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277556",
                          "updatedAt": "2022-08-02T05:55:25Z",
                          "publishedAt": "2021-01-12T20:30:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "50 Errors (the MAX before it quits) usually means an easy fix. Something fundamentally wrong. Not necessarily a compiler version or the like.\nAs Andrew asked, seeing the actual error helps tremendously. The details of these errors can be found if you scroll up in your terminal history.",
                          "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277597",
                          "updatedAt": "2022-08-02T05:55:25Z",
                          "publishedAt": "2021-01-12T20:45:37Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jinca"
                  },
                  "bodyText": "OK, When I set the following modules:\nmodule load rhel7/global\nmodule load slurm\nmodule load gcc/7\nmodule load openmpi-3.1.3-gcc-7.2.0-b5ihosk\nmodule load python/3.8\nI got this error while building petsc:\n===============================================================================                                                                                                                     Configuring STRUMPACK with cmake; this may take several minutes                                                                                                                         ===============================================================================                                                                                                                                                                                                                                                                                                             *******************************************************************************\nUNABLE to CONFIGURE with GIVEN OPTIONS    (see configure.log for details):\nError configuring STRUMPACK with cmake\n\nThere was an error. Exiting...",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277621",
                  "updatedAt": "2022-08-02T05:55:25Z",
                  "publishedAt": "2021-01-12T21:01:21Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Without PETSc, you wouldn't (shouldn't) be able to build libMesh. Without libMesh... you wouldn't be able to build MOOSE for an attempt at ./run_test. I see lots of tests passed as well as reaching the max failure. The only thing I can think of, is that your moose repo was previously built, and has a partial successful build.\nIts hard to tell if you copied exactly what you attempted, or if there is an error during your attempt:\nwhich mpicc\ncd\nmkdir projects                 # <-------- failed to enter projects before cloning moose\ngit clone https://github.com/idaholab/moose\ncd moose\ngit checkout master\ncd ~/projects/moose            # <-------- not the moose you just cloned. A possible older version of moose?",
                          "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277700",
                          "updatedAt": "2022-08-02T05:55:25Z",
                          "publishedAt": "2021-01-12T21:34:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jinca"
                  },
                  "bodyText": "Thank you so much for the quick replies guys. Many days ago, I was able to run MOOSE only with 7 FAILS. Now I am trying to reinstalling again, I can not install at least PETSC. I am very aware that I have to build PETSC, then LibMesh and at the end MOOSE.\nI will clean and delete all, turn off the machine and do it again in order to send the info today asap. Thanks again.",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277720",
                  "updatedAt": "2022-08-02T05:55:25Z",
                  "publishedAt": "2021-01-12T21:44:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jinca"
                  },
                  "bodyText": "Hi guys, this is the configuration I used:\nmodule purge\nmodule load slurm\nmodule load dot\nmodule load turbovnc/2.0.1\nmodule load vgl/2.5.1/64\nmodule load singularity/current\nmodule load rhel7/global\nmodule load cmake/latest\nmodule load gcc/7\nmodule load openmpi-3.1.3-gcc-7.2.0-b5ihosk\nmodule load python/3.8\nThe PETSC was built as the libMesh, and therefore MOOSE.\nI am attaching the output of the tests I've done recently.\nHope you can help me soon.\nBest,\nJulita Inca\nmesh/checkpoint.test_8 .................................................. [insufficient slots,min_cpus=8] SKIP\nmesh/checkpoint.test_8a ................................................. [insufficient slots,min_cpus=8] SKIP\npartitioners/random_partitioner.test ......................................................... [min_cpus=4] OK\nmesh/nemesis.nemesis_repartitioning_test ..................................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.ptscotch_weight_both .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis ...................................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_element ....................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_side .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_both .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_presplit_mesh ........................................ [min_cpus=2] OK\nauxkernels/ghosting_aux.no_algebraic_ghosting ................................................ [min_cpus=4] OK\npostprocessors/num_nodes.test_split .......................................................... [min_cpus=4] OK\nreporters/mesh_info.info/files ........................................... [min_cpus=2] FAILED (MISSING FILES)\noutputs/json/distributed.info/default .................................... [min_cpus=2] FAILED (MISSING FILES)\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nmisc/exception.parallel_error_residual_transient_non_zero_rank .. [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank ................ [min_cpus=2] FAILED (NO CRASH)\nbcs/dmg_periodic.1d ............................................................. [min_cpus=2] FAILED (ERRMSG)\nvectorpostprocessors/work_balance.work_balance/replicated ...................... [min_cpus=2] FAILED (CSVDIFF)\noutputs/variables.nemesis_hide ................................................. [min_cpus=2] FAILED (EXODIFF)\nmeshgenerators/distributed_rectilinear/generator.3D_hierarch ................... [min_cpus=4] FAILED (EXODIFF)\nmeshgenerators/distributed_rectilinear/generator.3d_scomm_out .................. [min_cpus=3] FAILED (EXODIFF)\nRan 2490 tests in 586.0 seconds.\n2480 passed, 96 skipped, 0 pending, 10 FAILED",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-278002",
                  "updatedAt": "2022-08-02T05:55:25Z",
                  "publishedAt": "2021-01-13T00:27:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Wrap three ticks around text when wanting to use a pre-rendered block (instead of one). Somehow, doing that entire thing around one broke GitHub! As I only got most of it, from email.\ntick tick tick\npre-rendered block\n\ntick tick tick\nHere is an excerpt of what was sent to me by email:\nproblems/eigen_problem/eigensolvers.nonlinear_power ....................................................... OK\nics/depend_on_uo.ic_depend_on_uo: Working Directory: /home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo\nics/depend_on_uo.ic_depend_on_uo: Running command: mpiexec -n 2 /home/ir-inca1/projects/moose/test/moose_test-opt -i geometric_neighbors_ic.i --error --error-unused --error-override --no-gdb-backtrace\nics/depend_on_uo.ic_depend_on_uo: srun: error: Unable to allocate resources: Invalid account or account/partition combination specified\nics/depend_on_uo.ic_depend_on_uo: srun: error: Unable to allocate resources: Invalid account or account/partition combination specified\nics/depend_on_uo.ic_depend_on_uo: Running exodiff: /home/ir-inca1/projects/moose/framework/contrib/exodiff/exodiff -m -F 1e-10 -t 5.5e-06 /home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/gold/geometric_neighbors_ic_out.e /home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/geometric_neighbors_ic_out.e\nics/depend_on_uo.ic_depend_on_uo: ERROR:\nics/depend_on_uo.ic_depend_on_uo: *****************************************************************\nics/depend_on_uo.ic_depend_on_uo: EXODIFF (Version: 2.90) Modified: 2018-02-15\nics/depend_on_uo.ic_depend_on_uo: Authors: Richard Drake, rrdrake@sandia.gov\nics/depend_on_uo.ic_depend_on_uo: Greg Sjaardema, gdsjaar@sandia.gov\nics/depend_on_uo.ic_depend_on_uo: Run on 2021/01/13 00:11:20 GMT\nics/depend_on_uo.ic_depend_on_uo: *****************************************************************\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo: exodiff: ERROR: Couldn't open file \"/home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/geometric_neighbors_ic_out.e\".\nics/depend_on_uo.ic_depend_on_uo: Reading first file ...\nics/depend_on_uo.ic_depend_on_uo: Reading second file ...\nics/depend_on_uo.ic_depend_on_uo: exodiff: ERROR: exodiff: ERROR: Couldn't open file \"/home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/geometric_neighbors_ic_out.e\". File does not exist.\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo: ################################################################################\nics/depend_on_uo.ic_depend_on_uo: Tester failed, reason: EXODIFF\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo ............................................... [min_cpus=2] FAILED (EXODIFF)\n\nThe issue is with srun:\nsrun: error: Unable to allocate resources: Invalid account or account/partition combination specified\nsrun: error: Unable to allocate resources: Invalid account or account/partition combination specified\n\nWhatever this is, it prevented the following command from running:\nmpiexec -n 2 /home/ir-inca1/projects/moose/test/moose_test-opt -i geometric_neighbors_ic.i --error --error-unused --error-override --no-gdb-backtrace\n\nWhich means, there would be no exodiff file generated, which would spawn the error:\nexodiff: ERROR: exodiff: ERROR: Couldn't open file \"/home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/geometric_neighbors_ic_out.e\". File does not exist.\n\nThe same is happening with every error posted (srun: error: Unable to allocate resources: Invalid account or account/partition combination specified). It would seem something to do with the machine you are running on. Allocating resources incorrectly, something like that.",
                          "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-279284",
                          "updatedAt": "2022-08-02T05:56:05Z",
                          "publishedAt": "2021-01-13T14:41:53Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}