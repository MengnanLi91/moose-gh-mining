{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyNS0wMS0yM1QwNzo1MTowMC0wNjowMM4Ad9le"
    },
    "edges": [
      {
        "node": {
          "title": "SuperLU in Electromagnetics module",
          "author": {
            "login": "TheoAM97"
          },
          "bodyText": "I am trying to find an alternative to COMSOL for magnetic flux calculations that will support GPU acceleration. Apologies if this is clarified somewhere but I can't anywhere in the online documentation if this is possible on MOOSE. Does the EM module support SuperLU or any GPU-accelerated solver?",
          "url": "https://github.com/idaholab/moose/discussions/29707",
          "updatedAt": "2025-01-28T19:13:47Z",
          "publishedAt": "2025-01-18T12:09:47Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIf you compile from source using a GPU enabled compiler you can solve an EM problem using petsc on GPUs. However the assembly of the systems will still be done on the CPU\nOur planned MFEM integration will be the way to do this fully on the GPU",
                  "url": "https://github.com/idaholab/moose/discussions/29707#discussioncomment-11876506",
                  "updatedAt": "2025-01-18T17:16:09Z",
                  "publishedAt": "2025-01-18T17:16:08Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "TheoAM97"
                          },
                          "bodyText": "That's good to know, thank you - is MFEM integration planned for the near future? Or a long term goal?",
                          "url": "https://github.com/idaholab/moose/discussions/29707#discussioncomment-11893394",
                          "updatedAt": "2025-01-20T17:13:32Z",
                          "publishedAt": "2025-01-20T17:13:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Near future for being able to use it / develop in it\nBut I ll let @lindsayad comment on the timeline",
                          "url": "https://github.com/idaholab/moose/discussions/29707#discussioncomment-11893691",
                          "updatedAt": "2025-01-20T17:45:30Z",
                          "publishedAt": "2025-01-20T17:45:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "You can follow this pull request: #29633\nI thought that I had heard that GPUs are quite inefficient at triangular solves like in an LU decomposition. But I suppose that even if they are inefficient there might be some gain?",
                          "url": "https://github.com/idaholab/moose/discussions/29707#discussioncomment-11987965",
                          "updatedAt": "2025-01-28T19:13:48Z",
                          "publishedAt": "2025-01-28T19:13:47Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Timoshenko beam element",
          "author": {
            "login": "kiliyan"
          },
          "bodyText": "Is there a special Timoshenko beam element or do I define it like how it is mentioned on https://mooseframework.inl.gov/modules/solid_mechanics/beam_vandv.html\nI did see the https://mooseframework.inl.gov/modules/solid_mechanics/C0TimoshenkoBeam.html but wanted to confirm there isn't an element type = Timoshenko beam or something along those lines.",
          "url": "https://github.com/idaholab/moose/discussions/29756",
          "updatedAt": "2025-01-27T20:48:32Z",
          "publishedAt": "2025-01-27T19:33:34Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "There is not. You ll want to follow the example you found",
                  "url": "https://github.com/idaholab/moose/discussions/29756#discussioncomment-11975614",
                  "updatedAt": "2025-01-27T20:43:00Z",
                  "publishedAt": "2025-01-27T20:42:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Extrusion of element type QUADSHELL4 error",
          "author": {
            "login": "garcs2"
          },
          "bodyText": "Hello,\nI'm currently trying to extrude a mesh of a 2D slice of a core for heat conduction. The mesh was generated in cubit and I'm attempting to extrude it in MOOSE. Below is an image of the mesh in paraview:\n\nAll I need is to extrude the mesh in the z direction, with some slight variation in the extrusion layer heights. To do this I use AdvancedExtruderGenerator and get the following error:\n\nExtrusion is not implemented for element type QUADSHELL4\ufffd[39m```\n\nIs there any way around this? Am I to understand that QUADSHELL4 isn't supported for any extrusions in `MeshGenerator`?",
          "url": "https://github.com/idaholab/moose/discussions/29753",
          "updatedAt": "2025-01-27T15:47:51Z",
          "publishedAt": "2025-01-27T15:42:49Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "in cubit you should specify the element type to be QUAD4 instead of QUADSHELL4",
                  "url": "https://github.com/idaholab/moose/discussions/29753#discussioncomment-11971718",
                  "updatedAt": "2025-01-27T15:47:51Z",
                  "publishedAt": "2025-01-27T15:47:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "granularity of multiapp transfer and update",
          "author": {
            "login": "mcacace"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nDear all,\nI might have a naive question, but  ... I'm working on linking an external library to a moose application, doing that via the multiapp system. My moose app runs as master and the external library as sub app at timestep_end. Coupling is done via transfer from the master to the sub (all is fine) and then back from the sub to the master (and here my question). The sub app passes a source/sink contribution as a auxvariable (monomial) which I then use in the master app to compute a kernel contribution). So my question: given that the transfer is done at the timestep_end, what would be the value of the auxvariable in the master app at the new time step? Is the value I passed from the previous solve? Or, should I \"lag in time\" the auxvariable (taking the old value)? Or, how can I control the syncing between the two?\nThanks for any feedback,\nmauro",
          "url": "https://github.com/idaholab/moose/discussions/29740",
          "updatedAt": "2025-01-27T09:44:59Z",
          "publishedAt": "2025-01-24T09:13:56Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nUse Problem/verbose_multiapps to see when the transfers and multiapps are being executed and that will let you know which field is at the end of the time step and which field is at the beginning.\nIf you use fixed point iterations between the two applications, by setting fixed_point_max_its in the executioner. That can help you do an implicit coupling between the two solves\nLagging is not always a bad idea, but it s usually first order in time. As in the time integration scheme error will be first order of the time step\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/29740#discussioncomment-11946467",
                  "updatedAt": "2025-01-24T18:32:44Z",
                  "publishedAt": "2025-01-24T18:32:43Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "mcacace"
                          },
                          "bodyText": "Thanks @GiudGiud. Will try to follow your suggestion. Sure I'll be back as my coding (slowly) develops further.",
                          "url": "https://github.com/idaholab/moose/discussions/29740#discussioncomment-11966926",
                          "updatedAt": "2025-01-27T09:44:59Z",
                          "publishedAt": "2025-01-27T09:44:58Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Overwriting material property with region average",
          "author": {
            "login": "RPitsinger"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nI have a mesh in which a material property (logG) varies spatially (by element - monomial constant variables - ) over the ROI and is 0 elsewhere. Inside that same mesh, I also have a scale variable, which is 1 inside the ROI and 0 elsewhere. I want to compute the average of logG over the ROI and then overwrite the material property (logG) with this averaged value across the ROI and output this new mesh.\nLike this formula :\n\nAny idea how to implement this in a __.i file?",
          "url": "https://github.com/idaholab/moose/discussions/29750",
          "updatedAt": "2025-01-26T10:59:26Z",
          "publishedAt": "2025-01-25T21:49:35Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "I would set up logG and scale as auxiliary variables using a ParsedAux (setting the 0 outside the ROI can be done here btw, as long as the definition of the ROI can be done in a if statement)\nThen integrate them separately using an ElementIntegralVariablePostprocessor.\nIf either of these are already available as material properties, you can skip the ParsedAux step and use an ElementIntegralMaterialPostprocessor.\nThen use a ParsedPostprocessor to perform the quotient between the two integrals",
                  "url": "https://github.com/idaholab/moose/discussions/29750#discussioncomment-11955135",
                  "updatedAt": "2025-01-26T00:07:05Z",
                  "publishedAt": "2025-01-26T00:07:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "RPitsinger"
                          },
                          "bodyText": "Yup, that worked, thanks.\nI read the variable from the mesh with UserObject - SolutionUserObject\nRead that into an AuxKernel - SolutionAux.\nPerformed integral on the (Aux) variable with Postprocessor - ElementIntegralVariablePostprocessor\nDid the quotient with Postprocessor - ParsedPostprocessor.\nOverwrote the variable with ParsedAux - AuxKernel, where I call the quotient value with functor_names.",
                          "url": "https://github.com/idaholab/moose/discussions/29750#discussioncomment-11957403",
                          "updatedAt": "2025-01-26T10:59:23Z",
                          "publishedAt": "2025-01-26T10:59:22Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "non converging phase field fracture test",
          "author": {
            "login": "amassaf"
          },
          "bodyText": "Hi I have a phase field fracture test tension-compression on a holed square plate (mesh attached) but it is not converging properly. Could it be a problem with the executioner ?\n02mesh.inp.txt\n[Mesh]\n    [file]\n    type = FileMeshGenerator\n    file = 02mesh.inp\n  []\n  [cornernode]\n    type = ExtraNodesetGenerator\n    new_boundary = BL\n    coord = '0 0 0'\n    input = file\n  []\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Variables]\n  [./disp_x]\n  [../]\n  [./disp_y]\n  [../]\n  [./c]\n  [../]\n[]\n[AuxKernels]\n[./stress_xx]\n    type = RankTwoAux\n    variable = stress_xx\n    rank_two_tensor = stress\n    index_j = 0\n    index_i = 0\n    execute_on = timestep_end\n  [../]\n[./stress_xy]\n    type = RankTwoAux\n    variable = stress_xy\n    rank_two_tensor = stress\n    index_j = 0\n    index_i = 1\n    execute_on = timestep_end\n  [../]\n\n [./Von_Mises_stress]\n    type = RankTwoScalarAux\n    variable = Von_Mises_stress\n    rank_two_tensor = stress\n    scalar_type = VonMisesStress\n  [../]\n [./total_strain_yy]\n    type = RankTwoAux\n    rank_two_tensor = total_strain\n    variable = total_strain_yy\n    index_i = 1\n    index_j = 1\n  [../]\n[./total_strain_xx]\n    type = RankTwoAux\n    rank_two_tensor = total_strain\n    variable = total_strain_xx\n    index_i = 0\n    index_j = 0\n  [../]\n[./total_strain_xy]\n    type = RankTwoAux\n    rank_two_tensor = total_strain\n    variable = total_strain_xy\n    index_i = 0\n    index_j = 1\n  [../]\n[]\n\n[AuxVariables]\n  [./resid_x]\n  [../]\n  [./resid_y]\n  [../]\n  [./bounds_dummy]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n[./stress_xx]\n    order = CONSTANT\n    family = MONOMIAL\n  [../]\n[./stress_xy]\n    order = CONSTANT\n    family = MONOMIAL\n  [../]\n [./Von_Mises_stress]\n    family = MONOMIAL\n    order = CONSTANT\n  [../]\n [./total_strain_yy]\n    order = CONSTANT\n    family = MONOMIAL\n  [../]\n  [./total_strain_xx]\n    order = CONSTANT\n    family = MONOMIAL\n  [../]\n[./total_strain_xy]\n    order = CONSTANT\n    family = MONOMIAL\n  [../]\n\n[]\n\n\n[Modules]\n  [./TensorMechanics]\n    [./Master]\n      [./All]\n        add_variables = true\n        strain = SMALL\n        additional_generate_output = 'strain_yy stress_yy'\n        planar_formulation = PLANE_STRAIN\n        save_in = 'resid_x resid_y'\n      [../]\n    [../]\n  [../]\n[]\n\n[Kernels]\n  [./solid_x]\n    type = PhaseFieldFractureMechanicsOffDiag\n    variable = disp_x\n    component = 0\n    c = c\n  [../]\n  [./solid_y]\n    type = PhaseFieldFractureMechanicsOffDiag\n    variable = disp_y\n    component = 1\n    c = c\n  [../]\n    [./off_disp]\n    type = AllenCahnElasticEnergyOffDiag\n    variable = c \n    displacements = 'disp_x disp_y'\n    mob_name = L\n  [../]\n   [./ACbulk]\n    type = AllenCahn\n    variable = c\n    f_name = F\n  [../]\n  [./ACInterfaceCleavageFracture]\n    type = ACInterface\n    variable = c\n  [../]\n\n[]\n\n[BCs]\n  [./ydisp]\n    type = FunctionDirichletBC\n    variable = disp_y\n    boundary = top\n    function = 'fux'\n  [../]\n  [./yfix]\n    type = DirichletBC\n    variable = disp_y \n    boundary = bottom\n    value = 0 \n  [../]\n  [./xfix]\n    type = DirichletBC\n    variable = disp_x\n    boundary = BL\n    value = 0\n  [../]\n[]\n\n[Functions]\n [./fux]\n   type = PiecewiseLinear\n   x =  '0.0  1.0e-2  2.0e-2   3e-2'\n   y =  '0.0  0.010   0.0      -0.010'\n[]\n[]\n\n[Materials]\n  [./pfbulkmat]\n    type = GenericConstantMaterial\n    prop_names = 'gc_prop l visco'\n    prop_values = '2.7 0.03  1e-5'\n  [../]\n  [./elasticity_tensor]\n    type = ComputeElasticityTensor\n    C_ijkl = '120.0e3 80.e3'\n    fill_method = symmetric_isotropic\n  [../]\n  [./damage_stress]\n    type = ComputeLinearElasticPFFractureStress\n    c = c\n    E_name = 'elastic_energy'\n    D_name = 'degradation'\n    F_name = 'local_fracture_energy'\n    decomposition_type = stress_spectral \n    use_snes_vi_solver = true\n  [../]\n  [./degradation]\n    type = DerivativeParsedMaterial\n    f_name = degradation\n    args = 'c'\n    function = '(1.0-c)^2*(1.0 - eta) + eta'\n    constant_names       = 'eta'\n    constant_expressions =  '1.0e-6'\n    derivative_order = 2\n  [../]\n  [./define_mobility]\n    type = ParsedMaterial\n    material_property_names = 'gc_prop visco'\n    f_name = L\n    function = '1.0/(gc_prop * visco)'\n  [../]\n  [./define_kappa]\n    type = ParsedMaterial\n    material_property_names = 'gc_prop l'\n    f_name = kappa_op\n    function = 'gc_prop * l * 3 / 4' #gc_prop * l'\n  [../]\n  [./local_fracture_energy]\n    type = DerivativeParsedMaterial\n    f_name = local_fracture_energy\n    args = 'c'\n    material_property_names = 'gc_prop l'\n    function = '3 * gc_prop / (8 * l) * c' \n    derivative_order = 2\n  [../]\n  [./fracture_driving_energy]\n    type = DerivativeSumMaterial\n    args = c\n    sum_materials = 'elastic_energy local_fracture_energy'\n    derivative_order = 2\n    f_name = F\n  [../]\n[]\n\n[Postprocessors]\n [./top_x]\n    type = NodalExtremeValue\n    variable = disp_x\n    boundary = top\n    value_type = max\n  [../]\n\n[./top_y]\n    type = NodalExtremeValue\n    variable = disp_y\n    boundary = top\n    value_type = max\n  [../]\n\n  [./resid_x]\n    type = NodalSum\n    variable = resid_x\n    boundary = bottom\n  [../]\n  [./resid_y]\n    type = NodalSum\n    variable = resid_y\n    boundary = bottom\n  [../]\n[]\n\n\n[Bounds]\n  [./c_upper_bound]\n    type = ConstantBoundsAux\n    variable = bounds_dummy\n    bounded_variable = c\n    bound_type = upper\n    bound_value = 1.0\n  [../]\n  [./c_lower_bound]\n    type = VariableOldValueBoundsAux\n    variable = bounds_dummy\n    bounded_variable = c\n    bound_type = lower\n  [../]\n[]\n\n\n[Preconditioning]\n  [./smp]\n    type = SMP\n    full = true\n  [../]\n[]\n\n[Executioner]\n  type = Transient\n  #solve_type = newton #PJFNK\n  #scheme=BDF2\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package -snes_type'\n  petsc_options_value = 'lu       mumps                  vinewtonrsls'\n  nl_rel_tol = 1e-6\n  nl_abs_tol = 1e-10\n  dt = 5e-5\n  end_time = 0.2\n  automatic_scaling = true\n[]\n\n\n\n[Outputs]\n  exodus = true\n  csv = true\n[]",
          "url": "https://github.com/idaholab/moose/discussions/29717",
          "updatedAt": "2025-01-25T08:59:04Z",
          "publishedAt": "2025-01-21T08:31:07Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "selarem"
                  },
                  "bodyText": "Very strange ! The strain_spectral decomposition generally works fine in tension and in compression. Here it does not converge even in tension.",
                  "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11916678",
                  "updatedAt": "2025-01-22T13:25:20Z",
                  "publishedAt": "2025-01-22T13:25:19Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "selarem"
                  },
                  "bodyText": "@jiangwen84\ndo you have any idea, on why this have convergence issues ?\nregards,",
                  "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11940739",
                  "updatedAt": "2025-01-24T09:16:42Z",
                  "publishedAt": "2025-01-24T09:16:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "hello\n\nbut it is not converging properly.\n\ncan you please attach the log?\nwhich techniques have you tried here?\nhttps://mooseframework.inl.gov/application_usage/failed_solves.html",
                  "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11944487",
                  "updatedAt": "2025-01-24T15:16:39Z",
                  "publishedAt": "2025-01-24T15:16:38Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "amassaf"
                          },
                          "bodyText": "Hello\nit seems to be due to the nonlinear solve fail. I've tried using the line_search = 'none' and setting a nl_abs_tol=1e-10 and the nl_rel_tol=1e-6, set automatic_scale = true and I've also tried the solve_type = Newton. I've also tried having a finer mesh and changing the phase field problem parameters like gc or l but it seems that the problem with the non-linear convergence emerges when the time is around 0.1 (when there is a tension to compression transition but it sometimes also fails before then - by fail i mean the solve doesn't converge and time step is reduced to reach dt_min) I didn't have the problem with the other non spectral decomposition methods in the ComputeLinearElasticPFFractureStress.\nthe log : moose_trial.txt",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11945830",
                          "updatedAt": "2025-01-24T17:22:05Z",
                          "publishedAt": "2025-01-24T17:22:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Why is the c variable residual never going down?\nDid you set super loose tolerances that allow convergence with any value of that residual?",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11946655",
                          "updatedAt": "2025-01-24T18:51:56Z",
                          "publishedAt": "2025-01-24T18:51:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "amassaf"
                          },
                          "bodyText": "The units are mm MPa s so I think the nl_abs_tol=1e-10 and the nl_rel_tol=1e-6 are fine, or is there other tolerances for the phase field problem specifically ?",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11946679",
                          "updatedAt": "2025-01-24T18:54:52Z",
                          "publishedAt": "2025-01-24T18:54:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This does not explain the residual staying above 1.\nIf it s not the tolerances maybe it s the number of iterations.\nCan you set nl_forced_its to 10 and see if the residual goes down for c ?",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11946700",
                          "updatedAt": "2025-01-24T18:57:19Z",
                          "publishedAt": "2025-01-24T18:57:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "amassaf"
                          },
                          "bodyText": "I added the nl_forced_its = 10, but nothing has changed, c residual is still at 1, goes down to 0.8 for some iterations :\nmoose_trial.txt",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11946848",
                          "updatedAt": "2025-01-24T19:16:27Z",
                          "publishedAt": "2025-01-24T19:16:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "this isn't normal.\nTry the \"Ill-conditioned or ill-posed problem\" instructions on that page\nhttps://mooseframework.inl.gov/application_usage/failed_solves.html\nI think you ll find the problem is ill-posed in some capacity\nEither that or something with bounds is not showing us the true residual. In that case we should try adding -ksp_monitor_true_residual on the command line or in petsc options",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11947470",
                          "updatedAt": "2025-01-24T20:47:48Z",
                          "publishedAt": "2025-01-24T20:47:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "amassaf"
                          },
                          "bodyText": "The -ksp_monitor_true_residual gives this log:\nmoose_trial.txt",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11947666",
                          "updatedAt": "2025-01-24T21:15:10Z",
                          "publishedAt": "2025-01-24T21:15:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "amassaf"
                          },
                          "bodyText": "and using this executioner:\n[Executioner]\n  type = Transient\n  #solve_type = newton #PJFNK\n  #scheme=BDF2\n  petsc_options = '-pc_svd_monitor '\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package -snes_type'\n  petsc_options_value = 'svd       mumps                  vinewtonrsls'\n\n#petsc_options = '-ksp_monitor_true_residual'\n\n#petsc_options_iname = '-pc_type -snes_linesearch_damping -ksp_gmres_restart -pc_factor_mat_solver_package -snes_type'\n#petsc_options_value = 'lu  0.5 1000    mumps                  vinewtonrsls'\n\n # line_search = 'basic'\n line_search = none\n\n  nl_rel_tol = 1e-6\n  #nl_abs_tol = 1e-10\n  dt = 5e-4\n  end_time = 0.2\n  automatic_scaling = true\n  nl_forced_its = 20\n[]\n\ngives me this:\n  Executioner:             Transient\n  TimeStepper:             ConstantDT\n  TimeIntegrator:          ImplicitEuler\n  Solver Mode:             Preconditioned JFNK\n  PETSc Preconditioner:    svd \n  MOOSE Preconditioner:    SMP\n\n\nTime Step 0, time = 0\n\nPostprocessor Values:\n+----------------+----------------+----------------+----------------+----------------+\n| time           | resid_x        | resid_y        | top_x          | top_y          |\n+----------------+----------------+----------------+----------------+----------------+\n|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n+----------------+----------------+----------------+----------------+----------------+\n\n\nTime Step 1, time = 0.0005, dt = 0.0005\n\nPerforming automatic scaling calculation\n\n    |residual|_2 of individual variables:\n                     disp_x: 0.000175511\n                     disp_y: 0.00248219\n                     c:      1.17445\n 0 Nonlinear |R| = 2.488389e-03\n\n===================================================================================\n=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES\n=   PID 2707847 RUNNING AT \n=   EXIT CODE: 9\n=   CLEANING UP REMAINING PROCESSES\n=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES\n===================================================================================\nYOUR APPLICATION TERMINATED WITH THE EXIT STRING: Killed (signal 9)\nThis typically refers to a problem with your application.\nPlease see the FAQ page for debugging suggestions",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11947710",
                          "updatedAt": "2025-01-24T21:22:02Z",
                          "publishedAt": "2025-01-24T21:22:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "YOUR APPLICATION TERMINATED WITH THE EXIT STRING: Killed (signal 9)\n\nyou are runnig out of memory. make the mesh a lot smaller to run SVD, down to 1-5k elements max",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11948192",
                          "updatedAt": "2025-01-24T22:23:17Z",
                          "publishedAt": "2025-01-24T22:23:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "amassaf"
                          },
                          "bodyText": "Ok, thanks it worked, I got this:\nmoose_trial.txt\nTime Step 1, time = 0.0005, dt = 0.0005\n\nPerforming automatic scaling calculation\n\n    |residual|_2 of individual variables:\n                     disp_x: 9.61617e-05\n                     disp_y: 0.000970133\n                     c:      5.62412\n 0 Nonlinear |R| = 9.748876e-04\n      SVD: condition number 2.002505168263e+04, 0 of 990 singular values are (nearly) zero\n      SVD: smallest singular values: 9.121985765161e-05 7.583532795909e-04 3.134453905996e-03 4.096421736808e-03 5.132670171111e-03\n      SVD: largest singular values : 1.768595240078e+00 1.773327750385e+00 1.790825275654e+00 1.810794194333e+00 1.826682363956e+00\n      0 Linear |R| = 9.748876e-04\n      1 Linear |R| = 2.344575e-17\n    |residual|_2 of individual variables:\n                     disp_x: 1.66047e-17\n                     disp_y: 1.67193e-17\n                     c:      5.62412\n 1 Nonlinear |R| = 2.356378e-17\n  Finished Solving                                                                       [ 11.80 s] [  192 MB]\n Solve Converged!\n\nOutlier Variable Residual Norms:\n  disp_x: 1.660470e-17\n  disp_y: 1.671932e-17\n  c: 5.624117e+00\n\nPostprocessor Values:\n+----------------+----------------+----------------+----------------+----------------+\n| time           | resid_x        | resid_y        | top_x          | top_y          |\n+----------------+----------------+----------------+----------------+----------------+\n|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n|   5.000000e-04 |  -7.810948e-13 |  -8.176373e+01 |  -2.311164e-07 |   5.000000e-04 |\n+----------------+----------------+----------------+----------------+----------------+\n\n\nTime Step 2, time = 0.001, dt = 0.0005\n    |residual|_2 of individual variables:\n                     disp_x: 9.61617e-05\n                     disp_y: 0.000970133\n                     c:      5.61655\n 0 Nonlinear |R| = 9.748876e-04",
                          "url": "https://github.com/idaholab/moose/discussions/29717#discussioncomment-11948416",
                          "updatedAt": "2025-01-24T22:59:58Z",
                          "publishedAt": "2025-01-24T22:59:57Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Debuging UMAT",
          "author": {
            "login": "M16R24"
          },
          "bodyText": "A custom UMAT is used to get the material response under compressive loading with symmetric boundary conditions.  The issue I am facing is that after a certain amount of strain is reached, the simulation diverges in terms of \"dt\" and stops. I have tried debugging using gdb and rectified certain things. But I also want to check the Jacobian as well. So, how can I debug the Jacobian matrix that goes back into MOOSE?",
          "url": "https://github.com/idaholab/moose/discussions/29725",
          "updatedAt": "2025-01-25T05:31:44Z",
          "publishedAt": "2025-01-22T09:51:00Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nSee the troubleshooting failed solves instructions on the website\nThe Jacobian tester will tell you which coefficient is wrong then the dofmap output will tell you to which variables it is tied",
                  "url": "https://github.com/idaholab/moose/discussions/29725#discussioncomment-11949798",
                  "updatedAt": "2025-01-25T05:31:45Z",
                  "publishedAt": "2025-01-25T05:31:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MultiApp transfer variable to initial condition of subapp",
          "author": {
            "login": "garcs2"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nHello everyone,\nI am working on making a neutronics-heat transfer-thermohydraulics coupled simulation, using Griffin as the neutronics solver and heat transfer-thermohydraulics (HT-TH) being solved with the base modules in MOOSE. I have Griffin and HT-TH running successfuly separately. The HT-TH is solved using TransientMultiApp where HT acts as the main app and TH is the sub-app. Ideally, the workflow would look like the following:\n\nFor clarity, the FullSolveMultiApp executes on timestep_end so that Griffin solves first, then transfers the power density to HT-TH where HT solves first. My question is then: how do I designate the initial condition of HT to be the transferred power density? Since the power density is a scalar, I would like to use ConstantIC but a value is required as input. Below is the relevant portion of my input file, with the IC block not filled in since I don't know what to do:\n[Variables]\n  [T]\n  []\n[]\n[AuxVariables]\n  [fluid_temp]\n  initial_condition = '${inlet_T}'\n  []\n  [flux]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [power_density]\n    family = L2_LAGRANGE\n    order = First\n    block = ${fuel_blocks}\n  []\n[]\n[Kernels]\n  [diffusion]\n    type = HeatConduction\n    variable = T\n  []\n  [source]\n    type = CoupledForce\n    variable = T\n    v = power_density\n    block = ${fuel_blocks}\n  []\n[]\n\n[BCs]\n  [pin_outer]\n    type = MatchedValueBC\n    variable = T\n    v = fluid_temp\n    boundary = 'fluid_solid_interface'\n  []\n[]\n\n[ICs]\n  [power_density]\n    type = ConstantIC\n    variable = power_density\n    value = ???\n    block = ${fuel_blocks}\n  []\n[]",
          "url": "https://github.com/idaholab/moose/discussions/29676",
          "updatedAt": "2025-01-24T06:34:45Z",
          "publishedAt": "2025-01-10T20:37:53Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "hello\nInstead of using a ConstantIC, if you want a value from another application, you should transfer it.\nTransfers can be executed on INITIAL.\nbut here you should be good already?\n\nthe FullSolveMultiApp executes on timestep_end so that Griffin solves first, then transfers the power density to HT-TH where HT solves first\n\nsince Griffin executes first and sends its power field to the HT solve, the HT solve has the updated power density already for its first solve?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11802418",
                  "updatedAt": "2025-01-10T20:49:26Z",
                  "publishedAt": "2025-01-10T20:49:26Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "garcs2"
                          },
                          "bodyText": "Hi Guillaume\nI see so what you're saying is that because I already transfer power density from Griffin, then there is no need to designate an IC explicitly. I was under the impression that I had to tell MOOSE explicitly where the IC was coming from if that makes sense. This helps and now the job is at least running, I just deleted the ICs block. I will ask more follow-up questions on this thread if any come up.",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11803093",
                          "updatedAt": "2025-01-10T23:26:55Z",
                          "publishedAt": "2025-01-10T22:10:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "garcs2"
                          },
                          "bodyText": "Hi Guillaume,\nThe input file is now running and I get a complete run of Griffin-HT-TH. However, I'm finding that the power_density computed via Griffin is not being transferred correctly into HT. My solution in Griffin completes and then transfers to HT. I realized that since Griffin technically computes a time step by setting up the transfer I have to have the multi app execute on FINAL. After the Griffin solution is computed I find that I'm getting this as part of the output as HT-TH step forward in time:\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m| time           | flux_integral  | htm_Tfuel      | htm_Tref       | max_fuel_T     | max_power      | min_power      | power_source   |\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0: \ufffd[39m|   5.000000e-02 |  -8.876084e-05 |   8.660000e+02 |   8.660000e+02 |   8.660000e+02 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\nIs there something that I'm doing incorrectly that comes to mind for you? For reference this is my multiapp and transfer blocks in my Griffin input:\n[MultiApps]\n    [Griffin_htm]\n    type = FullSolveMultiApp\n    #app_type = GriffinApp\n    input_files = 'SNAP_solid_test3_1.i'\n    positions = '0 0 0'\n    execute_on = 'FINAL'\n    []\n[]\n\n[Transfers]\n    [to_htm_power_density]\n        type = MultiAppProjectionTransfer\n        to_multi_app = Griffin_htm\n        source_variable = griffin_power_density\n        variable = power_density\n    []\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11848082",
                          "updatedAt": "2025-01-15T21:46:21Z",
                          "publishedAt": "2025-01-15T21:46:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "execute_on = 'FINAL'\n\n\nthis seems weird. Griffin should not be using time steps as power iterations. Maybe you are using an old executioner in Griffin?\nWhich executioner / transport systems are you using in Griffin?",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11849278",
                          "updatedAt": "2025-01-16T01:16:47Z",
                          "publishedAt": "2025-01-16T01:16:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "garcs2"
                          },
                          "bodyText": "For the executioner I'm using SweepUpdate and for the transport system it's an eigenvalue problem where I'm using DFEM-SN. I can also copy my transport system and executioner blocks here, I wasn't sure if I'm able to since Griffin is export-controlled.",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11855184",
                          "updatedAt": "2025-01-16T13:49:42Z",
                          "publishedAt": "2025-01-16T13:49:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Do you have a contact in the Griffin team?\nWhich institution are you affiliated with?\nYes we are fairly limited in what we can say about Griffin code here.",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11855200",
                          "updatedAt": "2025-01-16T13:51:18Z",
                          "publishedAt": "2025-01-16T13:51:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "garcs2"
                          },
                          "bodyText": "I do not have contact with the Griffin team, though I can post to their discussion board. I thought this was more of a transfers issue not specific to Griffin which is why I made the post here. I am affiliated with UW-Madison.",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11856125",
                          "updatedAt": "2025-01-16T15:14:51Z",
                          "publishedAt": "2025-01-16T15:14:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Alright so try:\n\ndont use execute_on FINAL, use TIMESTEP_END\nplay with the richardson iterations parameters to get the multiapp executed inside the power iteration loop",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11857787",
                          "updatedAt": "2025-01-16T17:51:07Z",
                          "publishedAt": "2025-01-16T17:51:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "garcs2"
                          },
                          "bodyText": "Okay so I've done what you suggested, and I've made the Griffin execution verbose to track the solver. This is part of the output where you see that the power_source in HT is not updated but that the multiapp is being executed within the power iteration loop:\nTransfers on TIMESTEP_END To MultiApps\ufffd[39m:\n----------------------------------------------------------------------------\n|         Name         |            Type            |  From  |     To      |\n----------------------------------------------------------------------------\n| to_htm_power_density | MultiAppProjectionTransfer | Parent | Griffin_htm |\n----------------------------------------------------------------------------\n  Currently Executing Transfers\n    Transferring variables through projection                                            [\ufffd[33m 11.38 s\ufffd[39m] [\ufffd[33m12424 MB\ufffd[39m]\n\ufffd[36mTransfers on TIMESTEP_END Are Finished\n\ufffd[39m\n\ufffd[36m\nNo Transfers on TIMESTEP_END Between MultiApps\n\ufffd[39m\n\ufffd[36m\nExecuting MultiApps on TIMESTEP_END\ufffd[39m\n  Finished Executing Transfers                                                           [\ufffd[33m 11.39 s\ufffd[39m] [\ufffd[33m12424 MB\ufffd[39m]\nWarning, Exodus files cannot have titles longer than 80 characters.  Your title will be truncated.\n\ufffd[36mGriffin_htm0: \ufffd[39mOutputting\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m  Outputting Step                                                          [\ufffd[33m  9.04 s\ufffd[39m] [\ufffd[33m12424 MB\ufffd[39m]\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mTime Step 0, time = 0\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mPostprocessor Values:\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m| time           | flux_integral  | htm_Tfuel      | htm_Tref       | max_fuel_T     | max_power      | min_power      | power_source   |\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mFinished Outputting                                                        [\ufffd[33m  9.05 s\ufffd[39m] [\ufffd[33m12424 MB\ufffd[39m]\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mTime Step 1, time = 0.05\n\ufffd[36mGriffin_htm0: \ufffd[39m         old time = 0\n\ufffd[36mGriffin_htm0: \ufffd[39m               dt = 0.05\n\ufffd[36mGriffin_htm0: \ufffd[39m           old dt = 0.05\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m 0 Nonlinear |R| = \ufffd[32m4.001707e+05\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m 1 Nonlinear |R| = \ufffd[32m2.515758e+03\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m 2 Nonlinear |R| = \ufffd[32m1.600207e-01\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m 3 Nonlinear |R| = \ufffd[32m2.271778e-06\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m\ufffd[32m Solve Converged!\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mFinished Solving                                                           [\ufffd[33m 56.94 s\ufffd[39m] [\ufffd[33m12424 MB\ufffd[39m]\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39mTime Step 1, time = 0.05\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m         old time = 0\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m               dt = 0.05\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m           old dt = 0.05\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m 0 Nonlinear |R| = \ufffd[32m4.064714e+05\ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m 1 Nonlinear |R| = \ufffd[32m4.700860e+01\ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m 2 Nonlinear |R| = \ufffd[32m1.092626e-03\ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\ufffd[32m Solve Converged!\ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39mPostprocessor Values:\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m| time           | Nu_avg         | Re_avg         | T_avg          | T_wall_avg     | coolant_T_in   | coolant_T_out  |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   8.660000e+02 |   0.000000e+00 |   8.660000e+02 |   8.660000e+02 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   5.000000e-02 |   3.263650e+00 |   7.514410e+03 |   8.660000e+02 |   8.660000e+02 |   8.660000e+02 |   8.660000e+02 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m| time           | coolant_p_in   | coolant_p_out  | f_avg          | htc_avg        | q_avg          | vel_avg        |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   0.000000e+00 |   2.537271e+05 |   2.537271e+05 |   1.940000e-02 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   5.000000e-02 |   2.561549e+05 |   2.537894e+05 |   1.940000e-02 |   2.914777e+04 |  -1.354264e-05 |   5.203557e-01 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mPostprocessor Values:\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m| time           | flux_integral  | htm_Tfuel      | htm_Tref       | max_fuel_T     | max_power      | min_power      | power_source   |\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0: \ufffd[39m|   5.000000e-02 |  -8.876084e-05 |   8.660000e+02 |   8.660000e+02 |   8.660000e+02 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mSteady-State Relative Differential Norm: 20\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mTime Step 2, time = 0.1\n\ufffd[36mGriffin_htm0: \ufffd[39m         old time = 0.05\n\ufffd[36mGriffin_htm0: \ufffd[39m               dt = 0.05\n\ufffd[36mGriffin_htm0: \ufffd[39m           old dt = 0.05\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m 0 Nonlinear |R| = \ufffd[32m1.380532e-02\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m 1 Nonlinear |R| = \ufffd[32m1.249788e-06\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m 2 Nonlinear |R| = \ufffd[32m4.534208e-10\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m\ufffd[32m Solve Converged!\ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mFinished Solving                                                           [\ufffd[33m 31.89 s\ufffd[39m] [\ufffd[33m12432 MB\ufffd[39m]\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39mTime Step 2, time = 0.1\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m         old time = 0.05\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m               dt = 0.05\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m           old dt = 0.05\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m 0 Nonlinear |R| = \ufffd[32m9.826201e+00\ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m 1 Nonlinear |R| = \ufffd[32m8.550787e-06\ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\ufffd[32m Solve Converged!\ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39mPostprocessor Values:\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m| time           | Nu_avg         | Re_avg         | T_avg          | T_wall_avg     | coolant_T_in   | coolant_T_out  |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   8.660000e+02 |   0.000000e+00 |   8.660000e+02 |   8.660000e+02 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   5.000000e-02 |   3.263650e+00 |   7.514410e+03 |   8.660000e+02 |   8.660000e+02 |   8.660000e+02 |   8.660000e+02 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   1.000000e-01 |   3.263650e+00 |   7.514410e+03 |   8.660001e+02 |   8.660001e+02 |   8.660000e+02 |   8.660001e+02 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m| time           | coolant_p_in   | coolant_p_out  | f_avg          | htc_avg        | q_avg          | vel_avg        |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   0.000000e+00 |   2.537271e+05 |   2.537271e+05 |   1.940000e-02 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   5.000000e-02 |   2.561549e+05 |   2.537894e+05 |   1.940000e-02 |   2.914777e+04 |  -1.354264e-05 |   5.203557e-01 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m|   1.000000e-01 |   2.566503e+05 |   2.538021e+05 |   1.940000e-02 |   2.914777e+04 |  -1.819328e-04 |   5.203580e-01 |\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0_thm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mPostprocessor Values:\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m| time           | flux_integral  | htm_Tfuel      | htm_Tref       | max_fuel_T     | max_power      | min_power      | power_source   |\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0: \ufffd[39m|   5.000000e-02 |  -8.876084e-05 |   8.660000e+02 |   8.660000e+02 |   8.660000e+02 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0: \ufffd[39m|   1.000000e-01 |  -1.291114e-09 |   8.660000e+02 |   8.660000e+02 |   8.660000e+02 |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n\ufffd[36mGriffin_htm0: \ufffd[39m+----------------+----------------+----------------+----------------+----------------+----------------+----------------+----------------+\n\ufffd[36mGriffin_htm0: \ufffd[39m\n\ufffd[36mGriffin_htm0: \ufffd[39mSteady-State Solution Achieved at time: 0.1\n\ufffd[32mSubapp Griffin_htm0 solve converged!\ufffd[39m\n    Finished Executing MultiApps                                                         [\ufffd[33m109.79 s\ufffd[39m] [\ufffd[33m12432 MB\ufffd[39m]\n\ufffd[36mFinished Executing MultiApps on TIMESTEP_END\n\ufffd[39m\n  Finished Executing MultiApps                                                           [\ufffd[33m109.79 s\ufffd[39m] [\ufffd[33m12432 MB\ufffd[39m]\n\ufffd[36m\nTransfers on TIMESTEP_END From MultiApps\ufffd[39m:\n----------------------------------------------------------------------\n|      Name      |            Type            |    From     |   To   |\n----------------------------------------------------------------------\n| from_htm_Tfuel | MultiAppUserObjectTransfer | Griffin_htm | Parent |\n----------------------------------------------------------------------\n\ufffd[36mTransfers on TIMESTEP_END Are Finished\n\ufffd[39m\n  Performing Richardson Iteration\n\nI cut the output before and after it goes back to the power iteration in Griffin. In summary:\n\ntransfer occurs on TIMESTEP_END\nthe multiapp is executed inside the power iteration loop\nthe transfer seems to be happening within MOOSE but the value itself isn't properly transferred",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11868491",
                          "updatedAt": "2025-01-17T15:37:30Z",
                          "publishedAt": "2025-01-17T15:34:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so transfers is not often enough?\nYou can try MULTIAPP_FIXED_POINT_END instead then",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11868525",
                          "updatedAt": "2025-01-17T15:37:24Z",
                          "publishedAt": "2025-01-17T15:37:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "garcs2"
                          },
                          "bodyText": "I tried MULTIAPP_FIXED_POINT_END and I'm still not getting the transfer to pass on properly, I also tried different configurations between TIMESTEP_END and MULTIAPP_FIXED_POINT_END between the main app and sub app and haven't gotten the transfers to function the way that I would like to. Given this issue I'm considering having the scheme shift in the other direction, where HT-TH solve first (with  the power density set in HT) and then pass that information onto Griffin. I figure this would work as when I explicitly provide an IC HT-TH solve just fine, but I'm still curious why the transfer isn't solving the way that I was originally intending.",
                          "url": "https://github.com/idaholab/moose/discussions/29676#discussioncomment-11870851",
                          "updatedAt": "2025-01-17T19:37:08Z",
                          "publishedAt": "2025-01-17T19:32:43Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Parallel debugging on MacOS Sequoia",
          "author": {
            "login": "gambka"
          },
          "bodyText": "I need to debug a simulation that only hits a segfault when using a split-mesh. Once split, you have to run on the same number of processes as splits of the mesh. I follow the instructions here for parallel debugging and I'm running into issues with this command:\nmpiexec -n 4 ./yourapp-dbg -i inputfile.i --start-in-debugger='sudo lldb'\n\nAfter changing to myapp-dbg and the input file name of interest. I receive\nsh: xterm: command not found\nsh: xterm: command not found\nsh: xterm: command not found\nsh: xterm: command not found\n\nand the simulation just continues in the terminal as a normal run of the dbg executable.\nI think this is because --start-in-debugger uses xterm, however, on Sequoia if you do an echo $TERM it reports xterm-256color. Is there a way to  use --start-in-debugger with xterm-256color instead of xterm?",
          "url": "https://github.com/idaholab/moose/discussions/29730",
          "updatedAt": "2025-01-23T20:33:01Z",
          "publishedAt": "2025-01-22T18:53:50Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Not that I know. Did you try aliasing xterm to xterm-256color?",
                  "url": "https://github.com/idaholab/moose/discussions/29730#discussioncomment-11920592",
                  "updatedAt": "2025-01-22T19:03:43Z",
                  "publishedAt": "2025-01-22T19:03:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "gambka"
                          },
                          "bodyText": "That didn't work.",
                          "url": "https://github.com/idaholab/moose/discussions/29730#discussioncomment-11920947",
                          "updatedAt": "2025-01-22T19:39:08Z",
                          "publishedAt": "2025-01-22T19:39:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "gambka"
                          },
                          "bodyText": "The issue here was due to an issue with the internally provided XQuartz installation upon upgrading from Sonoma to Sequoia as well as xterm not being properly added to the PATH. This can be closed.",
                          "url": "https://github.com/idaholab/moose/discussions/29730#discussioncomment-11932063",
                          "updatedAt": "2025-01-23T16:08:28Z",
                          "publishedAt": "2025-01-23T16:08:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Initiation Issue",
          "author": {
            "login": "julianseb"
          },
          "bodyText": "This is in continuation of #29644. So maybe @cpgr or @lynnmunday could help me out again. Any other help is also very appreciated.\nAfter the other model worked out fine, i tried to implement the open water body i was talking about in the previous discussion. I set up another PorousFlowDictator (dictator_wasser) with only one component and one phase, because i see the open water body as fully saturated. I set the porosity to 0.9999.\nI set up some OutflowBCs for temperature and the porepressure at the bottom of the water body (boundary: grund) in each direction with the complimentary dictators. I set the porepressure at the water surface (boundary: wasserspiegel) the same as the gas pressure in the ground at atmospheric pressure (101325 Pa).\nI've managed to get a 1-phase model of it running, but there i only used one PorousFlowDictator.\nBut when i try to run the inputfile it initializes, tabulates the fluidproperties for the gas phase but then shows this error:\nExecution Information:\n  Executioner:             Transient\n  TimeStepper:             IterationAdaptiveDT\n  TimeIntegrator:          ImplicitEuler\n  Solver Mode:             NEWTON\n  PETSc Preconditioner:    asm \n  MOOSE Preconditioner:    SMP\n\ndampf_tabulated: Generating (p, T) tabulated data\n\n\n*** ERROR ***\nThe following error occurred in the Problem 'MOOSE Problem' of type FEProblem.\n\nThe following parallel-communicated exception was detected during INITIAL evaluation:\nPressure 0 is outside the range of tabulated pressure (12523, 3e+06).\nBecause this did not occur during residual evaluation, there is no way to handle this, so the solution is aborting.\n\nSo there must be a problem with the gas pressure, because the gas phase is the one with the tabulated properties. But i didn't do any changes to that in comparrison to the aformentioned case that was running.\nDoes anybody have an idea why the pressure apperently drops to 0 Pa? Is this because of a shared boundary between the dictators?\nThis is my mesh:\n\n\nHere are my inputfile and the mesh:\nCurrent_state.zip\nI also added my inputfile here:\n[Mesh]\n  [mesh]\n    type = FileMeshGenerator\n    file = Gewaesser_nah_klein.msh\n  []\n[]\n\n[GlobalParams]\n  gravity = '0 -9.8065 0'\n[]\n\n[Variables]\n  [temperature]\n    initial_condition = 293.15\n  []\n  [pp]\n  []\n  [gp]\n  []\n[]\n\n[ICs]\n  [InitialConditionBoden]\n    type = FunctionIC\n    variable = pp\n    function = gradient\n    block = 'boden'\n  []\n  [ICDampf]\n    type = ConstantIC\n    value = 101325\n    variable = gp\n    block = 'boden'\n  []\n  [InitialConditionWasser]\n    type = FunctionIC\n    variable = pp\n    function = wasser\n    block = 'wasser'\n  []\n[]\n\n[Functions]\n  [gradient]\n    type = ParsedFunction\n    symbol_names = 'rho g gradient left_head'\n    symbol_values = '1000 9.8065 0.1 10'\n    expression = '101325 + (left_head-y-(x*gradient))*(g*rho)'\n  []\n  [t_kabel]\n      type = ParsedFunction\n      expression = 'if(t <= 14400, (293.15 + (363.15 - 293.15) * t / 14400), 363.15)'\n  []\n  [wasser]\n    type = ParsedFunction\n    symbol_names = 'rho g left_head'\n    symbol_values = '1000 9.8065 10'\n    expression = '101325 + (left_head-y)*(g*rho)'\n  []\n[]\n\n[BCs]\n    [flow]\n      type = FunctionDirichletBC\n      function = gradient      \n      variable = 'pp'\n      boundary = 'rechts links unten oben'\n    []\n    [outflow_gp]\n      type = PorousFlowOutflowBC \n      flux_type = fluid\n      variable = 'gp'\n      boundary = 'rechts'\n      PorousFlowDictator = dictator_boden\n      mass_fraction_component = 1\n    []\n    [top]\n      type = DirichletBC\n      boundary = 'oben'\n      value = 101325\n      variable = 'gp'\n    []    \n    [top_wasserspiegel]\n      type = FunctionDirichletBC\n      variable = pp\n      boundary = wasserspiegel\n      function = wasser\n    []\n    # [temp_kabel]\n    #   type = FunctionDirichletBC\n    #   boundary = 'kabel'\n    #   function = t_kabel\n    #   variable = temperature\n    # []\n    [temp_out]\n      type = PorousFlowOutflowBC\n      PorousFlowDictator = dictator_boden\n      boundary = 'rechts'\n      variable = temperature\n      flux_type = heat\n    []\n    [temp_in]\n      type = DirichletBC\n      boundary = links\n      value = 293.15\n      variable = temperature\n    []\n    [temp_unten]\n      type = ADConvectiveHeatFluxBC\n      T_infinity = 293.15\n      boundary = 'unten rechts'\n      heat_transfer_coefficient = 20\n      variable = temperature\n    []\n    [temp_atm]\n      type = ADConvectiveHeatFluxBC\n      T_infinity = 293.15\n      boundary = 'oben wasserspiegel'\n      heat_transfer_coefficient = 5\n      variable = temperature\n    []\n    [outflow_pp_wasser]\n      type = PorousFlowOutflowBC\n      variable = pp\n      flux_type = FLUID\n      boundary = 'grund'\n      PorousFlowDictator = dictator_wasser\n    []\n    [outflow_pp_boden]\n      type = PorousFlowOutflowBC\n      variable = pp\n      flux_type = FLUID\n      boundary = 'grund'\n      PorousFlowDictator = dictator_boden\n    []\n\n    [t_grund_boden]\n      type = PorousFlowOutflowBC\n      PorousFlowDictator = dictator_boden\n      boundary = grund\n      variable = temperature\n      flux_type = HEAT\n    []\n    [t_grund_wasser]\n      type = PorousFlowOutflowBC\n      variable = temperature\n      boundary = grund\n      PorousFlowDictator = dictator_wasser\n      flux_type = HEAT\n    []\n[]\n\n[Kernels]\n  [temperature_dt_boden]\n    type = PorousFlowEnergyTimeDerivative\n    variable = temperature\n    PorousFlowDictator = dictator_boden\n    block = 'boden'\n  []\n  [advectionT_boden]\n    type = PorousFlowHeatAdvection\n    variable = temperature\n    block = 'boden'\n    PorousFlowDictator = dictator_boden\n  []\n  [conductionT_boden]\n    type = PorousFlowHeatConduction\n    variable = temperature\n    block = 'boden'\n    PorousFlowDictator = dictator_boden\n  []\n  [pressure_dt_pp_boden]\n    type = PorousFlowMassTimeDerivative\n    variable = pp\n    block = 'boden'\n    PorousFlowDictator = dictator_boden\n    fluid_component = 0\n  []\n  [pressure_dt_gp]\n    type = PorousFlowMassTimeDerivative\n    variable = gp\n    PorousFlowDictator = dictator_boden\n    block = 'boden'\n    fluid_component = 1\n  []\n  [pp_disp_boden]\n    type = PorousFlowDispersiveFlux\n    disp_long = '1E-4 1E-4'\n    disp_trans = '1E-5 1E-5'\n    fluid_component = 0\n    block = 'boden'\n    variable = pp\n    PorousFlowDictator = dictator_boden\n  []\n  [gp_disp_boden]\n    type = PorousFlowDispersiveFlux\n    disp_long = '5E-4 5E-4'\n    disp_trans = '5E-5 5E-5'\n    fluid_component = 1\n    block = 'boden'\n    variable = gp\n    PorousFlowDictator = dictator_boden\n  []\n  [pp_advective_boden]\n    type = PorousFlowAdvectiveFlux\n    variable = pp\n    block = 'boden'\n    PorousFlowDictator = dictator_boden\n    fluid_component = 0\n  []\n  [gp_advective]\n    type = PorousFlowAdvectiveFlux\n    variable = gp\n    PorousFlowDictator = dictator_boden\n    block = 'boden'\n    fluid_component = 1\n  []\n  [temperature_dt_wasser]\n    type = PorousFlowEnergyTimeDerivative\n    variable = temperature\n    PorousFlowDictator = dictator_wasser\n    block = 'wasser'\n  []\n  [advectionT_wasser]\n    type = PorousFlowHeatAdvection\n    variable = temperature\n    block = 'wasser'\n    PorousFlowDictator = dictator_wasser\n  []\n  [conductionT_wasser]\n    type = PorousFlowHeatConduction\n    variable = temperature\n    block = 'wasser'\n    PorousFlowDictator = dictator_wasser\n  []\n  [pressure_dt_pp_wasser]\n    type = PorousFlowMassTimeDerivative\n    variable = pp\n    block = 'wasser'\n    PorousFlowDictator = dictator_wasser\n    fluid_component = 0\n  []\n  [pp_disp_wasser]\n    type = PorousFlowDispersiveFlux\n    disp_long = '1E-6'\n    disp_trans = '1E-7'\n    block = 'wasser'\n    variable = pp\n    fluid_component = 0\n    PorousFlowDictator = dictator_wasser\n  []\n  [pp_advective_wasser]\n    type = PorousFlowAdvectiveFlux\n    variable = pp\n    block = 'wasser'\n    fluid_component = 0\n    PorousFlowDictator = dictator_wasser\n  []\n[]\n\n[AuxVariables]\n  [darcy_x_p0]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [darcy_y_p0]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [darcy_x_p1]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [darcy_y_p1]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [sat_p0]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [sat_p1]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [cp]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [vis]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [density]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n[]\n\n[AuxKernels]\n  [d_v_x_boden_p0]\n    type = PorousFlowDarcyVelocityComponent\n    component = x\n    variable = darcy_x_p0\n    block = 'boden'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n    fluid_phase = 0\n  []\n  [d_v_y_boden_p0]\n    type = PorousFlowDarcyVelocityComponent\n    component = y\n    variable = darcy_y_p0\n    block = 'boden'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n    fluid_phase = 0\n  []\n  [d_v_x_boden_p1]\n    type = PorousFlowDarcyVelocityComponent\n    component = x\n    variable = darcy_x_p1\n    block = 'boden'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n    fluid_phase = 1\n  []\n  [d_v_y_boden_p1]\n    type = PorousFlowDarcyVelocityComponent\n    component = y\n    variable = darcy_y_p1\n    block = 'boden'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n    fluid_phase = 1\n  []\n  [d_v_x_wasser_p0]\n    type = PorousFlowDarcyVelocityComponent\n    component = x\n    variable = darcy_x_p0\n    block = 'wasser'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_wasser\n    fluid_phase = 0\n  []\n  [d_v_y_wasser_p0]\n    type = PorousFlowDarcyVelocityComponent\n    component = y\n    variable = darcy_y_p0\n    block = 'wasser'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_wasser\n    fluid_phase = 0\n  []\n  [saturation_boden_p0]\n    type = PorousFlowPropertyAux\n    variable = sat_p0\n    property = saturation\n    block = 'boden'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n    phase = 0\n  []\n  [saturation_boden_p1]\n    type = PorousFlowPropertyAux\n    variable = sat_p1\n    property = saturation\n    block = 'boden'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n    phase = 1\n  []\n  [saturation_wasser]\n    type = PorousFlowPropertyAux\n    variable = sat_p0\n    property = saturation\n    block = 'wasser'\n    execute_on = 'INITIAL TIMESTEP_END'\n    PorousFlowDictator = dictator_wasser\n    phase = 0\n  []\n  [capillary_pressure]\n    type = PorousFlowPropertyAux\n    property = capillary_pressure\n    variable = cp\n    block = 'boden'\n    execute_on = 'LINEAR TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n    liquid_phase = 0\n  []\n  [viscosity_boden]\n    type = PorousFlowPropertyAux\n    variable = vis\n    property = viscosity\n    block = boden\n    execute_on = 'LINEAR TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n  []\n  [viscosity_wasser]\n    type = PorousFlowPropertyAux\n    variable = vis\n    property = viscosity\n    block = wasser\n    execute_on = 'LINEAR TIMESTEP_END'\n    PorousFlowDictator = dictator_wasser\n  []\n  [density_boden]\n    type = PorousFlowPropertyAux\n    variable = density\n    property = density\n    block = boden\n    execute_on = 'LINEAR TIMESTEP_END'\n    PorousFlowDictator = dictator_boden\n  []\n  [density_wasser]\n    type = PorousFlowPropertyAux\n    variable = density\n    property = density\n    block = wasser\n    execute_on = 'LINEAR TIMESTEP_END'\n    PorousFlowDictator = dictator_wasser\n  []\n[]\n\n[FluidProperties]\n  [wasser]\n    type = Water97FluidProperties\n  []\n  [dampf]\n    type = NitrogenFluidProperties\n  []\n  [dampf_tabulated]\n    type = TabulatedBicubicFluidProperties\n    fp = dampf\n    interpolated_properties = 'density enthalpy internal_energy viscosity k c cv cp'\n    construct_pT_from_ve = false\n    construct_pT_from_vh = false\n    temperature_min = 274\n    temperature_max = 400\n    pressure_min = 12523\n    pressure_max = 3e6\n    tolerance = 1e-8\n    T_initial_guess = 293.15\n    p_initial_guess = 1e5\n  []\n[]\n\n\n\n[Materials]\n    [internal_energy_boden]\n      type = PorousFlowMatrixInternalEnergy\n      density = 2600\n      specific_heat_capacity = 1000\n      block = 'boden'\n      PorousFlowDictator = dictator_boden\n    []\n    [thermal_conductivity_boden]\n      type = PorousFlowThermalConductivityIdeal\n      dry_thermal_conductivity = '0.4 0 0  0 0.4 0 0 0 0.4'\n      wet_thermal_conductivity = '1 0 0    0 1 0   0 0 1'\n      block = 'boden'\n      PorousFlowDictator = dictator_boden\n    []\n    [porosity_boden]\n      type = PorousFlowPorosityConst\n      porosity = 0.3\n      block = 'boden'\n      PorousFlowDictator = dictator_boden\n    []\n    [permeablity_boden]\n      type = PorousFlowPermeabilityConst\n      block = 'boden'\n      permeability = '1E-12 0 0    0 1E-12 0    0 0 1E-12'\n      PorousFlowDictator = dictator_boden\n    []\n    [fluid_boden]\n      type = PorousFlowSingleComponentFluid\n      fp = 'wasser'\n      phase = 0\n      PorousFlowDictator = dictator_boden\n      block = 'boden'\n    []\n    [dampf]\n      type = PorousFlowSingleComponentFluid\n      fp = 'dampf_tabulated'\n      phase = 1\n      PorousFlowDictator = dictator_boden\n      block = 'boden'\n    []\n    [temperature_qp_boden]\n      type = PorousFlowTemperature\n      temperature = temperature\n      PorousFlowDictator = dictator_boden\n      block = 'boden'\n    []\n    [saturation_calculator_boden]\n      type = PorousFlow2PhasePP\n      PorousFlowDictator = dictator_boden\n      phase0_porepressure = pp\n      phase1_porepressure = gp\n      capillary_pressure = capillary_pressure_boden\n      block = 'boden'\n    []\n    [relperm_boden_dampf]\n      type = PorousFlowRelativePermeabilityCorey\n      n = 2\n      phase = 1\n      block = 'boden'\n      PorousFlowDictator = dictator_boden\n    []\n    [relperm_boden_fluid]\n      type = PorousFlowRelativePermeabilityCorey\n      n = 2\n      s_res = 0.1\n      sum_s_res = 0.1\n      phase = 0\n      block = 'boden'\n      PorousFlowDictator = dictator_boden\n    []\n    [mass_frac_boden]\n      type = PorousFlowMassFraction\n      PorousFlowDictator = dictator_boden\n      block = 'boden'\n      mass_fraction_vars = '1 0'\n    []\n    [diff_boden]\n      type = PorousFlowDiffusivityConst\n      diffusion_coeff = '1E-6 1E-5 1E-6 1E-5'\n      tortuosity = '0.5 0.5'\n      block = 'boden'\n      PorousFlowDictator = dictator_boden\n    []\n    [internal_energy_wasser]\n      type = PorousFlowMatrixInternalEnergy\n      density = 1000\n      specific_heat_capacity = 4200\n      block = 'wasser'\n      PorousFlowDictator = dictator_wasser\n    []\n    [thermal_conductivity_wasser]\n      type = PorousFlowThermalConductivityIdeal\n      dry_thermal_conductivity = '0.6 0 0    0 0.6 0   0 0 0.6'\n      wet_thermal_conductivity = '0.6 0 0    0 0.6 0   0 0 0.6'\n      block = 'wasser'\n      PorousFlowDictator = dictator_wasser\n    []\n    [porosity_wasser]\n      type = PorousFlowPorosityConst\n      porosity = 0.9999\n      block = 'wasser'\n      PorousFlowDictator = dictator_wasser\n    []\n    [permeablity_wasser]\n      type = PorousFlowPermeabilityConst\n      block = 'wasser'\n      permeability = '1E-4 0 0    0 1E-4 0    0 0 1E-4'\n      PorousFlowDictator = dictator_wasser\n    []\n    [fluid_wasser]\n      type = PorousFlowSingleComponentFluid\n      fp = 'wasser'\n      phase = 0\n      PorousFlowDictator = dictator_wasser\n      block = 'wasser'\n    []\n    [temperature_qp_wasser]\n      type = PorousFlowTemperature\n      temperature = temperature\n      PorousFlowDictator = dictator_wasser\n      block = 'wasser'\n    []\n    [saturation_calculator_wasser]\n      type = PorousFlow1PhaseP\n      capillary_pressure = capillary_pressure_wasser\n      PorousFlowDictator = dictator_wasser\n      porepressure = pp\n      block = 'wasser'\n    []\n    [relperm_wasser]\n      type = PorousFlowRelativePermeabilityConst\n      PorousFlowDictator = dictator_wasser\n      phase = 0\n      kr = 1\n      block = 'wasser'\n    []\n    [mass_frac_wasser]\n      type = PorousFlowMassFraction\n      PorousFlowDictator = dictator_wasser\n      block = 'wasser'\n    []\n    [diff_wasser]\n      type = PorousFlowDiffusivityConst\n      diffusion_coeff = '1.4E-7'\n      tortuosity = '1'\n      block = 'wasser'\n      PorousFlowDictator = dictator_wasser\n    []\n[]\n\n[UserObjects]\n  [dictator_boden]\n    type = PorousFlowDictator\n    number_fluid_components = 2\n    number_fluid_phases = 2\n    porous_flow_vars = 'pp temperature gp'\n  []\n  [dictator_wasser]\n    type = PorousFlowDictator\n    number_fluid_components = 1\n    number_fluid_phases = 1\n    porous_flow_vars = 'pp temperature'\n  []\n  [capillary_pressure_boden]\n    type = PorousFlowCapillaryPressureVG\n    alpha = 1E-4\n    m = 0.5\n    block = 'boden'\n  []\n  [capillary_pressure_wasser]\n    type = PorousFlowCapillaryPressureConst\n    block = 'wasser'\n    pc = 0\n  []\n[]\n\n# [Postprocessors]\n#   [heat]\n#     type = PorousFlowHeatEnergy\n#     outputs = csv\n#     PorousFlowDictator = dictator_boden\n#   []\n#   [Fluid]\n#     type = PorousFlowFluidMass\n#     PorousFlowDictator = dictator_boden\n#     outputs = csv\n#     fluid_component = 0\n#   []\n#   [Gas]\n#     type = PorousFlowFluidMass\n#     PorousFlowDictator = dictator_boden\n#     outputs = csv\n#     fluid_component = 1\n#   []\n# []\n\n\n[Preconditioning]\n  [basic]\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type'\n    petsc_options_value = ' asm      lu           NONZERO'\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = NEWTON\n  automatic_scaling = true\n  compute_scaling_once = true\n  end_time = 1E9\n  nl_abs_tol = 1E-6\n  nl_rel_tol = 1E-6\n  nl_max_its = 20\n  \n  [TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 1\n    growth_factor = 2\n    cutback_factor = 0.5\n  []\n[]\n\n[Debug]\n  show_var_residual_norms = true\n[]\n\n[Outputs]\n  exodus = true\n  print_linear_residuals = true\n  # [csv]\n  #   type = CSV\n  # []\n[]",
          "url": "https://github.com/idaholab/moose/discussions/29720",
          "updatedAt": "2025-02-06T08:23:16Z",
          "publishedAt": "2025-01-21T20:10:31Z",
          "category": {
            "name": "Q&A Modules: Porous Flow"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lynnmunday"
                  },
                  "bodyText": "I've never seen a simulation with more than one PorousFlowDictator objects.  I didn't think you could do that.  I looked through the test suite and I didn't find an example that used more than one.  Did you find any tests that do something similar with only one PorousFlowDictator?  Can you use one?",
                  "url": "https://github.com/idaholab/moose/discussions/29720#discussioncomment-11908028",
                  "updatedAt": "2025-01-21T21:34:45Z",
                  "publishedAt": "2025-01-21T21:34:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "julianseb"
                          },
                          "bodyText": "No i did not find tests doing that. But i thought since the PorousFlowDictator is a UserObject that is called upon by the Kernels, Materials, etc. i could use a arbitrary number of Dictators as long as they are used by different Objects. At least it was worth a try.\nI guess i will have to use one PorousFlowDictator then or maybe look at an multiapp system (which seems a bit overwhelming at the moment).\nBut anyway. Thanks a lot :)",
                          "url": "https://github.com/idaholab/moose/discussions/29720#discussioncomment-11913154",
                          "updatedAt": "2025-01-22T08:54:48Z",
                          "publishedAt": "2025-01-22T08:54:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lynnmunday"
                          },
                          "bodyText": "@WilkAndy or @cpgr can multiple porousFlowDictator's be used in a single input file?",
                          "url": "https://github.com/idaholab/moose/discussions/29720#discussioncomment-11919809",
                          "updatedAt": "2025-01-22T17:49:17Z",
                          "publishedAt": "2025-01-22T17:49:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "julianseb"
                  },
                  "bodyText": "So i rewrote my inputfile so it only contains one PorousFlowDictator.\nIf i don't have any BC on the border between the ground and the open water body it runs as expected like #29644 but with a no flow boundary in its way.\nBut i want to have a coupling of the to blocks so that heat and porepressure are exchanged. I tried to implement this with the OutflowBCs. For only the porepressure it seems fine. When i only engage the temperature it also runs, but i get artifacts on the border from the waterbody to the unsaturated zone and there seem to be some cooling i can not explain.\n\nSometimes i get an error message like this that would explain that there is cooling but unfortunately not why it is happening:\nTemperature 270.67 is out of range in wasser: inRegion()\nTo recover, the solution will fail and then be re-attempted with a reduced time step.\n\nA MooseException was raised during FEProblemBase::computeResidualTags\nTemperature 270.67 is out of range in wasser: inRegion()\nTo recover, the solution will fail and then be re-attempted with a reduced time step.\n\n  Nonlinear solve did not converge due to DIVERGED_FNORM_NAN iterations 8\n Solve Did NOT Converge!\n\nAnd when i engage the temperature and the porepressure OutflowBCs at the same time the convergence is really bad and it cuts the timestep constantly.\n...\nTime Step 1, time = 0.03125, dt = 0.03125\n    |residual|_2 of individual variables:\n                temperature: 0.0676454\n                pp:          0.00275448\n                gp:          1.55676\n 0 Nonlinear |R| = 1.558232e+00\n      0 Linear |R| = 1.558232e+00\n      1 Linear |R| = 3.733900e-09\n    |residual|_2 of individual variables:\n                temperature: 0.180709\n                pp:          0.00227147\n                gp:          0.000387099\n 1 Nonlinear |R| = 1.807236e-01\n      0 Linear |R| = 1.807236e-01\n      1 Linear |R| = 2.343165e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0450336\n                pp:          0.000574951\n                gp:          1.89311e-09\n 2 Nonlinear |R| = 4.503732e-02\n      0 Linear |R| = 4.503732e-02\n      1 Linear |R| = 1.773325e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0236775\n                pp:          0.000298875\n                gp:          7.56092e-10\n 3 Nonlinear |R| = 2.367936e-02\n      0 Linear |R| = 2.367936e-02\n      1 Linear |R| = 2.251880e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0171082\n                pp:          0.000216034\n                gp:          5.8726e-10\n 4 Nonlinear |R| = 1.710954e-02\n      0 Linear |R| = 1.710954e-02\n      1 Linear |R| = 5.675462e-11\n    |residual|_2 of individual variables:\n                temperature: 0.0142076\n                pp:          0.000178531\n                gp:          3.70942e-10\n 5 Nonlinear |R| = 1.420868e-02\n      0 Linear |R| = 1.420868e-02\n      1 Linear |R| = 3.797841e-11\n    |residual|_2 of individual variables:\n                temperature: 0.0106693\n                pp:          0.00013447\n                gp:          2.0394e-10\n 6 Nonlinear |R| = 1.067016e-02\n      0 Linear |R| = 1.067016e-02\n      1 Linear |R| = 1.301987e-10\n    |residual|_2 of individual variables:\n                temperature: 0.00889562\n                pp:          0.00011211\n                gp:          2.42013e-10\n 7 Nonlinear |R| = 8.896326e-03\n      0 Linear |R| = 8.896326e-03\n      1 Linear |R| = 6.176494e-11\n    |residual|_2 of individual variables:\n                temperature: 0.00763035\n                pp:          9.6169e-05\n                gp:          2.65802e-10\n 8 Nonlinear |R| = 7.630958e-03\n      0 Linear |R| = 7.630958e-03\n      1 Linear |R| = 4.222395e-11\n    |residual|_2 of individual variables:\n                temperature: 0.00747023\n                pp:          9.40895e-05\n                gp:          2.80426e-10\n 9 Nonlinear |R| = 7.470826e-03\n      0 Linear |R| = 7.470826e-03\n      1 Linear |R| = 2.566226e-11\n    |residual|_2 of individual variables:\n                temperature: 0.00707274\n                pp:          8.90493e-05\n                gp:          2.89188e-10\n10 Nonlinear |R| = 7.073304e-03\n  Nonlinear solve did not converge due to DIVERGED_MAX_IT iterations 10\n  Finished Solving                                                                       [103.61 s] [  343 MB]\n Solve Did NOT Converge!\nAborting as solve did not converge\n\nSolve failed, cutting timestep.\n\nTime Step 1, time = 0.015625, dt = 0.015625\n    |residual|_2 of individual variables:\n                temperature: 0.0676454\n                pp:          0.00275448\n                gp:          1.55676\n 0 Nonlinear |R| = 1.558232e+00\n      0 Linear |R| = 1.558232e+00\n      1 Linear |R| = 3.131965e-09\n    |residual|_2 of individual variables:\n                temperature: 0.21916\n                pp:          0.00273185\n                gp:          0.000386187\n 1 Nonlinear |R| = 2.191774e-01\n      0 Linear |R| = 2.191774e-01\n      1 Linear |R| = 2.852329e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0555337\n                pp:          0.00070943\n                gp:          4.09194e-09\n 2 Nonlinear |R| = 5.553823e-02\n      0 Linear |R| = 5.553823e-02\n      1 Linear |R| = 3.365325e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0301049\n                pp:          0.000374472\n                gp:          5.37534e-09\n 3 Nonlinear |R| = 3.010720e-02\n      0 Linear |R| = 3.010720e-02\n      1 Linear |R| = 2.476050e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0252734\n                pp:          0.000314313\n                gp:          4.5801e-09\n 4 Nonlinear |R| = 2.527531e-02\n      0 Linear |R| = 2.527531e-02\n      1 Linear |R| = 1.504118e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0202637\n                pp:          0.00025191\n                gp:          3.51368e-09\n 5 Nonlinear |R| = 2.026522e-02\n      0 Linear |R| = 2.026522e-02\n      1 Linear |R| = 5.070768e-11\n    |residual|_2 of individual variables:\n                temperature: 0.0169667\n                pp:          0.000212282\n                gp:          2.28632e-10\n 6 Nonlinear |R| = 1.696800e-02\n      0 Linear |R| = 1.696800e-02\n      1 Linear |R| = 1.885023e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0151194\n                pp:          0.000189108\n                gp:          2.69138e-10\n 7 Nonlinear |R| = 1.512060e-02\n      0 Linear |R| = 1.512060e-02\n      1 Linear |R| = 1.330392e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0135125\n                pp:          0.000168974\n                gp:          2.85915e-10\n 8 Nonlinear |R| = 1.351356e-02\n      0 Linear |R| = 1.351356e-02\n      1 Linear |R| = 1.037414e-10\n    |residual|_2 of individual variables:\n                temperature: 0.0121035\n                pp:          0.000151336\n                gp:          3.11829e-10\n 9 Nonlinear |R| = 1.210447e-02\n      0 Linear |R| = 1.210447e-02\n      1 Linear |R| = 8.473254e-11\n    |residual|_2 of individual variables:\n                temperature: 0.011714\n                pp:          0.000146466\n                gp:          3.34518e-10\n10 Nonlinear |R| = 1.171495e-02\n  Nonlinear solve did not converge due to DIVERGED_MAX_IT iterations 10\n Solve Did NOT Converge!\n  Finished Solving                                                                       [103.06 s] [  343 MB]\nAborting as solve did not converge\n\nSolve failed, cutting timestep.\n\nTime Step 1, time = 0.0078125, dt = 0.0078125\n    |residual|_2 of individual variables:\n                temperature: 0.0676454\n                pp:          0.00275448\n                gp:          1.55676\n 0 Nonlinear |R| = 1.558232e+00\n...\n\nAlso adding the gaspressure would be nice, but isn't really necessary.\nIs ther a better way to couple two blocks like there is no boundary than with OutflowBCs?\nThis is my current BC block. I also added my current inputfile and mesh:\nCurrent_state.zip\n[BCs]\n    [flow]\n      type = FunctionDirichletBC\n      function = gradient      \n      variable = 'pp'\n      boundary = 'rechts links unten oben'\n    []\n    [outflow_gp]\n      type = PorousFlowOutflowBC \n      flux_type = fluid\n      variable = 'gp'\n      boundary = 'rechts'\n      PorousFlowDictator = dictator_boden\n      mass_fraction_component = 1\n    []\n    [top]\n      type = DirichletBC\n      boundary = 'oben wasserspiegel'\n      value = 101325\n      variable = 'gp'\n    []\n    [top_wasserspiegel]\n      type = FunctionDirichletBC\n      variable = pp\n      boundary = wasserspiegel\n      function = wasser\n    []\n    # [temp_kabel]\n    #   type = FunctionDirichletBC\n    #   boundary = 'kabel'\n    #   function = t_kabel\n    #   variable = temperature\n    # []\n    [temp_out]\n      type = PorousFlowOutflowBC\n      PorousFlowDictator = dictator_boden\n      boundary = 'rechts oben'\n      variable = temperature\n      flux_type = heat\n    []\n    [temp_in]\n      type = DirichletBC\n      boundary = links\n      value = 293.15\n      variable = temperature\n    []\n    [temp_unten]\n      type = ADConvectiveHeatFluxBC\n      T_infinity = 293.15\n      boundary = 'unten rechts'\n      heat_transfer_coefficient = 20\n      variable = temperature\n    []\n    [temp_atm]\n      type = DirichletBC\n      boundary = 'oben wasserspiegel'\n      variable = temperature\n      value = 293.15\n    []\n    [pp_grund]\n      type = PorousFlowOutflowBC\n      variable = pp\n      flux_type = FLUID\n      boundary = 'grund'\n      PorousFlowDictator = dictator_boden\n      mass_fraction_component = 0\n    []\n    [t_grund]\n      type = PorousFlowOutflowBC\n      PorousFlowDictator = dictator_boden\n      boundary = grund\n      variable = temperature\n      flux_type = HEAT\n    []\n    # [gp_grund]\n    #   type = PorousFlowOutflowBC\n    #   variable = gp\n    #   flux_type = FLUID\n    #   boundary = 'grund'\n    #   PorousFlowDictator = dictator_boden\n    #   mass_fraction_component = 1\n    # []\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/29720#discussioncomment-11930282",
                  "updatedAt": "2025-01-23T14:02:58Z",
                  "publishedAt": "2025-01-23T13:51:00Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}