{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wMy0yOVQxNDowMzo0NS0wNTowMM4AMbsd"
    },
    "edges": [
      {
        "node": {
          "title": "Survey: Development of spline-based FEM (IGA) technologies in MOOSE",
          "author": {
            "login": "GregVernon"
          },
          "bodyText": "MOOSE Community,\nAs you may have heard, we at Coreform have been partnering with INL on some MOOSE developments.  Primarily these have included adding spline basis function technologies to MOOSE and the MOOSE ecosystem, thereby enabling spline-based FEM (often referred to as Isogeometric Analysis or \"IGA\").  We also develop Coreform Cubit, a meshing and model-preparation software that is often used with MOOSE.\nTo help identify and prioritize features, we've put together a short nine-question survey to solicit your input and feedback.  We will share the results of the survey with the MOOSE community once the survey is completed (maybe after a week or two). Your responses will help Coreform, and other MOOSE developers, identify and prioritize desired features to add to MOOSE and Coreform Cubit for the MOOSE community!\n- Coreform\nLink to Survey",
          "url": "https://github.com/idaholab/moose/discussions/17021",
          "updatedAt": "2022-07-20T10:10:42Z",
          "publishedAt": "2021-02-16T19:45:06Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GregVernon"
                  },
                  "bodyText": "Thank you to everyone who responded to the survey!  I've attached the results of the survey to this comment in the hopes that you find them insightful, if not helpful for your own endeavors!\nMOOSE_Survey_Results.pdf",
                  "url": "https://github.com/idaholab/moose/discussions/17021#discussioncomment-568446",
                  "updatedAt": "2022-07-20T10:10:52Z",
                  "publishedAt": "2021-04-04T19:10:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "3D spinodal decomposition",
          "author": {
            "login": "Amir1361"
          },
          "bodyText": "Hello,\nI am trying to have a 3D simulation of spinodal decomposition. First, I used \"GeneratedMesh,\" and it works with a small mesh number. But when I increased the nx, ny, and nz, I was out of memory.\n\"mpiexec noticed that process rank 0 with PID 66797 on node cpu110 exited on signal 9 (Killed).\"\nThen, I changed it to \"DistributedRectilinearMeshGenerator\" to run it with more mesh numbers. But I got the following error:\n\"^[[31m\n*** ERROR ***\nTask init_mesh is not registered to build MeshGenerator derived objects^[[39m\n\nMPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD\nwith errorcode 1.\nNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nYou may or may not see output from other processes, depending on\nexactly when Open MPI kills them.\n[cpu102:07986] 7 more processes have sent help message help-mpi-api.txt / mpi-abort\n[cpu102:07986] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\"\nHere is my mesh part in the input file:\n[Mesh]\ntype = DistributedRectilinearMeshGenerator\ndim = 3\nelem_type = QUAD4\nnx = 25\nny = 25\nnz = 25\nxmax = 100\nymax = 100\nzmax = 100\n[]\nCould you please help me to solve this?\nBest,\nAmir",
          "url": "https://github.com/idaholab/moose/discussions/17427",
          "updatedAt": "2022-08-04T10:26:38Z",
          "publishedAt": "2021-03-24T19:59:52Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "Do you have mesh adaptivity enabled?",
                  "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-527766",
                  "updatedAt": "2022-08-04T10:27:03Z",
                  "publishedAt": "2021-03-25T03:03:10Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "On top of @aeslaughter 's question, are you using a direct solver, i.e. LU?\nBTW, your element_type = QUAD4 does not make sense for a 3D domain.",
                          "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-527848",
                          "updatedAt": "2022-08-04T10:27:03Z",
                          "publishedAt": "2021-03-25T03:46:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Amir1361"
                          },
                          "bodyText": "Thank you for your replies.\nThis is my input file: FeCrCo_3D.txt\nI removed the \"element_type = QUAD4\", but it doesn't work yet. How can I enable the mesh adaptivity?",
                          "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-528108",
                          "updatedAt": "2022-08-04T10:27:04Z",
                          "publishedAt": "2021-03-25T06:05:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "permcody"
                          },
                          "bodyText": "Even with DistributedRectilinearMeshGenerator you still need to set your parallel_type = distributed to run with distributed mesh. However, I don't think this problem is big enough to require distributed so I tried it without. It ran OK as the per core memory usage was about just over 2GB after 3 time steps. Depending on your system and how many MPI ranks you used, you can figure out if that would exceed your available memory. I would expect that number to continue to climb, but not too much beyond that. I then tried it with distributed and the memory usage stayed well under 2GB (~1.3GB).\nAnother very interesting thing I noted was how slow DistributedRectilinearMeshGenerator was. It took a very long time to generate the mesh. Using the simple \"GeneratoredMesh\" object it was only a split second. I know the former uses less memory but I'm a little surprised with how much slower it was. We probably haven't tried it much with medium size problems like this. It really is for very large problems.\nSo your input file works, just make sure you aren't running yourself out of memory!",
                          "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-530094",
                          "updatedAt": "2022-08-04T10:27:05Z",
                          "publishedAt": "2021-03-25T14:59:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "Adding a sub-block will fix the error.\n[Mesh]\n[drmg]\ntype = DistributedRectilinearMeshGenerator\ndim = 3\nnx = 25\nny = 25\nnz = 25\nxmax = 100\nymax = 100\nzmax = 100\n[]\n[]\n\n\nI am not sure if switching to distributed mesh will resolve your memory issue. I ran your input file with 10 MPI procs, and roughly each process requires a maximum 3-4 Gb memory. That seems to be reasonable to me. You might want to request more memory on you HPC to large 3D simulation.",
                          "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-530128",
                          "updatedAt": "2023-04-19T19:43:32Z",
                          "publishedAt": "2021-03-25T15:07:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Amir1361"
                          },
                          "bodyText": "Thank you very much! I do not have any problem with following mesh size:\n[Mesh]\n[drmg]\ntype = DistributedRectilinearMeshGenerator\ndim = 3\nnx = 25\nny = 25\nnz = 25\nxmax = 100\nymax = 100\nzmax = 100\n[]\n[]\nBut when I want to increase nx, ny, nz and xmax, ymax, zmax:\n[Mesh]\n[drmg]\ntype = DistributedRectilinearMeshGenerator\ndim = 3\nnx = 100\nny = 100\nnz = 100\nxmax = 200\nymax = 200\nzmax = 200\n[]\nI get the memory error:\n\"mpiexec noticed that process rank 0 with PID 142272 on node cpu102 exited on signal 9 (Killed).\"\nI am running my job on cluster computer (Boise State University). Is there any way to solve this issue? Thank you very much for your help!\nAmir",
                          "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-531298",
                          "updatedAt": "2023-04-19T19:43:33Z",
                          "publishedAt": "2021-03-25T19:28:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "To run a 3D problem with 100x100x100 definitely requires a lot of memory usage:-) So you have to work with your HPC admin to increase the memory for your simulation.",
                          "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-536719",
                          "updatedAt": "2023-04-19T19:43:36Z",
                          "publishedAt": "2021-03-27T03:07:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "permcody"
                          },
                          "bodyText": "Amir, I suggest that when you run, you start up a \"top\" process on the lead node in your batch job and watch memory usage to see where you are at. If you are only a little over (it runs for past the mesh stage and dies in the solver), one way to get the extra memory you need is to simply not use all the cores on each node (e.g. if your nodes have 32 processors each, try running your problem using only 24 or 28 cores on the nodes). Doing this with a batch scheduling system is fairly straightforward. Your million element mesh shouldn't take too much memory (Several hundred MB on each rank), so there are other factors that may be causing you to run out of memory related to the sizes of your equation systems or other resident memory. Let us know if you need any further guidance.",
                          "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-560349",
                          "updatedAt": "2023-04-19T19:43:36Z",
                          "publishedAt": "2021-04-01T21:54:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Amir1361"
                          },
                          "bodyText": "@permcody Thank you very much for your comments. I will test the different options. Thank you for your help; I appreciate it.",
                          "url": "https://github.com/idaholab/moose/discussions/17427#discussioncomment-561047",
                          "updatedAt": "2023-04-19T19:43:36Z",
                          "publishedAt": "2021-04-02T03:53:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "No energy evolution in Grand Potential Phase Field model",
          "author": {
            "login": "scottedmuller"
          },
          "bodyText": "Hi All,\nI am developing a Grand Potential Phase Field model of a binary system in moose and it is having some really odd issues. The energies look right in both phases but I can supersaturate one phase a ton without causing any evolution. I can look at the free energy and see that it is increased appropriately in the given phase, indicating there should be a driving force for the energy to minimize, but no changes occur. Does anyone have any specific advice for troubleshooting a lack of energy minimization in my simulations? The energy is not increasing either there is just no change at all.\nThanks,\nScott",
          "url": "https://github.com/idaholab/moose/discussions/17437",
          "updatedAt": "2022-07-18T07:10:52Z",
          "publishedAt": "2021-03-26T17:44:57Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "laagesen"
                  },
                  "bodyText": "Hi Scott- when you say the total energy is not decreasing, how are you determining that? Are you using a postprocessor to sum the local free energy density? Bear in mind that when you are using a grand potential model, it is the total grand potential that should decrease and not the Helmholtz free energy (although I think the first should imply the second). Also, what are your initial conditions? Just want to make sure it is not stuck in a metastable state.",
                  "url": "https://github.com/idaholab/moose/discussions/17437#discussioncomment-545469",
                  "updatedAt": "2022-07-18T07:11:02Z",
                  "publishedAt": "2021-03-29T20:14:18Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "scottedmuller"
                          },
                          "bodyText": "Hi Larry,\nI output both the grand potential and \"regular\" free energy as a Material block (just for looking at in Paraview). I've summed over the domain of the Grand Potential free energy density, and it is also not changing, neither increasing or decreasing (unless I change the mobilities in dramatic non-physical ways). The boundary conditions are periodic, and I'm currently considering a 1-D case for simplicity, with a domain of 10 microns. For the initial conditions, I use BoundingBoxIC to make a 4 micron region in the middle of the domain the solid (eta = 1) and everywhere else liquid (eta = 0), and I set the interface width (int_width) to 0.5 microns.\n\nI use a similar BoundingBoxIC for the chemical potential, with the solid region being set to zero (mu = 0), and the liquid region chosen based on the desired level of saturation.\n\nI've tested a variety of mu values for the liquid, including absurdly high values corresponding to concentrations close to the solid, but it makes no difference in the evolution of the system.\nDiffusion in the liquid and solid is on the order of 10^-3 and 10^-5 um^2/us respectively, and chi (partial of omega wrt mu) is on the order of 10^-5. When the mobility of the order parameter is set close to (but still above) the mobility of the chemical potential, nothing evolves at all.\nNot sure how helpful this is, but when I artificially increase the diffusion by a few orders of magnitude, and increase the order parameter mobility (eta) to make sure evolution is diffusion-limited, it produces odd behavior in the order parameter after a large number of steps:\n\nA state that I would think would be impossible. The chemical potential in this case is\n\nAgain, not sure how useful this is. I know there are a lot of random things that can be done to mess with a PF model to produce weird non-physical effects, but maybe it can help with troubleshooting?\nPlease let me know what other information I can give you to help diagnose the problem. Thank you very much for your help!\n-Scott",
                          "url": "https://github.com/idaholab/moose/discussions/17437#discussioncomment-555232",
                          "updatedAt": "2022-07-18T07:11:04Z",
                          "publishedAt": "2021-04-01T00:06:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "Hi Scott, a couple of questions for you:\n-Are you starting from the example file moose/modules/phase_field/test/tests/GrandPotentialPFM/GrandPotentialPFM.i? If so please make sure your MOOSE repo is up to date; there was a bug fix to this implemented a few months ago (see #16343)\n-What are the free energies you are using for the solid and liquid phases?\n-If you set diffusivities in both phases to 1 does the problem persist?\n-If so can you please paste in the solver output from the first time step?",
                          "url": "https://github.com/idaholab/moose/discussions/17437#discussioncomment-555309",
                          "updatedAt": "2022-07-18T07:11:10Z",
                          "publishedAt": "2021-04-01T00:21:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "scottedmuller"
                          },
                          "bodyText": "I developed my model by hand, based mostly on Plapp's formulation from his 2011 paper (http://dx.doi.org/10.1103/PhysRevE.84.031601). I'm using a parabolic free energy for the solid, and I was originally using a free energy derived from the ideal solution model for the liquid, but I interpolated it as a parabolic free energy during troubleshooting, in case there were compatibility issues between the free energies.\nI just tested setting the diffusivities in both phases equal to 1, and the problem actually goes away! For the most part. I tested it with different values for the order parameter L's mobility, and evolution of the system changes quite a bit. When I set both the mobility and both diffusion coefficients to 1, I get the final state as\n\nWhich, as I understand it, is close to what we would expect for the chemical potential mu. The solver output is:\nMesh:\n  Parallel Type:           replicated\n  Mesh Dimension:          1\n  Spatial Dimension:       1\n  Nodes:\n    Total:                 101\n    Local:                 14\n  Elems:\n    Total:                 100\n    Local:                 13\n  Num Subdomains:          1\n  Num Partitions:          8\n  Partitioner:             metis\n\nNonlinear System:\n  Num DOFs:                202\n  Num Local DOFs:          28\n  Variables:               { \"eta\" \"mu\" }\n  Finite Element Types:    \"LAGRANGE\"\n  Approximation Orders:    \"FIRST\"\n\nAuxiliary System:\n  Num DOFs:                1500\n  Num Local DOFs:          195\n  Variables:               { \"local_GP_FE\" \"local_FE\" \"M_mu\" \"chi\" \"drho_eta\" ... \"rho\" \"rhoEq_l\" \"w_l\"\n                             \"w_loc\" \"w_s\" }\n  Finite Element Types:    \"MONOMIAL\"\n  Approximation Orders:    \"CONSTANT\"\n\nExecution Information:\n  Executioner:             Transient\n  TimeStepper:             IterationAdaptiveDT\n  Solver Mode:             Preconditioned JFNK\n  MOOSE Preconditioner:    SMP\n\n\nTime Step 0, time = 0\n\nPostprocessor Values:\n+----------------+----------------+----------------+\n| time           | total_FE       | total_GP_FE    |\n+----------------+----------------+----------------+\n|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n+----------------+----------------+----------------+\n\n\nTime Step 1, time = 1e-08, dt = 1e-08\n\nPerforming automatic scaling calculation\n\n 0 Nonlinear |R| = 3.167606e-06\n      0 Linear |R| = 3.167606e-06\n      1 Linear |R| = 1.755020e-09\n  Linear solve converged due to CONVERGED_RTOL iterations 1\n 1 Nonlinear |R| = 1.755160e-09\nNonlinear solve converged due to CONVERGED_FNORM_ABS iterations 1\n Solve Converged!\n\nPostprocessor Values:\n+----------------+----------------+----------------+\n| time           | total_FE       | total_GP_FE    |\n+----------------+----------------+----------------+\n|   0.000000e+00 |   0.000000e+00 |   0.000000e+00 |\n|   1.000000e-08 |   2.117288e-01 |  -2.158171e-01 |\n+----------------+----------------+----------------+\n\nJust to give more information, if I keep diffusion equal to one and set L = 10, the system evolves to\n\nAnd if L = 100, I get\n\nIf I instead reduce L to 0.1, I get\n\nThe best of these is when L and the diffusivities are equal to 1, which almost feels like blind luck that I happened to find values that behave well. When I return the diffusivity to its original value, or to a constant value on the order to the liquid diffusivity, and then reduce L proportionally, I get the same problem as before where nothing evolves at all. I'm not sure how I can exploit the working model (where L=D=1) to choose a value for L that will work, there must be another piece I'm missing that plays a role in the mobilities' interplay.",
                          "url": "https://github.com/idaholab/moose/discussions/17437#discussioncomment-560352",
                          "updatedAt": "2022-07-18T07:11:10Z",
                          "publishedAt": "2021-04-01T21:55:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "Hey Scott, the example in the repository I mentioned before is also based on Plapp's 2011 paper. So you could compare that to the model you've developed to see if there are any differences in the implementation.\nI think that you may need to make some adjustments to your solver tolerances, time step, or problem scaling. Looking at your solver output, I notice that the original residual at the start of your first time step is |R| = 3.167606e-06. Let me explain what the residual means. When we set the governing equations up, we normally move all terms to the LHS of the equation so that the RHS of the equation is 0. The \"residual\" is what the numerical solver is finding for the RHS of the equation (which we would like to be zero or close to it).\nIn your case, the solver executes one nonlinear iteration and exits with a residual |R| = 1.755160e-09. Normally we would like to see the residual decrease much more than that, like 8 or 9 orders of magnitude decrease. So I think the anomalous results you are getting is because the problem is not being solved to sufficiently high accuracy. Although 10^-9 seems like a small number, what we consider small in this context depends on how your problem is scaled and your time step size. Since you start out with a pretty small residual, it needs to decrease quite a lot more than 3 orders of magnitude.\nFirst thing to do is figure out why the solver is exiting. The solver will exit if either the absolute or relative tolerance criteria is met. The relative tolerance criteria is set by the parameter nl_rel_tol in your input file. For that to succeed, the final residual divided by original residual (at the beginning of the time step) must be < nl_rel_tol. This is how we normally like the solver to exit, and a value of 10^-8 is usually a good value to use. You may also exit if the absolute tolerance criterion is met. This is controlled by the parameter nl_abs_tol; if your residual is smaller than this the solver declares success. The problem with this is that what constitutes a small number varies on your time step size, problem scaling and other factors. My guess of what is happening is that you have a value of something like nl_abs_tol = 2e-9 set such that your solver is exiting prematurely. So if that is the case I would set it to a much lower number or remove that parameter completely (in which case the default value of 1e-50 will be used).\nOnce you figure that out and make sure that your nl_rel_tol is set to a reasonable number, you should be able to increase your time step size significantly. That should increase the initial residual and then when the relative tolerance criterion is met the solve will proceed.\nI think the reason you saw some evolution with L=D=1 and none at all for the smaller numbers that you were previously using is that with smaller L and D, the initial residual was probably below a set value of nl_abs_tol, so the solver declared success at the beginning of the problem and didn't actually do anything.",
                          "url": "https://github.com/idaholab/moose/discussions/17437#discussioncomment-560449",
                          "updatedAt": "2022-07-19T22:22:03Z",
                          "publishedAt": "2021-04-01T22:23:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Mixed element types?",
          "author": {
            "login": "joe61vette"
          },
          "bodyText": "I am using Cubit and for one region I would like to use \"tetmesh\".  When I do, the result is a mixture of \"tets\" and \"pyramids\".  This region would be one block for which I would normally set an element type.  But..\nIs it possibly to run a problem with a MOOSE-based code that has such a mix of element types?  If so, how should I set the element type?\nThanks,\nJoe Kelly",
          "url": "https://github.com/idaholab/moose/discussions/17478",
          "updatedAt": "2022-12-13T22:10:07Z",
          "publishedAt": "2021-03-31T21:30:02Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "You can certainly use MOOSE to run a model containing a mix of element types, and you shouldn't need to set the element type manually - MOOSE will just handle everything automatically.  There are only a few examples of this in the tests, for instance, many of the tests using AnnularMeshGenerator that creates QUAD4 and TRI3 elements in a single mesh.  However,  because of the constraints from the Exodus mesh definition, you're going to have to put your TETs into one block (also called \"subdomain\" in parts of MOOSE) and PYRAMIDs into another block.  By the way, this gives you the freedom of prescribing different physics to the TETs than the PYRAMIDs, if you needed to, although it doesn't sound like you actually want to do that.",
                  "url": "https://github.com/idaholab/moose/discussions/17478#discussioncomment-554883",
                  "updatedAt": "2022-12-13T22:10:07Z",
                  "publishedAt": "2021-03-31T21:39:25Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Thanks Andy.\nUnfortunately, there is no easy way to separate the pyramids from the tets.  The pyramids get automatically generated to bridge from a neighboring region that uses quads. Maybe it is possible to convert the pyramids to tets?  I'll look at doing that.\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17478#discussioncomment-554894",
                          "updatedAt": "2022-12-13T22:11:03Z",
                          "publishedAt": "2021-03-31T21:44:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "So this has become a CUBIT question.  I'm not a CUBIT expert - is there some way to \"select all PYRAMIDs within a region and add them to a block\"?",
                          "url": "https://github.com/idaholab/moose/discussions/17478#discussioncomment-554907",
                          "updatedAt": "2022-12-13T22:11:03Z",
                          "publishedAt": "2021-03-31T21:49:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Thanks again Andy.  I found a way to mesh this to avoid using tetmesh.  So, for now at least, my problem is solved.  May have to revisit this again later as pyramids provide a great transition from quads.  As I have a lot of long relatively thin regions, a map mesh that is swept is generally the best way to go. Unfortunately, there are sometimes thin wedge shaped adjoining regions for which tets are the best.  Pyramids provide the transition but with no clearly defined boundary.\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17478#discussioncomment-557440",
                          "updatedAt": "2022-12-13T22:11:04Z",
                          "publishedAt": "2021-04-01T13:01:42Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "Andy, thanks to your suggestion about finding a command to group all pyramids in a block, I believe that I have found a solution for this.  Namely:\nBlock 1 add volume 1                    # puts all elements of volume 1 in block 1\nBlock 1 remove pyramid all           # removes all pyramid elements from block 1\nBlock 2 add volume 1                   # puts all elements of volume 1 in block 2\nBlock 2 remove tet all                  # removes all tet elements from block 2\nI think that will solve my problem.  You know Cubit has a lot of capability but sometimes it is difficult for a casual user to find it.\nJoe",
                  "url": "https://github.com/idaholab/moose/discussions/17478#discussioncomment-558132",
                  "updatedAt": "2022-12-13T22:11:42Z",
                  "publishedAt": "2021-04-01T15:02:52Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "Hello Again:\nEven better, Cubit can actually do this for you automatically.  Just use:\nset block mixed element output offset\nIt then adds an \"offset\" to the block ID.  One can specify the value of the offset (eg, 1000) for each element type.\nJoe",
                  "url": "https://github.com/idaholab/moose/discussions/17478#discussioncomment-558466",
                  "updatedAt": "2022-12-13T22:11:56Z",
                  "publishedAt": "2021-04-01T15:47:01Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "permcody"
                          },
                          "bodyText": "Yes, I think I remember doing this years ago. Thanks for the reminder Joe!",
                          "url": "https://github.com/idaholab/moose/discussions/17478#discussioncomment-560211",
                          "updatedAt": "2022-12-13T22:12:00Z",
                          "publishedAt": "2021-04-01T21:17:37Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Shooting methods",
          "author": {
            "login": "ke7kto"
          },
          "bodyText": "It seems that this might be a little out of scope, but is there anything like a shooting method within moose, where one could vary, e.g. a material property, with a boundary condition acting as a constraint? I would imagine this could be highly useful, but also a very heavy lift to implement.",
          "url": "https://github.com/idaholab/moose/discussions/17477",
          "updatedAt": "2021-03-31T21:47:24Z",
          "publishedAt": "2021-03-31T20:33:16Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nI have a PR #17420 up for performing small optimization problems like this using the Control system, and more precisely a PID controller in a pseudo-transient calculation.\nMerging is likely to be pretty delayed as I want to add more capability to this but you could try it out.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/17477#discussioncomment-554705",
                  "updatedAt": "2021-03-31T20:49:01Z",
                  "publishedAt": "2021-03-31T20:48:07Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ke7kto"
                          },
                          "bodyText": "I'll take a look. Thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/17477#discussioncomment-554902",
                          "updatedAt": "2021-03-31T21:47:10Z",
                          "publishedAt": "2021-03-31T21:47:10Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Moose install",
          "author": {
            "login": "Draper18"
          },
          "bodyText": "Hello everyone,\nI am trying to install MOOSE on Linux Ubuntu 20.04.2 LTS. I followed the install guide but when I got to run tests at the end they all crashed. In the troubleshooting guide I cannot even get the always ok test to not crash. While working through the trouble shooting guide I was able to get the hello world example to work though. I have tried deactivating and removing MOOSE then going back through the install guide only to end up with the same results. Any help is greatly appreciated.\nThanks,\nAlex Draper",
          "url": "https://github.com/idaholab/moose/discussions/17464",
          "updatedAt": "2022-09-24T11:54:32Z",
          "publishedAt": "2021-03-30T20:10:56Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nCould you please post the error messages you are running into?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/17464#discussioncomment-550071",
                  "updatedAt": "2022-09-24T11:54:36Z",
                  "publishedAt": "2021-03-30T20:23:50Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Draper18"
                  },
                  "bodyText": "Here is the error message when running the tests\nMoose Error messages.txt",
                  "url": "https://github.com/idaholab/moose/discussions/17464#discussioncomment-550516",
                  "updatedAt": "2022-09-24T11:54:40Z",
                  "publishedAt": "2021-03-30T22:52:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "It seems you are missing libquadmath. sudo apt install libquadmath or something close should help",
                  "url": "https://github.com/idaholab/moose/discussions/17464#discussioncomment-550618",
                  "updatedAt": "2022-09-24T11:54:41Z",
                  "publishedAt": "2021-03-30T23:36:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PETSc crash maybe?",
          "author": {
            "login": "makeclean"
          },
          "bodyText": "I have a problem, I#m happy to share - it runs on 1120 MPI tasks when using presplit mode. If I dont pre-split, its a segfault on launch (presumably out of memory). If I run with 20 MPI tasks and 56 threads, it runs until after the 2nd timestep which is great\nmpirun noticed that process rank 5 with PID 226281 on node cpu-p-333 exited on signal 11 (Segmentation fault).\nTime Step 2, time = 0.000585938, dt = 0.000390625\nComputing initial residual ...........................                                     [ 28.34 s]\nUpdating displaced mesh ..........                                                         [ 11.14 s]\n 0 Nonlinear |R| = 5.922167e+02\nUpdating displaced mesh .........                                                          [ 10.98 s]\nAt line 80 of file xerbla.f (unit = 6, file = 'stdout')\nFortran runtime error: Missing initial left parenthesis in format\n\nAt line 80 of file xerbla.f (unit = 6, file = 'stdout')\nFortran runtime error: Missing initial left parenthesis in format\n\nAt line 80 of file xerbla.f (unit = 6, file = 'stdout')\nFortran runtime error: Missing initial left parenthesis in format\n\nAt line 80 of file xerbla.f (unit = 6, file = 'stdout')\nFortran runtime error: Missing initial left parenthesis in format\n\n--------------------------------------------------------------------------\nPrimary job  terminated normally, but 1 process returned\na non-zero exit code. Per user-direction, the job has been aborted.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nmpirun noticed that process rank 5 with PID 226281 on node cpu-p-333 exited on signal 11 (Segmentation fault).\n\nAnyone seen anything like it?",
          "url": "https://github.com/idaholab/moose/discussions/17382",
          "updatedAt": "2022-07-04T07:27:09Z",
          "publishedAt": "2021-03-19T07:48:44Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "makeclean"
                  },
                  "bodyText": "When it does run i get some lovely stress fields",
                  "url": "https://github.com/idaholab/moose/discussions/17382#discussioncomment-501985",
                  "updatedAt": "2022-07-04T07:27:07Z",
                  "publishedAt": "2021-03-19T07:50:16Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "makeclean"
                  },
                  "bodyText": "Ahh its a blas error",
                  "url": "https://github.com/idaholab/moose/discussions/17382#discussioncomment-501992",
                  "updatedAt": "2022-07-04T07:27:20Z",
                  "publishedAt": "2021-03-19T07:53:08Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "Looks like a runtime error trying to print the actual error.\nhttps://www.netlib.org/lapack/explore-3.1.1-html/xerbla.f.html\nMy guess is, something is going horribly wrong way earlier, leading to bogus data going into some blas call.",
                          "url": "https://github.com/idaholab/moose/discussions/17382#discussioncomment-514970",
                          "updatedAt": "2022-07-04T07:27:21Z",
                          "publishedAt": "2021-03-22T20:02:42Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "Did you get the error only when using threads?  How did you install blas?",
                  "url": "https://github.com/idaholab/moose/discussions/17382#discussioncomment-526466",
                  "updatedAt": "2022-07-04T07:27:21Z",
                  "publishedAt": "2021-03-24T19:14:23Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "I couldnt actually run the problem without threads tbh",
                          "url": "https://github.com/idaholab/moose/discussions/17382#discussioncomment-540486",
                          "updatedAt": "2022-07-04T07:27:29Z",
                          "publishedAt": "2021-03-28T13:27:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Hmmm, I guess you should use a distributed mesh.",
                          "url": "https://github.com/idaholab/moose/discussions/17382#discussioncomment-549935",
                          "updatedAt": "2022-07-04T07:27:29Z",
                          "publishedAt": "2021-03-30T19:41:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "It was running with a pre-splti mesh, but crashes, using mpi tasks, but fine with threading",
                          "url": "https://github.com/idaholab/moose/discussions/17382#discussioncomment-549944",
                          "updatedAt": "2022-07-04T07:27:30Z",
                          "publishedAt": "2021-03-30T19:43:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PorousFlow THM poroelastic eqs",
          "author": {
            "login": "AmandaLug"
          },
          "bodyText": "Hi all,\nI'm modeling a THM problem in PorousFlow and two questions came to my mind during the mechanical coupling and I could not make my mind around. I'm sorry if this is too simple and not MOOSE related...\n1.- In the earth_tide_fullsat.i model, FunctionDirichletBC are implemented to model strains. From the governing equations, Dirichlet BCs should be used for displacement and not strain, right?\n2.- The barometric_fully_confined.i model, works fine in that simple set up. However, if gravity is implemented, is not possible to obtain the barometric efficiency of the system anymore? I'm not sure which is the relation between effective stress and density.\nThanks!\nAmanda",
          "url": "https://github.com/idaholab/moose/discussions/17453",
          "updatedAt": "2022-09-16T11:13:46Z",
          "publishedAt": "2021-03-29T19:05:40Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "Hi @AmandaLug ,\nThanks for your questions.  They'll help future readers too!\n\nYou are correct, DirichletBC sets a condition on the displacement.  Because this particular model has only 1 element, this immediately sets the strain.  However, now imagine a real situation with complicated, heterogeneous geology.  We know that earth-tide software such as ETERNA-x will predict strains at all points within the earth, as a function of time.  The simplest thing to assume is that those strains are correct, on-average, throughout your complicated, heterogeneous geology.  At some points, the strains in your geology are a little larger than ETERNA-x, and at other points, the strains are a little smaller, but their mean is the same as the ETERNA-x prediction.  So, we use a suitable DirichletBC on the displacements to enforce the average strain throughout our model to be the same as the ETERNA-x prediction.  Given those DirichletBC, MOOSE will solve for the displacements (and strains) throughout our model, finding some strains are a little larger than average, some are a little smaller, but the mean strain is fixed by the BCs, to be equal to the ETERNA-x strains.  Make sense?\nYou can still compute barometric efficiency when you have gravity.  The setup is the same.  The atmospheric pressure applies a total stress to the top of your model: it applies a FunctionNeumannBC to the displacement variable, and a FunctionDirichletBC to your porepressure variable (which you would have in a realistic situation, but barometric_fully_confined.i does not have).  See atm_tides.i for an example.  Looking at Fig4 of the documentation, i estimate barometric efficiency to be around 0.4 (ratio of orange amplitude to blue amplitude).",
                  "url": "https://github.com/idaholab/moose/discussions/17453#discussioncomment-545719",
                  "updatedAt": "2022-09-16T11:13:46Z",
                  "publishedAt": "2021-03-29T21:23:34Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "AmandaLug"
                          },
                          "bodyText": "Thanks Andy for the great answers! I will need some time to process this.",
                          "url": "https://github.com/idaholab/moose/discussions/17453#discussioncomment-549594",
                          "updatedAt": "2022-09-16T11:13:46Z",
                          "publishedAt": "2021-03-30T18:03:04Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Instance UO inside Material",
          "author": {
            "login": "joe61vette"
          },
          "bodyText": "Hello:\nBeginner developer help needed here.  I want to instance a user object during the initialization of a material object.  To be specific, I have an ADMaterial \"SPWallDragMaterial\" that will produce a material property for wall drag.  I want the user to be able to select from a list of wall drag models.  So, I have a UO base class \"WallDragModel\", that has subclasses for the various models.\nIn SPWallDragMaterial.h, I have the declaration:\nconst WallDragModel & _wd;    // user object for wall drag model\nIn SPWallDragMaterial::validParams, there is a MooseEnum for the various models, along with the necessary input params that will need to be passed to the model UOs.  In the constructor for SPWallDragMaterial, I have:\nconst std::string uo_name = this->name() + \"_wall_drag\";\nstd::string uo_type = \"WallDragModel\";\nInputParameters params = WallDragModel::validParams();\nswitch (_wall_drag_model)\n{\ncase ASP::CONSTANT:\nuo_type = \"ConstantWallDrag\";\nparams.set(\"value\") = getParam(\"value\");\nbreak;\n..... other cases.....\n}\n_fe_problem.addUserObject(uo_type, uo_name, params);\n_wd = _fe_problem.getUserObject(uo_name);\nI included \"FEProblemBase.h\".  I get the following compile error for the last line of code above (along with the error that I did not initialize _wd):\nUsers/joe/projects/ASP/src/materials/SPWallDragMaterial.C:82:7: error: no viable overloaded '='\n_wd = _fe_problem.getUserObject(uo_name);\n/Users/joe/projects/ASP/build/header_symlinks/WallDragModel.h:5:7: note: candidate function (the implicit copy assignment operator) not viable: 'this' argument has type 'const WallDragModel', but method is not marked const\nclass WallDragModel : public GeneralUserObject\n\nWhat is the proper way to instance a UO inside of a Material class?  \n\nThanks for the help,\nJoe Kelly",
          "url": "https://github.com/idaholab/moose/discussions/17418",
          "updatedAt": "2022-06-22T09:09:28Z",
          "publishedAt": "2021-03-23T20:05:54Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "Dear Joe Kelly,\nThe way I use user objects inside materials is shown in this example:\nhttps://github.com/ngrilli/c_pfor_am/blob/main/src/materials/ComputeElasticityTensorCPGrain.C\nIn the class parameters you add:\nparams.addParam(\"read_prop_user_object\",\n\"The GrainPropertyReadFile \"\n\"GeneralUserObject to read element \"\n\"specific property values from file\");\nThen in the constructor:\n_read_prop_user_object(isParamValid(\"read_prop_user_object\")\n? &getUserObject(\"read_prop_user_object\")\n: nullptr),\nThen methods of the user object can be called as:\n_read_prop_user_object->getData(_current_elem, 0);\nIn the header file, there is:\nconst GrainPropertyReadFile * const _read_prop_user_object;\nIn this case you don't need the addUserObject function.\nBest Regards,\nNicol\u00f2 Grilli\nNational University of Singapore",
                  "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-521638",
                  "updatedAt": "2022-06-22T09:09:28Z",
                  "publishedAt": "2021-03-24T08:22:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ngrilli"
                          },
                          "bodyText": "@joe61vette",
                          "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-521640",
                          "updatedAt": "2022-06-22T09:09:27Z",
                          "publishedAt": "2021-03-24T08:22:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Dear Nicolo:\nThanks so much for your reply.  If I understand this correctly, the \"read_prop_user_object\" would have to already exist.  Namely be part of the input file.  Thus it would already be instanced and initialized.  The coding you provided would then \"use\" that UO.  Is that correct?\nWhat I am hoping to do is actually instance the UO inside the material.  So, no UO in the input model.  The input params would actually be read by the material and then passed to the UO when it is instanced.  I would like to follow this approach as a user convenience feature.\nThanks for the help,\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-525155",
                          "updatedAt": "2022-06-22T09:09:19Z",
                          "publishedAt": "2021-03-24T15:03:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "rwcarlsen"
                  },
                  "bodyText": "Would it be better to maybe just make all these things materials?  You could create a material for each drag model - e.g. FooDragModel, BarDragModel, etc.  Then you could have this material create properties with its own name plus a suffix.  The wall drag material itself would then take the name of the drag model material as a parameter - and then add the pre-coded suffixes to retrieve the appropriate values supplied by the drag model.  Something like this:\n[Materials]\n  [drag_model]\n    type = FooDragModel\n    # this material creates properties named \"drag_model_prop1\", \"drag_model_prop2\" - etc.\n  []\n  [wall_drag]\n    type = WallDrag\n    model = drag_model # uses this suffix to calc+retrieve properties \"drag_model_prop1\", \"drag_model_prop2\", etc.\n  []\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-545125",
                  "updatedAt": "2022-06-22T09:09:14Z",
                  "publishedAt": "2021-03-29T18:50:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Thanks Robert.  I think that would work.  I will need to look at it as I would like to instance the drag_model specific materials from the wall_drag material rather than have the user need to put them in the input model.  Wasn't sure if I needed an action for this or not.  In some of the drag_models, there is a significant amount of setup and data to store.  That is why I want them in their own objects.\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-548034",
                          "updatedAt": "2022-06-22T09:09:14Z",
                          "publishedAt": "2021-03-30T12:50:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rwcarlsen"
                          },
                          "bodyText": "If you really want to streamline input file syntax, then an action that sets things up would be the way to go.  MOOSE really isn't designed for MooseObject/system subclasses (nearly all classes/objects in moose) to be created ouside of input files and/or actions.  Anything else you try will just be a bit wonky and won't be a \"first-class\" work-flow.  It's also generally good to try to stick with materials as often as reasonably doable over other objects (userobjects, etc.) - they offer some power/flexibility that can be quite convenient as your code evolves.",
                          "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-548532",
                          "updatedAt": "2022-06-22T09:09:14Z",
                          "publishedAt": "2021-03-30T14:43:01Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Periodic BC on cubit generated, 3D mesh",
          "author": {
            "login": "bhollra"
          },
          "bodyText": "Hello,\nI am working on a steady state 3D solid heat conduction problem. I would like to implement a periodic boundary condition in my model, but adding the following into the [BCs] section of my input file causes the residuals to stagnate well before an appropriate convergence tolerance.\n\n[./Periodic]\n\t[./x]\n\t\tvariable = T_solid\n\t\tprimary = left\n\t\tsecondary = right\n\t\ttranslation = '1 0 0'\n\t[../]\n\t[./y]\n\t\tvariable = T_solid\n\t\tprimary = front\n\t\tsecondary = back\n\t\ttranslation = '0 1 0'\n\t[../]\n[../]\n\nTo demonstrate this, I have created a simpler version of my case where the issue is recreated. In a case where I remove the periodic BC, the solution has no trouble solving, as shown below:\n\nAny insight into any potential mistakes in my implementation or a more appropriate method for implementing a periodic BC for this case would be much appreciated.\nBrent\nAttached are the input file and mesh I am using.\nperiodic_bc.zip",
          "url": "https://github.com/idaholab/moose/discussions/17267",
          "updatedAt": "2022-10-20T05:23:43Z",
          "publishedAt": "2021-03-10T04:10:21Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "bhollra"
                  },
                  "bodyText": "It seems there was an error running the original files I uploaded with MOOSE (I was using SAM to run them with no issues). Attached are updated files, tested with MOOSE only, which reproduce the issue described above.\nperiodic_bc_1.zip",
                  "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-465113",
                  "updatedAt": "2023-09-27T17:20:25Z",
                  "publishedAt": "2021-03-10T20:08:11Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Tagging @permcody @andrsd who should know a lot about periodic BCs",
                  "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-495428",
                  "updatedAt": "2023-09-27T17:20:28Z",
                  "publishedAt": "2021-03-17T19:50:07Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "andrsd"
                  },
                  "bodyText": "Your problem is in your PETSc options. When I comment them out, I get this convergence:\n 0 Nonlinear |R| = 5.744566e+00\n      0 Linear |R| = 5.744566e+00\n      1 Linear |R| = 3.609398e-01\n      2 Linear |R| = 8.977029e-02\n      3 Linear |R| = 1.969694e-02\n      4 Linear |R| = 1.959308e-02\n      5 Linear |R| = 1.350552e-02\n      6 Linear |R| = 1.279056e-03\n      7 Linear |R| = 7.000715e-04\n      8 Linear |R| = 3.249532e-04\n      9 Linear |R| = 8.143903e-05\n     10 Linear |R| = 2.449361e-05\n 1 Nonlinear |R| = 2.449361e-05\n      0 Linear |R| = 2.449361e-05\n      1 Linear |R| = 2.279705e-05\n      2 Linear |R| = 2.255615e-05\n      3 Linear |R| = 2.128266e-05\n      4 Linear |R| = 1.389275e-05\n      5 Linear |R| = 4.483888e-06\n      6 Linear |R| = 9.006490e-07\n      7 Linear |R| = 3.828971e-07\n      8 Linear |R| = 1.659223e-07\n      9 Linear |R| = 7.817783e-08\n     10 Linear |R| = 4.299557e-08\n     11 Linear |R| = 2.047900e-08\n     12 Linear |R| = 8.901540e-09\n     13 Linear |R| = 4.517054e-09\n     14 Linear |R| = 1.658610e-09\n     15 Linear |R| = 7.478177e-10\n     16 Linear |R| = 3.790151e-10\n     17 Linear |R| = 9.637592e-11\n 2 Nonlinear |R| = 9.637587e-11\n Solve Converged!\n\nAnd this result:",
                  "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-499167",
                  "updatedAt": "2023-09-27T17:20:32Z",
                  "publishedAt": "2021-03-18T15:56:47Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "bhollra"
                          },
                          "bodyText": "Thanks for your response. Removing the PETSc options did the trick for me!",
                          "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-535090",
                          "updatedAt": "2023-09-27T17:20:36Z",
                          "publishedAt": "2021-03-26T16:35:54Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mangerij"
                  },
                  "bodyText": "Very nice mesh... what sort of CUBIT options did you use to do this?\nI always have difficulty getting periodic nodes when using the tetrahedrons.",
                  "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-544983",
                  "updatedAt": "2023-09-27T17:20:36Z",
                  "publishedAt": "2021-03-29T18:11:10Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "bhollra"
                          },
                          "bodyText": "I output the blocks using the \"hex8\" element type, but it looks like the actual output is a combination of hexahedral and wedge element types, so I don't have any experience using tetrahedrons with a periodic boundary.\nHere is the .jou file I used if you're interested.\ntest_mesh.txt",
                          "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-545160",
                          "updatedAt": "2023-09-27T17:20:36Z",
                          "publishedAt": "2021-03-29T19:03:45Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}