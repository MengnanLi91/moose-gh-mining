{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0wNC0yOFQxMToyNjowMi0wNTowMM4APaiP"
    },
    "edges": [
      {
        "node": {
          "title": "Solving Eigenvalue Problems within Eigenvalue Executioner",
          "author": {
            "login": "LagrangeW"
          },
          "bodyText": "Respected experts and peers\uff0c\nI'm solving a simple multi-group neutron diffusion problem within Eigenvalue Executioner, while couldn't get correct eigenvalue by imitating the given example input file. Is there some more detailed input syntax guidance of this executioner?\nAny help would be appreciated, thanks!",
          "url": "https://github.com/idaholab/moose/discussions/20239",
          "updatedAt": "2022-06-19T01:40:32Z",
          "publishedAt": "2022-02-06T13:04:15Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThis is the best documentation we have on solving eigenvalue problems.\nhttps://mooseframework.inl.gov/source/problems/EigenProblem.html\nInput file syntax wise, there is this page:\nhttps://mooseframework.inl.gov/source/executioners/Eigenvalue.html\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2126141",
                  "updatedAt": "2022-06-19T01:40:36Z",
                  "publishedAt": "2022-02-07T15:16:35Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "LagrangeW"
                          },
                          "bodyText": "thanks so much for your reply\uff01",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2155222",
                          "updatedAt": "2022-06-19T01:40:37Z",
                          "publishedAt": "2022-02-11T06:52:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "AnthonyB08"
                  },
                  "bodyText": "Hi LagrangeW,\nDid you have any success in modeling multi-group neutron transport? I too am having difficulty with the eigenvalue problem. Mine seems to not want to converge.",
                  "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2642434",
                  "updatedAt": "2022-06-19T01:40:36Z",
                  "publishedAt": "2022-04-26T23:06:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "LagrangeW"
                          },
                          "bodyText": "Hello\uff0c\nI\u2018ve done some simple 2D diffusion benchmarks, and the the calculated value is in good agreement with the reference.\nI met some non-converge problems before also, and the everytime it was found to be input error, I give wrong group constants or group cross section.\nMaybe you can try to change your input parameters.",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2644078",
                          "updatedAt": "2022-08-10T03:07:53Z",
                          "publishedAt": "2022-04-27T06:40:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AnthonyB08"
                          },
                          "bodyText": "When you found your eigenvalue, lambda, how did you go about solving the criticality diffusion problem using MOOSE. All of my simulations come back 0",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2650661",
                          "updatedAt": "2022-08-10T03:07:58Z",
                          "publishedAt": "2022-04-27T23:10:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AnthonyB08"
                          },
                          "bodyText": "I understand that the criticality solves for the differential to be equal to zero, but how does one then analyze the transport for the flux? Is this just not a problem of finding criticality for the respected macroscopic cross-section and diffusion coefficient?",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2650749",
                          "updatedAt": "2022-08-10T03:07:58Z",
                          "publishedAt": "2022-04-27T23:42:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "LagrangeW"
                          },
                          "bodyText": "I use eigenvalue executioner to solve this type of eigenvalue problem, as @GiudGiud suggests, which could solve out eigenvalue together with it's eigenvector, or neutron flux.",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2651272",
                          "updatedAt": "2022-08-10T03:08:16Z",
                          "publishedAt": "2022-04-28T02:23:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AnthonyB08"
                          },
                          "bodyText": "One last question Largrange W, what kernel did you use to couple to two groups together? This is my governing problem",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2663822",
                          "updatedAt": "2022-08-10T03:08:17Z",
                          "publishedAt": "2022-04-29T20:09:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "LagrangeW"
                          },
                          "bodyText": "I think diffusion kernel https://mooseframework.inl.gov/source/kernels/Diffusion.html can be used in neutron diffusion term(leakage term), and reaction term https://mooseframework.inl.gov/source/kernels/Reaction.html can be used in other terms (removal, fission and scattering).",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2677391",
                          "updatedAt": "2022-08-10T03:08:18Z",
                          "publishedAt": "2022-05-03T01:31:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Decoupled Navier-Stokes and energy equations using multiapp",
          "author": {
            "login": "am-tc01"
          },
          "bodyText": "Hi,\nI am trying to solve steady state Navier Stokes and energy equations in a decoupled manner using multiapp such that the main app solves for the temperature (T) and the sub-app solves for the velocity (v) and pressure (P). The driving force in my case is temperature (like in welding or AM) so the two systems are tightly coupled i.e. the temperature distribution depends on the flow field, while the flow field is driven by the gradient in temperature (Marangoni effect). My question now is, given that it is a steady state problem, do I need one sub-app like I described above or two sub-apps in the following way:\n\nMain app solves for T without v, transfers T to sub-app\nSub-app solves for v and P with T as known, transfers v to sub-sub-app\nSub-sub-app solves for T with v as known\n\nThanks!",
          "url": "https://github.com/idaholab/moose/discussions/20887",
          "updatedAt": "2022-06-05T05:27:11Z",
          "publishedAt": "2022-04-27T10:30:07Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou dont need to have 2 subapps or 3 apps in total. You just need to iterate this:\n\nMain app solves for T with v (starts with initial guess), transfers T to sub-app\nSub-app solves for v and P with T as known, transfers v to main app\n\nEven if the driving force is T, you can solve for velocity as the main app and solve for T in the subapp btw. The decision on subapp/main app depends on which app needs to take the smallest time steps.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2649980",
                  "updatedAt": "2022-06-05T05:27:13Z",
                  "publishedAt": "2022-04-27T20:31:36Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "am-tc01"
                          },
                          "bodyText": "Hi Guillaume,\nThanks! Given the fact that it's a steady state problem, would it still work with 1 subapp? Do I need Fixed Point iteration in that case?",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2652432",
                          "updatedAt": "2022-06-05T05:27:14Z",
                          "publishedAt": "2022-04-28T07:14:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You still need fixed point in that problem. Fluid flow is often easier to solve with relaxation-to-steady-state transients. DId you get a solution for steady state already?",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2655420",
                          "updatedAt": "2022-06-05T05:27:14Z",
                          "publishedAt": "2022-04-28T14:46:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "am-tc01"
                          },
                          "bodyText": "Yes I did get a solution with steady state but the convergence for the fluid flow app is really slow.\nI am not sure if I understood what you meant by \"relaxation-to-steady-state transients\"? Solve the steady state fluid flow as a transient problem with steady_state_detection, or?\nBR.",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2662875",
                          "updatedAt": "2022-06-05T05:27:14Z",
                          "publishedAt": "2022-04-29T16:56:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yes that's the idea. The flow can reach the steady solution through a transient.\nThis generally works better than trying to solve the steady problem directly.",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2674349",
                          "updatedAt": "2022-06-12T05:00:59Z",
                          "publishedAt": "2022-05-02T15:04:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "am-tc01"
                          },
                          "bodyText": "I see. But I assume it would be slower than solving for steady state directly, given that one can actually get a solution with steady state, right? I should try it anyway. Thanks for the tip!",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2676102",
                          "updatedAt": "2022-06-12T05:00:59Z",
                          "publishedAt": "2022-05-02T19:46:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yes it's slower, but it ll work more reliably.\ngood luck, let us know how the coupling goes!",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2676126",
                          "updatedAt": "2022-06-12T05:00:59Z",
                          "publishedAt": "2022-05-02T19:50:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Execution of RVE sub-model for all macro-gauss points",
          "author": {
            "login": "abarun22"
          },
          "bodyText": "Dear all,\nI am working on a computational homogenization scheme where i am required to run my RVE sub-model for each of the macro gauss point and return the homogenized stress measures to the relevant gauss points. For a mesh with 100 elements and 8 GP's per element, i may need to launch 800 sub-apps for performing the computation and each of them will return the results to the individual gauss points. Obviously it's computationally intensive, but parallelization might help here. I wonder whether any activity of this kind has been attempted before. I welcome any related ideas/suggestions here that would help me come up with right simulation framework for my model.\nKind regards,\nArun",
          "url": "https://github.com/idaholab/moose/discussions/20213",
          "updatedAt": "2022-06-14T20:47:44Z",
          "publishedAt": "2022-02-02T18:56:47Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "hello\nThe multiapps system is designed for this. it should spread the multiapps across the processes nicely.\nOne thing that can help performance is to create the smallest type of app possible. So if you can limit your subapp to use a single MOOSE module, heat conduction for example, then specifying app_type=HeatConductionApp to the Multiapp will speed things up.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2099350",
                  "updatedAt": "2022-06-14T20:47:45Z",
                  "publishedAt": "2022-02-02T19:44:24Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Guillaume,\nThanks for the suggestions. Concerning my requirements, i guess 'CentroidMultiApp' is the closest one i can think of doing the job, but however executed only at the element centroids. I want the apps to be called at the element gauss points instead of the centroids and so can i look at adapting 'CentroidMultiApp' to implement this scenario. Also i need a transfer facility to copy the stress tensor at each gauss point on to the sub-apps and return the homogenized stress tensor from the sub-apps on to the macro gauss points.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2105368",
                          "updatedAt": "2022-06-14T20:47:45Z",
                          "publishedAt": "2022-02-03T17:12:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYes you will have to create a QuadraturePointMultiApp I think.\nSome of the trasnfers will still work if you are working with quadrature points instead of centroids.\nFor example, towards the multiapp, I suppose you will use this one https://mooseframework.inl.gov/source/transfers/MultiAppVariableValueSamplePostprocessorTransfer.html, and since it is based on the location of the multiapp, it should work.\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2105810",
                          "updatedAt": "2022-06-14T20:47:45Z",
                          "publishedAt": "2022-02-03T18:14:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "I started with QuadraturePointMultiApp. Towards that i am looking at methods to access the spatial locations of the quadrature points in the whole domain. Quite a few approaches are available to access them, but most of them start with solving a system of equations which eventually branch out to objects relating to the QP locations. As far as i know there aren't any direct methods which can do this job, say for example a start from the mesh connectivity and then reaching out to individual QP locations.\nThe following code might do the trick in a straight forward manner, but for some reasons i ended up with a seg fault, when it reaches get_elem().\n  System & system = _fe_problem.getNonlinearSystemBase().system();\n  std::unique_ptr<FEMContext> mesh_context = libmesh_make_unique<FEMContext>(system);\n  unsigned short dim = mesh_context->get_elem().dim();\n  FEBase * fe_mesh = nullptr;\n  mesh_context->get_element_fe(0, fe_mesh, dim);\n  std::vector<Point> quad_point = fe_mesh->get_xyz();\n\nAny ideas on getting this done in a neater way is most welcome.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2157326",
                          "updatedAt": "2022-07-05T19:00:03Z",
                          "publishedAt": "2022-02-11T13:28:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nSo what kind of variable are you using? If the quadrature is low-order enough you could possibly get away with using the nodes. It's just a thought though.\nMost likely, what you need to compute quadrature points is to \"reinit\" at quadrature points.\nSee an example here:\n\n  \n    \n      moose/framework/src/base/Assembly.C\n    \n    \n         Line 3817\n      in\n      1bf7c59\n    \n  \n  \n    \n\n        \n          \n           Assembly::elementVolume(const Elem * elem) const \n        \n    \n  \n\n\nthere should be examples starting from just an element, which is what you are probably having a 'for' loop on in this context.\nI ll see if i find one.\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2170733",
                          "updatedAt": "2022-07-11T09:16:37Z",
                          "publishedAt": "2022-02-14T05:43:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "yeah better example here:\n\n  \n    \n      moose/framework/src/loops/ComputeUserObjectsThread.C\n    \n    \n         Line 115\n      in\n      4ee8eed\n    \n  \n  \n    \n\n        \n          \n           _fe_problem.reinitElem(elem, _tid); \n        \n    \n  \n\n\nthis is while looping over elements for user objects. Every time we 'enter' a new element, we 'reinit' it, which computes the quadrature points.\nIn which routine are you adding code exactly?",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2170743",
                          "updatedAt": "2022-07-11T09:16:36Z",
                          "publishedAt": "2022-02-14T05:45:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "As was the case conventionally, i want this implementation to be based on quadrature points alone. I tried the first approach as outlined in Assembly.C, but to my dismay the quadrature points object (q_points) was never populated with QP locations. Please below the related code snippet.\n  MooseMesh & master_mesh = _fe_problem.mesh();\n  auto & mesh = master_mesh.getMesh();\n  for (auto & elem : mesh.active_local_element_ptr_range()){\n    FEType fe_type(elem->default_order(),LAGRANGE);\n    std::unique_ptr<FEBase> fe(FEBase::build(elem->dim(), fe_type));\n    const std::vector<Point> & q_points = fe->get_xyz();\n\nAre you sure the second approach works? I do not see any lines of code that calculates the coordinates whenever we reinit elements.\nI am adding this code in a new source file QuadraturePointMultiapp.C which is an exact replica of CentroidMultiApp.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2182976",
                          "updatedAt": "2022-07-11T09:20:36Z",
                          "publishedAt": "2022-02-15T18:02:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYou dont pass enough information to get qps in that construction call. Only the element order and dimension are passed.\nYou need to re-init the FEBase with the element.\nSee https://libmesh.github.io/doxygen/classlibMesh_1_1FEGenericBase.html\nand this function, the reinit for example\nreinit\u00a0(const\u00a0Elem\u00a0*elem, const std::vector<\u00a0Point\u00a0> *const pts=nullptr, const std::vector<\u00a0Real\u00a0> *const weights=nullptr)=0\n\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2183737",
                          "updatedAt": "2022-07-11T09:20:45Z",
                          "publishedAt": "2022-02-15T20:17:58Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Hi Guillaume,\nThe code below works.\n MooseMesh & master_mesh = _fe_problem.mesh();\n  auto & mesh = master_mesh.getMesh();\n  for (auto & elem : mesh.active_local_element_ptr_range()){\n    const FEFamily mapping_family = FEMap::map_fe_type(*elem);\n    FEType fe_type(elem->default_order(),mapping_family);\n    std::unique_ptr<FEBase> fe(FEBase::build(elem->dim(), fe_type));\n    const std::vector<Point> & q_points = fe->get_xyz();\n    const int extraorder = 0;\n    std::unique_ptr<QBase> qrule (fe_type.default_quadrature_rule (2, extraorder));\n    fe->attach_quadrature_rule (qrule.get());\n    fe->reinit(elem);\n     for (auto i=0; i<q_points.size(); ++i){ \n      _positions.push_back(q_points[i]);\n}\n  }\n\nThe key here is the positioning of reinit and get_xyz().\nKind regards,\nArun",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2199099",
                  "updatedAt": "2022-07-11T09:16:30Z",
                  "publishedAt": "2022-02-17T18:09:53Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Guillaume,\nI am currently working on a transfer scheme very similar to 'MultiAppVariableValueSamplePostprocessorTransfer' but for a tensor value across the multiapp system. The issue here being that this tensor (deformation gradient) is to be extracted for a collection of quadrature points across whole of the mesh and these are required to be transferred to individual sub-apps fixed to each quadrature point. I have the following questions in mind that needs clarification before i move on to the implementation phase.\n\nThe deformation gradient for all quadrature points can be stored in a single array like defgrad_tensor(3,3,tnqp) and then transfer individual tensors to the sub-apps with the following segment of code.\n\nfor (unsigned int i = 0; i < _multi_app->numGlobalApps(); i++){\nif (_multi_app->hasLocalApp(i))\n          _multi_app->appProblemBase(i).setPostprocessorValueByName(_postprocessor_name, defgrad_tensor(3,3,i));\n}\n\nI do not want to go this way as this array will grow in size when large meshes are used. Are there any efficient approaches we have in place to map the individual tensors on to the problem base in the sub-apps without having to go for bulky arrays. Some sort of functional mapping or a variable substitution method that can connect the tensors and sub-app problem base would be beneficial here.\n\nThis question mainly depend on the type of approach we choose for data transfer. How do we consolidate the collection of deformation gradients, given that these are material properties and are only referenced at a particular quadrature point when called from material routines? Could we compile them together for the whole set of integration points from a single place?\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2295815",
                          "updatedAt": "2022-07-11T09:21:07Z",
                          "publishedAt": "2022-03-04T15:16:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "So this transfer goes:\nvariable (single valued field) to postprocessor (single value for an app)\nAnd you want to go:\ntensor (multi-valued field, 9 values?) to a few postprocessors (they can only hold one value)?\nYou could consider using a vectorpostprocessor on the receiving hand, would cut down on the number of transfers\nHow bulky do you expect these arrays that you want to transfer at each Qp ?\nIf you want to store the values only once, that's a no-go. Subapp should have its own copy, not retrieve it from the main app every time. Subapps do not know that they are subapps, they behave exactly as if they were the main application.\nSo the deformation gradients are material properties?\nMaterial properties typically are re-computed every time you consider a new element. They are not stored for the whole domain at once. You could transfer each component of the tensor to an auxiliary variable.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2298824",
                          "updatedAt": "2022-07-11T09:21:10Z",
                          "publishedAt": "2022-03-05T03:21:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Guillaume,\nYes i want deformation gradient tensor(3x3=9 values, yes they are material properties) to be passed on to whatever sub-app data fields. In return i want PK1 stress tensor (3x3) to be passed from individual sub-apps to the macro quadrature points.  I see that 'MultiAppVariableValueSamplePostprocessorTransfer' does not implement the return transfer for some reasons. So you say that each set of tensor specific to a quadrature point can be saved on to an auxiliary variable in the sub-app side? Given below is a typical input file that is supposed to run on all the sub-apps. I am interested in setting the parameter 'targets' with the tensor values we are transferring from the macro quadrature points. Currently this has been done manually within the inputfile through functions . Can this be done more efficiently collecting the values from all QP's?\n# 3D test with just strain control\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n  large_kinematics = true\n  constraint_types = 'strain strain strain strain strain strain strain strain strain'\n  ndim = 3\n  macro_gradient = hvar\n[]\n\n[Mesh]\n  [base]\n    type = FileMeshGenerator\n    file = '3d.e'\n  []\n\n  [sidesets]\n    type = SideSetsFromNormalsGenerator\n    input = base\n    normals = '-1 0 0\n                1 0 0\n                0 -1 0\n                0 1 0\n              '\n              ' 0 0 -1\n                0 0  1  '\n    fixed_normal = true\n    new_boundary = 'left right bottom top back front'\n  []\n[]\n\n[Variables]\n  [disp_x]\n  []\n  [disp_y]\n  []\n  [disp_z]\n  []\n  [hvar]\n    family = SCALAR\n    order = NINTH\n  []\n[]\n\n[AuxVariables]\n  [s11]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s21]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s31]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s12]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s22]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s32]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s13]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s23]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s33]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n\n  [F11]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F21]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F31]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F12]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F22]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F32]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F13]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F23]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F33]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n[]\n\n[AuxKernels]\n  [s11]\n    type = RankTwoAux\n    variable = s11\n    rank_two_tensor = pk1_stress\n    index_i = 0\n    index_j = 0\n  []\n  [s21]\n    type = RankTwoAux\n    variable = s21\n    rank_two_tensor = pk1_stress\n    index_i = 1\n    index_j = 0\n  []\n  [s31]\n    type = RankTwoAux\n    variable = s31\n    rank_two_tensor = pk1_stress\n    index_i = 2\n    index_j = 0\n  []\n  [s12]\n    type = RankTwoAux\n    variable = s12\n    rank_two_tensor = pk1_stress\n    index_i = 0\n    index_j = 1\n  []\n  [s22]\n    type = RankTwoAux\n    variable = s22\n    rank_two_tensor = pk1_stress\n    index_i = 1\n    index_j = 1\n  []\n  [s32]\n    type = RankTwoAux\n    variable = s32\n    rank_two_tensor = pk1_stress\n    index_i = 2\n    index_j = 1\n  []\n  [s13]\n    type = RankTwoAux\n    variable = s13\n    rank_two_tensor = pk1_stress\n    index_i = 0\n    index_j = 2\n  []\n  [s23]\n    type = RankTwoAux\n    variable = s23\n    rank_two_tensor = pk1_stress\n    index_i = 1\n    index_j = 2\n  []\n  [s33]\n    type = RankTwoAux\n    variable = s33\n    rank_two_tensor = pk1_stress\n    index_i = 2\n    index_j = 2\n  []\n\n  [F11]\n    type = RankTwoAux\n    variable = F11\n    rank_two_tensor = deformation_gradient\n    index_i = 0\n    index_j = 0\n  []\n  [F21]\n    type = RankTwoAux\n    variable = F21\n    rank_two_tensor = deformation_gradient\n    index_i = 1\n    index_j = 0\n  []\n  [F31]\n    type = RankTwoAux\n    variable = F31\n    rank_two_tensor = deformation_gradient\n    index_i = 2\n    index_j = 0\n  []\n  [F12]\n    type = RankTwoAux\n    variable = F12\n    rank_two_tensor = deformation_gradient\n    index_i = 0\n    index_j = 1\n  []\n  [F22]\n    type = RankTwoAux\n    variable = F22\n    rank_two_tensor = deformation_gradient\n    index_i = 1\n    index_j = 1\n  []\n  [F32]\n    type = RankTwoAux\n    variable = F32\n    rank_two_tensor = deformation_gradient\n    index_i = 2\n    index_j = 1\n  []\n  [F13]\n    type = RankTwoAux\n    variable = F13\n    rank_two_tensor = deformation_gradient\n    index_i = 0\n    index_j = 2\n  []\n  [F23]\n    type = RankTwoAux\n    variable = F23\n    rank_two_tensor = deformation_gradient\n    index_i = 1\n    index_j = 2\n  []\n  [F33]\n    type = RankTwoAux\n    variable = F33\n    rank_two_tensor = deformation_gradient\n    index_i = 2\n    index_j = 2\n  []\n[]\n\n[UserObjects]\n  [integrator]\n    type = HomogenizationConstraintIntegral\n    targets = 'strain11 strain21 strain31 \n\t             strain12 strain22 strain32 \n\t\t     strain13 strain23 strain33'\n    execute_on = 'initial linear'\n  []\n[]\n\n[Kernels]\n  [sdx]\n    type = HomogenizedTotalLagrangianStressDivergence\n    variable = disp_x\n    component = 0\n  []\n  [sdy]\n    type = HomogenizedTotalLagrangianStressDivergence\n    variable = disp_y\n    component = 1\n  []\n  [sdz]\n    type = HomogenizedTotalLagrangianStressDivergence\n    variable = disp_z\n    component = 2\n  []\n[]\n\n[ScalarKernels]\n  [enforce]\n    type = HomogenizationConstraintScalarKernel\n    variable = hvar\n    integrator = integrator\n  []\n[]\n\n[Functions]\n  [strain11]\n    type = ParsedFunction\n    value = '8.0e-2*t'\n  []\n  [strain22]\n    type = ParsedFunction\n    value = '-4.0e-2*t'\n  []\n  [strain33]\n    type = ParsedFunction\n    value = '8.0e-2*t'\n  []\n  [strain23]\n    type = ParsedFunction\n    value = '2.0e-2*t'\n  []\n  [strain13]\n    type = ParsedFunction\n    value = '-7.0e-2*t'\n  []\n  [strain12]\n    type = ParsedFunction\n    value = '1.0e-2*t'\n  []\n  [strain32]\n    type = ParsedFunction\n    value = '1.0e-2*t'\n  []\n  [strain31]\n    type = ParsedFunction\n    value = '2.0e-2*t'\n  []\n  [strain21]\n    type = ParsedFunction\n    value = '-1.5e-2*t'\n  []\n  [zero]\n    type = ConstantFunction\n    value = 0\n  []\n[]\n\n[BCs]\n  [Periodic]\n    [x]\n      variable = disp_x\n      auto_direction = 'x y z'\n    []\n    [y]\n      variable = disp_y\n      auto_direction = 'x y z'\n    []\n    [z]\n      variable = disp_z\n      auto_direction = 'x y z'\n    []\n  []\n\n  [fix1_x]\n    type = DirichletBC\n    boundary = \"fix_all\"\n    variable = disp_x\n    value = 0\n  []\n  [fix1_y]\n    type = DirichletBC\n    boundary = \"fix_all\"\n    variable = disp_y\n    value = 0\n  []\n  [fix1_z]\n    type = DirichletBC\n    boundary = \"fix_all\"\n    variable = disp_z\n    value = 0\n  []\n\n  [fix2_x]\n    type = DirichletBC\n    boundary = \"fix_xy\"\n    variable = disp_x\n    value = 0\n  []\n  [fix2_y]\n    type = DirichletBC\n    boundary = \"fix_xy\"\n    variable = disp_y\n    value = 0\n  []\n\n  [fix3_z]\n    type = DirichletBC\n    boundary = \"fix_z\"\n    variable = disp_z\n    value = 0\n  []\n[]\n\n[Materials]\n  [elastic_tensor_1]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 100000.0\n    poissons_ratio = 0.3\n    block = '1'\n  []\n  [elastic_tensor_2]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 120000.0\n    poissons_ratio = 0.21\n    block = '2'\n  []\n  [elastic_tensor_3]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 80000.0\n    poissons_ratio = 0.4\n    block = '3'\n  []\n  [elastic_tensor_4]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 76000.0\n    poissons_ratio = 0.11\n    block = '4'\n  []\n  [compute_stress]\n    type = ComputeLagrangianLinearElasticStress\n  []\n  [compute_strain]\n    type = ComputeLagrangianStrain\n    homogenization_gradient_names = 'homogenization_gradient'\n  []\n  [compute_homogenization_gradient]\n    type = ComputeHomogenizedLagrangianStrain\n  []\n[]\n\n[Preconditioning]\n  [smp]\n    type = SMP\n    full = true\n  []\n[]\n\n[Postprocessors]\n  [s11]\n    type = ElementAverageValue\n    variable = s11\n    execute_on = 'initial timestep_end'\n  []\n  [s21]\n    type = ElementAverageValue\n    variable = s21\n    execute_on = 'initial timestep_end'\n  []\n  [s31]\n    type = ElementAverageValue\n    variable = s31\n    execute_on = 'initial timestep_end'\n  []\n  [s12]\n    type = ElementAverageValue\n    variable = s12\n    execute_on = 'initial timestep_end'\n  []\n  [s22]\n    type = ElementAverageValue\n    variable = s22\n    execute_on = 'initial timestep_end'\n  []\n  [s32]\n    type = ElementAverageValue\n    variable = s32\n    execute_on = 'initial timestep_end'\n  []\n  [s13]\n    type = ElementAverageValue\n    variable = s13\n    execute_on = 'initial timestep_end'\n  []\n  [s23]\n    type = ElementAverageValue\n    variable = s23\n    execute_on = 'initial timestep_end'\n  []\n  [s33]\n    type = ElementAverageValue\n    variable = s33\n    execute_on = 'initial timestep_end'\n  []\n\n  [F11]\n    type = ElementAverageValue\n    variable = F11\n    execute_on = 'initial timestep_end'\n  []\n  [F21]\n    type = ElementAverageValue\n    variable = F21\n    execute_on = 'initial timestep_end'\n  []\n  [F31]\n    type = ElementAverageValue\n    variable = F31\n    execute_on = 'initial timestep_end'\n  []\n  [F12]\n    type = ElementAverageValue\n    variable = F12\n    execute_on = 'initial timestep_end'\n  []\n  [F22]\n    type = ElementAverageValue\n    variable = F22\n    execute_on = 'initial timestep_end'\n  []\n  [F32]\n    type = ElementAverageValue\n    variable = F32\n    execute_on = 'initial timestep_end'\n  []\n  [F13]\n    type = ElementAverageValue\n    variable = F13\n    execute_on = 'initial timestep_end'\n  []\n  [F23]\n    type = ElementAverageValue\n    variable = F23\n    execute_on = 'initial timestep_end'\n  []\n  [F33]\n    type = ElementAverageValue\n    variable = F33\n    execute_on = 'initial timestep_end'\n  []\n[]\n\n[Executioner]\n  type = Transient\n\n  solve_type = 'newton'\n  line_search = none\n\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n\n  l_max_its = 2\n  l_tol = 1e-14\n  nl_max_its = 20\n  nl_rel_tol = 1e-8\n  nl_abs_tol = 1e-10\n\n  start_time = 0.0\n  dt = 0.2\n  dtmin = 0.2\n  end_time = 0.2\n[]\n\n[Outputs]\n#  file_base = strain_3d\n  exodus = true\n  csv = true\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2318782",
                          "updatedAt": "2022-07-11T09:21:09Z",
                          "publishedAt": "2022-03-08T18:36:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so there is no material property transfers as far as I know.\nSo copy everything to aux variables then transfer.\nOr you'll have to code something to make material property transfers work.\nIn the direction:\nauxvariable -> postprocessor (so main app to subapp, from the material property first) use\nhttps://mooseframework.inl.gov/source/transfers/MultiAppVariableValueSamplePostprocessorTransfer.html\npostprocessor -> auxvariable (so subapp to main app, use\nhttps://mooseframework.inl.gov/source/transfers/MultiAppPostprocessorInterpolationTransfer.html",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2319862",
                          "updatedAt": "2022-11-10T23:41:21Z",
                          "publishedAt": "2022-03-08T22:10:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "I guess the material property transfers are really hard to implement. Howver i am currently looking in to the other option you mentioned.\nI am kind of caught up at how to copy the deformation gradient (present in ComputeFiniteStrain.C) on to an auxvariable. Do you see the tensors to be copied seperately for each quadrature point or put everything in a single array and copy once. The possibility of later is debatable given that ComputeFiniteStrain::computeProperties() is called in an outer loop of elements and therefore collection in a single array will be tricky. Once we are clear of this method, we could then think about altering MultiAppVariableValueSamplePostprocessorTransfer and MultiAppPostprocessorInterpolationTransfer to transfer the data across the multiapps.\nIf you could provide some examples of copying material properties on to auxvariables, i would be pleased.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2373004",
                          "updatedAt": "2022-11-10T23:41:21Z",
                          "publishedAt": "2022-03-16T13:49:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "I further see another problem while returning the homogenized stresses from the sub model to the macro quadrature points. A look at the material module below suggest that i should be reading and writing the stresses at the class ComputeFiniteStrainElasticStress\n[Materials]\n  [./elasticity_tensor]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 2.1e5\n    poissons_ratio = 0.3\n  [../]\n  [./stress]\n    type = ComputeFiniteStrainElasticStress\n  [../]\n[]\n\n\nThe issue here being that we are reading (the input stress tensor) and writing (homogenized stresses) at the same place in the macro model and more importantly through a single variable _stress[_qp] which creates a problem while calling the sub-model. After reading this tensor we may want to transfer focus to the sub-model, compute the homogenized stress tensor and return to the main model and use that to overwrite _stress[qp]. This action must repeat for all of the integration points in the macro model. My first question here is that how do we break from the main model at this place, run sub-model  and return to the same place to resume the macro computation? Could we make use of EXEC_NONLINEAR to implement this? How about usage of custom transfers ? Will that be of any help here? Another problem  with this approach being the difficulty associated with the parallelization, as we are addressing the workflow QP by QP and never see the creation of multiple number of apps.\nAlternatively we could collect the tensors for all of the QP's, create equivalent number of multiapps, solve the sub-models and return the homogenized stresses to _stress[qp]. This way the parallelization is ensured. The main problem i see here is with regarding to the re-writing of stresses as we are already out of this location where this should happen. But i'm sure there should be a workaround to this, possibly by function overriding or something similar.\nAny suggestion/pointers on adopting the right approach here would be extremely helpful.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2416602",
                          "updatedAt": "2022-11-10T23:41:22Z",
                          "publishedAt": "2022-03-22T17:32:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Just thought i should make further progress in to this deadlock. The intention here is to read and over-write the stresses (in ComputeFiniteStrainElasticStress) within the scope of a macro timestep. For this to happen we need (a) collection of stresses from the macro model (b) a proper transfer mechanism (c) proper choice of a control parameter (execute_on) in QuadraturePointMultiapp that executes the sub-model upon occurence of a certain action. We can somehow come up with an idea to implement (a) and (b). My concern is that what parameter we should use in (c) to signal the execution of sub-model. Do you think CUSTOM or NONLINEAR options for 'execute_on' will be of any help here. This is very much essential as it not only initiates the sub-model at the right place, but also help us maintain MOOSE's natural way of working, especially not disturbing the inbuilt parallelization capabilities of the multi-apps system. Any suggestions here would be much appreciated.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2480431",
                          "updatedAt": "2022-11-10T23:41:58Z",
                          "publishedAt": "2022-03-31T18:05:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nOk seems I missed three messages here.\n1)\nYou're working with a matrix here right? For the tensor.\nIf so then this auxkernel\nhttps://mooseframework.inl.gov/source/auxkernels/MaterialRankTwoTensorAux.html\nwill copy the material property into an auxvariable.\nThen\n\nMultiAppVariableValueSamplePostprocessorTransfer will do the transfer of the auxvariable value at each point to a postprocessor\nMultiAppPostprocessorInterpolationTransfer can be used to send data back (as postprocessors) from the subapp to an auxvariable\n\n\nThere's a lot to unpack there. I ll try to provide some elements.\n\nAre you expecting to have to converge this coupling at every non linear iteration? Like the stress the main app send to the subapp influences the stress received from the subapp, and it impacts the stress in the main app ?\nOr is this a one-off calculation where the stress in the main app is simply overwritten, then you move on. I think in that case you do not want to run on EXEC_NONLINEAR, simply on TIMESTEP_END.\nI would not worry about parallelization for this, MOOSE will take care of running each subapp in an orderly fashion, at every quadrature point one after the other, using as many processes in parallel as it can (given the arguments to mpirun)\nYou want one multiapp, with subapps at every quadrature point.\nMy main concern is actually writing to _stress. I dont think we'll be able to do that in a straight forward way. You ll definitely have to dig in the code to make it happen. Possibly write in a if statement in the stress computation : if(before multiapps, do this, else: load this)\n\n\n\n(a) and (b) should be OK now\n(c) it depends on your answer for the question above",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2481448",
                          "updatedAt": "2022-11-10T23:42:04Z",
                          "publishedAt": "2022-03-31T21:16:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Yes we are dealing mainly with tensors essentially the matrices. I think MaterialRankTwoTensorAux might be a good reference to carry the material properties on to auxvariables. I do not think the coupling happens in the midst of some non-linear iterations. It would be a one-off calculation with an intention to overwrite the macro stresses with the homogenized stresses from the sub-model. It can possibly happen with 'TIMESTEP_END' parameter in QuadraturePointMultiapp. When this object is called you are ready to create the sub-apps for equivalent number of integration points and you would have collected the stresses that needs transfer across these apps. These sub-apps are now in a position to return the stresses back to the integration points. The issue now is we have now lost access to the material module (ComputeFiniteStrainElasticStress) where the over-writing should happen. This is where the purpose of if condition for determining the status of multiapps will not work, as we will never be able to get in to the element computation loop. But we could do a workaround here. If we understand how these stresses are used for further calculation before the start of next time step we might try and develop that additional bit of code and proceed with further timesteps. In my opinion this will be a highly unlikely task given how implicity MOOSE is handling those element computations behind the scenes. The main problem being that we are out of the element loop that does the basic maths such as element integrations and extrapolations needed for the nodal stress calculations. These are my thoughts and i welcome your suggestions here.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2487224",
                          "updatedAt": "2022-11-10T23:42:23Z",
                          "publishedAt": "2022-04-01T16:43:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I dont think you need to worry about the basic math acts here. You ll just need to create a new material to handle this switch between values from the main app and values of stress from the subapps, stored in aux variables.\nHow far along are you? Do you have the:\n\nsaving stress in auxvariables\nquadraturepoint multiapp\ntransfers of stress to the subapp\ntransfers of stress to the main app\n?",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2487657",
                          "updatedAt": "2022-11-10T23:42:28Z",
                          "publishedAt": "2022-04-01T17:49:32Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "I have a question for a similar problem, however my mesh is exactly the\nsame for the rve and the only thing that changes from rve to rve are the\nBCS which come from the master app. It seems an awful waste to make n qp\nsub apps for such an application. It seems to me that perhaps we could\nconsolidate all it into a single subapp and.lrescibe BCS through the\ntransfer mechanism\n\nAny suggestions are most welcome.\n\nCheers\n\u2026\nOn Fri, Apr 29, 2022, 11:16 AM Guillaume Giudicelli < ***@***.***> wrote:\n If you want to avoid using 48 auxkernels you will have to use an array\n auxvariables, or seriously hack auxkernels to make them act on more than\n one variable. The latter is not recommended.\n\n You can write an array auxkernel to work on array auxvariables. The build\n array variable aux is actually a good example of that\n\n I dont think there's a setVectorPostprocessorByName. I think looking at\n Reporters is a good idea, it's better at dealing with miscealenous data\n types like a vector here.\n\n https://mooseframework.inl.gov/source/transfers/MultiAppReporterTransfer.html\n is able to transfer vectorpostprocessors, treating them as reporters\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#20213 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ACYC6LKGEWARASSHAZYK6BTVHP4L3ANCNFSM5NM327XQ>\n .\n You are receiving this because you are subscribed to this thread.Message\n ID: ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2664541",
                  "updatedAt": "2022-07-11T09:23:16Z",
                  "publishedAt": "2022-04-30T00:12:10Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "This means that the RVE computations for all the quadrature points meant to be happening sequentiually in a single sub-app. I am not pretty sure whether the parallel computation makes any impact here.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2672367",
                          "updatedAt": "2022-07-11T09:23:16Z",
                          "publishedAt": "2022-05-02T09:26:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Just a terminology point, we are planning to use one MultiApp, creating multiple subapp, one for each quadrature point.\nThe subapps are going to be ran in parallel, with usually near perfect scaling.\nSee the docs here https://mooseframework.inl.gov/syntax/MultiApps/",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674128",
                          "updatedAt": "2022-07-11T09:23:24Z",
                          "publishedAt": "2022-05-02T14:32:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "@abarun22 Not sure what you are referring to, the parallel part of the\ncomputation has 2 parts, solution of the governing equations for each\nsub-app and splitting sub-apps in parallel. What I would like to have\nhappen is have a single-sub-app for all qp's and the resulting governing\nequations of the sub-app will be solved in parallel. Is there a mechanism\nin moose to accomplish this?\n\n@GiudGiud i cannot understand the scaling part, are you suggesting that in\nan example with 100 qp's and n procs, the 100 sub-app solution with n procs\n(n < number of apps) will have the same time scaling as a single sub-app\nwith n proc solution ??\n\u2026\nOn Mon, May 2, 2022 at 5:26 AM abarun22 ***@***.***> wrote:\n This means that the RVE computations for all the quadrature points meant\n to be happening sequentiually in a single sub-app. I am not pretty sure\n whether the parallel computation makes any impact here.\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#20213 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ACYC6LKHZPDDPUVA76KCBVDVH6NT5ANCNFSM5NM327XQ>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674414",
                  "updatedAt": "2022-07-11T09:23:17Z",
                  "publishedAt": "2022-05-02T15:12:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "So the form of scaling I was referring to was weak scaling. In a problem with N qps and n<N processors, if N and n go to infinity at the same rate, the solution time stays roughly constant.\nWhat you are describing is a mix of strong scaling and some other concept. Typically I would expect 100 subapps with n=10 procs to solve faster than 1 app with 10 procs. The nonlinear system for each subapp is smaller hence cheaper. But it could go either way, as in the first case the setup is done 10 times per app, whereas in the latter the setup is done once, and with 10 procs.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674459",
                          "updatedAt": "2022-07-11T09:23:30Z",
                          "publishedAt": "2022-05-02T15:19:03Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "Thanks for the explanation. i just a did a quick manual check. I will post\nthe results in a day or so, where i manually created the sub-apps for a 1d\nsystem at all the qp's since the qp locations are known. I also\nmanually created a single sub-app with a combinergenerator to create the\npattern of interest. Initial results seem as if the sub-app is considerably\nslower.\n\nWhat about a mechanism for creating a sub-app like what i am suggesting.\nCan such a thing be done ?\n\nCheers\n\u2026\nOn Mon, May 2, 2022 at 11:19 AM Guillaume Giudicelli < ***@***.***> wrote:\n So the form of scaling I was referring to was weak scaling. In a problem\n with N qps and n<N processors, if N and n go to infinity at the same rate,\n the solution time stays roughly constant.\n\n What you are describing is a mix of strong scaling and some other concept.\n Typically I would expect 100 subapps with n procs to solve faster than 1\n app with 10 procs. The nonlinear system for each subapp is smaller hence\n cheaper. But it could go either way, as in the first case the setup is done\n 10 times per app, whereas in the latter the setup is done once, and with 10\n procs.\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#20213 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ACYC6LJ2V3AOPC3NCA5PESLVH7W7HANCNFSM5NM327XQ>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674583",
                  "updatedAt": "2022-07-11T09:23:24Z",
                  "publishedAt": "2022-05-02T15:38:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "For sure. If you want to automate a complex process you can create an action for it:\nhttps://mooseframework.inl.gov/source/actions/Action.html\nThis will be some specialized syntax that creates the objects you need. You could write a MultiApp in an action though it's not usually done.\nIn the single sub-app approach, you can probably get more performance by working on the solve method. Each qp's solve is independent so you should use some sort block-method to solve it.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674624",
                          "updatedAt": "2022-07-11T09:23:21Z",
                          "publishedAt": "2022-05-02T15:44:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Thats a nice suggestion having a single sub-app and allowing all the governing equations per qp to be happening in a parallel fashion. Given that the sub-app computation is generally quicker makes it sensible to do this way. I am not very sure about the features in MOOSE to handle this type of computation.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674640",
                          "updatedAt": "2022-07-11T09:23:21Z",
                          "publishedAt": "2022-05-02T15:46:01Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "Thanks, I am going to start working on it soon.\nJust to clarify, what you are suggesting would amount to the following\n1) i would follow something similar to @abarun22 method of extracting the\nqp positions (posted here earlier),\n2) Instead of populating the positions parameters in the multi-app, i would\nuse it to programmatically create an input file for the single sub-app\nbased on these xyz positions ...(either using generated meshes or read an\ninput mesh... etc)\nSo in effect this would be an action inside the multiapp\n\n@guidgiud, are the petsc solvers with LU preconditioning not fast on block\ndiagonal matrices ? I assumed that since you are using such solvers for the\nFV part in MOOSE. Is this not true ??\n\nCheers\n\u2026\nOn Mon, May 2, 2022 at 11:46 AM abarun22 ***@***.***> wrote:\n Thats a nice suggestion having a single sub-app and allowing all the\n governing equations per qp to be happening in a parallel fashion. Given\n that the sub-app computation is generally quicker makes it sensible to do\n this way. I am not very sure about the features in MOOSE to handle this\n type of computation.\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#20213 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ACYC6LJGP44GPZUP6XVWVBTVH72EJANCNFSM5NM327XQ>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674970",
                  "updatedAt": "2022-07-11T09:23:23Z",
                  "publishedAt": "2022-05-02T16:33:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "There's a lot more blocks here than we have in the finite volume of the Navier Stokes equations.\nLU works well for us as a placeholder while we are working on segregated solvers",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2675016",
                          "updatedAt": "2022-11-10T23:21:19Z",
                          "publishedAt": "2022-05-02T16:40:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "[Multiapp][Turn off subapp]",
          "author": {
            "login": "coskrrb2002"
          },
          "bodyText": "Dear users,\nIs there any way to turn off subapp in the mainapp at the certain time or at the certain value?\nCheers",
          "url": "https://github.com/idaholab/moose/discussions/20901",
          "updatedAt": "2022-07-08T08:00:14Z",
          "publishedAt": "2022-04-28T12:26:58Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "You can use the Controls system (https://mooseframework.inl.gov/moose/syntax/Controls/) to change the enabled flag on a MultiApp.",
                  "url": "https://github.com/idaholab/moose/discussions/20901#discussioncomment-2654647",
                  "updatedAt": "2022-07-08T08:00:14Z",
                  "publishedAt": "2022-04-28T13:11:41Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "coskrrb2002"
                          },
                          "bodyText": "Thank you for your advice!",
                          "url": "https://github.com/idaholab/moose/discussions/20901#discussioncomment-2671373",
                          "updatedAt": "2022-07-08T08:00:14Z",
                          "publishedAt": "2022-05-02T04:58:02Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Boundary condition that couples Neumann data to the time derivative of a variable",
          "author": {
            "login": "aaelmeli"
          },
          "bodyText": "Hi\nI need to define the following boundary condition:\ndu/dx=coeff * du/dt\nCorrect me if I am wrong, CoupledVarNeumannBC will not be useful in this case.\nAny ideas!",
          "url": "https://github.com/idaholab/moose/discussions/20850",
          "updatedAt": "2022-06-15T15:32:24Z",
          "publishedAt": "2022-04-22T17:34:24Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou could still use CoupledVarNeumannBC, if the coupled variable was an auxvariable equal to the derivative. I dont see an auxkernel to do that but it d be pretty easy. Please let know if you want to do that\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2617699",
                  "updatedAt": "2022-06-15T15:32:26Z",
                  "publishedAt": "2022-04-22T18:36:03Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "Hello\nYou could still use CoupledVarNeumannBC, if the coupled variable was an auxvariable equal to the derivative. I dont see an auxkernel to do that but it d be pretty easy. Please let know if you want to do that\nGuillaume\n\nThanks, @GiudGiud. Yes, that would be great.",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2617796",
                          "updatedAt": "2022-06-25T15:49:26Z",
                          "publishedAt": "2022-04-22T18:51:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Ok I ll make an object today and make a PR to MOOSE",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2618176",
                          "updatedAt": "2022-06-25T15:49:33Z",
                          "publishedAt": "2022-04-22T20:02:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Could you please try out the code in #20853\nand see if it works for you.",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2618971",
                          "updatedAt": "2022-06-25T15:49:33Z",
                          "publishedAt": "2022-04-22T23:55:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "Could you please try out the code in #20853 and see if it works for you.\n\nHi @GiudGiud\nWhat command should I use to pull this? I think I am doing something wrong when pulling this to my local.",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622368",
                          "updatedAt": "2022-08-23T16:04:26Z",
                          "publishedAt": "2022-04-23T23:36:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "There's multiple ways. One would be\n\nadd my branch as remote\nfetch it\nadd the two commits from my PR\n\ngit remote add guillaume git@github.com:GiudGiud/moose.git\ngit fetch guillaume\ngit cherry-pick 3f51a3271b1e7c2637aa4e7607118955bf5b66e1\ngit cherry-pick b233d0b0a77ade27c811d550d274f7891da2f663",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622399",
                          "updatedAt": "2022-08-23T16:04:26Z",
                          "publishedAt": "2022-04-23T23:56:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "Thank you, but I am still getting this error.\n(moose) aaelmeli@CCEE-DT-284:~/projects/moose$ git remote add guillaume git@github.com:GiudGiud/moose.git\n(moose) aaelmeli@CCEE-DT-284:~/projects/moose$ git fetch guillaume\nWarning: Permanently added the ECDSA host key for IP address '140.82.112.3' to the list of known hosts.\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622462",
                          "updatedAt": "2022-08-23T16:04:33Z",
                          "publishedAt": "2022-04-24T00:42:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "This is a screenshot of the branches that I have. I think I mistakenly made the moose.git branch with some command. I do not know what is the significance of the \"+\" sign on its left. This might be the problem.\nI tried to delete it but I am getting the following error message:\nerror: Cannot delete branch 'moose.git' checked out at '/home/aaelmeli/projects/moose/git@github.com:GiudGiud/moose.git'",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622470",
                          "updatedAt": "2022-08-23T16:04:33Z",
                          "publishedAt": "2022-04-24T00:51:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "never seen that\nwhat does git remote -v return?\ndid you clone MOOSE with ssh or with https?",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622563",
                          "updatedAt": "2022-08-23T16:04:35Z",
                          "publishedAt": "2022-04-24T01:47:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "never seen that what does git remote -v return?\n\nabdorepo        https://github.com/aaelmeli/moose.git (fetch)\nabdorepo        https://github.com/aaelmeli/moose.git (push)\ngithub-desktop-giudgiud https://github.com/GiudGiud/moose.git (fetch)\ngithub-desktop-giudgiud https://github.com/GiudGiud/moose.git (push)\nguid    git@github.com:GiudGiud/moose.git (fetch)\nguid    git@github.com:GiudGiud/moose.git (push)\nguillaume       git@github.com:GiudGiud/moose.git (fetch)\nguillaume       git@github.com:GiudGiud/moose.git (push)\norigin  https://github.com/idaholab/moose.git (fetch)\norigin  https://github.com/aaelmeli/moose_remote_aaelmeli.git (push)\norigin2 git@github.com:aaelmeli/moose_remote_aaelmeli.git (fetch)\norigin2 git@github.com:aaelmeli/moose_remote_aaelmeli.git (push)\nupstream        https://github.com/idaholab/moose.git (fetch)\nupstream        https://github.com/idaholab/moose.git (push)\n\n\ndid you clone MOOSE with ssh or with https?\n\nI have tried both actually with no success.\nI could get you pr with the following command\ngit fetch https://github.com/GiudGiud/moose.git PR_timeaux:timeaux\nupdated moose and conda, and I am getting another interesting error when do make as follows:\n(moose) aaelmeli@CCEE-DT-284:~/projects/first_test$ make\nRebuilding symlinks in /home/aaelmeli/projects/first_test/build/header_symlinks\nCompiling C++ (in opt mode) /home/aaelmeli/projects/moose/framework/build/unity_src/actions_Unity.C...\nIn file included from /home/aaelmeli/projects/moose/framework/build/header_symlinks/MaterialBase.h:34,\n                 from /home/aaelmeli/projects/moose/framework/build/header_symlinks/Material.h:13,\n                 from /home/aaelmeli/projects/moose/framework/src/actions/CheckOutputAction.C:12,\n                 from /home/aaelmeli/projects/moose/framework/build/unity_src/actions_Unity.C:49:\n/home/aaelmeli/projects/moose/framework/build/header_symlinks/Assembly.h:24:10: fatal error: libmesh/elem_side_builder.h: No such file or directory\n   24 | #include \"libmesh/elem_side_builder.h\"\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake: *** [/home/aaelmeli/projects/moose/framework/build.mk:145: /home/aaelmeli/projects/moose/framework/build/unity_src/actions_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622646",
                          "updatedAt": "2022-08-23T16:04:37Z",
                          "publishedAt": "2022-04-24T02:30:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so to make the ssh cloning work you ll have to go to Github settings and add your ssh public key from your machine.\nThis does look like libmesh is out of date.\nWhat does mamba list | grep moose return?",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622697",
                          "updatedAt": "2022-08-23T16:04:40Z",
                          "publishedAt": "2022-04-24T02:57:55Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Frictional contact with locking correction has bad convergence",
          "author": {
            "login": "Flolaffel"
          },
          "bodyText": "Hello,\nI'm running a simulation where frictional contact is applied between two bodies. Without locking correction, the results are incorrect (I have an Abaqus simulation for comparison) and the convergence is good. When I turn locking correction on, the results are correct but the convergence takes a huge hit. The solve time increases from 3 to 10 minutes. I already played with a lot of options like the ones in the contact-block, scaling and PETsc preconditioning but I can't get it to converge properly.\nAny ideas to fix this behaviour?",
          "url": "https://github.com/idaholab/moose/discussions/20856",
          "updatedAt": "2022-06-02T06:52:26Z",
          "publishedAt": "2022-04-24T19:44:55Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDo you know what methods Abaqus is using to solve that same problem?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2625631",
                  "updatedAt": "2022-06-02T06:52:26Z",
                  "publishedAt": "2022-04-24T20:02:50Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Abaqus uses so called hybrid elements that introduce an additional DOF to compute the pressure stress in the elements to model (nearly) incompressible materials (I'm using a rubber part in my contact).",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2629001",
                          "updatedAt": "2022-06-02T06:52:42Z",
                          "publishedAt": "2022-04-25T11:15:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "how does it solve the nonlinear problem? like the numerical method (newton? pjfnk? and the linear problems at each step if one of those)",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2630765",
                          "updatedAt": "2022-06-02T06:52:41Z",
                          "publishedAt": "2022-04-25T15:06:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Abaqus used the Newton method with a direct solver based on Gauss elimination for the linear equations.",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631534",
                          "updatedAt": "2022-06-02T06:52:42Z",
                          "publishedAt": "2022-04-25T17:01:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "We don't do many hyperelastic material simulations: Do you know it's friction that triggers this behavior? I.e. is it harder for the solver to converge with frictionless contact?",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631650",
                          "updatedAt": "2022-06-02T06:54:32Z",
                          "publishedAt": "2022-04-25T17:18:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Frictionless contact convergence is a lot better than frictional. Same simulation with frictionless formulation only takes 3 minutes instead of 10 and needs a lot less iterations.",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631747",
                          "updatedAt": "2022-06-02T06:54:31Z",
                          "publishedAt": "2022-04-25T17:33:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "Yeah, but I was mostly referring to the interaction with the correction to volumetric locking.",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631776",
                          "updatedAt": "2022-06-02T06:54:32Z",
                          "publishedAt": "2022-04-25T17:37:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "Also, could you paste your contact block?",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631808",
                          "updatedAt": "2022-06-02T06:54:32Z",
                          "publishedAt": "2022-04-25T17:42:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "I've run some tests and here are the numbers I obtained:\n\nfrictionless, no locking correction: 83 nonlinear iterations, 229 linear iterations\nfrictionless, locking correction: 83 nonlinear iteration, 285 linear iterations\nfrictional, no locking correction: 111 nonlinear iterations, 2961 linear iterations\nfrictional, locking correction: 191 nonlinear iterations, 13115 linear iterations\n\nAs you can see, the frictional contact convergence definitely takes a bigger hit from the locking correction than the frictionless contact. Frictionless contact convergence remains fine.\nMy contact block looks as follows\n[Contact]\n  [kontakt]\n    primary = top\n    secondary = rmax\n    formulation = penalty\n    penalty = 1e4\n    model = frictionless/coulomb\n    (friction_coefficient = 0.6)\n    normal_smoothing_distance = 0.1\n  []\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2639163",
                          "updatedAt": "2022-06-02T06:54:32Z",
                          "publishedAt": "2022-04-26T13:37:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "bwspenc"
                          },
                          "bodyText": "I don't know what to say about the connection between volumetric locking and poor convergence. Frictional contact problems are always tough, so it doesn't surprise me that you're in general having more trouble with friction.\nIt's interesting that your nonlinear convergence isn't too bad, but the linear convergence is what's really suffering. That indicates that your preconditioner isn't very effective. That could be due to a poor preconditioning method or a poor approximation of the Jacobian provided by your mechanics kernels (or a combination of the two). You'll have your best luck with a direct solver like superlu_dist and a full preconditioning matrix. I'm not sure what your settings are in your mechanics materials/kernels, but there are some options there that can help as well.",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2640863",
                          "updatedAt": "2022-06-02T06:55:43Z",
                          "publishedAt": "2022-04-26T17:13:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "I played with a lot of different preconditioning settings. The results posted above were obtained with PJFNK, full SMP and superlu_dist. When using fricitonless contact the best settings seem to be Newton, -ksp_type preonly and mumps. But these fail to converge with frictional contact.\nCould you please provide some insight on the settings you mentioned?",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2640980",
                          "updatedAt": "2022-06-02T06:55:59Z",
                          "publishedAt": "2022-04-26T17:35:18Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "recuero"
                  },
                  "bodyText": "I'd recommend following the setup of existing frictional cases in the repository.",
                  "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2666225",
                  "updatedAt": "2022-06-02T06:52:41Z",
                  "publishedAt": "2022-04-30T13:42:46Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Elastic response to mesh modification",
          "author": {
            "login": "AdrienWehrle"
          },
          "bodyText": "Hi everyone,\nI have been working on an application based on MASTODON to study the elastic response of a domain on an inclined surface, after the removal of some support at its front.\nWhat I did first was to apply a Pressure boundary condition, with the frontal_pressure function depending on time and taking two different values before and after a given time t (simulating the loss of support in front of the domain).\n[BCs]\n  [Pressure]\n    [front_pressure]  \n    boundary = front\n    function = frontal_pressure\n    displacements = 'disp_x disp_y disp_z'\n    []\n  []\n\nThis worked great and gave interesting results. But this only represents the result of a material removal, not the actual removal itself. Next step was therefore to actively remove some material at the front of the domain and get the pressure release as a result, instead of a preset. Just as it actually happens in a real case.\nBased on #20601, I looked into two potential solutions:\n\nhttps://mooseframework.inl.gov/source/userobject/CoupledVarThresholdElementSubdomainModifier.html\nhttps://mooseframework.inl.gov/source/meshgenerators/BlockDeletionGenerator.html\n\nMy progress (and issues) on those two options:\n\n\nThis is on the fly. One input file, and a moving boundary. However, I need to assign the newly deactivated block (considered as removed, chopped off) to an existing subdomain.\nAnd that is where I got stuck... How to assign it to a subdomain that doesn't have any, literally any, impact on the main domain anymore?\nHere is how far I got into that first direction with my input file: chop_dynamic.i\n\n\nThis is the restart option. Two input files, the first one for initialization, and the second one where a new mesh is created with the material chopped off and the solutions remapped based on the first simulation. It also seems like a good idea, because in my case I remove material only once and this is irreversible.\nI think I got pretty far here, but somehow I don't get significant elastic response (only a very slow displacement in x) in my transient simulation although I removed 500m off my domain and initialized with displacements corresponding to the entire loaded domain... So I should have a significant extension-dominated reaction...! Also the solve is very slow, much slower than with a pressure release of similar magnitude, somehow suggesting something is wrong...\nHere is how far I got into that second direction with my input files: chop_init.i (initialization) and chop_restart.i (transient after removal)\n\n\nAny help/thoughts on one of these options or both would be much appreciated! Thank you a lot in advance!",
          "url": "https://github.com/idaholab/moose/discussions/20836",
          "updatedAt": "2022-06-16T19:41:20Z",
          "publishedAt": "2022-04-21T14:25:37Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nFor option 1, if you move elements to a domain that is outside the block restrictions of the kernels/variables, then it is effectively removed from the simulation.\nThere might also be material property values you could set in that domain that negate all form of displacement (like incredibly stiff). This might not be a good idea as I think you may pay a steep price in convergence properties.\nFor your inputs in 1 and 2, I would set displacement = 'disp_x disp_y ...' in GlobalParams. it s too easy to have forgotten it somewhere.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2609486",
                  "updatedAt": "2022-06-16T19:41:35Z",
                  "publishedAt": "2022-04-21T14:50:42Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "AdrienWehrle"
                          },
                          "bodyText": "Hi @GiudGiud ! Thank you a lot for your help!\n\nFor option 1, if you move elements to a domain that is outside the block restrictions of the kernels/variables, then it is effectively removed from the simulation.\n\nIt sounds like option 1 is the better option here, so I tried to move elements to a domain outside of the block restrictions of my problem. To this end, I think I need to create some elements disconnected from my domain, defined in channel.e.\nI therefore tried the following in chop_dynamic.i:\n[Mesh]\n  [channel]      \n  type = FileMeshGenerator\n  file = channel.e\n  []\n  \n  [chop_event]\n    type = SubdomainBoundingBoxGenerator\n    input = channel\n    block_id = 2\n    block_name = chop_event\n    bottom_left = '29500  -100 0'\n    top_right = '30000 2100 1000'\n  []\n\n  [deactivated]\n    type = SubdomainBoundingBoxGenerator\n    input = calving_event\n    block_id = 3\n    block_name = deactivated_domain\n    bottom_left = '40000  1000 1000'\n    top_right = '40001  1001 1001'\n  []\n[]\n\nwith the CoupledVarThresholdElementSubdomainModifier subdomain_id set to 3. And where deactivated has somehow random coordinates, just to be out of the main domain boundaries.\nHowever I get Requested subdomain 3 does not exist.. I suppose the subdomain wasn't created because no element are in the bounding box. I looked at other objects in https://mooseframework.inl.gov/syntax/Mesh/index.html but I couldn't find the one that I should use here so far... Or maybe am I not interpreting your advice the right way? Thank you a lot for your help!",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2613614",
                          "updatedAt": "2022-06-16T19:41:36Z",
                          "publishedAt": "2022-04-22T07:01:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "hello\nYou could use an ElementGenerator and CombinerGenerator for this:\nhttps://mooseframework.inl.gov/source/meshgenerators/ElementGenerator.html\nhttps://mooseframework.inl.gov/source/meshgenerators/CombinerGenerator.html\njust put an element real far and block restrict the physics out of it.\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2616231",
                          "updatedAt": "2022-06-16T19:41:45Z",
                          "publishedAt": "2022-04-22T14:44:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AdrienWehrle"
                          },
                          "bodyText": "Thank you a lot! What don't get right with https://mooseframework.inl.gov/source/meshgenerators/ElementGenerator.html is that it only generates an element if I use existing nodes for element_connectivity, although I want to create an element that is not connected at all to my main domain, hence with new nodes...\nHere is the mesh I'm working on:\n\n[Mesh]\n  \n  [channel]      \n  type = FileMeshGenerator\n  file = channel.e\n  []\n\n  [chop_event]\n    type = SubdomainBoundingBoxGenerator\n    input = channel\n    block_id = 2\n    block_name = chop_event\n    bottom_left = '29500  -100 0'\n    top_right = '30000 2100 1000'\n  []\n\n  [single_element]\n    type = ElementGenerator\n    input = chop_event\n    nodal_positions = '0 0 0\n                       -1000 0 0\n                       -1000 -1000 0\n                       0 -1000 0\n                       0 0 -500\n                       -1000 0 -500\n                       -1000 -1000 -500\n                       0 -1000 -500'\n    element_connectivity = '100000 100001 100002 100003 100005 100006 100007 100008'\n    elem_type = \"QUAD8\"\n  []\n\n  [combined]\n    type = CombinerGenerator\n    inputs = 'chop_event single_element'\n  []\n\n  final_generator = combined\n[]\n\nI set element_connectivity higher than my maximum node number to try to create new ones, but it fails with a segmentation fault (because those nodes don't exist).... If I set element_connectivity in my range of existing nodes, then it uses those... Somehow I'm using it in the wrong way since it doesn't allow me make it do what's meant to...!\nThank you a lot for your help!",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2630647",
                          "updatedAt": "2022-06-16T19:41:44Z",
                          "publishedAt": "2022-04-25T14:56:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "umm. Well if it's not in a block that has the physics it shouldnt matter at all where it is. Or how many elements it has\nIf that mesh generator doesnt work for you you can use any random mesh, load it with FileMeshGenerator and combine it, unstitched, with the CombinerMeshGenerator. And it ll just sit there, doing nothing.\nYou'll need this btw\n[Problem]\n  kernel_coverage_check = false\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2631421",
                          "updatedAt": "2022-07-11T08:22:57Z",
                          "publishedAt": "2022-04-25T16:43:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AdrienWehrle"
                          },
                          "bodyText": "Thank you a lot for you help on that @GiudGiud! I think I'm getting close, but still have an issue.\nIn the updated version of chop_dynamic.i, I:\n\nLoad my main domain + a single isolated element\nSet a new block id for the isolated element only (value is 255)\nCombine the two meshes\nRestrict all the physics to blocks 1 and 2 (main mesh and area that I want to chop off, respectively)\nChange block 2 to 255 at the chopping time to move the chopped area out of my problem.\nSet kerner_coverage_check and material_coverage_check to False.\n\nIt somehow sounds good but I get the following error, which is linked to the element doing nothing at the solve level but I can't really backtrack it nor solve it (error is at the end of the log):\nThe following total 1 aux variables:\n  von_mises\nare added for automatic output by MaterialOutputAction.\nFinished Setting Up                                                                      [  0.65 s] [  187 MB]\nFramework Information:\nMOOSE Version:           git commit 6f309b1abd on 2022-02-07\nLibMesh Version:         \nPETSc Version:           3.15.1\nSLEPc Version:           3.15.1\nCurrent Time:            Tue Apr 26 09:30:20 2022\nExecutable Timestamp:    Fri Mar 11 16:19:52 2022\n\nParallelism:\n  Num Processors:          1\n  Num Threads:             11\n\nMesh: \n  Parallel Type:           replicated\n  Mesh Dimension:          3\n  Spatial Dimension:       3\n  Nodes:                   11987\n  Elems:                   9864\n  Num Subdomains:          3\n\nNonlinear System:\n  Num DOFs:                35961\n  Num Local DOFs:          35961\n  Variables:               { \"disp_x\" \"disp_y\" \"disp_z\" } \n  Finite Element Types:    \"LAGRANGE\" \n  Approximation Orders:    \"FIRST\" \n\nAuxiliary System:\n  Num DOFs:                182549\n  Num Local DOFs:          182549\n  Variables:               { \"vel_x\" \"accel_x\" \"vel_y\" \"accel_y\" \"vel_z\" \"accel_z\" } { \"stress_xx\" \"stress_xy\" \n                             \"stress_xz\" \"stress_yx\" \"stress_yy\" \"stress_yz\" \"stress_zx\" \"stress_zy\" \n                             \"stress_zz\" } \"t_calving\" \"von_mises\" \n  Finite Element Types:    \"LAGRANGE\" \"MONOMIAL\" \"LAGRANGE\" \"MONOMIAL\" \n  Approximation Orders:    \"FIRST\" \"CONSTANT\" \"FIRST\" \"CONSTANT\" \n\nExecution Information:\n  Executioner:             Transient\n  TimeStepper:             ConstantDT\n  Solver Mode:             NEWTON\n  MOOSE Preconditioner:    SMP\n\nLEGACY MODES ENABLED:\n This application uses the legacy material output option: material properties are output only on TIMESTEP_END, not INITIAL. To remove this message, set 'use_legacy_material_output' to false in this application. If there are gold output files that contain material property output for which output occurs on INITIAL, then these will generate diffs due to zero values being stored, and these tests should be re-golded.\n\n\n    Setting Up Materials\n      Finished Computing Initial Material Values                                         [  0.21 s] [  131 MB]\n    Finished Setting Up Materials                                                        [  0.21 s] [  131 MB]\n  Finished Performing Initial Setup                                                      [  2.77 s] [  185 MB]\n\nTime Step 0, time = 0\nWarning, Exodus files cannot have titles longer than 80 characters.  Your title will be truncated.\n\nTime Step 1, time = 0.02, dt = 0.02\n\nPerforming automatic scaling calculation\n\n 0 Nonlinear |R| = 9.256317e-01\nAt column 2, pivotL() encounters zero diagonal at line 722 in file /opt/civet/build_2/conda_builds/conda_envs/next-aeb8c4f62f/conda-bld/moose-petsc_1643733610091/work/arch-conda-c-opt/externalpackages/git.superlu_dist/SRC/symbfact.c\n\nThank you a lot for your precious help!",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2637628",
                          "updatedAt": "2022-07-11T08:22:59Z",
                          "publishedAt": "2022-04-26T09:50:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "oh did you block restrict the variables outside of the elements with no physics? This is actually important, otherwise the matrix has a zero for the qps there.\nThe block restriction of the variables is usually inherited by the kernels btw",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2640349",
                          "updatedAt": "2022-07-11T08:23:04Z",
                          "publishedAt": "2022-04-26T15:58:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AdrienWehrle"
                          },
                          "bodyText": "oh did you block restrict the variables outside of the elements with no physics?\n\nI am not sure I completely understand. Variables are not block-restricted at the moment, but they probably should? Should I use inactive on block 255 (inactive element) for all variables?",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2640473",
                          "updatedAt": "2022-07-11T08:23:06Z",
                          "publishedAt": "2022-04-26T16:15:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "They should be. Pass the block =  in the variables blocks (each of them) not the kernels.\ninactive is not for this. inactive is to turn off an object in the input file, like entirely not just block restrict",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2640618",
                          "updatedAt": "2022-07-11T08:23:05Z",
                          "publishedAt": "2022-04-26T16:34:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AdrienWehrle"
                          },
                          "bodyText": "Thank you a lot, that indeed solved the zero qps issue!\nOnly remaining problem is I restrict the physics to block 1 and 2 (main domain and chopped area, respectively), then change the block ID of the chopped area from 2 to 255 (to deactivate it). Therefore block 2 doesn't exist anymore, but is passed in block = in variable blocks, resulting in ExodusII_IO_Helper: block id 2 not found in block_ids. at chop time (t=1).\nI think one workaround is to have not only the one isolated element (block_id=255), but also a second one with block_id=2 (useless but containing the physics) so that block id 2 doesn't disappear. Do you think that is a good idea?\nThank you a lot for your help!",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2645516",
                          "updatedAt": "2022-07-11T08:23:06Z",
                          "publishedAt": "2022-04-27T10:15:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yeah I think you ll have to do that.\nOr you start with just all elements in block 1, then move some of the chopped elements in block 1 to block 255?",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2647419",
                          "updatedAt": "2022-07-11T08:23:06Z",
                          "publishedAt": "2022-04-27T14:55:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "AdrienWehrle"
                  },
                  "bodyText": "Or you start with just all elements in block 1, then move some of the chopped elements in block 1 to block 255?\n\nIndeed, I managed to trade my initial SubdomainBoundingBoxGenerator + CoupledVarThresholdElementSubdomainModifier with threshold on time, with one single CoupledVarThresholdElementSubdomainModifier on a variable depending on space and time! Thank you so much for your help on that!",
                  "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2661012",
                  "updatedAt": "2022-06-16T19:41:51Z",
                  "publishedAt": "2022-04-29T12:01:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "AdrienWehrle"
                  },
                  "bodyText": "One last thing that I can't get right for this problem (sorry to disturb you with that!):\nI added the following function:\n[Functions]\n  [chop_criterion]\n    type = ParsedFunction\n    value = 'if((x>29300.)&(t>1.), 1., 0.)'\n  []\n[]\n\nthat I then apply in my subdomain modifier:\n[UserObjects]\n  [calving_event]\n    type = CoupledVarThresholdElementSubdomainModifier\n    coupled_var = 'chop_boolean'\n    block = 1\n    criterion_type = ABOVE\n    threshold = 0.5\n    subdomain_id = 255\n    moving_boundary_name = downstream \n    execute_on = 'INITIAL TIMESTEP_BEGIN'\n  []\n[]\n\nchop_boolean is the variable set by chop_criterion.\nI thought I solved my issue, but I get a weird chop where not all expected elements are moved to the 255 subdomain (see screenshots below) although the condition is a clear cut and not at the border of an element row (x=29000 and x=29500 are the two element rows before and after the condition x=29300)... As you can see on the sfcreenshots, the outer band of elements is entirely removed but not issue appears in the second one....\nThis is independent of the threshold, I always get such elements in and out no matter what value I take...\nI was earlier using the same spatial condition with SubdomainBoundingBoxGenerator and I got a nice and clear cut as expected... Any ideas on that? Thank you a lot",
                  "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2661132",
                  "updatedAt": "2022-06-16T19:41:50Z",
                  "publishedAt": "2022-04-29T12:27:48Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I think the centroid of the element is considered. If you move this x=29300 closer to the next layer (3rd) does it cut properly?",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2662092",
                          "updatedAt": "2022-06-16T19:41:59Z",
                          "publishedAt": "2022-04-29T15:10:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "actually no scratch that.\nThis is how the element value is determined:\n  Real avg_val = 0;\n\n  for (unsigned int qp = 0; qp < _qrule->n_points(); ++qp)\n    avg_val += _v[qp] * _JxW[qp] * _coord[qp];\n  avg_val /= _current_elem_volume;\n\n  return avg_val;\n\nsp you have to make that average below the threshold value.\nEither smooth the transition with a linear function in between the jump, or jump to -1 instead of 0, or raise/lower the threshold appropriately",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2662327",
                          "updatedAt": "2022-06-16T19:42:00Z",
                          "publishedAt": "2022-04-29T15:43:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AdrienWehrle"
                          },
                          "bodyText": "Oh woh, I wasn't looking in the right direction, I also initially thought it was about the spatial threshold.\nWhat I have for the moment is my chop_criterion taking -1000 or 1000 and threshold at 0 so there can't be any overrunning.\nThank you a lot for all your help on that! \ud83d\udc4d",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2662711",
                          "updatedAt": "2022-06-16T19:42:00Z",
                          "publishedAt": "2022-04-29T16:29:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AdrienWehrle"
                          },
                          "bodyText": "Next step will be to try and have my chop_criterion to depend on a damage value I compute from stresses. Challenge is my damage is a material property and not an (aux)variable, therefore I can't access it directly with a CoupledVarThresholdElementSubdomainModifier... I tried to find a way to convert a material property to a variable, but that sounds like a bad idea. Would you  by any chance have an advice on that before I mark this question as answered? Thank you a lot!\nThe damage implementation is visible here.",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2662775",
                          "updatedAt": "2022-07-11T08:23:33Z",
                          "publishedAt": "2022-04-29T16:40:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nit s fine to convert material property to auxiliary variable in your case. You are only interested about the value.\nit s not fine when you care about transferring derivatives.\nTo do that use MaterialRealAux or one of the other template instantiations depending on the type of your material property\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/20836#discussioncomment-2664641",
                          "updatedAt": "2022-07-11T08:23:33Z",
                          "publishedAt": "2022-04-30T01:18:50Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Do not convergence of heat conduction",
          "author": {
            "login": "Salma-Mao"
          },
          "bodyText": "Hi,\nI am studying  about heat conduction. Material properties change with temperature as shown in the picture.  I define the material like:\nif \uff08temperature[_qp] < 1000\uff09\n{\n     specific_heat[_qp] = 1.23*temperature[_qp] + 1.23;\n}\nelse\n{\n     specific_heat[_qp] = 4.56*temperature[_qp] + 4.56;\n}\n\n\nHowever, does not converge after a certain time step. At the same time, some units are just above 1000 K.  I guess this error has something to do with my judgment on temperature. But I do not know how to  solve this problem.\n\nThanks,\nMS",
          "url": "https://github.com/idaholab/moose/discussions/20891",
          "updatedAt": "2022-06-22T09:15:41Z",
          "publishedAt": "2022-04-27T14:24:41Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nFrom what you copy pasted it looks like the code skips the center portion of this graph. It jumps straight from 1200 J/m3 to 4500.\nIs this normal? It seems to miss the T=1200 to 3500 portion.\nif the solve struggles with too much discontinuity, you could smooth this profile.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20891#discussioncomment-2647297",
                  "updatedAt": "2022-06-22T09:16:02Z",
                  "publishedAt": "2022-04-27T14:43:47Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Salma-Mao"
                          },
                          "bodyText": "Thanks  for your help.",
                          "url": "https://github.com/idaholab/moose/discussions/20891#discussioncomment-2659711",
                          "updatedAt": "2022-06-22T09:16:19Z",
                          "publishedAt": "2022-04-29T07:28:19Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error on reading the abaqus file (.inp)",
          "author": {
            "login": "avtarsinghh1991"
          },
          "bodyText": "Hello MOOSE Experts\nI generated a ABAQUS mesh input file (.inp) using CUBIT with several grains. After I am trying to read the file in the MOOSE as follows:\n[Mesh]\n  [initial_mesh]\n     type = FileMeshGenerator\n     file = Abaqus_file.inp\n  []\n    [./scale]\n    type = TransformGenerator\n    input = initial_mesh\n    transform = SCALE\n    vector_value ='11.0e-6 11.0e-6 0'\n  []\n   construct_side_list_from_node_list=true\n[]\n\nAfterwards, I am running the command with --mesh-only option and getting the following error:\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n/home/asingh/projects/babbler/babbler-opt: Relink `/home/asingh/mambaforge3/envs/moose/lib/libgfortran.so.5' with `/lib64/librt.so.1' for IFUNC symbol `clock_gettime'\n\n\n\n*** Warning, This code is deprecated and will be removed in future versions:\nThe parameter direction is deprecated.\nSpecifying direction+multiapp is deprecated. Specify the to_multi_app and from_multi_app\n\nError: Needed to read 2 nodes, but read 4 instead!\n[0] ../src/mesh/abaqus_io.C, line 723, compiled Apr 13 2022 at 21:08:08\n\n\n*** ERROR ***\nError: Needed to read 2 nodes, but read 4 instead!\n\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n\nIf the number of grains are less than 30, it is working absolutely fine. However, if the number of grains increases even to 31, the above mentioned error is popping out.\nPlease give me any suggestions to solve the issue. I have attached the mesh file as well.\nmesh_file.zip\nBest\nAvtar",
          "url": "https://github.com/idaholab/moose/discussions/20903",
          "updatedAt": "2022-06-22T15:59:57Z",
          "publishedAt": "2022-04-28T16:00:48Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nFor the first errors, I'd remove the moose environment in mamba and re-install. Something changed in the libgfortran library since it was compiled, and easiest way is to reinstall.\nFor the deprecation warning, your multiapp syntax is deprecated. Instead of\n direction = TO_MULTIAPP\n  multiapp = multiapp_name\n\nwe now do\nto_multiapp = multiapp_name\n\nthis is to support transfers from multiapp to multiapp.\nWhat's a good reader for abaqus meshes? I dont have Abaqus on my machine.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20903#discussioncomment-2656152",
                  "updatedAt": "2022-06-22T16:00:08Z",
                  "publishedAt": "2022-04-28T16:24:32Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Hi @GiudGiud\nThanks. I solved the multiapp issue.\nFor the abaqus file, I tried this file to read in ABAQUS, its working absolutely fine in ABAQUS.\nBest\nAvtar",
                          "url": "https://github.com/idaholab/moose/discussions/20903#discussioncomment-2656185",
                          "updatedAt": "2022-06-22T16:00:16Z",
                          "publishedAt": "2022-04-28T16:27:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Well the error isnt explicit enough. My guess is that there are mixed element types in a domain, like a single triangle in a quad block.\nMOOSE doesnt support this.\nI converted it to exodus with meshio. Do you get a better error message with this format?\nmesh_file_exo.zip",
                          "url": "https://github.com/idaholab/moose/discussions/20903#discussioncomment-2656337",
                          "updatedAt": "2022-06-22T16:00:16Z",
                          "publishedAt": "2022-04-28T16:51:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "@GiudGiud\nThanks for the insight. I have also written the abaqus input using my own MATLAB subroutine. For that I am generating structured mesh in which there is no possibility of mixed element type.\nI am facing the same issue in there as well with more than 30 grains.\nI am getting the following error upon reading the file you shared in MOOSE\nEXODUS: Error: Attempting to open the netcdf-4 file:\n        '/home/singh/projects/babbler/Anisotropic_2D/Abaqus_file.e'\n        with a netcdf library that does not support netcdf-4\nError opening ExodusII mesh file: /home/singh/projects/babbler/Anisotropic_2D/Abaqus_file.e\n[0] ../src/mesh/exodusII_io_helper.C, line 643, compiled Dec 16 2021 at 14:15:31\n\n\n*** ERROR ***\nError opening ExodusII mesh file: /home/singh/projects/babbler/Anisotropic_2D/Abaqus_file.e\n\n\nI have tried importing the abaqus file in cubit and export it as exodus file. It is working fine if doing so. But i want to directly use the abaqus file. Any suggestions?\nBest\nAvtar",
                          "url": "https://github.com/idaholab/moose/discussions/20903#discussioncomment-2656409",
                          "updatedAt": "2022-06-22T16:00:23Z",
                          "publishedAt": "2022-04-28T17:04:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok you cant read my file because I m using an hdf5 exodus and I dont think your moose install is setup that way.\nOnly suggestion I would have is to debug the abaqus file reader in MOOSE. Ill create an issue and the thermomechanics people at INL may look at it if they have time",
                          "url": "https://github.com/idaholab/moose/discussions/20903#discussioncomment-2656459",
                          "updatedAt": "2022-09-16T18:20:24Z",
                          "publishedAt": "2022-04-28T17:12:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thanks. Can you suggest any other file format which I can write including the nodesets and sidesets? I am thinking of using the .msh file format",
                          "url": "https://github.com/idaholab/moose/discussions/20903#discussioncomment-2657001",
                          "updatedAt": "2022-09-16T18:20:26Z",
                          "publishedAt": "2022-04-28T18:53:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The list of supported (well except for inp in your case) is here:\nhttps://mooseframework.inl.gov/source/meshgenerators/FileMeshGenerator.html",
                          "url": "https://github.com/idaholab/moose/discussions/20903#discussioncomment-2657077",
                          "updatedAt": "2022-09-16T18:20:27Z",
                          "publishedAt": "2022-04-28T19:09:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Block Material as a Function of Time",
          "author": {
            "login": "jrwill11"
          },
          "bodyText": "I'm trying to simulate the transient of a moving material. The properties of the material doesn't change, but instead its block number does.\nSo say I have three blocks: block 1 is LEU, and blocks 2 and 3 are air at time step 1. At time step 2, block 1 is air, block 2 is LEU, and block 3 is air. Does MOOSE have any functions where I can change the material as a function of time in a single input?",
          "url": "https://github.com/idaholab/moose/discussions/20902",
          "updatedAt": "2022-05-31T04:35:54Z",
          "publishedAt": "2022-04-28T15:55:10Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThe only block modifier right now is this one:\nhttps://mooseframework.inl.gov/source/userobject/CoupledVarThresholdElementSubdomainModifier.html\nYou will have to encode the block change in a variable to make it work.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20902#discussioncomment-2656166",
                  "updatedAt": "2022-05-31T04:37:01Z",
                  "publishedAt": "2022-04-28T16:26:02Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}