{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0wNi0yMVQxMjowNToxNy0wNTowMM4AO-hb"
    },
    "edges": [
      {
        "node": {
          "title": "Test suite runs on head vs fails on compute node",
          "author": {
            "login": "mntbighker"
          },
          "bodyText": "I'm an OpenHPC cluster manager trying to help my team install MOOSE. I'm to the point where the test suite runs with few failures (mostly lack of CPUs). But on the compute nodes pretty much every test is skipped, and ultimately MPI errors kill the run. I'm not having much success debugging this. OpenHPC has no MOOSE meta package. Most of the MOOSE dependencies are already part of OpenHPC, but so many of the packages are built with MOOSE compile dependencies missing that we pretty much built it per the instructions. PETSc being a prime example. Some examples of things I had to add to get MOOSE tests to work: libtirpc-devel, python36-devel, and...\npython -m pip install --upgrade pip\npython -m pip install -t /home/shared/petsc/python3.6 pandas\npython -m pip install -t /home/shared/petsc/python3.6 matplotlib\npython -m pip install -t /home/shared/petsc/python3.6 sympy\npython -m pip install -t /home/shared/petsc/python3.6 deepdiff\npython -m pip install -t /home/shared/petsc/python3.6 numpy\npython -m pip install -t /home/shared/petsc/python3.6 scipy\npython -m pip install -t /home/shared/petsc/python3.6 vtk\nMost of the python stuff has OpenHPC packages, that don't work with MOOSE either.\nI have re-run the libmesh upgrade script a few times as these things were added, in case that was necessary.",
          "url": "https://github.com/idaholab/moose/discussions/21241",
          "updatedAt": "2022-06-25T05:55:10Z",
          "publishedAt": "2022-06-09T00:45:19Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nCould you please give us the test log for a few of the errors?\nCan you run one of the failing test in an interactive job and paste the error here if the log isnt available?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2909668",
                  "updatedAt": "2022-06-09T00:57:42Z",
                  "publishedAt": "2022-06-09T00:57:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mntbighker"
                  },
                  "bodyText": "vectorpostprocessors/work_balance.work_balance/distributed ..................... [MESH_MODE!=DISTRIBUTED] SKIP\n[c1.Thor2.local:37446] [[44689,0],0] ORTE_ERROR_LOG: Data unpack had inadequate space in file util/show_help.c at line 513\n[c1.Thor2.local:37446] [[44689,0],0] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file util/show_help.c at line 507\n[c1.Thor2.local:37446] [[44689,0],0] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file util/show_help.c at line 501\n--------------------------------------------------------------------------\nIt looks like orte_init failed for some reason; your parallel process is\nlikely to abort.  There are many reasons that a parallel process can\nfail during orte_init; some of which are due to configuration or\nenvironment problems.  This failure appears to be an internal failure;\nhere's some additional information (which may only be relevant to an\nOpen MPI developer):\n\n  getting local rank failed\n  --> Returned value No permission (-17) instead of ORTE_SUCCESS\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nIt looks like orte_init failed for some reason; your parallel process is\nlikely to abort.  There are many reasons that a parallel process can\nfail during orte_init; some of which are due to configuration or\nenvironment problems.  This failure appears to be an internal failure;\nhere's some additional information (which may only be relevant to an\nOpen MPI developer):\n\n  orte_ess_init failed\n  --> Returned value No permission (-17) instead of ORTE_SUCCESS\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nIt looks like MPI_INIT failed for some reason; your parallel process is\nlikely to abort.  There are many reasons that a parallel process can\nfail during MPI_INIT; some of which are due to configuration or environment\nproblems.  This failure appears to be an internal failure; here's some\nadditional information (which may only be relevant to an Open MPI\ndeveloper):\n\n  ompi_mpi_init: ompi_rte_init failed\n  --> Returned \"No permission\" (-17) instead of \"Success\" (0)\n--------------------------------------------------------------------------\nmisc/check_error.steady_no_converge: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.steady_no_converge: Running command: /home/mwmoorcroft/moose/test/moose_test-opt -i steady_no_converge.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.steady_no_converge: *** An error occurred in MPI_Init_thread\nmisc/check_error.steady_no_converge: *** on a NULL communicator\nmisc/check_error.steady_no_converge: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\nmisc/check_error.steady_no_converge: ***    and potentially your MPI job)\nmisc/check_error.steady_no_converge: [c1.Thor2.local:37573] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nmisc/check_error.steady_no_converge: ################################################################################\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: Unable to match the following pattern against the program's output:\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: Aborting as solve did not converge\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: ################################################################################\nmisc/check_error.steady_no_converge: Tester failed, reason: EXPECTED OUTPUT MISSING\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge ......................................... FAILED (EXPECTED OUTPUT MISSING)\nmisc/check_error.steady_no_converge: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.steady_no_converge: Running command: /home/mwmoorcroft/moose/test/moose_test-opt -i steady_no_converge.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.steady_no_converge: *** An error occurred in MPI_Init_thread\nmisc/check_error.steady_no_converge: *** on a NULL communicator\nmisc/check_error.steady_no_converge: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\nmisc/check_error.steady_no_converge: ***    and potentially your MPI job)\nmisc/check_error.steady_no_converge: [c1.Thor2.local:37561] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nmisc/check_error.steady_no_converge: ################################################################################\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: Unable to match the following pattern against the program's output:\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: Aborting as solve did not converge\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: ################################################################################\nmisc/check_error.steady_no_converge: Tester failed, reason: EXPECTED OUTPUT MISSING\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge ......................................... FAILED (EXPECTED OUTPUT MISSING)\nmisc/check_error.missing_mesh_test: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.missing_mesh_test: Running command: /home/mwmoorcroft/moose/test/moose_test-opt -i missing_mesh_test.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.missing_mesh_test: *** An error occurred in MPI_Init_thread\nmisc/check_error.missing_mesh_test: *** on a NULL communicator\nmisc/check_error.missing_mesh_test: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\nmisc/check_error.missing_mesh_test: ***    and potentially your MPI job)\nmisc/check_error.missing_mesh_test: [c1.Thor2.local:37576] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nmisc/check_error.missing_mesh_test: ################################################################################\nmisc/check_error.missing_mesh_test:\nmisc/check_error.missing_mesh_test: Unable to match the following pattern against the program's output:\nmisc/check_error.missing_mesh_test:\nmisc/check_error.missing_mesh_test: Unable to open file \\S+\nmisc/check_error.missing_mesh_test:\nmisc/check_error.missing_mesh_test: ################################################################################\nmisc/check_error.missing_mesh_test: Tester failed, reason: EXPECTED ERROR MISSING\nmisc/check_error.missing_mesh_test:\nmisc/check_error.missing_mesh_test ........................................... FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.function_file_test14: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.function_file_test14: Running command: /home/mwmoorcroft/moose/test/moose_test-opt -i function_file_test14.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.function_file_test14: *** An error occurred in MPI_Init_thread\nmisc/check_error.function_file_test14: *** on a NULL communicator\nmisc/check_error.function_file_test14: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\nmisc/check_error.function_file_test14: ***    and potentially your MPI job)\nmisc/check_error.function_file_test14: [c1.Thor2.local:37560] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nmisc/check_error.function_file_test14: ################################################################################\nmisc/check_error.function_file_test14:\nmisc/check_error.function_file_test14: Unable to match the following pattern against the program's output:\nmisc/check_error.function_file_test14:\nmisc/check_error.function_file_test14: In \\S+: Read more than two rows of data from file '\\S+'.  Did you mean to use \"format = columns\" or set \"xy_in_file_only\" to false?\nmisc/check_error.function_file_test14:\nmisc/check_error.function_file_test14: ################################################################################\nmisc/check_error.function_file_test14: Tester failed, reason: EXPECTED ERROR MISSING\nmisc/check_error.function_file_test14:\nmisc/check_error.function_file_test14 ........................................ FAILED (EXPECTED ERROR MISSING)\nkernels/ad_max_dofs_per_elem_error.ad_max_dofs_per_elem_error ...................... [AD_MODE!=NONSPARSE] SKIP\nkernels/ad_max_dofs_per_elem_error.ad_max_dofs_per_elem_error ...................... [AD_MODE!=NONSPARSE] SKIP\nkernels/ad_max_dofs_per_elem_error.ad_max_dofs_per_elem_error ...................... [AD_MODE!=NONSPARSE] SKIP\nkernels/ad_max_dofs_per_elem_error.ad_max_dofs_per_elem_error ...................... [AD_MODE!=NONSPARSE] SKIP\nminimal_app.minimal ........................................................................ [NO DISPLAY] SKIP\nminimal_app.minimal ........................................................................ [NO DISPLAY] SKIP\nminimal_app.minimal ........................................................................ [NO DISPLAY] SKIP\nminimal_app.minimal ........................................................................ [NO DISPLAY] SKIP\n[c1.Thor2.local:37446] 3 more processes have sent help message help-orte-runtime / orte_init:startup:internal-failure\n[c1.Thor2.local:37446] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n[c1.Thor2.local:37446] 3 more processes have sent help message help-mpi-runtime.txt / mpi_init:startup:internal-failure\nmisc/check_error.dynamic_check_name_block_mismatch_test .............................................. RUNNING\nmisc/check_error.missing_mesh_test .................................................................",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2915382",
                  "updatedAt": "2022-06-09T17:10:17Z",
                  "publishedAt": "2022-06-09T16:51:38Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mntbighker"
                  },
                  "bodyText": "",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2915527",
                  "updatedAt": "2022-06-14T16:58:14Z",
                  "publishedAt": "2022-06-09T17:08:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you can start interactive jobs on most clusters, for pbs it s qsub -i iirc.\nthen once you have a bash shell on a node you just run tests either with:\nspecifying a restriction on the test suite\n./run_tests --re \"pattern_matching_a_test\"\n\nor like a regular MOOSE input:\n./moose_test-opt -i input_file_for_that_test.i\n\nyou are getting MPI errors.\nCan you add to your job script (or run interactively)\nwhich mpicc\nwhich mpirun \nmodule list\n\nso we know which mpi is being used.\nAlso what does ldd <your_executable> return?",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2915662",
                          "updatedAt": "2022-06-09T17:25:11Z",
                          "publishedAt": "2022-06-09T17:25:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "[mwmoorcroft@thor2-head ~]$ cd moose/test/\n[mwmoorcroft@thor2-head test]$ module load moose-dev-gcc\n[mwmoorcroft@thor2-head test]$ salloc -N1 -n4 -t 240\nsalloc: Granted job allocation 1275\n[mwmoorcroft@c4 test]$ which mpicc\n/opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/bin/mpicc\n[mwmoorcroft@c4 test]$ which mpirun\n/opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/bin/mpirun\n[mwmoorcroft@c4 test]$ module list\n\nCurrently Loaded Modules:\n  1) autotools   3) gnu9/9.4.0    5) ucx/1.11.2         7) openmpi4/4.1.1   9) phdf5/1.10.8\n  2) prun/2.2    4) hwloc/2.5.0   6) libfabric/1.13.0   8) ohpc            10) moose-dev-gcc/4.0.1\n\n[mwmoorcroft@c4 test]$ python -m pip list\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\naiohttp (3.8.1)\naiosignal (1.2.0)\nasync-timeout (4.0.2)\nasynctest (0.13.0)\nattrs (21.4.0)\ncharset-normalizer (2.0.12)\ncycler (0.11.0)\ndeepdiff (5.7.0)\nfrozenlist (1.2.0)\nidna (3.3)\nidna-ssl (1.1.0)\nkiwisolver (1.3.1)\nmatplotlib (3.3.4)\nmpi4py (3.1.3)\nmpmath (1.2.1)\nmultidict (5.2.0)\nnumpy (1.19.5)\nordered-set (4.0.2)\npandas (1.1.5)\nPillow (8.4.0)\npip (9.0.3)\npyparsing (3.0.9)\npython-dateutil (2.8.2)\npytz (2022.1)\nPyYAML (3.12)\nscipy (1.5.4)\nsetuptools (39.2.0)\nsix (1.16.0)\nsympy (1.9)\ntyping-extensions (4.1.1)\nvtk (9.1.0)\nwslink (1.6.5)\nyarl (1.7.2)\n\n[mwmoorcroft@c4 test]$ ldd ../libmesh/installed/bin/meshtool-opt\n\tlinux-vdso.so.1 (0x00007ffe64b99000)\n\tlibmesh_opt.so.0 => /home/mwmoorcroft/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 (0x00007f17efe08000)\n\tlibnetcdf.so.13 => /home/mwmoorcroft/moose/scripts/../libmesh/installed/lib/libnetcdf.so.13 (0x00007f17efaf9000)\n\tlibtimpi_opt.so.10 => /home/mwmoorcroft/moose/scripts/../libmesh/installed/lib/libtimpi_opt.so.10 (0x00007f17ef8da000)\n\tlibz.so.1 => /lib64/libz.so.1 (0x00007f17ef6c2000)\n\tlibslepc.so.3.16 => /home/shared/petsc/lib/libslepc.so.3.16 (0x00007f17ef104000)\n\tlibpetsc.so.3.16 => /home/shared/petsc/lib/libpetsc.so.3.16 (0x00007f17ed735000)\n\tlibHYPRE-2.23.0.so => /home/shared/petsc/lib/libHYPRE-2.23.0.so (0x00007f17ed10c000)\n\tlibstrumpack.so => /home/shared/petsc/lib/libstrumpack.so (0x00007f17ec5fe000)\n\tlibsuperlu_dist.so.7 => /home/shared/petsc/lib/libsuperlu_dist.so.7 (0x00007f17ec2ed000)\n\tlibhdf5_hl.so.100 => /opt/ohpc/pub/libs/gnu9/openmpi4/hdf5/1.10.8/lib/libhdf5_hl.so.100 (0x00007f17ec0ca000)\n\tlibhdf5.so.103 => /opt/ohpc/pub/libs/gnu9/openmpi4/hdf5/1.10.8/lib/libhdf5.so.103 (0x00007f17ebac4000)\n\tlibparmetis.so => /home/shared/petsc/lib/libparmetis.so (0x00007f17eb884000)\n\tlibmetis.so => /home/shared/petsc/lib/libmetis.so (0x00007f17eb621000)\n\tlibmpi_usempif08.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi_usempif08.so.40 (0x00007f17eb3e3000)\n\tlibmpi_usempi_ignore_tkr.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi_usempi_ignore_tkr.so.40 (0x00007f17eb1d8000)\n\tlibmpi_mpifh.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi_mpifh.so.40 (0x00007f17eaf6c000)\n\tlibmpi.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi.so.40 (0x00007f17eac40000)\n\tlibgfortran.so.5 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libgfortran.so.5 (0x00007f17ea7ad000)\n\tlibgcc_s.so.1 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libgcc_s.so.1 (0x00007f17ea595000)\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f17ea375000)\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00007f17ea16d000)\n\tlibquadmath.so.0 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libquadmath.so.0 (0x00007f17e9f26000)\n\tlibstdc++.so.6 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libstdc++.so.6 (0x00007f17e9b46000)\n\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007f17e9942000)\n\tlibtirpc.so.3 => /lib64/libtirpc.so.3 (0x00007f17e970f000)\n\tlibmpi_cxx.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi_cxx.so.40 (0x00007f17e94f2000)\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007f17e9170000)\n\tlibgomp.so.1 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libgomp.so.1 (0x00007f17e8f3a000)\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007f17e8b75000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f17f159a000)\n\tlibopen-rte.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libopen-rte.so.40 (0x00007f17e88bb000)\n\tlibopen-orted-mpir.so => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libopen-orted-mpir.so (0x00007f17e86b9000)\n\tlibopen-pal.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libopen-pal.so.40 (0x00007f17e83f0000)\n\tlibutil.so.1 => /lib64/libutil.so.1 (0x00007f17e81ec000)\n\tlibhwloc.so.15 => /opt/ohpc/pub/libs/hwloc/lib/libhwloc.so.15 (0x00007f17e7f94000)\n\tlibgssapi_krb5.so.2 => /lib64/libgssapi_krb5.so.2 (0x00007f17e7d3f000)\n\tlibkrb5.so.3 => /lib64/libkrb5.so.3 (0x00007f17e7a55000)\n\tlibk5crypto.so.3 => /lib64/libk5crypto.so.3 (0x00007f17e783e000)\n\tlibcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007f17e763a000)\n\tlibxml2.so.2 => /lib64/libxml2.so.2 (0x00007f17e72d2000)\n\tlibkrb5support.so.0 => /lib64/libkrb5support.so.0 (0x00007f17e70c1000)\n\tlibkeyutils.so.1 => /lib64/libkeyutils.so.1 (0x00007f17e6ebd000)\n\tlibcrypto.so.1.1 => /lib64/libcrypto.so.1.1 (0x00007f17e69d4000)\n\tlibresolv.so.2 => /lib64/libresolv.so.2 (0x00007f17e67bd000)\n\tliblzma.so.5 => /lib64/liblzma.so.5 (0x00007f17e6596000)\n\tlibselinux.so.1 => /lib64/libselinux.so.1 (0x00007f17e636c000)\n\tlibpcre2-8.so.0 => /lib64/libpcre2-8.so.0 (0x00007f17e60e8000)",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2949218",
                          "updatedAt": "2022-06-14T17:54:03Z",
                          "publishedAt": "2022-06-14T17:10:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "What is this module? moose-dev-gcc/4.0.1\nIs it a module that includes the other ones?\nHas the petsc module been compiled with this mpi?\nFinally just a hunch, could you try compiling on the compute node? Like from scratch. just in case the compute node is too different from the head node\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2949598",
                          "updatedAt": "2022-06-14T18:05:38Z",
                          "publishedAt": "2022-06-14T18:05:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "[root@thor2-head ~]# cat /opt/ohpc/pub/moduledeps/gnu9-openmpi4/moose-dev-gcc/4.0.1\n#%Module1.0#####################################################################\n## MOOSE module\n\nset base_path   /home/shared\n\nproc ModulesHelp { } {\n\nputs stderr \" \"\nputs stderr \"This module loads moose gnu9\"\nputs stderr \"toolchain and the openmpi MPI stack.\"\nputs stderr \" \"\n\nputs stderr \"\\nVersion 4.0.1\\n\"\n\n}\nmodule-whatis \"Name: moose gnu9 compiler and openmpi MPI\"\nmodule-whatis \"Version: 4.0.1\"\n\nset     version                     4.0.1\n\nconflict petsc\nconflict openmc\ndepends-on phdf5\n\nsetenv CC       mpicc\nsetenv CXX      mpicxx\nsetenv F90      mpif90\nsetenv F77      mpif77\nsetenv FC       mpif90\nsetenv PETSC_ARCH\tarch-moose\nsetenv PETSC_DIR        $base_path/petsc\n# setenv PMIX_MCA_gds\thash\n\nprepend-path PYTHONPATH         /home/shared/petsc/python3.6\n\nYes, I compiled the shared petsc with the same mpi.\nI can look into compiling on a node. But I believe they are set up on OpenHPC with a lot of the \"build\" environment excluded from the node boot image. At present there is not even a make command on the nodes. As far as I knew the dev environment (build) is intended to be on the head. Then the binaries are run on the nodes. So all you need is the built shared libs.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2949771",
                          "updatedAt": "2022-06-14T19:01:58Z",
                          "publishedAt": "2022-06-14T18:28:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@milljm I m not sure what is going on here, i dont see anything wrong",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2950037",
                          "updatedAt": "2022-06-14T19:02:12Z",
                          "publishedAt": "2022-06-14T19:02:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "You can probably drop PETSC_ARCH in the 4.0.1.lua file. As it looks like PETSc libs are located in\n/home/shared/petsc/lib/libpetsc.so.3.16\n\nand not in a $PETSC_DIR/$PETSC_ARCH path:\n/home/shared/petsc/arch-moose/lib/libpetsc.so.3.16\n\nCan you log into a compute node, and perform an ldd command on the moose_test-opt binary? If we're missing libraries that would be a good indicator. If we're not, does moose_test-opt run correctly enough to print a help page (as @GiudGiud asked)?\n<obtain a compute node>\n<module load>\ncd /home/mwmoorcroft/moose/test/\n./moose_test-opt -h\nAnd then if that works... try with MPI:\nmpiexec -n 2 ./moose_test-opt -h",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2950368",
                          "updatedAt": "2022-06-14T19:57:20Z",
                          "publishedAt": "2022-06-14T19:56:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "I made some mods to the node environment, and shockingly the recompile seems to be working so far. I'm not very far in yet.\nconfigure: Detected BOOST_ROOT; continuing with --with-boost=/opt/ohpc/pub/libs/gnu9/openmpi4/boost/1.76.0\nchecking for Boost headers version >= 1.47.0... /opt/ohpc/pub/libs/gnu9/openmpi4/boost/1.76.0/include\nchecking for Boost's header version... 1_76\nchecking for the toolset name used by Boost for mpicxx -std=gnu++17... configure: WARNING: could not figure out which toolset name to use for mpicxx -std=gnu++17\nchecking for the flags needed to use pthreads... conftest.cpp: In function 'int main()':\nconftest.cpp:34:24: warning: null argument where non-null required (argument 1) [-Wnonnull]\n34 |     pthread_attr_init(0); pthread_cleanup_push(0, 0);\n|                        ^\nconftest.cpp:35:27: warning: null argument where non-null required (argument 1) [-Wnonnull]\n35 |     pthread_create(0,0,0,0); pthread_cleanup_pop(0);\n|                           ^\nconftest.cpp:35:27: warning: null argument where non-null required (argument 3) [-Wnonnull]\nconftest.cpp:33:27: warning: 'th' is used uninitialized in this function [-Wuninitialized]\n33 | pthread_t th; pthread_join(th, 0);\n|               ~~~~~~~~~~~~^~~~~~~\n-pthread",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2950655",
                          "updatedAt": "2022-06-14T20:44:20Z",
                          "publishedAt": "2022-06-14T20:44:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "I removed arch-moose, but In order to compile on a node directly it appears I must follow this page...\nhttps://mooseframework.inl.gov/getting_started/installation/offline_installation.html\nThe nodes have no access to the internet.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2950845",
                          "updatedAt": "2022-06-14T21:21:44Z",
                          "publishedAt": "2022-06-14T21:21:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I'd like to come back to the first issue of building on the head-node and then attempting running said built binaries on the compute node.\nI am more familiar with this method, as this is how our HPC clusters operate. This is in response to you creating a new issue: #21304. I don't think we can solve both at the same time, because you are operating from a shared location (that is my understanding anyway).\nSolving build issues on head-nodes is simpler, than solving build issues on what is supposed to be compute-only nodes (while I agree, building on compute nodes should work too).\nI am still curious what the following reports (after you build successfully on the head-node):\n<obtain a compute node>\n<module load>\ncd /home/mwmoorcroft/moose/test/\n./moose_test-opt -h\nAnd then if that works... try with MPI:\nmpiexec -n 2 ./moose_test-opt -h",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2957835",
                          "updatedAt": "2022-06-15T17:12:33Z",
                          "publishedAt": "2022-06-15T17:12:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "Ok, I appreciate the help. I'm going to complete the build on the head and I'll report back here when I'm ready to test again. I was under the impression that if you can't build moose on a compute node, you can't use moose on a compute node. It would be great if that's not true. I thought compiling was part of running moose. I'm the sysadmin, not the nuclear scientist.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2958689",
                          "updatedAt": "2022-06-15T19:24:43Z",
                          "publishedAt": "2022-06-15T19:22:13Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mntbighker"
                  },
                  "bodyText": "I'll check, but....\n[mwmoorcroft@c4 test]$ prun ./moose_test-opt -i ./tests/mesh/concentric_circle_mesh/concentric_circ\nle_mesh.i --mesh-only\n[prun] Master compute host = c4\n[prun] Resource manager = slurm\n[prun] Launch cmd = mpirun ./moose_test-opt -i ./tests/mesh/concentric_circle_mesh/concentric_circle_mesh.i --mesh-only (family=openmpi4)\nIn UnstructuredMesh::stitch_meshes:\nThis mesh has 53 nodes on boundary 1.\nOther mesh has 53 nodes on boundary 3.\nMinimum edge length on both surfaces is 0.002625.\nIn UnstructuredMesh::stitch_meshes:\nFound 53 matching nodes.\nIn UnstructuredMesh::stitch_meshes:\nThis mesh has 105 nodes on boundary 2.\nOther mesh has 105 nodes on boundary 4.\nMinimum edge length on both surfaces is 0.002625.\nIn UnstructuredMesh::stitch_meshes:\nFound 105 matching nodes.\nMesh Information:\nelem_dimensions()={2}\nspatial_dimension()=2\nn_nodes()=1225\nn_local_nodes()=228\nn_elem()=1212\nn_local_elem()=202\nn_active_elem()=1212\nn_subdomains()=9\nn_partitions()=6\nn_processors()=6\nn_threads()=1\nprocessor_id()=0\nis_prepared()=true\nis_replicated()=true\nMesh Bounding Box:\nMinimum: (x,y,z)=(-0.710315, -0.710315,        0)\nMaximum: (x,y,z)=(0.710315, 0.710315,        0)\nDelta:   (x,y,z)=( 1.42063,  1.42063,        0)\nMesh Element Type(s):\nQUAD4\nMesh Nodesets:\nNodeset 1, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 2, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 3, 7 nodes\nBounding box minimum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 4, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nMesh Sidesets:\nSideset 1 (left), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=(2.22045e-16,  1.42063,        0)\nSideset 2 (bottom), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063, 3.33067e-16,        0)\nSideset 3 (right), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=(2.22045e-16,  1.42063,        0)\nSideset 4 (top), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063, 1.11022e-16,        0)\nMesh Edgesets:\nNone\nMesh Subdomains:\nSubdomain 1: 276 elems (QUAD4, 276 active), 289 active nodes\nVolume: 0.201323\nBounding box minimum: (x,y,z)=( -0.2546,  -0.2546,        0)\nBounding box maximum: (x,y,z)=(  0.2546,   0.2546,        0)\nBounding box delta: (x,y,z)=(  0.5092,   0.5092,        0)\nSubdomain 2: 144 elems (QUAD4, 144 active), 161 active nodes\nVolume: 0.150984\nBounding box minimum: (x,y,z)=( -0.3368,  -0.3368,        0)\nBounding box maximum: (x,y,z)=(  0.3368,   0.3368,        0)\nBounding box delta: (x,y,z)=(  0.6736,   0.6736,        0)\nSubdomain 3: 96 elems (QUAD4, 96 active), 120 active nodes\nVolume: 0.0502081\nBounding box minimum: (x,y,z)=(   -0.36,    -0.36,        0)\nBounding box maximum: (x,y,z)=(    0.36,     0.36,        0)\nBounding box delta: (x,y,z)=(    0.72,     0.72,        0)\nSubdomain 4: 96 elems (QUAD4, 96 active), 120 active nodes\nVolume: 0.0502251\nBounding box minimum: (x,y,z)=( -0.3818,  -0.3818,        0)\nBounding box maximum: (x,y,z)=(  0.3818,   0.3818,        0)\nBounding box delta: (x,y,z)=(  0.7636,   0.7636,        0)\nSubdomain 5: 96 elems (QUAD4, 96 active), 101 active nodes\nVolume: 0.0252443\nBounding box minimum: (x,y,z)=( -0.3923,  -0.3923,        0)\nBounding box maximum: (x,y,z)=(  0.3923,   0.3923,        0)\nBounding box delta: (x,y,z)=(  0.7846,   0.7846,        0)\nSubdomain 6: 48 elems (QUAD4, 48 active), 72 active nodes\nVolume: 0.0251788\nBounding box minimum: (x,y,z)=( -0.4025,  -0.4025,        0)\nBounding box maximum: (x,y,z)=(  0.4025,   0.4025,        0)\nBounding box delta: (x,y,z)=(   0.805,    0.805,        0)\nSubdomain 7: 48 elems (QUAD4, 48 active), 72 active nodes\nVolume: 0.021476\nBounding box minimum: (x,y,z)=(  -0.411,   -0.411,        0)\nBounding box maximum: (x,y,z)=(   0.411,    0.411,        0)\nBounding box delta: (x,y,z)=(   0.822,    0.822,        0)\nSubdomain 8: 144 elems (QUAD4, 144 active), 162 active nodes\nVolume: 0.176113\nBounding box minimum: (x,y,z)=(  -0.475,   -0.475,        0)\nBounding box maximum: (x,y,z)=(   0.475,    0.475,        0)\nBounding box delta: (x,y,z)=(    0.95,     0.95,        0)\nSubdomain 9: 264 elems (QUAD4, 264 active), 288 active nodes\nVolume: 1.31744\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063,  1.42063,        0)\nGlobal mesh volume = 2.01819\n[mwmoorcroft@c4 test]$ module unload pmix\n[mwmoorcroft@c4 test]$ prun ./moose_test-opt -i ./tests/mesh/concentric_circle_mesh/concentric_circ\nle_mesh.i --mesh-only\n[prun] Master compute host = c4\n[prun] Resource manager = slurm\n[prun] Launch cmd = mpirun ./moose_test-opt -i ./tests/mesh/concentric_circle_mesh/concentric_circle_mesh.i --mesh-only (family=openmpi4)\nIn UnstructuredMesh::stitch_meshes:\nThis mesh has 53 nodes on boundary 1.\nOther mesh has 53 nodes on boundary 3.\nMinimum edge length on both surfaces is 0.002625.\nIn UnstructuredMesh::stitch_meshes:\nFound 53 matching nodes.\nIn UnstructuredMesh::stitch_meshes:\nThis mesh has 105 nodes on boundary 2.\nOther mesh has 105 nodes on boundary 4.\nMinimum edge length on both surfaces is 0.002625.\nIn UnstructuredMesh::stitch_meshes:\nFound 105 matching nodes.\nMesh Information:\nelem_dimensions()={2}\nspatial_dimension()=2\nn_nodes()=1225\nn_local_nodes()=228\nn_elem()=1212\nn_local_elem()=202\nn_active_elem()=1212\nn_subdomains()=9\nn_partitions()=6\nn_processors()=6\nn_threads()=1\nprocessor_id()=0\nis_prepared()=true\nis_replicated()=true\nMesh Bounding Box:\nMinimum: (x,y,z)=(-0.710315, -0.710315,        0)\nMaximum: (x,y,z)=(0.710315, 0.710315,        0)\nDelta:   (x,y,z)=( 1.42063,  1.42063,        0)\nMesh Element Type(s):\nQUAD4\nMesh Nodesets:\nNodeset 1, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 2, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 3, 7 nodes\nBounding box minimum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 4, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nMesh Sidesets:\nSideset 1 (left), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=(2.22045e-16,  1.42063,        0)\nSideset 2 (bottom), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063, 3.33067e-16,        0)\nSideset 3 (right), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=(2.22045e-16,  1.42063,        0)\nSideset 4 (top), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063, 1.11022e-16,        0)\nMesh Edgesets:\nNone\nMesh Subdomains:\nSubdomain 1: 276 elems (QUAD4, 276 active), 289 active nodes\nVolume: 0.201323\nBounding box minimum: (x,y,z)=( -0.2546,  -0.2546,        0)\nBounding box maximum: (x,y,z)=(  0.2546,   0.2546,        0)\nBounding box delta: (x,y,z)=(  0.5092,   0.5092,        0)\nSubdomain 2: 144 elems (QUAD4, 144 active), 161 active nodes\nVolume: 0.150984\nBounding box minimum: (x,y,z)=( -0.3368,  -0.3368,        0)\nBounding box maximum: (x,y,z)=(  0.3368,   0.3368,        0)\nBounding box delta: (x,y,z)=(  0.6736,   0.6736,        0)\nSubdomain 3: 96 elems (QUAD4, 96 active), 120 active nodes\nVolume: 0.0502081\nBounding box minimum: (x,y,z)=(   -0.36,    -0.36,        0)\nBounding box maximum: (x,y,z)=(    0.36,     0.36,        0)\nBounding box delta: (x,y,z)=(    0.72,     0.72,        0)\nSubdomain 4: 96 elems (QUAD4, 96 active), 120 active nodes\nVolume: 0.0502251\nBounding box minimum: (x,y,z)=( -0.3818,  -0.3818,        0)\nBounding box maximum: (x,y,z)=(  0.3818,   0.3818,        0)\nBounding box delta: (x,y,z)=(  0.7636,   0.7636,        0)\nSubdomain 5: 96 elems (QUAD4, 96 active), 101 active nodes\nVolume: 0.0252443\nBounding box minimum: (x,y,z)=( -0.3923,  -0.3923,        0)\nBounding box maximum: (x,y,z)=(  0.3923,   0.3923,        0)\nBounding box delta: (x,y,z)=(  0.7846,   0.7846,        0)\nSubdomain 6: 48 elems (QUAD4, 48 active), 72 active nodes\nVolume: 0.0251788\nBounding box minimum: (x,y,z)=( -0.4025,  -0.4025,        0)\nBounding box maximum: (x,y,z)=(  0.4025,   0.4025,        0)\nBounding box delta: (x,y,z)=(   0.805,    0.805,        0)\nSubdomain 7: 48 elems (QUAD4, 48 active), 72 active nodes\nVolume: 0.021476\nBounding box minimum: (x,y,z)=(  -0.411,   -0.411,        0)\nBounding box maximum: (x,y,z)=(   0.411,    0.411,        0)\nBounding box delta: (x,y,z)=(   0.822,    0.822,        0)\nSubdomain 8: 144 elems (QUAD4, 144 active), 162 active nodes\nVolume: 0.176113\nBounding box minimum: (x,y,z)=(  -0.475,   -0.475,        0)\nBounding box maximum: (x,y,z)=(   0.475,    0.475,        0)\nBounding box delta: (x,y,z)=(    0.95,     0.95,        0)\nSubdomain 9: 264 elems (QUAD4, 264 active), 288 active nodes\nVolume: 1.31744\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063,  1.42063,        0)\nGlobal mesh volume = 2.01819",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2960335",
                  "updatedAt": "2022-06-16T01:40:10Z",
                  "publishedAt": "2022-06-16T01:40:09Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Some tests are designed to only work in serial, while other are designed to only work in parallel. It would seem you need to prefix every moose based app command with prun. You can do so by exporting the following influential environment variable:\nexport MOOSE_MPI_COMMAND=prun\n./run_test -j 16\nThere are two tests which require a minimum of 16 cores:\ncd moose/test/tests\ngit grep \"min_parallel\"\n<trimmed>\npartitioners/hierarchical_grid_partitioner/tests:    min_parallel = 16\nfvkernels/fv_adapt/tests:    min_parallel = 16\n<trimmed>\nIf you do not provide -j 16, these two tests will be skipped with \"insufficient slots\"\nBut anyway... lets see if running with prun changes the game at all first!",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2964037",
                          "updatedAt": "2022-06-16T12:54:18Z",
                          "publishedAt": "2022-06-16T12:54:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I can't really go into anything about Python being our issue. The TestHarness is dying very early. Probably because the TestHarness is breaking some rules while on a node, and being killed externally (not running with prun I suspect).\nWhen the TestHarness is killed prematurely you will commonly see a stack trace caused by Python threading:\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2964058",
                          "updatedAt": "2022-06-16T12:59:25Z",
                          "publishedAt": "2022-06-16T12:56:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "With the prun env variable it dies pretty quickly. The prun command is a wrapper script that runs mpiexec if I recall.\n[mwmoorcroft@c1 test]$ prun\nThis OpenHPC utility is used to launch parallel (MPI) applications\nwithin a supported resource manager.\nUsage: prun  executable [arguments]\nwhere available options are as follows:\n-h        generate help message and exit\n-v        enable verbose output\nmisc/check_error.dot_integrity_check: Tester failed, reason: EXPECTED ERROR MISSING\nmisc/check_error.dot_integrity_check:\nmisc/check_error.dot_integrity_check ......................................... FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.multi_precond_test: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.multi_precond_test: Running command: prun -n 1 /home/mwmoorcroft/moose/test/moose_test-opt -i multi_precond_test.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.multi_precond_test: /opt/ohpc/pub/utils/prun/2.2/prun: illegal option -- n\nmisc/check_error.multi_precond_test: Invalid option: -",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965587",
                          "updatedAt": "2022-06-16T16:42:27Z",
                          "publishedAt": "2022-06-16T16:41:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "MOOSE_MPI_COMMAND=mpirun\nand\n#> ./run_script -j 6 \n\nSeems to be giving the best result.\nAt this point it appears to me we are back to python modules that exist on head that are missing on nodes. But it's big progress.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965667",
                          "updatedAt": "2022-06-16T16:55:13Z",
                          "publishedAt": "2022-06-16T16:55:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "Not surprisingly, still lots of these...\nusability.command-line/empty: Working Directory: /home/mwmoorcroft/moose/test/tests/usability\nusability.command-line/empty: Running command: /home/mwmoorcroft/moose/test/moose_test-opt\nusability.command-line/empty: [c1.Thor2.local:158018] OPAL ERROR: Unreachable in file pmix3x_client.c at line 112\nusability.command-line/empty: --------------------------------------------------------------------------\nusability.command-line/empty: The application appears to have been direct launched using \"srun\",\nusability.command-line/empty: but OMPI was not built with SLURM's PMI support and therefore cannot\nusability.command-line/empty: execute. There are several options for building PMI support under\nusability.command-line/empty: SLURM, depending upon the SLURM version you are using:\nusability.command-line/empty:\nusability.command-line/empty:   version 16.05 or later: you can use SLURM's PMIx support. This\nusability.command-line/empty:   requires that you configure and build SLURM --with-pmix.\nusability.command-line/empty:\nusability.command-line/empty:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or\nusability.command-line/empty:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually\nusability.command-line/empty:   install PMI-2. You must then build Open MPI using --with-pmi pointing\nusability.command-line/empty:   to the SLURM PMI library location.\nusability.command-line/empty:\nusability.command-line/empty: Please configure as appropriate and try again.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965691",
                          "updatedAt": "2022-06-16T17:53:00Z",
                          "publishedAt": "2022-06-16T16:58:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "It's still dying with python errors without getting the end statement with skipped, failed, etc.\nindicators/laplacian_jump_indicator.group/test_biharmonic ................................................. OK\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/RunParallel.py\", line 44, in run\n    job.run()\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Job.py\", line 227, in run\n    self.__tester.run(self.timer, self.options)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/testers/bench.py\", line 163, in run\n    t.run(timer, timeout=p['max_time'])\n  File \"/home/mwmoorcroft/moose/python/TestHarness/testers/bench.py\", line 98, in run\n    raise RuntimeError('command {} returned nonzero exit code'.format(cmd))\nRuntimeError: command ['/home/mwmoorcroft/moose/test/moose_test-opt', '-i', '/home/mwmoorcroft/moose/test/tests/kernels/simple_diffusion/simple_diffusion.i', '--check-input', 'Mesh/nx=100', 'Mesh/ny=100', 'Outputs/console=false', 'Outputs/exodus=false', 'Outputs/csv=false', '--no-gdb-backtrace'] returned nonzero exit code\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 412, in runJob\n    self.run(job) # Hand execution over to derived scheduler\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/RunParallel.py\", line 74, in run\n    tester.setStatus(tester.error, 'PYTHON EXCEPTION')\nAttributeError: 'SpeedTest' object has no attribute 'error'\n\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 260, in queueJobs\n    self.run_pool.apply_async(self.runJob, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\n\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 260, in queueJobs\n    self.run_pool.apply_async(self.runJob, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\n\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 255, in queueJobs\n    self.status_pool.apply_async(self.jobStatus, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 260, in queueJobs\n    self.run_pool.apply_async(self.runJob, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\n\n\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 255, in queueJobs\n    self.status_pool.apply_async(self.jobStatus, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965700",
                          "updatedAt": "2022-06-16T17:48:21Z",
                          "publishedAt": "2022-06-16T17:00:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "What I will say is it's not leaving a dozen running threads behind like it was before when it dies. I just ran on 2 nodes with 12 cores allocated and the tests seem to still be running.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965709",
                          "updatedAt": "2022-06-16T18:02:20Z",
                          "publishedAt": "2022-06-16T17:02:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "something is killing the TestHarness not by the doing of the TestHarness itself. When you see these errors. e.g:\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\n\nThe TestHarness was sent a kill signal by an external program.\nIt means we are not executing moose_test-opt correctly while on a node. The test where it failed at was a benchmark test. I believe the benchmark tests do not honor the MOOSE_MPI_COMMAND. Can you try this:\nexport MOOSE_MPI_COMMAND=mpirun\n./run_tests -j 6 -i tests\nThis basically tells the TestHarness to only run specification files with the name 'tests'.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2966105",
                          "updatedAt": "2022-06-16T18:13:20Z",
                          "publishedAt": "2022-06-16T18:13:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "The following is just for informational purposes....\nAs for prun failures, it is because we assume all MPI commands accepts -n and then a number... If it comes down to it, we can HAK the TestHarness to drop this, just to see if that works on your HPC system.\nThe file/line responsible for the additional -n argument is here: \n  \n    \n      moose/python/TestHarness/testers/RunApp.py\n    \n    \n         Line 207\n      in\n      74571c4\n    \n  \n  \n    \n\n        \n          \n           command = self.mpi_command + ' -n ' + str(ncpus) + ' ' + command \n        \n    \n  \n\n\nFor your purposes, you would want to alter that to:\n            command = self.mpi_command + ' ' + command\nHowever, if my understanding of prun is correct, running prun throws all available resources (cores) at one binary. Not something we want! The TestHarness manages resource allocation for you. e.g: we can't have two programs doing the same things (the TestHarness would try to run multiple pruns).",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2966197",
                          "updatedAt": "2022-06-16T18:26:19Z",
                          "publishedAt": "2022-06-16T18:26:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "It's running. For some reason some of the python modules I manually installed are not detected, so they get skipped. One example is vtk. A lot of the ones I installed are detected though.\nEverything seems pretty good just using MOOSE_MPI_COMMAND=mpirun. Probably don't need to bother about prun since it's just a convenience wrapper.\nThe tests are STILL running. :-)",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2966207",
                          "updatedAt": "2022-06-16T19:10:17Z",
                          "publishedAt": "2022-06-16T18:27:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mntbighker"
                  },
                  "bodyText": "For the record there has been a bug in OpenHPC (maybe it's slurm) that prevented pmix from working correctly.\nApparently they STILL have not fixed pmix.\n[]openhpc/ohpc#1320\nThey pushed the milestone from OHPC 2.5 to OHPC 2.6.  :-(  The only answer is to rebuild openmpi yourself. I have have done that, but then patches don't arrive with dnf.\nedit: I'm told OpenMPI PMIx may get fixed after July.",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2960339",
                  "updatedAt": "2022-06-22T18:12:43Z",
                  "publishedAt": "2022-06-16T01:41:01Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Anisotropic elasticity problem",
          "author": {
            "login": "avtarsinghh1991"
          },
          "bodyText": "Hello\nI am trying to incorporate anisotropic material properties with euler angles for solving simple elasticity problem\nFollowing is the input file.\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Mesh]\n  [generated]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = 10\n    ny = 10\n    xmax = 1\n    ymax = 1\n  []\n  [pin]\n    type = ExtraNodesetGenerator\n    input = generated\n    new_boundary = pin\n    coord = '0 0 0'\n  []\n[]\n\n[Modules/TensorMechanics/Master]\n  [all]\n    add_variables = true\n    strain = FINITE\n    generate_output = 'stress_xx stress_yy vonmises_stress'\n  []\n[]\n\n#\n# Added boundary/loading conditions\n# https://mooseframework.inl.gov/modules/tensor_mechanics/tutorials/introduction/step02.html\n#\n[BCs]\n  [left_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = left\n    value = 0\n  []\n  [bottom_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = bottom\n    value = 0\n  []\n  [right_x]\n    type = FunctionDirichletBC\n    variable = disp_y\n    boundary = top\n    function = '0.001*t'\n  []\n  [pin_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = pin\n    value = 0\n  []\n  [pin_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = pin\n    value = 0\n  []\n[]\n\n[Materials]\n  [./elasticity_matrix]\n    type = ComputeElasticityTensor\n    base_name = 'rotation_matrix'\n    fill_method = symmetric9\n    C_ijkl = '1.684e9 0.176e9 0.176e9 1.684e9 0.176e9 1.684e9 0.754e9 0.754e9 0.754e9'\n    # rotation matrix for rotating a vector 30 degrees about the z-axis\n    rotation_matrix = '0.8660254 -0.5       0.\n                       0.5        0.8660254 0\n                       0          0         1'\n  [../]\n  [./elasticity_euler]\n    type = ComputeElasticityTensor\n    base_name = 'euler'\n    fill_method = symmetric9\n    C_ijkl = '1.684e9 0.176e9 0.176e9 1.684e9 0.176e9 1.684e9 0.754e9 0.754e9 0.754e9'\n    euler_angle_1 = -30.  # same as above but opposite direction because _transpose_ gets built from these angles\n    euler_angle_2 = 0.\n    euler_angle_3 = 0.\n  [../]\n\n  [stress]\n    type = ComputeFiniteStrainElasticStress\n  []\n[]\n\n\n# consider all off-diagonal Jacobians for preconditioning\n[Preconditioning]\n  [SMP]\n    type = SMP\n    full = true\n  []\n[]\n\n[Executioner]\n  type = Transient\n  # we chose a direct solver here\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n  end_time = 100\n  dt = 1\n[]\n\nWhen I am running it. I am getting the following error\n\n*** ERROR ***\nThe following error occurred in the object \"MOOSE Problem\", of type \"FEProblem\".\n\nMaterial property 'elasticity_tensor', requested by 'stress' is not defined on block 0\nMaterial property 'elasticity_tensor', requested by 'stress_face' is not defined on block 0\nMaterial property 'elasticity_tensor', requested by 'stress_neighbor' is not defined on block 0\n\nFurther, i have my own abaqus mesh file with few grains. How can I read the euler angles and youngs modulus for different grains from the file. Please suggest some reference example.\nPlease help. Thank you in advance.",
          "url": "https://github.com/idaholab/moose/discussions/19576",
          "updatedAt": "2022-06-22T16:38:56Z",
          "publishedAt": "2021-12-07T23:58:14Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "you should not define two ComputeElasticityTensor\nIt seems that you want to model polycrystalline materials. If so, please take a look the crystal plasticity models in MOOSE. see https://mooseframework.inl.gov/source/materials/crystal_plasticity/ComputeMultipleCrystalPlasticityStress.html",
                  "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1769881",
                  "updatedAt": "2022-06-13T08:09:01Z",
                  "publishedAt": "2021-12-08T02:14:06Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thanks for your reply. Actually, I am trying to rotate the elastic tensor depending on the crystallographic plane.\nC' = RCR^T\nin which R is the rotation matrix.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770071",
                          "updatedAt": "2022-06-13T08:09:02Z",
                          "publishedAt": "2021-12-08T03:07:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "You can keep just one ComputeElasticityTensor and then remove the base_name.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770174",
                          "updatedAt": "2022-06-13T08:09:11Z",
                          "publishedAt": "2021-12-08T03:43:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Yes, I did the same. I even did a few runs to make sure the orientation is fine. It's working now. Thank you very much.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770178",
                          "updatedAt": "2022-06-13T08:09:11Z",
                          "publishedAt": "2021-12-08T03:43:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thanks. Just a follow-up question. I also want to give different Euler angles for different grains. and I want to read those angles from the file. I have my own mesh file with different volume tags for different grains. Can you please suggest some example problems?\nMost of the CP model examples are single-crystal only.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770193",
                          "updatedAt": "2022-06-19T00:50:28Z",
                          "publishedAt": "2021-12-08T03:51:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "You can use ElementPropertyReadFile with read_type option to read in the Euler angles for each block.\nHere is one example\nmoose/modules/tensor_mechanics/test/tests/crystal_plasticity/user_object_based/prop_block_read.i",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770293",
                          "updatedAt": "2022-06-19T00:50:28Z",
                          "publishedAt": "2021-12-08T04:30:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thank you. I implemented it and its working.\nI am just wondering is that only works with \"ComputeElasticityTensorCP\" or any material type. Because I can only able to run it for \"ComputeElasticityTensorCP\".\nBelow is the input file.\n[GlobalParams]\ndisplacements = 'disp_x disp_y'\n[]\n[Mesh]\n[generated]\ntype = GeneratedMeshGenerator\ndim = 2\nnx = 10\nny = 10\nxmax = 1\nymax = 1\n[]\n[pin]\ntype = ExtraNodesetGenerator\ninput = generated\nnew_boundary = pin\ncoord = '0 0 0'\n[]\n[]\n[UserObjects]\n[./euler_angle_read]\ntype = ElementPropertyReadFile\nprop_file_name = 'input_file.txt'\nnprop = 3\nread_type = block\nnblock= 1\n[../]\n[]\n[Modules/TensorMechanics/Master]\n[all]\nadd_variables = true\nstrain = FINITE\ngenerate_output = 'stress_xx stress_yy vonmises_stress'\n[]\n[]\n[BCs]\n[left_x]\ntype = DirichletBC\nvariable = disp_x\nboundary = left\nvalue = 0\n[]\n[bottom_y]\ntype = DirichletBC\nvariable = disp_y\nboundary = bottom\nvalue = 0\n[]\n[right_x]\ntype = FunctionDirichletBC\nvariable = disp_x\nboundary = right\nfunction = '0.001t'\n[]\n[top_y]\ntype = FunctionDirichletBC\nvariable = disp_y\nboundary = top\nfunction = '0.001t'\n[]\n[pin_x]\ntype = DirichletBC\nvariable = disp_x\nboundary = pin\nvalue = 0\n[]\n[pin_y]\ntype = DirichletBC\nvariable = disp_y\nboundary = pin\nvalue = 0\n[]\n[]\n[Materials]\n[./elasticity_tensor_with_Euler]\ntype = ComputeElasticityTensorCP\nfill_method = axisymmetric9\nC_ijkl = '1.984e9 0.196e9 0.176e9 1.684e9 0.176e9 1.584e9 0.754e9 0.754e9 0.754e9'\nread_prop_user_object = euler_angle_read\n[../]\n[stress]\ntype = ComputeFiniteStrainElasticStress\n[]\n[]\n[Preconditioning]\n[SMP]\ntype = SMP\nfull = true\n[]\n[]\n[Executioner]\ntype = Transient\npetsc_options_iname = '-pc_type'\npetsc_options_value = 'lu'\nend_time = 100\ndt = 1\n[]\n[Outputs]\nexodus = true\ninterval =10\n[]\nIf I am putting material type = \"ComputeElasticityTensor\" instead of \"ComputeElasticityTensorCP\". It says \"unused parameter 'Materials/elasticity_tensor_with_Euler/read_prop_user_object'\"\nOr Can I use \"ElementPropertyReadFile\" for providing other properties to different materials.\nThanks in advance.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1775393",
                          "updatedAt": "2022-06-19T00:50:28Z",
                          "publishedAt": "2021-12-08T21:08:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "For elasticity tensors, only ComputeElasticityTensorCP is implemented to take Euler angles from ElementPropertyReadFile UO.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1775963",
                          "updatedAt": "2022-06-19T00:51:03Z",
                          "publishedAt": "2021-12-08T23:51:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "latmarat"
                          },
                          "bodyText": "@jiangwen84,\nwhen assigning euler angles to grains as \"blocks\", how do you set the correspondence between moose mesh elements and block ids?\nAlso, do you know how moose mesh elements are ordered along x,y,z (i.e., in which order the element labels are increasing)?\nThanks.\n@asingh-mit, I am also trying to run an elastic simulation with a polycrystal. Would you mind sharing your input file that worked for your elastic polycrystal?\nThank you all!",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-2978523",
                          "updatedAt": "2022-06-19T00:57:13Z",
                          "publishedAt": "2022-06-19T00:57:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "@latmarat You can either use MOOSE's SubdomainBoundingBoxGenerator or use external meshing tools, e.g. Cubit or Gmsh to assign block ids.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-3003954",
                          "updatedAt": "2022-06-22T16:38:56Z",
                          "publishedAt": "2022-06-22T16:38:55Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Question about random seed with ConservedLangevinNoise",
          "author": {
            "login": "mangerij"
          },
          "bodyText": "I have four calculations that I am running simultaneously on a cluster with 32 processors each (parallel across the nodes).\nEach of the calculations has the same random seed given to the variables (set in the UserObject as instructed).\nCan I trust that the noise that is given to my four calculations is the same because I have the same number of processors?",
          "url": "https://github.com/idaholab/moose/discussions/21379",
          "updatedAt": "2022-06-25T05:54:12Z",
          "publishedAt": "2022-06-22T09:06:00Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "The noise should be the consistent even with different parallel setting:\n\nMOOSE currently distributes a high-quality efficient Pseudo Random Number Generator package (mtwist) that is stable across different machine architectures. This random number generator is tied into MOOSE's random number generator system that can generate consistent spatial random number fields as parallel discretization changes (e.g. the number of threads/processors does not impact generated fields). The random number interface is very straightforward to use.\n\nSee https://mooseframework.inl.gov/source/interfaces/RandomInterface.html",
                  "url": "https://github.com/idaholab/moose/discussions/21379#discussioncomment-3003369",
                  "updatedAt": "2022-06-22T15:30:15Z",
                  "publishedAt": "2022-06-22T15:30:13Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "You can always confirm this by either\n\noutputting the generated field for visualization\noutputting some metrics about the sampled field using postprocessors",
                          "url": "https://github.com/idaholab/moose/discussions/21379#discussioncomment-3003383",
                          "updatedAt": "2022-06-22T15:32:28Z",
                          "publishedAt": "2022-06-22T15:32:28Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How does Materials object call member functions in VectorPostprocessor object?",
          "author": {
            "login": "PengWei97"
          },
          "bodyText": "Dear MOOSE experts,\nRecently I wanted to construct a grain boundary energy as a function of grain size in the material object GBEvolution. By searching the code, I found only the case where the material class is coupled with the userobject, like: _grain_tracker(getUserObject<GrainTracker>(\"grain_tracker\")), but not the case where the VectorPostprocessor is called. In addition, the FeatureVolumeVectorPostprocessor object calculates the volume of each feature, and creates the pubilc function getFeatureVolume so that we can get the volume of each feature based on the feature id.\nSo my question is, how to call FeatureVolumeVectorPostprocessor.getFeatureVolume in material object GBEvolution to get feature volume.\nAny suggestions or recommendations to fix the problem would be greatly appreciated.\nThank you\nWei",
          "url": "https://github.com/idaholab/moose/discussions/21369",
          "updatedAt": "2022-06-22T15:21:22Z",
          "publishedAt": "2022-06-21T15:54:42Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIf this is a standard vectorpostprocessor you should  _fe_problem.getVectorPostprocessorValueByName(_vpp_name, vec_name);\nto retrieve the values of the VPP. You can bind this to a reference in the constructor so that you only call this once.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21369#discussioncomment-2996602",
                  "updatedAt": "2022-06-21T19:13:30Z",
                  "publishedAt": "2022-06-21T19:13:30Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Ok, Thank you for your very kind reply.\nThis function getVectorPostprocessorValueByName can achieve my requirements, with the following code,\n  const auto & k1 = _fe_problem.getVectorPostprocessorValueByName(_vector_postprocessor_name, _vector_name); // _fe_problem. VectorPostprocessorValue \n  std::cout << \"the size of k1 is \" << k1.size() <<std::endl;\n  for(unsigned int i = 0; i < k1.size(); i++)\n      std::cout << \"the value of k1 is \" << k1[i] <<std::endl;  \nBut I'm not quite sure why _fe_problem is added? Please can you give me guidance on this.\nSimilarly, if you do not use _fe_problem, you can also get the desired result, and the result is as follows.\n\nwei",
                          "url": "https://github.com/idaholab/moose/discussions/21369#discussioncomment-3002355",
                          "updatedAt": "2022-06-22T13:38:54Z",
                          "publishedAt": "2022-06-22T13:38:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "it depends on which object has the VPPInterface to use this routine. The problem object does. I guess your material also does.\nEither way is fine",
                          "url": "https://github.com/idaholab/moose/discussions/21369#discussioncomment-3003151",
                          "updatedAt": "2022-06-22T15:05:41Z",
                          "publishedAt": "2022-06-22T15:05:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "good, the problem is solved, thank you very much!",
                          "url": "https://github.com/idaholab/moose/discussions/21369#discussioncomment-3003287",
                          "updatedAt": "2022-06-22T15:21:20Z",
                          "publishedAt": "2022-06-22T15:21:18Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Peacock's problem",
          "author": {
            "login": "tmewhy"
          },
          "bodyText": "Hi all,\nI installed moose on ubuntu20.04 offline using the official document solution. After I finished the installation, I used a simple case for testing. When opening peacock, the following reminder appeared:\n\nQCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 180, 180, 180, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 180, 180, 180, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'\n\nWhen I use peacock to open the output file (.e), peacock crashes with the following error:\n\nQCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 180, 180, 180, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 180, 180, 180, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255\nSegmentation fault (core dumped)\n\nMy guess is that qt or vtk has not been installed well, but I have reinstalled them several times and it still has no effect. Thanks for the help!",
          "url": "https://github.com/idaholab/moose/discussions/18048",
          "updatedAt": "2022-06-22T15:03:40Z",
          "publishedAt": "2021-06-10T02:35:55Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "It's impressive you were able to get Peacock to launch at all using the Offline instructions :) I am not seeing anything about Peacock and its myriad amount of dependencies one needs to install in order to make it work.\nHow did you manage (just curious)? Did you build each from source, or use a package manager?\nIndeed this feels like a python binding to one of the VTK or Qt libraries. Probably VTK. Since Peacock at the very least opens without more than a few notices. If it were Qt and it's stack, I would recon Peacock wouldn't load up at all.\nWe can use 'chigger' to test pieces of your install. Try the following simple test:\nexport MOOSE_DIR=/path/to/moose\nexport PYTHONPATH=/path/to/moose/python:$PYTHONPATH\n# where /path/to/moose is your absolute path to moose\n\ncd /path/to/moose/python/chigger/tests/simple\n./simple.py\nThe simple.py script should run for a bit, and produce a file: simple.png (if all goes well). It probably won't in your case, and I am hoping for a bit more information other than Seg fault. Regardless, this moose/python/chigger/tests/ location is littered with tests we can run to try and zero in on the issue. @aeslaughter would know best (tagging if he would like chime in).",
                  "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-851864",
                  "updatedAt": "2022-06-22T03:58:58Z",
                  "publishedAt": "2021-06-10T13:35:40Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "tmewhy"
                          },
                          "bodyText": "Thanks, I used \"apt-offline\" and \"pip download\" to obtain the relevant installation package on an online computer, and then install it on an offline computer. It did take some time : ) Although it has passed the make -j 6 test, I am not sure that I installed it correctly.\nI ran simple.pyand got the following prompt:\n\nAborted (core dumped)",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-856208",
                          "updatedAt": "2022-06-22T03:58:58Z",
                          "publishedAt": "2021-06-11T01:47:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Is this still an issue? Thinking of closing, we dont have too many resources on peacock these days.\nTo get a backtrace on a seg fault in python, should look at https://docs.python.org/3/library/faulthandler.html",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-1829264",
                          "updatedAt": "2022-06-22T03:59:26Z",
                          "publishedAt": "2021-12-16T23:04:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jianqixi"
                          },
                          "bodyText": "Did you solve the problem? I also met the same issue of \"Aborted (core dumped)\" when call peacock",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-2998793",
                          "updatedAt": "2022-06-22T04:00:43Z",
                          "publishedAt": "2022-06-22T04:00:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "@jianqixi, what OS are you operating from?\nThe following has nothing to do with your current situation. But I feel I must mention:\nI am currently removing support for Peacock in our next large update to our Conda packages: #21324\nI am opting instead to create a separate package (lets say: moose-peacock) which will most likely need to be installed into a separate environment e.g. something like this:\n(this doesn't work/exist yet, this is just for demonstration purposes)\nmamba create -n moose moose-libmesh moose-tools    # For compiling moose-based application\nmamba create -n peacock moose-peacock              # Strictly for running Peacock\nYou would then activate which ever environment you needed at the time.\nThe reason for separating them is due to the increasingly difficult-to-solve constraints the dependency chain required for Peacock operation V's the dependency chain required for the compilers/libraries necessary to build/run MOOSE based application.",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-3002444",
                          "updatedAt": "2022-06-22T21:47:08Z",
                          "publishedAt": "2022-06-22T13:47:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Posting this more for myself;\nPeacock (on MacOS) requires these Conda packages:\nmamba create -n peacock pyqt vtk matplotlib pandas python=3.x  # (where x is whatever\n                                                                  version of python that\n                                                                  your moose environment\n                                                                  is using)",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-3003130",
                          "updatedAt": "2022-06-22T15:03:41Z",
                          "publishedAt": "2022-06-22T15:03:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Output at each fixed point iteration",
          "author": {
            "login": "BoZeng1997"
          },
          "bodyText": "Hi,\nI want to know if there is a way to output exodus file at each picard iteration within a timestep? Or where can I find an example of creating custom flag for picard iteration?\nBo",
          "url": "https://github.com/idaholab/moose/discussions/21117",
          "updatedAt": "2022-06-25T05:53:36Z",
          "publishedAt": "2022-05-24T18:43:28Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nwe currently do not have this feature.\nif this is just for debugging I d recommend setting the max number of fixed point iterations at all the desired times (running the code multiple times) then setting the accept_on_max_fixed_point_iteration parameter of the executioner to true\nguillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814653",
                  "updatedAt": "2022-05-24T19:26:57Z",
                  "publishedAt": "2022-05-24T19:26:56Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "Not for debugging. I want to see how picard iteration find its path. The cost to run it till the critical step is high and the iteration in that step is of hundreds. Is there a way I can probably create an object to achieve this?",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814685",
                          "updatedAt": "2022-05-24T19:32:43Z",
                          "publishedAt": "2022-05-24T19:32:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If you\u2019re open to coding it s not that difficult to create an execute_on flag to output on fixed point iterations.\n@lindsayad has something like this in one of his branches I think",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814695",
                          "updatedAt": "2022-05-24T19:35:19Z",
                          "publishedAt": "2022-05-24T19:35:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "is Alex Lindsay open to inquire here? If so, shall I wait till he replies? I will be grateful if I can follow an example.",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814725",
                          "updatedAt": "2022-05-24T19:41:10Z",
                          "publishedAt": "2022-05-24T19:41:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "He s busy right now I believe but he ll get back to us when he has time",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814799",
                          "updatedAt": "2022-05-24T19:56:26Z",
                          "publishedAt": "2022-05-24T19:56:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "This is what I have for an example: lindsayad/moose@7d0e9a3228cd221",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2815809",
                          "updatedAt": "2022-05-24T23:41:45Z",
                          "publishedAt": "2022-05-24T23:41:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "Hi @lindsayad . Is there a way to pack each iteration within a step into one exodus file in sequence? I did not find the way to append iteration step in exodus output.",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2934483",
                          "updatedAt": "2022-06-12T22:44:30Z",
                          "publishedAt": "2022-06-12T22:44:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Probably not for Picard iterations. What would you even want to write the timestep as?",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2997413",
                          "updatedAt": "2022-06-21T21:43:32Z",
                          "publishedAt": "2022-06-21T21:43:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "need help mapping external field to nodes",
          "author": {
            "login": "jessecarterMOOSE"
          },
          "bodyText": "Hi all -\nI'm trying to apply an external field (temperature) to nodes on my mesh. I've got a list of node numbers and/or nodesets and the required temperature, which can also vary through time. I want to read in the values and apply them at the nodes, and also be able to interpolate across the element to the quadrature points for my material model. I figured a Nodal AuxKernel is the way to go for the last part, but what's a good way to read in nodal values, that change with time (piecewise linearly), from a file?",
          "url": "https://github.com/idaholab/moose/discussions/21371",
          "updatedAt": "2022-06-25T05:53:30Z",
          "publishedAt": "2022-06-21T17:00:57Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThe only time-dependent CSV reader I know of is using a Function.\nhttps://mooseframework.inl.gov/source/functions/PiecewiseMulticonstant.html\nand\nhttps://mooseframework.inl.gov/source/functions/PiecewiseMultilinear.html\nthis does not store info by nodes though, only XYZT\nso what you want here would be a time-dependent PropertyReadeFile user object.\nWe dont have that but we could use one\nhttps://mooseframework.inl.gov/source/userobject/PropertyReadFile.html\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21371#discussioncomment-2996646",
                  "updatedAt": "2022-06-21T19:20:50Z",
                  "publishedAt": "2022-06-21T19:20:49Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Thanks. Let's say I manage to get all the data read in. How do I get those values to the nodes and then to the quadrature points? Is there an AuxKernel that calls the property data?",
                          "url": "https://github.com/idaholab/moose/discussions/21371#discussioncomment-2996688",
                          "updatedAt": "2022-06-21T19:28:22Z",
                          "publishedAt": "2022-06-21T19:28:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so the way we do it with the propertyReadFile is that there is a different read routine/workflow depending on whether the data is stored by node / element / block / nearest-neighbor region.\nI would imagine you do the same, but add a time / time_index argument to the read routine.\nYes you may use an AuxKernel. We have not made a XXXXXFromCSVAuxKernel but it wouldnt be hard to make, following the same patterns as the PiecewiseConstantFromCSV Function\nFor getting data to nodes just store it by nodes and read by nodes.\nFor Qps, either do the same (store by Qp read by Qp) or the usual workflow does the conversion node-qp qp-nodes with the shape functions and what not.",
                          "url": "https://github.com/idaholab/moose/discussions/21371#discussioncomment-2996746",
                          "updatedAt": "2022-06-21T19:40:00Z",
                          "publishedAt": "2022-06-21T19:40:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Plate with centered circular hole.",
          "author": {
            "login": "Leni-Yeo"
          },
          "bodyText": "Good afternoon, I am trying to create a plate with centered circular hole for uni-axial tension test. I was wondering if moose has a set up already that I may have missed or if anyone has a recommendation on how to set the mesh.\nThank you!",
          "url": "https://github.com/idaholab/moose/discussions/21373",
          "updatedAt": "2022-06-21T19:21:44Z",
          "publishedAt": "2022-06-21T18:11:28Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "Yes, we have a way of doing that. Hang on, I'll find an example....",
                  "url": "https://github.com/idaholab/moose/discussions/21373#discussioncomment-2996244",
                  "updatedAt": "2022-06-21T18:15:36Z",
                  "publishedAt": "2022-06-21T18:15:31Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "It boils down to using ConcentricCircleMeshGenerator with has_outer_square = true and then using BlockDeletionGenerator to delete the inner circular block...",
                          "url": "https://github.com/idaholab/moose/discussions/21373#discussioncomment-2996301",
                          "updatedAt": "2022-06-21T18:23:20Z",
                          "publishedAt": "2022-06-21T18:23:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Leni-Yeo"
                          },
                          "bodyText": "Thank you!. I will try that.",
                          "url": "https://github.com/idaholab/moose/discussions/21373#discussioncomment-2996312",
                          "updatedAt": "2022-06-21T18:25:17Z",
                          "publishedAt": "2022-06-21T18:25:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "To get a 3D mesh you can add a MeshExtruderGenerator.",
                          "url": "https://github.com/idaholab/moose/discussions/21373#discussioncomment-2996321",
                          "updatedAt": "2022-06-21T18:26:17Z",
                          "publishedAt": "2022-06-21T18:26:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Leni-Yeo"
                          },
                          "bodyText": "It worked. Thank you!",
                          "url": "https://github.com/idaholab/moose/discussions/21373#discussioncomment-2996648",
                          "updatedAt": "2022-06-21T19:21:11Z",
                          "publishedAt": "2022-06-21T19:21:10Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error in compilation using remote desktop",
          "author": {
            "login": "bha112"
          },
          "bodyText": "Hello,\nUsing this command in my remote desktop for compilation of the input file **mpirun -np 4 ../../home/projects/mastodon/mastodon-opt -i big_model.i ** . Its just printing the same steps 4 times in the terminal, Instead it should use 4 processors for solving the problem.\nPlease find the 3 attached Terminal Screenshots , First figure shows that after giving 4 processors command its only Using\none processors\n\n, Second and Third figures shows that the same steps is displaying 4 times in the Terminal\n\n\nCould you please help me to solve the issue?\nThank you very much!\nBhavesh",
          "url": "https://github.com/idaholab/moose/discussions/21368",
          "updatedAt": "2022-06-25T05:53:16Z",
          "publishedAt": "2022-06-21T15:14:10Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nI've seen this before when the mpirun does not match the MPI compiler used.\nWhat did you use to compile?\nWhat does\nwhich mpirun\necho $CXX\nwhich mpicc\n\nreturn ?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21368#discussioncomment-2995028",
                  "updatedAt": "2022-06-21T15:16:18Z",
                  "publishedAt": "2022-06-21T15:16:18Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "bha112"
                          },
                          "bodyText": "Thanks for your response. You were right, I was using a different mpi version (open mpi 4.0.3) than the one that comes with moose tools. Once I activated conda moose environment, mpirun is working fine.",
                          "url": "https://github.com/idaholab/moose/discussions/21368#discussioncomment-2995610",
                          "updatedAt": "2022-06-21T16:30:17Z",
                          "publishedAt": "2022-06-21T16:30:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "This will change in the near future: #21324\nI am removing the dependency of MPICH within moose-tools. Many dependencies will be loosened in this regard. While others will be even tighter (compilers specifically).",
                          "url": "https://github.com/idaholab/moose/discussions/21368#discussioncomment-2995867",
                          "updatedAt": "2022-06-21T17:06:55Z",
                          "publishedAt": "2022-06-21T17:06:51Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error on running the input file in supercomputing cluster",
          "author": {
            "login": "avtarsinghh1991"
          },
          "bodyText": "Hello MOOSE experts\nI am trying to run the MOOSE application in the XSEDE cluster and found the following error. However, the code is running absolutely fine on my own computer and also in a small HPC cluster.\n*** ERROR ***\nA 'Concentration' is not a registered object.\n\nIf you are trying to find this object in a dynamically linked library, make sure that\nthe library can be found either in your \"Problem/library_path\" parameter or in the\nMOOSE_LIBRARY_PATH environment variable.\n\nStack frames: 21\n0: libMesh::print_trace(std::ostream&)\n1: moose::internal::mooseErrorRaw(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)\n2: void mooseError<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&)\n3: Factory::reportUnregisteredError(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const\n4: Factory::getValidParams(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)\n5: MooseObjectAction::MooseObjectAction(InputParameters)\n6: AddKernelAction::AddKernelAction(InputParameters)\n7: /home/x-avtar/projects/moose/framework/libmoose-opt.so.0(+0x11e97a3) [0x148b01ec47a3]\n8: std::shared_ptr<Action> Registry::build<AddKernelAction, Action>(InputParameters const&)\n9: ActionFactory::create(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, InputParameters&)\n10: Parser::walkRaw(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, hit::Node*)\n11: Parser::walk(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, hit::Node*)\n12: hit::Node::walk(hit::Walker*, hit::NodeType, hit::TraversalOrder)\n13: hit::Node::walk(hit::Walker*, hit::NodeType, hit::TraversalOrder)\n14: hit::Node::walk(hit::Walker*, hit::NodeType, hit::TraversalOrder)\n15: Parser::parse(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&)\n16: MooseApp::setupOptions()\n17: MooseApp::run()\n18: /home/x-avtar/projects/babbler/babbler-opt() [0x40adf5]\n19: __libc_start_main\n20: /home/x-avtar/projects/babbler/babbler-opt() [0x40b01e]\n--------------------------------------------------------------------------\nMPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD\nwith errorcode 1.\n\nNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nYou may or may not see output from other processes, depending on\nexactly when Open MPI kills them.\n\nI have already checked about the registration error. The concentration is a registered object as below in .C file\n#include \"Concentration.h\"\nregisterMooseObject(\"BabblerApp\", Concentration);\n\nI don't know why still it is giving the error. Please help.\nBest\nAvtar",
          "url": "https://github.com/idaholab/moose/discussions/20515",
          "updatedAt": "2024-05-01T06:04:19Z",
          "publishedAt": "2022-03-09T17:13:30Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nWhat are you running in the command line? Trying to check if the right application is ran\nWhat is the output of ls src/* include/* in babbler? Trying to check if the file is here\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2325923",
                  "updatedAt": "2022-06-16T00:09:41Z",
                  "publishedAt": "2022-03-09T17:41:09Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Output of ls src/* include/*\nx-avtar@login01.anvil:[babbler] $ ls src/* include/*\nsrc/main.C  src/main.x86_64-pc-linux-gnu.opt.lo  src/main.x86_64-pc-linux-gnu.opt.lo.d\n\ninclude/base:\nbabblerApp.h\n\ninclude/kernels:\nAnisotropic_PF_Fracture.h  CoupledNeumannBC.h  DiffusionTimeDerivative.h  Phasefield_eta_pmi.h\nConcentration.h            DarcyPressure.h     Phasefield_eta_gb.h        PhasefieldFracture.h\n\ninclude/Materials:\nAnisoDiffusivity.h  Aniso_Eigen_Strain.h  Aniso_Elasticity_Tensor.h  Anisotropic_PF.h\n\nsrc/base:\nbabblerApp.C  babblerApp.x86_64-pc-linux-gnu.opt.lo  babblerApp.x86_64-pc-linux-gnu.opt.lo.d\n\nsrc/kernels:\nAnisotropic_PF_Fracture.C  CoupledNeumannBC.C  DiffusionTimeDerivative.C  Phasefield_eta_pmi.C\nConcentration.C            DarcyPressure.C     Phasefield_eta_gb.C        PhasefieldFracture.C\n\nsrc/Materials:\nAnisoDiffusivity.C  Aniso_Eigen_Strain.C  Aniso_Elasticity_Tensor.C  Anisotropic_PF.C\n\n\nrun command:\nx-avtar@login01.anvil:[home] $ cd x-avtar/\nx-avtar@login01.anvil:[~] $ ls\nprojects  Research\nx-avtar@login01.anvil:[~] $ cd projects/\nx-avtar@login01.anvil:[projects] $ cd babbler/\nx-avtar@login01.anvil:[babbler] $ ls\n3D_Particles  babbler-opt  build  doc  include  lib  LICENSE  Makefile  README.md  run_tests  scripts  src  test  testroot  unit\nx-avtar@login01.anvil:[babbler] $ cd 3D_Particles/\nx-avtar@login01.anvil:[3D_Particles] $ ls\nabaqus_mesh_88.inp  cleavage_planes.txt  DIM_sub_particle.i  myscript.sh        Particle_matrix_in.e   run.sh\nabaqus_mesh.inp     DIM_sub.i            input_file.txt      Particle_matrix.i  Particle_matrix_iso.i\n\nx-avtar@login01.anvil:[3D_Particles] $ /home/x-avtar/projects/babbler/babbler-opt -i Particle_matrix.i",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2325967",
                          "updatedAt": "2022-06-16T00:09:42Z",
                          "publishedAt": "2022-03-09T17:46:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Does Concentration show up in ./babbler_opt --registry ?\nIf not can you recompile?",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2326901",
                          "updatedAt": "2022-06-16T00:09:43Z",
                          "publishedAt": "2022-03-09T20:40:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "How can I do that? Please let me know.",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2326982",
                          "updatedAt": "2022-06-16T00:09:43Z",
                          "publishedAt": "2022-03-09T20:58:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "How to check that if Concentration is there in babbler-opt--registry?",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2326998",
                          "updatedAt": "2022-06-16T00:09:46Z",
                          "publishedAt": "2022-03-09T21:00:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "run the command ./babbler-opt --registry then look through the output on the screen.\nTo recompile, type make in the babbler folder",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2327426",
                          "updatedAt": "2022-06-16T00:09:46Z",
                          "publishedAt": "2022-03-09T22:17:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Following is the output of the ./babbler-opt --registry\nConcentration.C is there ....\nBabblerApp      object  Concentration   Concentration   /home/x-avtar/projects/babbler/src/kernels/Concentration.C\nBabblerApp      object  CoupledNeumannBC        CoupledNeumannBC        /home/x-avtar/projects/babbler/src/kernels/CoupledNeumannBC.C\nBabblerApp      object  DarcyPressure   DarcyPressure   /home/x-avtar/projects/babbler/src/kernels/DarcyPressure.C\nBabblerApp      object  DiffusionTimeDerivative DiffusionTimeDerivative /home/x-avtar/projects/babbler/src/kernels/DiffusionTimeDerivative.C\nBabblerApp      object  PhasefieldFracture      PhasefieldFracture      /home/x-avtar/projects/babbler/src/kernels/PhasefieldFracture.C\nBabblerApp      object  Phasefield_eta_gb       Phasefield_eta_gb       /home/x-avtar/projects/babbler/src/kernels/Phasefield_eta_gb.C\nBabblerApp      object  Phasefield_eta_pmi      Phasefield_eta_pmi      /home/x-avtar/projects/babbler/src/kernels/Phasefield_eta_pmi.C\nBabblerApp      object  AnisoDiffusivity        AnisoDiffusivity        /home/x-avtar/projects/babbler/src/Materials/AnisoDiffusivity.C",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2331657",
                          "updatedAt": "2022-06-16T00:09:46Z",
                          "publishedAt": "2022-03-10T13:50:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "what does the input file look like?",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2332454",
                          "updatedAt": "2022-06-16T00:09:46Z",
                          "publishedAt": "2022-03-10T15:31:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Following is the input file.\n[Mesh]\n  [initial_mesh]\n     type = FileMeshGenerator\n     file = 'abaqus_mesh.inp'\n  []\n\n   [rename_blocks]\n  type = RenameBlockGenerator\n  input = initial_mesh\n  old_block = '91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178'\n  new_block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87'\n  []\n\n   [./scale]\n     type = TransformGenerator\n     input = rename_blocks\n     transform = SCALE\n     vector_value ='10e-6 10e-6 10e-6'\n  []\n\n\n   [central_boundary]\n    type = SideSetsAroundSubdomainGenerator\n    input = scale\n    block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87'\n    new_boundary = flux\n  []\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n[]\n\n[Variables]\n  [./diffused]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n\n  [./damage]\n    family = LAGRANGE\n    order = FIRST\n  [../]\n\n  [./disp_x]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n\n  [./disp_y]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n\n  [./disp_z]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n[]\n\n[ICs]\n  [./ic_particle]\n    type = ConstantIC\n    variable = diffused\n    value = 4.8976e4\n  [../]\n[]\n\n\n[AuxVariables]\n  [./eta_gb]\n  [../]\n\n    [./bounds_dummy_c]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n\n      [./bounds_dummy_d]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n\n    [./vonmises]\n  order = FIRST\n  family = MONOMIAL\n[../]\n\n[]\n\n\n[Kernels]\n  [./diffusion]\n    type = Concentration \n    variable = diffused\n    cmax = 4.8976e4\n    ki = 0.0\n  [../]\n\n  [./euler]\n    type = TimeDerivative\n    variable = diffused\n  [../]\n\n  [./ACbulk]\n    type = AllenCahn\n    variable = damage\n    f_name = F\n  [../]\n\n  [./ACInterfaceCleavageFracture]\n    type = ACInterface # Anisotropic_PF_Fracture\n    variable = damage\n    kappa_name = kappa_op\n#    beta_penalty = 0\n  [../]\n\n  [./dcdt]\n    type = TimeDerivative\n    variable = damage\n  [../]\n\n  [./solid_x]\n    type = PhaseFieldFractureMechanicsOffDiag\n    variable = disp_x\n    component = 0\n    c = damage\n  [../]\n\n  [./solid_y]\n    type = PhaseFieldFractureMechanicsOffDiag\n    variable = disp_y\n    component = 1\n    c = damage\n  [../]\n\n  [./solid_z]\n    type = PhaseFieldFractureMechanicsOffDiag\n    variable = disp_z\n    component = 2\n    c = damage\n  [../]\n\n\n  [./off_disp]\n    type = AllenCahnElasticEnergyOffDiag\n    variable = damage\n    displacements = 'disp_x disp_y disp_z'\n    mob_name = L\n  [../]\n[]\n\n[Modules/TensorMechanics/Master]\n  [all]\n    add_variables = true\n    strain = SMALL\n    automatic_eigenstrain_names = true\n    generate_output = 'vonmises_stress' # hydrostatic_stress\n    eigenstrain_names = eigenstrain\n  []\n[]\n\n[UserObjects]\n  [./euler_angle_read]\n    type = ElementPropertyReadFile\n    prop_file_name = 'input_file.txt'\n    nprop = 3\n    read_type = block\n    nblock= 89\n  [../]\n    [./cleavage_angle_read]\n    type = ElementPropertyReadFile\n    prop_file_name = 'cleavage_planes.txt'\n    nprop = 3\n    read_type = block\n    nblock= 89\n  [../]\n[]\n\n\n[Materials]\n\n#  [./Cleavage_planes]\n#    type = Anisotropic_PF\n#    read_prop_user_object = cleavage_angle_read\n#  [../]\n\n  [./pfbulkmat_particle]\n    type = GenericConstantMaterial\n    prop_names = 'gc_prop_b gc_prop_gb   l    visco'\n    prop_values = '10           10      0.2e-6   3e-6'\n#    block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85'\n  [../]\n\n\n  [./define_gc_prop]\n    type = ParsedMaterial\n    material_property_names = 'gc_prop_b gc_prop_gb'\n    f_name = gc_prop\n    args = 'eta_gb'\n    function = '(1-eta_gb)*(1-eta_gb)*gc_prop_b + (1-(1-eta_gb)*(1-eta_gb))*gc_prop_gb'\n#    block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88'\n  [../]\n\n  [./diffusivity_tensor_with_Euler_particle]\n    type = AnisoDiffusivity\n    read_prop_user_object = euler_angle_read\n    diffusivity = '1.0e-14 0 0 0 1.0e-13 0 0 0 1.0e-13'\n#    block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88'\n    d = damage\n  [../]\n\n  [./elasticity_tensor_with_Euler_particle]\n    type = Aniso_Elasticity_Tensor\n    c = diffused\n    cmax =  51554.0\n    C1_ijkl = '230.0e9 125.0e9 65.0e9 230.0e9 65.0e9 190.0e9 30.0e9 30.0e9 52.5e9'\n    C0_ijkl = '230.0e9 125.0e9 65.0e9 230.0e9 65.0e9 190.0e9 30.0e9 30.0e9 52.5e9'\n    fill_method1 = symmetric9\n    fill_method0 = symmetric9\n    read_prop_user_object = euler_angle_read\n#    block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88'\n  [../]\n\n\n  [./eigen_strain_prefactor_particle]\n    type = DerivativeParsedMaterial\n    args = diffused\n    f_name = eigen_strain_prefactor\n    constant_names = 'c_ref'\n    constant_expressions = '4.8976e4'\n    function = (diffused-c_ref)/3\n#    block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88'\n  [../]\n\n\n  [./eigenstrain_particle]\n    type = Aniso_Eigen_Strain\n    eigen_base = '-1.1786e-06 0 0 0 1.3096e-06 0 0 0 1.3096e-06'\n    prefactor = eigen_strain_prefactor\n    eigenstrain_name = eigenstrain\n    read_prop_user_object = euler_angle_read\n#    block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88'\n  [../]\n\n\n  [./define_mobility]\n    type = ParsedMaterial\n    material_property_names = 'gc_prop visco'\n    f_name = L\n    function = '1.0/(gc_prop * visco)'\n  [../]\n  [./define_kappa]\n    type = ParsedMaterial\n    material_property_names = 'gc_prop l'\n    f_name = kappa_op\n    function = 'gc_prop * l'\n  [../]\n\n  [./damage_stress]\n    type = ComputeLinearElasticPFFractureStress\n    c = damage\n    E_name = 'elastic_energy'\n    D_name = 'degradation'\n    F_name = 'local_fracture_energy'\n    decomposition_type = stress_spectral  \n    use_snes_vi_solver = true\n  [../]\n\n  [./degradation]\n    type = DerivativeParsedMaterial\n    f_name = degradation\n    args = 'damage'\n    function = '(1.0-damage)^2*(1.0 - eta) + eta'\n    constant_names       = 'eta'\n    constant_expressions = '1.0e-5'\n    derivative_order = 2\n  [../]\n\n  [./local_fracture_energy]\n    type = DerivativeParsedMaterial\n    f_name = local_fracture_energy\n    args = 'damage'\n    material_property_names = 'gc_prop l'\n    function = 'damage^2 * gc_prop / 2 / l'\n    derivative_order = 2\n  [../]\n  \n  [./fracture_driving_energy]\n    type = DerivativeSumMaterial\n    args = damage\n    sum_materials = 'elastic_energy local_fracture_energy'\n    derivative_order = 2\n    f_name = F\n  [../]\n[]\n\n\n[AuxKernels]\n    [./vonmises]\n  type = RankTwoScalarAux\n  rank_two_tensor = stress\n  variable = vonmises\n  scalar_type = VonMisesStress\n[../]\n\n[]\n\n[Functions]\n  [./Particle_mass_surf_area]\n    type = ParsedFunction\n    value = (0.25*pi*Particle_diameter*Particle_diameter*1)*Density/(pi*Particle_diameter)  # mass (grams)/fluxarea (m^2)\n    vars = 'Density Particle_diameter'\n    vals = '4750000 10e-6'\n  [../]\n\n  [./FluxVal_temp]\n    type = ParsedFunction\n    value = In_Out_Switch*(Th_Cap/1000)*C_Rate/Faraday\n    vars = 'In_Out_Switch C_Rate Faraday Th_Cap Density'   # theoretical_capacity (Ah/g) *crate (1/h)/F(As/mol)\n    vals = '-1 1 96485.3415 300 4750000'\n  [../]\n\n  [./FluxVal]\n     type = CompositeFunction\n     functions = 'Particle_mass_surf_area FluxVal_temp'\n  [../]\n\n[]\n\n[BCs]\n [./flux_bc]\n    type = FunctionNeumannBC\n    variable = diffused\n    boundary = 'flux'\n    function = FluxVal\n  [../]\n[]\n\n\n[Bounds]\n  [./d_upper_bound]\n    type = ConstantBoundsAux\n    variable = bounds_dummy_d\n    bounded_variable = damage\n    bound_type = upper\n    bound_value = 1.0\n  [../]\n  [./d_lower_bound]\n    type = VariableOldValueBoundsAux\n    variable = bounds_dummy_d\n    bounded_variable = damage\n    bound_type = lower\n  [../]\n    [./c_upper_bound]\n    type = ConstantBoundsAux\n    variable = bounds_dummy_c\n    bounded_variable = diffused\n    bound_type = upper\n    bound_value = 4.8976e4\n  [../]\n  [./c_lower_bound]\n    type =  ConstantBoundsAux\n    variable = bounds_dummy_c\n    bounded_variable = diffused\n    bound_type = lower\n    bound_value = 1.4487e4\n  [../]\n[]\n\n#c_initial = 1.4487e4\n#c_final = 4.8976e4\n\n\n[Preconditioning]\n  [./smp]\n    type = SMP\n    full = true\n  [../]\n[]\n\n\n[Postprocessors]\n  [./avg_surface_concentration]\n    type = SideAverageValue\n    variable = diffused\n    boundary = 'flux'\n  [../]\n  \n  [./ave_damage]\n    type = ElementAverageValue  # Volumetric_average\n    variable = damage\n  [../]\n\n  [./ave_conc]\n    type = ElementAverageValue  # Volumetric_average\n    variable = diffused\n#    block = '0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88'\n  [../]\n\n  [./max_damage_value]\n    type = NodalExtremeValue\n    variable = damage\n  [../]\n\n[]\n\n[Executioner]\n  type = Transient\n  scheme = bdf2\n  \n#  solve_type = 'PJFNK'\n#  petsc_options_iname = '-pc_type -pc_hypre_type -ksp_gmres_restart -snes_type'\n#  petsc_options_value = 'hypre boomeramg 300 vinewtonrsls'\n#  automatic_scaling = true\n\n  solve_type = NEWTON\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package -snes_type'\n  petsc_options_value = 'lu       superlu_dist                  vinewtonrsls'\n  automatic_scaling = true\n  \n  l_max_its = 100\n  nl_max_its = 30\n  nl_abs_tol = 1e-6\n  l_tol = 1e-04\n  num_steps = 50000\n  dt = 2\n[]\n\n\n[Outputs]\n  execute_on = 'initial timestep_end'\n  interval = 10\n  exodus = true\n  csv = true\n[]\n\n[MultiApps]\n  [full_solve]\n    type = FullSolveMultiApp\n    execute_on = initial\n    positions = '0 0 0'\n    input_files = DIM_sub_particle.i\n    clone_master_mesh = true\n  []\n[]\n\n[Transfers]\n[from_eta_gb]\n    type = MultiAppCopyTransfer\n    multi_app = full_solve\n    direction = from_multiapp\n    variable = eta_gb\n    source_variable = eta_gb\n  []\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2332487",
                          "updatedAt": "2022-06-16T00:09:47Z",
                          "publishedAt": "2022-03-10T15:34:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I dont see a problem.\nWhat does Concentration inherit from in terms of classes?\n@loganharbour or @cticenhour I think we need a second pair of eyes here, I just dont see it",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2332810",
                          "updatedAt": "2022-06-16T00:09:49Z",
                          "publishedAt": "2022-03-10T16:11:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Here is the Concentration.h file\n#pragma once\n\n#include \"Kernel.h\"\n\nclass Concentration : public Kernel\n{\npublic:\n  static InputParameters validParams();\n\n  Concentration(const InputParameters & parameters);\n\n  virtual Real computeQpResidual() override;\n  virtual Real computeQpJacobian() override;\n\n  const Real & _ki; \n  const Real & _cmax; \n\n  const MaterialProperty<RankTwoTensor> & _Diff_Matrix;\n \n};\n\nand Concentration.C file\n#include \"Concentration.h\"\n\nregisterMooseObject(\"BabblerApp\", Concentration);\n\nInputParameters\nConcentration::validParams()\n{\n  InputParameters params = Kernel::validParams();\n  params.addClassDescription(\"Compute the concentration term for Fickian diffusion ($c$) equation: \"\n                             \"$-\\\\nabla \\\\cdot \\\\frac{\\\\mathbf{K}}{\\\\mu} \\\\nabla p = 0$\");\n  params.addRequiredParam<Real>(\"ki\", \"Interaction parameter\");\n  params.addRequiredParam<Real>(\"cmax\", \"Maximum Concentration\");\n  return params;\n}\n\nConcentration::Concentration(const InputParameters & parameters)\n  : Kernel(parameters),\n  _ki(getParam<Real>(\"ki\")),\n  _cmax(getParam<Real>(\"cmax\")),\n    // Get the parameters from the input file\n    _Diff_Matrix(getMaterialProperty<RankTwoTensor>(\"Diff_Matrix\"))\n{\n}\n\nReal\nConcentration::computeQpResidual()\n{\n   return  (1.0-2.0*_ki*(_u[_qp]/_cmax)*(1.0-_u[_qp]/_cmax)) *_Diff_Matrix[_qp] * _grad_test[_i][_qp]*_grad_u[_qp];\n}\n\nReal\nConcentration::computeQpJacobian()\n{\n  return (1.0-2.0*_ki*(_u[_qp]/_cmax)*(1.0-_u[_qp]/_cmax)) *_Diff_Matrix[_qp] * _grad_phi[_j][_qp] * _grad_test[_i][_qp]  +   (-2.0*_ki/_cmax + (4.0*_ki/_cmax)*(_u[_qp]/_cmax)) * _Diff_Matrix[_qp] *_grad_u[_qp] * _grad_test[_i][_qp];\n}",
                          "url": "https://github.com/idaholab/moose/discussions/20515#discussioncomment-2332864",
                          "updatedAt": "2022-06-16T00:09:48Z",
                          "publishedAt": "2022-03-10T16:16:15Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}